{
    "SurveyEntry": {
        "SurveyID": "SV_1BSk0CWt552laHI",
        "SurveyName": "Task 2 Test",
        "SurveyDescription": null,
        "SurveyOwnerID": "UR_2anmFMeTRFxFSsD",
        "SurveyBrandID": "qualtricsxm3rdbx99yv",
        "DivisionID": null,
        "SurveyLanguage": "EN",
        "SurveyActiveResponseSet": "RS_b8D5fo6RPBMAvyu",
        "SurveyStatus": "Active",
        "SurveyStartDate": "0000-00-00 00:00:00",
        "SurveyExpirationDate": "0000-00-00 00:00:00",
        "SurveyCreationDate": "2023-10-28 12:20:41",
        "CreatorID": "UR_2anmFMeTRFxFSsD",
        "LastModified": "2023-11-01 14:02:05",
        "LastAccessed": "0000-00-00 00:00:00",
        "LastActivated": "2023-10-29 09:22:45",
        "Deleted": null
    },
    "SurveyElements": [
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "BL",
            "PrimaryAttribute": "Survey Blocks",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": [
                {
                    "Type": "Default",
                    "Description": "Default Question Block",
                    "ID": "BL_cJ8Lixg9aZEEzhc",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID1"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Collapsed"
                    }
                },
                {
                    "Type": "Trash",
                    "Description": "Trash \/ Unused Questions",
                    "ID": "BL_9WY5lavHn5KrLgi",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID347"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID348"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID349"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID350"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID351"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID352"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID353"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID354"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID355"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID356"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID357"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID358"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID359"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID360"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID361"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID362"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID363"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID364"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID365"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID366"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID367"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID368"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID369"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID370"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID371"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID372"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID373"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID374"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID375"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID376"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID377"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID378"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID379"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID380"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID381"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID382"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID383"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID384"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID385"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID386"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID387"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID388"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID389"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID390"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID391"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID392"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID393"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID394"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID395"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID396"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID397"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID400"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID401"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID402"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID403"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID404"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID405"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID406"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID407"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID408"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID409"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID410"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID411"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID283"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID284"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID285"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID286"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID287"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID288"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID289"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID290"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID291"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID292"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID293"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID294"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID295"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID296"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID297"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID298"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID299"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID300"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID301"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID302"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID303"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID304"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID305"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID306"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID307"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID308"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID309"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID310"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID311"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID312"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID313"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID314"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID315"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID316"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID317"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID318"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID319"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID320"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID321"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID322"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID323"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID324"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID325"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID326"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID327"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID328"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID329"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID330"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID331"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID332"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID333"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID334"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID335"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID336"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID337"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID338"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID339"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID340"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID341"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID342"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID343"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID344"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID228"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID229"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID230"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID231"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID232"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID233"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID234"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID235"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID236"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID237"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID238"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID239"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID242"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID243"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID244"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID245"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID246"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID247"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID248"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID249"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID250"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID251"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID252"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID253"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID254"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID255"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID256"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID257"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID258"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID259"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID260"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID261"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID262"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID263"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID264"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID269"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID270"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID271"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID272"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID275"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID276"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID277"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID278"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID281"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID282"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID161"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID162"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID163"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID164"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID165"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID166"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID167"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID168"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID169"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID170"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID171"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID172"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID173"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID174"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID175"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID176"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID177"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID178"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID181"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID182"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID183"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID184"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID185"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID186"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID187"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID188"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID189"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID190"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID191"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID192"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID193"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID194"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID195"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID200"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID201"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID202"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID203"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID204"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID205"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID206"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID207"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID208"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID209"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID210"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID211"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID212"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID213"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID214"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID215"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID216"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID217"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID218"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID219"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID220"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID221"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID97"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID98"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID99"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID100"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID101"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID102"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID103"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID104"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID105"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID106"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID107"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID108"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID109"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID110"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID111"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID112"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID115"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID116"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID117"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID118"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID119"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID120"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID121"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID122"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID123"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID124"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID125"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID126"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID127"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID128"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID129"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID130"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID133"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID134"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID135"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID136"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID137"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID138"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID139"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID140"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID145"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID146"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID147"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID148"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID149"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID150"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID73"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID74"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID79"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID80"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID222"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID223"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID141"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID142"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID279"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID280"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID273"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID274"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID345"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID346"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Expanded"
                    }
                },
                {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Questions",
                    "ID": "BL_6GdrMCcDLZ2Ui8u",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID226"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID227"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID75"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID76"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID77"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID78"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID81"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID82"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID83"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID84"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID85"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID86"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID87"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID88"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID89"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID90"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID224"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID225"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID179"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID180"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID196"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID197"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID198"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID199"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID91"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID92"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID93"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID94"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID95"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID96"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID113"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID114"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID131"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID132"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID240"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID241"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID143"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID144"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID151"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID152"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID153"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID154"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID155"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID156"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID157"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID158"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID159"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID160"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID265"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID266"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID267"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID268"
                        },
                        {
                            "Type": "Page Break"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID398"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID399"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Expanded"
                    }
                }
            ]
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "FL",
            "PrimaryAttribute": "Survey Flow",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "Flow": [
                    {
                        "ID": "BL_cJ8Lixg9aZEEzhc",
                        "Type": "Block",
                        "FlowID": "FL_2"
                    },
                    {
                        "ID": "BL_6GdrMCcDLZ2Ui8u",
                        "Type": "Standard",
                        "FlowID": "FL_4"
                    }
                ],
                "Properties": {
                    "Count": 8
                },
                "FlowID": "FL_1",
                "Type": "Root"
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "PL",
            "PrimaryAttribute": "Preview Link",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "PreviewType": "Brand",
                "PreviewID": "424c25a8-ba9e-4154-9761-93d16dd52c04"
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SO",
            "PrimaryAttribute": "Survey Options",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "BackButton": "false",
                "SaveAndContinue": "true",
                "SurveyProtection": "PublicSurvey",
                "BallotBoxStuffingPrevention": "false",
                "NoIndex": "Yes",
                "SecureResponseFiles": "true",
                "SurveyExpiration": "None",
                "SurveyTermination": "DefaultMessage",
                "Header": "",
                "Footer": "",
                "ProgressBarDisplay": "NoText",
                "PartialData": "+1 week",
                "ValidationMessage": "",
                "PreviousButton": "",
                "NextButton": " \u2192 ",
                "SurveyTitle": "Qualtrics Survey | Qualtrics Experience Management",
                "SkinLibrary": "qualtricsxm3rdbx99yv",
                "SkinType": "templated",
                "Skin": {
                    "brandingId": null,
                    "templateId": "*2014",
                    "overrides": null
                },
                "NewScoring": 1,
                "SurveyMetaDescription": "The most powerful, simple and trusted way to gather experience data. Start your journey to experience management and try a free account today.",
                "SurveyName": "Task 2 Test",
                "CustomStyles": [],
                "ProtectSelectionIds": true
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SCO",
            "PrimaryAttribute": "Scoring",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "ScoringCategories": [],
                "ScoringCategoryGroups": [],
                "ScoringSummaryCategory": null,
                "ScoringSummaryAfterQuestions": 0,
                "ScoringSummaryAfterSurvey": 0,
                "DefaultScoringCategory": null,
                "AutoScoringCategory": null
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "PROJ",
            "PrimaryAttribute": "CORE",
            "SecondaryAttribute": null,
            "TertiaryAttribute": "1.1.0",
            "Payload": {
                "ProjectCategory": "CORE",
                "SchemaVersion": "1.1.0"
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "STAT",
            "PrimaryAttribute": "Survey Statistics",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "MobileCompatible": true,
                "ID": "Survey Statistics"
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID1",
            "SecondaryAttribute": " ",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<img src=\"https:\/\/qualtricsxm3rdbx99yv.qualtrics.com\/CP\/Graphic.php?IM=IM_6Pzx2LbhuG1KCCG\" \/>",
                "DefaultChoices": false,
                "DataExportTag": "Q1",
                "QuestionID": "QID1",
                "QuestionType": "DB",
                "Selector": "TB",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": " ",
                "ChoiceOrder": [],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID77",
            "SecondaryAttribute": "\"Alcohol Measures for Public Health Research Alliance (AMPHORA)\" AMPHORA is a Europe-wide project...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Alcohol Measures for Public Health Research Alliance (AMPHORA)&quot;<\/strong><br \/>\n<br \/>\nAMPHORA is a Europe-wide project involving researchers and research institutions from 14 European countries and counterparts and organizations from all 27 Member States. It aims to provide new scientific evidence for the best public health measures to reduce the harm caused by alcohol. This will be achieved through addressing various factors, including social and cultural determinants, marketing and advertising, taxes and pricing, availability and access, early diagnosis and treatment of diseases, interventions in drinking environments, and safer untaxed alcohol products. Cost-effectiveness analyses will be carried out in multiple settings, geographical regions, and for different gender and age groups to guide integrated policy making aimed at reducing alcohol-related harm.<br \/>\n<br \/>\nThe project will employ various research methodologies, such as time series analysis, longitudinal intervention research, policy mapping, cost-effectiveness analyses, and other policy-relevant approaches to evaluate recent and current alcohol policy changes in European Member States. It will also document the existing alcohol policy-related infrastructures and analyze their impact on effective policy development and implementation. The project will study the interaction between social and cultural determinants of alcohol policy and policy and preventive measures to determine the extent to which the implementation and impact of effective alcohol policies are culturally determined.<br \/>\n<br \/>\nMethodologies will be developed to create tools for benchmarking and comparative analysis at the European level, thus advancing the state of the art in alcohol policy research and enhancing cooperation between researchers in Europe and other geographic regions to promote integration and excellence of European research in alcohol policy.<br \/>\n<br \/>\nAMPHORA will provide the evidence base to inform policy and decision makers at European, national, and local levels, enabling the implementation of effective interventions to reduce the harm caused by alcohol through a wide range of policies implemented in different sectors and settings.",
                "DataExportTag": "COR61000",
                "QuestionID": "QID77",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Alcohol Measures for Public Health Research Alliance (AMPHORA)\" AMPHORA is a Europe-wide project...",
                "Choices": {
                    "1": {
                        "Display": "\"Cardiovascular and Metabolic Diseases Research\": cardiac, cvd, diabetes, heart_failure, cardiovascular, heart, vascular, atrial_fibrillation, cardiovascular_disease, liver, kidney, hf, lung, cellular, diabete"
                    },
                    "2": {
                        "Display": "\"Public Health and Preventive Medicine\": health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID267",
            "SecondaryAttribute": "\"AN AUGMENTED REALITY UAV-GUIDED GROUND NAVIGATION INTERFACE IMPROVE HUMAN PERFORMANCE IN MULTI-R...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<h3 dir=\"ltr\"><b id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">\"AN AUGMENTED REALITY UAV-GUIDED GROUND NAVIGATION INTERFACE IMPROVE HUMAN PERFORMANCE IN MULTI-ROBOT TELE-OPERATION\"<\/b><\/h3>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">This research proposes a human-multirobot system with semi-autonomous ground robots and UAV view for contaminant localization tasks. A novel Augmented Reality based operator interface has been developed. The interface uses an over-watch camera view of the robotic environment and allows the operator to direct each robot individually or in groups. It uses an A* path planning algorithm to ensure obstacles are avoided and frees the operator for higher-level tasks. It also displays sensor information from each individual robot directly on the robot in the video view. In addition, a combined sensor view can also be displayed which helps the user pin point source information. The sensors on each robot monitor the contaminant levels and a virtual display of the levels is given to the user and allows him to direct the multiple ground robots towards the hidden target. This paper reviews the user interface and describes several initial usability tests that were performed. This research demonstrates the development of a human multi robot interface that has the potential to improve cooperative robots for practical applications.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">INTRODUCTION&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">Multi-robot systems can often deal with tasks that are difficult for a single robot. For example, teams of robots may be able to complete tasks such as multipoint surveillance, cooperative transport, and explorations in hazardous environments more efficiently. Additionally, time-critical missions may require the use of multiple robots working simultaneously to efficiently accomplish the tasks. Controlling multiple robots is a challenging human operator task. In multi-robot scenarios, one of the main challenges for a human operator in search and detection missions is to remotely control the semi-autonomous robots [1]. Thus, there is a need to research and develop technologies that can enable an operator to control groups of semi-autonomous robots. Most human-robot interfaces for robot control have focused on providing users data collected by the robot and giving status messages about what the robot is doing. The conventional interface consists of several separate display windows to show information from the robot. [2] is an example of a conventional display from the Idaho National Laboratory (INL). The display may require the operator to integrate information, and this may increase the operator\u2019s workload. Another example of a conventional interface for multiple robot control was designed by Humphrey et al. [3]. Operators may have a high workload from needing to simultaneously integrate each multiple status bar. An alternative to conventional interface is a 3D virtual environment display based on a robot simulation. In contrast to direct interfaces, a virtual environment provides an external perspective which allows the operator to see the environment and drive the robot from viewpoints generated by the interface. Nguyen describes several Virtual Reality (VR) based interfaces for exploration, one of which has shown that VR interfaces can help the user understand and analyze the robot surroundings and improve the operator's situational awareness. However, in virtual environments, the operator\u2019s attention is drawn away from the actual environment which can reduce situational awareness and dynamic situations not modeled in the VR world could pose significant problems. In contrast to the virtual environment display, Augmented Reality (AR) is an advanced visualization technology which allows computer generated virtual images to merge with physical objects in real time. Unlike VR, the user enters and interacts with computer-generated 3D environments. AR allows the user to interact with the virtual images using real objects [5]. Researchers in robotics are beginning to use AR techniques in robotics because it provides direct views of the scene combined with the advantages of virtual displays for human-robot collaboration [6-8]. Communicating to robots and humans, on the other hand, touch-based input may allow users to perform complex tasks in an intuitive manner [9]. Micire et al. [10] studied the control of a single agent with a multi-touch table. Moreover, a multi-touch (DREAM) controller [11, 12] using a multitouch table is developed for multi-robot command and control [13, 14]. Figure 1: A group of semi-autonomous robots is controlled using the human-multi robot interface. We have developed a system to combine virtual and augmented reality interfaces capabilities with the human supervisor\u2019s ability to control the robots [15]. The role of this human-multi robot interface is to allow an operator to control a group of heterogeneous robots in real time and in a collaborative way. This paper presents results from a user evaluation of the real multiple robot system in which three interface conditions were evaluated (i.e. Joystick, Point-andGo, and Path Planning). Results show that the novel multi robot control (Point-and-Go and Path Planning) reduced their mission completion times compared to the traditional joystick control for target detection performance.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">SYSTEM DESCRIPTION&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">The human-multi robot interface is a top-down view from the stationary camera. We assumed that the top down view could be taken from a manned or unmanned aerial vehicle. We assumed that the top down view could be taken from a manned or unmanned aerial vehicle.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">Hardware&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">Four Mindstorms NXT robots were used as the remote robots. The NXT robot includes two NXT motors with encoders used for differential drive and a third passive caster wheel to maintain balance. An infrared sensor with a 240 degree view is attached on the NXT robot to search and detect infrared beacons. A marker on top of a NXT robot is used for position reading of the multi-robot system and for viewing robot status using AR software. The NXT robots are controlled through a Bluetooth connection. Proceedings of the 2011 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) An Augmented Reality UAV-Guided Ground Navigation Interface Improve Human Performance in Multi-Robot Tele-operation. A HiTechnic infrared electronic ball was used as a contaminant source. The infrared ball was hidden by one of the decoys. The testbed was equipped with a Logitech Webcam Pro 9000 with autofocus to obtain video frames at a resolution of 1280x1024 and at a refresh rate of 10 frames\/sec. The video was displayed on a 17\u201d liquid crystal display (LCD) computer monitor. A marker is attached on the top of a NXT robot. The marker is detected by AR software to measure a robot position and to generate virtual images to merge with a robot in real time. An infrared sensor with a 240 degree view is attached on the NXT robot to search and detect infrared beacons.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">Software&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">The ARToolKit augmented reality system is used to determine the position and orientation of each robot. A marker is attached to the top of each robot. Client\u2010server system for data communication has developed in the testbed. The robot server is programmed to communicate with NXT robots using Bluetooth. AR client delivers localization data obtained from an overhead camera to ground robots, and it displays the synthesized views. Visual C++ in Visual Studio 2008 is the programming language used for AR and robot communication API software development. Not eXactly C (NXC) in Bricx Command Center is the programming language used for NXT robots to configure the infrared sensor and robot communication.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">AR INTERFACE FOR MULTI-ROBOT CONTROL&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-c52757fc-7fff-708d-505f-84940ae406ce\">\nThis section describes three features of the interface used for user evaluation. First, we describe the Point-and-Go algorithm developed for multi-robot control and the Path Planning implemented for obstacle avoidance. Then, we describe the joystick control of the multiple robots. Finally, we explain the sensor data and robot messages visualization.&nbsp;<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p><p dir=\"ltr\"><b>- Point-and-Go Mode:<\/b> A point\u2010and\u2010go algorithm is developed for a single human operator controlling multiple robots. The operator is able to select any ground robot using a mouse left click, and then designates a goal location in the video feed from an overhead camera. A navigation algorithm is developed that allows the robot to turn toward the desired goal location, drive straight toward the goal, and then stop at the target. If a robot is stuck with an obstacle, the user is able to reverse the robot using a mouse right click. Point-and-Go is a high level instruction that allows an operator to control multiple semi-autonomous robots simultaneously.<\/p><p dir=\"ltr\"><span><br><\/span><\/p><p dir=\"ltr\"><b>- Path Planning Mode:<\/b> An A* algorithm path planning algorithm is implemented to the system. The A* algorithm allows the multiple robots to traverse to the target location with obstacle avoidance and shortest path.\n<\/p>",
                "DataExportTag": "AI967037",
                "QuestionID": "QID267",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"AN AUGMENTED REALITY UAV-GUIDED GROUND NAVIGATION INTERFACE IMPROVE HUMAN PERFORMANCE IN MULTI-R...",
                "Choices": {
                    "1": {
                        "Display": "\"Unmanned Aerial and Underwater Vehicles\": unmanned_aerial_vehicles, flight, mission, drone, aircraft, vehicle, unmanned_aerial, ship, autonomous, underwater, spacecraft, unmanned, vehicle_uav, landing, vessel"
                    },
                    "2": {
                        "Display": "\"Industrial Automation and Robotics\": robot, computer_vision, image, inspection, speech_recognition, industrial, remote, autonomous_robot, sensor, voice, autonomous, smart, microcontroller, raspberry_pi, wireless"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID91",
            "SecondaryAttribute": "\"Application of a Pattern-based Classification System for Invasive Endocervical Adenocarcinoma in...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Application of a Pattern-based Classification System for Invasive Endocervical Adenocarcinoma in Cervical Biopsy, Cone and Loop Electrosurgical Excision (LEEP) Material: Pattern on Cone and LEEP is Predictive of Pattern in the Overall Tumor&quot;<\/strong><br \/>\n<br \/>\nA pattern-based classification system has been recently proposed for invasive endocervical adenocarcinoma, which is predictive of the risk of nodal metastases. Identifying cases at risk of nodal involvement is most relevant at the time of biopsy and loop electrosurgical excision procedure (LEEP) to allow for optimal surgical planning, and, most importantly, consideration of lymphadenectomy.<br \/>\n<br \/>\nThis study aims to determine the topography of patterns of stromal invasion in invasive endocervical adenocarcinoma with emphasis on patterns in biopsy, cone, and LEEP. Invasive pattern was assessed following the pattern-based classification (Patterns A, B, and C) in 47 invasive endocervical adenocarcinomas treated with hysterectomy or trachelectomy and correlated with the pattern of invasion at the tumor surface (2 mm of tumor depth) and on preoperative biopsy and cone\/LEEP.<br \/>\n<br \/>\nPatterns A, B, and C were present in 21.3%, 36.2%, and 42.5% of cases, respectively. Most pattern A cases were Stage IA (90%), whereas most Pattern B and C cases were Stage IB (76.5% and 80%, respectively). Horizontal spread was on average larger in Pattern C (24.1 mm) than in Patterns A and B (7.7 and 12.3 mm, respectively).<br \/>\n<br \/>\nPattern at the tumor surface correlated with the overall pattern in 95.7% of cases. Concordance between patterns at cone\/LEEP and hysterectomy was 92.8%; the only discrepant case was upgraded from Pattern A on LEEP to C on final excision. Agreement between patterns in biopsy and the overall tumor, however, was only 37.5%. In all discrepant cases, biopsy failed to reveal destructive invasion, which was evident on excision. All discrepant biopsies with pattern A showed glandular complexity resembling exophytic papillary growth but did not meet criteria for destructive invasion. On excision, marked gland confluence with papillary architecture was evident.<br \/>\n<br \/>\nWe conclude that the pattern of invasion on cone\/LEEP is a good predictor of the pattern of invasion on hysterectomy, particularly if there is destructive invasion (B or C). Thus, the pattern-based classification can be successfully applied in these samples to guide definitive surgical treatment. Prediction of the overall pattern based on biopsy material alone appears to be suboptimal. However, glandular confluence and complexity on biopsy, regardless of its size, appears to be associated with destructive invasion in the overall tumor.",
                "DataExportTag": "CAN583940",
                "QuestionID": "QID91",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Application of a Pattern-based Classification System for Invasive Endocervical Adenocarcinoma in...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunohistochemical Analysis in Cancer Research\": staining, stain, immunohistochemical, tumour, immunoreactivity, antibody, ihc, cytoplasmic, canine, nuclear, paraffin_embed, mast, immunostaine, formalin_fix, nucleus"
                    },
                    "2": {
                        "Display": "\"Skin Cancer and Dermoscopy Analysis\": melanoma, cutaneous, melanocytic, melanocyte, dermoscopic, nevi, dermoscopy, nevus, pigment, melanocytic_nevi, dermoscopic_feature, pigmented, pigmentation, blue_nevus, lentigo_maligna"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID265",
            "SecondaryAttribute": "\"Automated Diagnosis of Physical Systems\" \u201cIf anything can go wrong, it will.\u201d (Murphy\u2019s Law) ",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<h3 dir=\"ltr\"><b id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">\"Automated Diagnosis of Physical Systems\"<\/b><\/h3><div><b><br><\/b><\/div>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">\u201cIf anything can go wrong, it will.\u201d (Murphy\u2019s Law) Although deceptive in its simplicity, the above quote has proved to be a profound insight into how things happen. Things are likely to fail no matter how well the system is designed. When a high degree of reliability and safety is desired the effects of these failures must be mitigated, and control must be maintained under all fault scenarios. Faults need to be detected close to their onset so that quick action can be taken by resetting control parameters to compensate for the fault or by reconfiguring the system to minimize the effects of the fault and thus prevent damage. In this paper we provide a brief introduction to a variety of automated techniques for diagnosing faults and then discuss in more detail one specific technology called HyDE.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">WHAT IS AUTOMATED DIAGNOSIS?&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">Fault diagnosis involves the detection of anomalous system behavior and the identification of the cause for the deviant behavior. Automated diagnosis refers to the use of software technologies to assist in diagnosing faults in a system. This is to be contrasted with autonomous diagnosis which deals with software technologies that operate autonomously to detect, isolate and compensate for faults in a system. We would like to introduce some definitions (taken from IFAC Technical Committee SAFEPROCESS) to set the context for the rest of the paper.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\"><b>- Fault:<\/b> A fault is an unpermitted deviation of at least one characteristic property or parameter of the system from acceptable, usual or standard conditions.&nbsp;<\/span><\/p><p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\"><b>- Fault Detection: <\/b>Fault detection is monitoring measured variables to determine if a fault has occurred in the plant. If a fault has occurred, it may be important to determine the time at which the fault occurred.&nbsp;<\/span><\/p><p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\"><b>- Fault Isolation:<\/b> Fault Isolation is determining the type and location of a fault once it is known that a fault has occurred. It typically follows fault detection but the two processes are often combined for additive faults.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\"><b>- Fault Identification:<\/b> Fault Identification is determining the size and time-variant behavior of a fault. It follows fault isolation.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">Fault may be of many types including:&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- Plant, actuator, sensor or controller faults&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- Additive or multiplicative faults&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- Abrupt or incipient faults&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- Persistent or intermittent faults&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">Diagnosis is made harder by several factors including and not limited to:&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- Typically only a few sensors are placed leading to limited observability into system behavior.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- The data from sensors are noisy due to inherent properties of the sensors&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- There are unknown inputs acting on the systems, due to lack of complete knowledge about conditions in which the system operates.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- The knowledge about how the system functions may be limited.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">- The effects of faults may be non-local and may take some time to manifest.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">In the rest of this paper we will introduce a sampling of automated diagnosis techniques that deal with some of the fault characteristics described above. We will briefly describe these techniques and then focus on one specific technique called HyDE.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">AUTOMATED DIAGNOSIS TECHNIQUES&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">The diagnosis problem has been dealt with in several domains from several angles resulting in a wide variety of diagnosis approaches. Some of these include Expert Systems, Case-based reasoning, Data-driven techniques and Model-based reasoning.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">Expert System Diagnosis<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">Traditionally diagnosis was performed by human troubleshooting experts who built up diagnostic knowledge based on their expertise and experience. The natural extension to this was to encode the diagnostic knowledge in a machine storable structure. These structures took the form of associations between observed symptoms and probable fault occurrences. Tools were built to assist in the creation of these structures and then to use these structures for the diagnosis task. Once these structures are built, users and non-expert operators could troubleshoot the system in case of faults.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-7b2b383d-7fff-052a-ea66-584cf099108e\">Some of the more commonly used structures to encode expert diagnostic knowledge are rules and fault trees. A rule describes the action(s) that should be taken if a symptom is observed. A set of rules describing the symptoms of all the possible faults is incorporated into a rule-based reasoning system. The reasoning may use a backward-chaining algorithm which starts at the hypothesis (consequences of rules) and collects and verifies evidence (antecedents of rules) that supports the hypothesis. Alternately forward-chaining may be used where rules whose antecedents match observed symptoms are examined. When several rules match, a chain of rule firings (based on predefined rule priority) is used to establish the diagnosis.<\/span><\/p>",
                "DataExportTag": "AI248278",
                "QuestionID": "QID265",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Automated Diagnosis of Physical Systems\" \u201cIf anything can go wrong, it will.\u201d (Murphy\u2019s Law) Alt...",
                "Choices": {
                    "1": {
                        "Display": "\"Operational Management and Automated Manufacturing Planning\": management, planning, plan, decision_making, manufacturing, workflow, automation, production, business, planner, support, automate, military, operational, framework"
                    },
                    "2": {
                        "Display": "\"Healthcare Expert System and Patient Diagnosis\": expert, diagnosis, healthcare, fault, acquisition, es, inference, reasoning, maintenance, kbs, design, inference_engine, patient, expertise, user"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID83",
            "SecondaryAttribute": "\"BIG DATA FOR SMART SOCIETY\" The general objective of GATE is to establish a Centre of Excellence...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"BIG DATA FOR SMART SOCIETY\"<\/strong><br>\n<br>\nThe general objective of GATE is to establish a Centre of Excellence on \"Big Data for Smart Society - GATE\" that will fulfill the vision of Open Innovation through building a sustainable University-Government-Industry-Society ecosystem. The CoE will be established as a joint initiative between Sofia University, the most prestigious educational and scientific hub in Bulgaria, and Chalmers University of Technology, Sweden, a leading European institution with extensive experience in research, education, and innovation.<br>\n<br>\nThe CoE's main research objective is to advance the state-of-the-art in the entire Big Data Value Chain, including the development of advanced methods and tools for data collection from a variety of structured and unstructured sources, data consistency checking and cleaning, data aggregation and linking, data processing, modeling, and analysis, and data delivery by providing both accessibility and proper visualization.<br>\n<br>\nFollowing the challenges of the Programme Horizon 2020 and the Bulgarian Innovation Strategy for Smart Specialization 2014-2020, the project team has selected the most promising Data Driven Innovation Pillars: Data Driven Government (Public Services based on Open Data); Data Driven Industry (Manufacturing and Production); Data Driven Society (Smart Cities); and Data Driven Science.<br>\n<br>\nThe specific objectives of Teaming 1 phase are to establish a solid background for the creation and sustainability of a CoE at a national, regional, and EU level through:<br>\n<br>\n1. Elaboration of a detailed and robust business plan with a long-term vision for setting up and sustaining the CoE.<br>\n2. Strengthening the research capacity and potential in Big Data.<br>\n3. Establishment of an international collaborative network of Big Data and related fields researchers.<br>\n4. Increasing the quality of education and training and offering measures for motivation and involvement of the next-generation Early-Stage Researchers.<br>\n5. Wide dissemination and promotion of project aims, activities, and expected outputs.",
                "DataExportTag": "COR51178",
                "QuestionID": "QID83",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"BIG DATA FOR SMART SOCIETY\" The general objective of GATE is to establish a Centre of Excellence...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\": computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "2": {
                        "Display": "\"Nuclear Transport and Digital Twin Experimentation\": nuclear, transport, interoperable, enabler, experimentation, agile, certification, toolbox, factory, testbed, bim, broad, federation, vertical, digital_twin"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID160",
            "SecondaryAttribute": "\"BRAHMS: SIMULATING PRACTICE FOR WORK SYSTEMS DESIGN\" \u00a0 A continuing problem in business today is..",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<h3 dir=\"ltr\"><b id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">\"BRAHMS: SIMULATING PRACTICE FOR WORK SYSTEMS DESIGN\"<\/b><\/h3>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">A continuing problem in business today is the design of human-computer systems that respect how work actually gets done. The overarching context of work consists of activities, which people conceive as ways of organizing their daily life and especially their interactions with each other. Activities include reading mail, going to workshops, meeting with colleagues over lunch, answering phone calls, and so on.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">Brahms is a multiagent simulation tool for modeling the activities of groups in different locations and the physical environment consisting of objects and documents, including especially computer systems. A Brahms model of work practice reveals circumstantial, interactional influences on how work actually gets done, especially how people involve each other in their work. In particular, a model of practice reveals how people accomplish a collaboration through multiple and alternative means of communication, such as meetings, computer tools, and written documents. Choices of what and how to communicate are dependent upon social beliefs and behaviors\u2014 what people know about each other\u2019s activities, intentions, and capabilities and their understanding of the norms of the group. As a result, Brahms models can help human-computer system designers to understand how tasks and information actually flow between people and machines, what work is required to synchronize individual contributions, and how tools hinder or help this process. In particular, workflow diagrams generated by Brahms are the emergent product of local interactions between agents and representational artifacts, not preordained, end-to-end paths built in by a modeler.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">We developed Brahms as a tool to support the design of work by illuminating how formal flow descriptions relate to the social systems of work; we accomplish this by incorporating multiple views\u2014 relating people, information, systems, and geography\u2014 in one tool. Applications of Brahms could also include system requirements analysis, instruction, implementing software agents, and a workbench for relating cognitive and social theories of human behavior.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">OVERVIEW OF OBJECTIVE, THEORETICAL STANCE, AND CONTRIBUTION&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">Brahms is a multi-agent simulation framework (Tokoro, 1996) for modeling work practice, incorporating state-of-the-art methods from artificial intelligence research and insights about work and learning from the social sciences. A Brahms model is a kind of theatrical play, intended to provoke conversation and stimulate insights in groups of people seeking to analyze or redesign their work. Rather than modeling technical knowledge in detail, Brahms models focus on the conventions by which people choose to use particular tools and interact with each other, such as how they communicate. The quality, methods, and evaluation criteria of technical problem solving\u2014 the focus of most computer systems design\u2014 are constrained by this social-interactional context (Sachs, 1995; Schon, 1983; Weickert, 1995; Zuboff, 1987).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">We hypothesize that multiple, complementary views\u2014 cognitive, social, physical\u2014 integrated into one model provide a better basis for understanding organizations than cognitive task models, which are disembodied and oriented around individuals, or business process models, which are overly abstract, and hence decontextualized. More generally, we are interested in how organizations change themselves, and thus how to design a workplace so that people will dynamically reconfigure their processes, use of tools, and collaboration to creatively affect how a job gets done (Nonaka and Takeuchi, 1995).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">In Brahms we apply and extend knowledge-based techniques in a way that seeks to understand how information and workflow actually happens. Our approach demonstrates how symbolic cognitive modeling, traditional business process modeling, and situated cognition theories may be brought together in a coherent approach to the design of human-computer systems. This introduction provides an overview of our objectives, theoretical stance, and contribution to human-computer system design. Subsequent sections in this paper describe the relation of Brahms to situated cognition and workflow modeling, the methodology, provide examples, present results, and analyze broader implications.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">Practice, work systems design, and modeling work&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">Broadly speaking, work practice may be contrasted with work process; practice concerns how people actually behave within a physical and social environment, as distinct from the functions they accomplish. For example, a description of work practice might include who picks up a fax, where it is delivered (to a desk? to a group of mailboxes?), and when this is done. In contrast, a typical description of a work process would only show that an order, for example, is sent from one organization to an agent who processes it. The faxing process and how it is carried out might not be mentioned at all. In short, a model of practice is oriented around agents\u2014 how they interact with their environment and what they do during the course of a day. A model of work process is typically oriented around work products (such as orders)\u2014 how they are transformed and flow from one transformation to the next.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">A key finding of our work is that a representation of the work process, such as work flow diagrams, can be derived from the result of a simulation of practice. The notion of \u201cpractice\u201d is central to work systems design, which has its roots in the design of socio-technical systems, a method developed in the 1950s by Eric Trist and Fred Emery (1959, 1960). Socio-technical systems design sought to analyze the relationship of the social system and the technical system, such as manufacturing machinery, and then design a \u201csocio-technical system\u201d that leveraged the advantages of each. Work design (Ehn, 1989; Greenbaum and Kyng, 1991; Pasmore, 1994; Weisbord, 1987 (see Chapter 16)) extends this tradition by focusing on both the formal features of work (explicit, intentional) and the informal features of work (as it is actually carried out \u201cin practice,\u201d analyzed with the use of ethnographic techniques).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">The aim of analyzing both the formal and the informal work practices is two-fold: to understand what it takes to actually accomplish a business function in order to use those insights in design, and to ensure that new designs of work can be effectively implemented. Socio-technical and work systems design aim at producing both a productive workplace and a positive work environment, which fosters human development by providing dignity, meaning and community. By contrast, business process reengineering focuses on the structuring of key business processes, eliminating duplication and unnecessary steps, formulating processes and procedures and using information technology extensively to improve work processing. Consequently, business process reengineering focuses more exclusively on tasks, does not take into account the informal nature of work, and does not hold as important the dignity of work or human development. In short, work design includes a focus on practice, while business process design exclusively focuses on process (Sachs, 1993). The methods of business process reengineering (BPR) and work system design differ considerably. BPR tends to be conducted by external consultants who analyze the flow of work products in terms of business functions, and not how products get from one place to another. Work design is more typically carried out by people who actually do the work (both workers and managers), and emphasizes not only what flows, but how and why work products manage to get from one place to another. For example, managers, office workers, crafts people, and researcher-facilitators collaborate to understand an existing work setting (such as new order processing for a telecommunications company) in order to develop a comprehensive design for the business organization, work process, computer tools, documents, facilities (such as seating arrangements), training, performance metrics and incentives, etc.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">Both approaches may also be contrasted with an economic or business strategy analysis, which invents the products and services that both business process reengineering and work system design seek to deliver. The two approaches differ in their view of how information technology should be used. In BPR, models of technical problem-solving, for example, tend to result in business process designs in which information technology is seen as an opportunity to \u201cdo the work\u201d (e.g., to automate as much of the work as possible). In work systems design, information technology is seen in terms of augmenting and supporting human work practices. These outlooks have profoundly different consequences for the design of software systems. We emphasize that in work design the design process attempts to treat everyday work as the result of a combination of interacting conceptual and physical influences and that practices will develop over time through learning and worker invention. Because work systems are not simply \u201cimplemented,\u201d but develop and grow through the learning of communities, workers and designers (such as software engineers) must collaborate in the design process (Greenbaum and Kyng, 1991).&nbsp;<\/span><\/p>\n<br>\n<span id=\"docs-internal-guid-cbc46a13-7fff-bbd4-f962-b0c4c30aa8e5\">The differences in these two approaches reflect theoretical underpinnings about the nature of knowledge and learning. Finding ways to effectively \u201csee\u201d process and practice at work has been a key challenge for us. We have developed Brahms because we think it provides a step forward in visualizing, analyzing, and thinking about the multiple influences on how work gets done.<\/span>",
                "DataExportTag": "AI307913",
                "QuestionID": "QID160",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"BRAHMS: SIMULATING PRACTICE FOR WORK SYSTEMS DESIGN\" \u00a0 A continuing problem in business today is...",
                "Choices": {
                    "1": {
                        "Display": "\"Parallel Computing Ecosystem Accelerators\": parallel, gpu, processor, compiler, thread, cpu, distribute, speedup, parallelism, parallelization, execution, parallelize, hpc, heterogeneous, cuda"
                    },
                    "2": {
                        "Display": "\"Neural Network Hardware and Software Tuning\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID82",
            "SecondaryAttribute": "\"Co-creation of service innovation in Europe\" There is growing consensus that public services can...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Co-creation of service innovation in Europe\"<\/strong><br>\n<br>\nThere is growing consensus that public services can be improved through experiments which bring together service providers and their users. This proposal is for H2020-SC6-Co-Creation-2016-217: Applied co-creation to deliver public services. The CoSIE project contributes to democratic dimensions and social inclusion through co-creating public services by engaging diverse citizen groups and stakeholders. Utilizing blended data sources (open data, social media) with innovative deployment of ICT (data analytics, Living Lab, Community reporting) in nine pilots, the project introduces the culture of experiments that encompasses various stakeholders for co-creating service innovations.<br>\n<br>\nThe CoSIE project has two overarching aims:&nbsp;<br>\n<br>\ni) Advance the active shaping of service priorities by end users and their informal support networks.<br>\nii) Engage citizens, especially groups often called 'hard to reach,' in the collaborative design of public services.<br>\n<br>\nThe aims are divided into six objectives:<br>\n<br>\n1) Develop practical resources grounded in asset-based knowledge to support new ways for public service actors to redefine operational processes.<br>\n2) Produce and deliver nine real-life pilots to co-create a set of relational public services with various combinations of public sector, civil society, and commercial actors.<br>\n3) Draw together cross-cutting lessons from pilots and utilize innovative visualization methods to share and validate new ideas and models of good governance.<br>\n4) Apply innovative approaches appropriate to local contexts and user groups to gather the necessary user insight to co-create services.<br>\n5) Ensure sustainability by establishing local trainers for animating dialogue and collating user voice, embedded in community networks.<br>\n6) Mobilize new knowledge from piloting and validating by creating an accessible, user-friendly roadmap to co-creation for service providers and their partners.<br>\n<br>\nThe project will be implemented as a joint venture with 24 partners from 10 EU countries.",
                "DataExportTag": "COR52830",
                "QuestionID": "QID82",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Co-creation of service innovation in Europe\" There is growing consensus that public services can...",
                "Choices": {
                    "1": {
                        "Display": "\"Rural Health and Sustainable Agriculture Management\": health, farming, agriculture, multi_actor, transdisciplinary, rural_area, empowerment, systemic, consultation, collective, pathway, inclusion, ce, tailor, pandemic"
                    },
                    "2": {
                        "Display": "\"Global History and Cultural Studies\": indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID81",
            "SecondaryAttribute": "\"Cold War Europe Beyond Borders: A Transnational History of Cross-Border Practices in the Alps-Ad...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Cold War Europe Beyond Borders: A Transnational History of Cross-Border Practices in the Alps-Adriatic area from World War II to the present\"<\/strong><br>\n<br>\nThis project aims to rethink the history of Cold War Europe by examining the development of transnational cross-border cooperation from the end of World War II to the present. Overcoming traditional narratives of a clear-cut European separation symbolized by the Berlin Wall, a decentralized analysis of recent European history will show us that the question of a divided continent should be reframed. The final objective is to challenge a dichotomous vision of two separate Europes, \"East\" and \"West,\" from a new, border perspective.<br>\n<br>\nTo this end, a highly qualified team of senior and junior scholars under my guidance will focus on the Alps-Adriatic region, a historical area now shared by Austria, Italy, Slovenia, and Croatia. This case involves a relatively narrow geographical area but an unusually broad typological range of subjects. During the Cold War, it was divided among socialist but non-aligned Yugoslavia, capitalist but neutral Austria, and NATO and EEC member Italy. Its development from the \"southern end\" of the Iron Curtain in 1946 to the \"most open border\" during the Cold War and a precursor to present-day Schengen Europe represents a paradigmatic case to study an alternative attitude towards borders, frontiers, and boundaries.<br>\n<br>\nDrawing on Cold War and borderland studies, social history, and the history of European integration, our innovative conceptual elaboration will demonstrate the interplay between top-down politics and bottom-up initiatives, offering a new and more nuanced history of Cold War Europe from the border perspective. Reconsidering the European past from this transnational angle, both in terms of geographic and methodological perspectives, will allow us to rediscover the human face of European integration and provide a new platform for contemporary discussions on sovereignty, territoriality, belonging, and the future role of borders in Europe and in the world.",
                "DataExportTag": "COR1505",
                "QuestionID": "QID81",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Cold War Europe Beyond Borders: A Transnational History of Cross-Border Practices in the Alps-Ad...",
                "Choices": {
                    "1": {
                        "Display": "\"Rural Health and Sustainable Agriculture Management\": health, farming, agriculture, multi_actor, transdisciplinary, rural_area, empowerment, systemic, consultation, collective, pathway, inclusion, ce, tailor, pandemic"
                    },
                    "2": {
                        "Display": "\"Global History and Cultural Studies\": indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID225",
            "SecondaryAttribute": "\"Collective Responsibility towards Nature and Future Generations\" This research proposal aims at...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Collective Responsibility towards Nature and Future Generations&quot;<\/strong><br \/>\n<br \/>\nThis research proposal aims at rethinking a collective and proactive concept of responsibility towards nature and future generations, on the basis of the understanding of nature developed by German Idealism in the early 19th century and the theory of responsibility first advanced by Hans Jonas in the 20th century. I will articulate my research into two steps, which aim at two objectives: 1) I will first work on the rational foundation of collective proactive responsibility towards the world in which we live, that is the duty to take care of it. This is the &#39;objective side&#39; of my research, in the sense that it grounds on a consideration of human beings as rational and responsible agents in themselves, in a Kantian sense. To reach this first objective (which is the most challenging one), a further step is needed: the elaboration of a concept of nature and its relation to human being able to take into account (1.1) the ontological continuity between nature and human being and with this the value of nature and life in themselves; (1.2) the primary responsibility of humans. All this will be possible analyzing and revitalizing the concept of nature in German Idealism and studying its ethical implications. 2) Second, I will investigate the role of human motivation, the context and the consequences of human action as essential part of any theory of responsibility. This is the &#39;subjective side&#39; of my research, that aims at providing the &#39;bridge&#39; between the formal foundation of the concept of collective responsibility and its application. In this part, my research will deal with the key concepts of &#39;respect,&#39; &#39;care&#39; towards vulnerable nature and future human beings, and &#39;fear&#39; of a collapse of nature and our future lives. The concept of collective responsibility generated by this project might show its fertility also with reference to contemporary debates (like those related to environmental issues).&quot;",
                "DataExportTag": "COR40200",
                "QuestionID": "QID225",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Collective Responsibility towards Nature and Future Generations\" This research proposal aims at...",
                "Choices": {
                    "1": {
                        "Display": "\"Political Science and Social Studies\": labour, ethnography, peace, military, legitimacy, indigenous, election, domestic, feminist, violent, ethnic, electoral, protest, victim, populism"
                    },
                    "2": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\": farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID197",
            "SecondaryAttribute": "\"Conformational states dynamically populated by a kinase determine its function A moving target A...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Conformational states dynamically populated by a kinase determine its function<\/strong><br \/>\n<br \/>\nA moving target Abl kinase is an important signaling protein that is dysregulated in leukemia and other cancers and is the target of inhibitors such as imatinib. Like other kinases, Abl kinase is dynamic, and regulating conformational dynamics is key to regulating activity. Xie et al. used nuclear magnetic resonance to show that the Abl kinase domain interconverts between one active and two inactive states. Imatinib stabilizes an inactive conformation, and several resistance mutations act by destabilizing this conformation. In a construct that includes the regulatory domain, depending on the relative arrangement of the kinase and regulatory domains, the kinase domain is stabilized in either the active state or one of the inhibited states. Understanding the conformational dynamics of kinases can be leveraged to design selective drugs. Nuclear magnetic resonance data allow dissection of regulatory and drug-resistance mechanisms in Abl kinase.<br \/>\n<br \/>\n<strong>INTRODUCTION<\/strong><br \/>\nProtein kinases mediate many cell signaling processes. Central to their physiological function is the regulation of their binding and enzymatic activities, which is typically achieved by conformational transitions between active and inactive states. Dysregulation of kinase activity by deletions or mutations often results in disease. Protein kinases are dynamic molecules that intrinsically sample a number of conformational states. However, it has been challenging to experimentally access their conformational ensemble and structurally characterize the discrete conformations associated with distinct activities. Such information could advance our understanding of activation and inhibition mechanisms in this protein family and aid in the development of selective inhibitors.<br \/>\n<br \/>\n<strong>RATIONALE<\/strong><br \/>\nWe used nuclear magnetic resonance spectroscopy to monitor in atomic-level detail how Abl kinase transitions between distinct conformational states and to elucidate how the conformational ensemble is exploited by mutants, ligands, posttranslational modifications, and inhibitors to regulate the kinase activity and function. We combined structural and energetic approaches to quantitate the contribution of key structural elements such as the activation loop, the Asp-Phe-Gly (DFG) motif, the regulatory spine, and the gatekeeper residue to kinase regulation and provide the mechanistic basis for drug resistance.<br \/>\n<br \/>\n<strong>RESULTS<\/strong><br \/>\nWe found that the Abl kinase domain interconverts between an active and two, transiently populated, conformational states that adopt discrete inactive structures. There are extensive differences in key structural elements between the conformational states that reveal multiple intrinsic regulatory mechanisms. The small energy difference between active and inactive states allows oncogenic mutations in the regulatory spine or the gatekeeper position to counteract inhibitory mechanisms and constitutively activate the kinase. By capturing and structurally characterizing the conformational state to which the cancer drug imatinib selectively binds, we explain a number of drug-resistance variants isolated in patients. These mutants confer resistance by depleting, through various mechanisms, the conformation to which imatinib binds.<br \/>\n<br \/>\nTo determine the basis for allosteric regulation, we studied a construct that includes the kinase domain and the regulatory domains and that can adopt an assembled and an extended conformation. In the assembled conformation, in which the regulatory domains dock onto the back of the kinase domain, one of the inactive states is selectively stabilized, thereby suppressing catalytic activity. In the extended conformation, wherein the regulatory domains dock on top of the N-lobe, the inactive state is eliminated, thus explaining the increased leukemogenic activity associated with this conformational state. Only one of the detected inactive states appears to be physiologically relevant. The inactive state with no apparent biological function can nevertheless be leveraged for the design of selective inhibitors. Targeting nonphysiological conformational states may be an effective strategy in the design of drugs with increased selectivity and reduced selection pressure for the occurrence of drug-resistance mutations. Although the structure of inactive states can, in principle, vary considerably among kinases, structural comparison of the Abl inactive states with those previously determined for other kinases reveals that there may be a limited number of structurally divergent inactive states intrinsic to kinases.<br \/>\n<br \/>\n<strong>CONCLUSION<\/strong><br \/>\nOur data demonstrate that the detection and structural characterization of the distinct conformational states populated by a kinase, coupled to the energetic dissection of the contribution of key structural elements to the selective stability of these states, are essential to advance our understanding of the mechanisms underpinning kinase regulation and function. The approaches presented here can be extended to other kinases to characterize transiently populated conformational states, with the goal of revealing the full repertoire of regulatory and drug-resistance mechanisms in the kinome.",
                "DataExportTag": "CAN552842",
                "QuestionID": "QID197",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Conformational states dynamically populated by a kinase determine its function A moving target A...",
                "Choices": {
                    "1": {
                        "Display": "\"Cellular Biology and Aging Research\": mitosis, stress, microtubule, homeostasis, aurora, aging, centrosome, membrane, cellular, clock, regulation, polarity, intracellular, dynamic, ion_channel"
                    },
                    "2": {
                        "Display": "\"Epigenetic Regulation and Gene Modification\": pten, chromatin, histone, regulation, stress, methylation, epigenetic, autophagy, metabolic, enzyme, gene, modification, translation, acetylation, control"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID227",
            "SecondaryAttribute": "\"Cyber-security Excellence Hub in Estonia and South Moravia (CHESS)\" The proposed Cyber-security...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Cyber-security Excellence Hub in Estonia and South Moravia (CHESS)&quot;<\/strong><br \/>\n<br \/>\nThe proposed Cyber-security Excellence Hub in Estonia and South Moravia (CHESS) will integrate leading cybersecurity institutions and capitalize on the strengths of both regions to address important Europe-wide challenges. South Moravia is a major ICT industry &amp; education powerhouse of the Czech Republic, with a very focused and coherent smart specialization strategy targeting cybersecurity. Estonia is among the most advanced digital societies globally, with exceptional e-government deployment &ndash; which, however, makes it vulnerable to various cyber threats.<br \/>\n<br \/>\nCHESS will directly follow the strategies and roadmaps of the European Cybersecurity Competence Pilots and build on the experience of CHESS partners involved in all four of these pilots, contributing to the safe transition of the EU to a full-scale digital society.<br \/>\n<br \/>\nThe CHESS Hub will conduct a thorough needs analysis of the two regions and develop a joint cross-border R&amp;I strategy for cybersecurity. The strategy development will be aided by the implementation of pilot R&amp;I projects that will reinforce the cross-regional collaboration, engage regional innovation ecosystems, and build evidence for future projects. Gaps in skills and expertise identified in the regions will be removed by training and knowledge transfer. Finally, dedicated task forces will ensure sustainability of CHESS by integration with regional, national, and EU-level strategies and funding programs.<br \/>\n<br \/>\nTo exploit the project outputs, especially the pilot project results, CHESS will aid with market potential assessment and link researchers and innovators with entrepreneurship training and business consultancy services available in the regions. The strategizing, skills-building, and pilot R&amp;I will cover the entirety of the cybersecurity field, with special attention to 6 Challenge Areas: Internet of Secure Things; Security Certification; Verification of Trustworthy Software; Blockchain; Post-Quantum Cryptography; and Human-centric Aspects of Cybersecurity.",
                "DataExportTag": "COR12334",
                "QuestionID": "QID227",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Cyber-security Excellence Hub in Estonia and South Moravia (CHESS)\" The proposed Cyber-security...",
                "Choices": {
                    "1": {
                        "Display": "\"Nuclear Security and Digital Infrastructure Management\": nuclear, cybersecurity, critical_infrastructure, interoperable, blockchain, certification, marketplace, toolbox, federation, digital_twin, factory, security_privacy, experimentation, deploy, cross"
                    },
                    "2": {
                        "Display": "\"Mobile Banking and E-commerce\": mobile, blockchain, payment, personal, store, bank, transaction, insurance, biometric, smartphone, retailer, purchase, retail, pay, e_commerce"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID87",
            "SecondaryAttribute": "\"Development of a Strategy to Treat Limb-Girdle Muscular Dystrophy (LGMD2A) Using Combined Cell a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Development of a Strategy to Treat Limb-Girdle Muscular Dystrophy (LGMD2A) Using Combined Cell and Gene Therapy Strategies\"<\/strong><br>\n<br>\nIn Duchenne muscular dystrophy, several mutations can be corrected by the exon-skipping technique on cells. In limb-girdle muscular dystrophy type 2A (LGMD2A), however, all the mutations described to date do not seem to be good candidates for the RNA correction technology. The recently identified c.1782+1072 G&gt;C mutation in LGMD2A patients could be potentially repaired at the RNA level. The principal aim of this project is to mask the effect of this mutation in stem cells from patients that can be derived to muscle and to test its therapeutic potential in a mouse model. This is the first step to be considered before planning a clinical trial in patients with this mutation. Our long-term goal is to generate new advanced therapy protocols (gene and cell therapies) against LGMD2A, a muscular dystrophy that is especially prevalent in the population of the Basque region. To this end, we will pursue the following aims:<br>\n<br>\n1. To develop a therapeutic approach for LGMD2A patients who carry c.1782+1072G&gt;C mutation.<br>\n2. Isolation, optimization of in vitro culture, and genetic correction of CD133+ cells from patients with this mutation.<br>\n3. Analysis of the efficacy of this combined therapeutic approach in murine models in vivo.<br>\n<br>\nIn order to accomplish aims 1 and 2, we will develop a descriptive study with fibroblasts (from skin) and CD133+ cells (from blood and muscle) from donors, in which we will determine the efficiency, yield, and myogenic potential for subsequent testing in mice. To accomplish aim 3, we will perform a double-blind experimental approach in mice (wild type versus C3KO\/scid and mdx\/scid mice), using CD133+ cells with the most myogenic potential (from aim 2) as an in vivo therapy to determine the correction effect in the human calpain 3. This work would contribute to the treatment of LGMD2A patients who carry c.1782+1072G&gt;C mutation in calpain 3 and it could also be helpful for the treatment of other patients with similar types of mutations.",
                "DataExportTag": "COR194",
                "QuestionID": "QID87",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Development of a Strategy to Treat Limb-Girdle Muscular Dystrophy (LGMD2A) Using Combined Cell a...",
                "Choices": {
                    "1": {
                        "Display": "\"Pharmaceutical Drug Delivery and Nanomedicine\": drug, delivery, cellular, therapeutic, nanoparticle, drug_delivery, drug_discovery, compound, toxicity, pharmaceutical, gene_therapy, nanomedicine, electric_vehicle, vivo, release"
                    },
                    "2": {
                        "Display": "\"Gene Therapy and Immune Response in Cardiac and Metabolic Diseases\": cellular, therapeutic, immune, genetic, inflammation, metabolic, gene_therapy, mechanism, liver, ra, cardiac, vivo, heart_failure, inflammatory, molecule"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID96",
            "SecondaryAttribute": "\"Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer Nutr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer Nutrition and Immune Function Objective\"<\/strong><br>\n<br>\nEarly nutritional support in patients with gastric cancer can improve their nutritional status, but the impact on immune function has not been confirmed. This study aimed to analyze the effects of Qihuang decoction combined with enteral nutrition on nutrition and the immune function of postoperative gastric cancer.<br>\n<br>\n<strong>Methods<\/strong><br>\n<br>\n- 120 patients with postoperative gastric cancer in the study group and 117 in the control group were selected as the study subjects from our hospital at random.<br>\n- Indications of nutrition and immune function, and the rates of complications were compared the day before surgery and 1, 3, 7, and 14 days after surgery.<br>\n<br>\n<strong>Results<\/strong><br>\n<br>\n- Indications of nutrition except hemoglobin (HB) in the study group were significantly higher than those before operation, and albumin (ALB) and prealbumin (TP) were significantly increased 7 and 14 days after surgery (P &lt; 0.001).<br>\n- Protein (PA) levels were higher 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group.<br>\n- Cellular and humoral immunity indications in the study group were significantly higher than those before operation.<br>\n- CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P &lt; 0.001).<br>\n- IgA, IgG, and IgM were higher 3, 7, and 14 days after surgery in the study group (P &lt; 0.001).<br>\n- Complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017).<br>\n<br>\n<strong>Conclusions<\/strong><br>\n<br>\nQihuang decoction combined with enteral nutrition can promote the absorption of enteral nutrition, improve the immune system, and reduce the risk of infection complications.",
                "DataExportTag": "CAN247180",
                "QuestionID": "QID96",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer Nutr...",
                "Choices": {
                    "1": {
                        "Display": "\"Statistical Modeling and Error Prediction\": simulation, error, prediction, bayesian, dynamic, modeling, estimator, uncertainty, mathematical_model, probability, measurement, noise, stochastic, equation, monte_carlo"
                    },
                    "2": {
                        "Display": "\"Medical Imaging and Cancer Treatment\": imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID131",
            "SecondaryAttribute": "\"Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer Nutr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer Nutrition and Immune Function Objective&quot;<\/strong><br \/>\n<br \/>\nEarly nutritional support in patients with gastric cancer can improve their nutritional status, but the impact on immune function has not been confirmed. This study aimed to analyze the effects of Qihuang decoction combined with enteral nutrition on nutrition and the immune function of postoperative gastric cancer.<br \/>\n<br \/>\n<strong>Methods<\/strong><br \/>\n<br \/>\n- 120 patients with postoperative gastric cancer in the study group and 117 in the control group were selected as the study subjects from our hospital at random.<br \/>\n- Indications of nutrition and immune function, and the rates of complications were compared the day before surgery and 1, 3, 7, and 14 days after surgery.<br \/>\n<br \/>\n<strong>Results<\/strong><br \/>\n<br \/>\n- Indications of nutrition except hemoglobin (HB) in the study group were significantly higher than those before operation, and albumin (ALB) and prealbumin (TP) were significantly increased 7 and 14 days after surgery (P &lt; 0.001).<br \/>\n- Protein (PA) levels were higher 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group.<br \/>\n- Cellular and humoral immunity indications in the study group were significantly higher than those before operation.<br \/>\n- CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P &lt; 0.001).<br \/>\n- IgA, IgG, and IgM were higher 3, 7, and 14 days after surgery in the study group (P &lt; 0.001).<br \/>\n- Complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P&chi;2=0.017).<br \/>\n<br \/>\n<strong>Conclusions<\/strong><br \/>\n<br \/>\nQihuang decoction combined with enteral nutrition can promote the absorption of enteral nutrition, improve the immune system, and reduce the risk of infection complications.",
                "DataExportTag": "CAN1491233",
                "QuestionID": "QID131",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer Nutr...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetic Sequencing and Genomic Diversity\": genome, gene, sequence, exon, deletion, somatic_mutation, mtdna, somatic, clone, sequencing, deoxyribonucleic_acid, diversity, transcript, splicing, region"
                    },
                    "2": {
                        "Display": "\"Genetic Disorders and Chromosomal Abnormalities in Acute Myelogenous Leukemia\": chromosome, deletion, loh, region, acute_myelogenous_leukemia, loss, cytogenetic, chromosomal, amplification, gene, translocation, fish, loss_heterozygosity, fusion, rearrangement"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID151",
            "SecondaryAttribute": "\"Enabling Precis Integrative Netw\" A key challenge in precision medicine lies in understanding mo...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Enabling Precis Integrative Netw\"<\/strong><br>\n<br>\nA key challenge in precision medicine lies in understanding molecular-level underpinnings of complex human disease. Biological networks in multicellular organisms can generate hypotheses about disease genes, pathways, and their behavior in disease-related tissues. Diverse functional genomic data, including expression, protein\u2013protein interaction, and relevant sequence and literature information, can be utilized to build integrative networks that provide both genome-wide coverage as well as contextual specificity and accuracy. By carefully extracting the relevant signal in thousands of heterogeneous functional genomics experiments through integrative analysis, these networks model how genes work together in specific contexts to carry out cellular processes, thereby contributing to a molecular-level understanding of complex human disease and paving the way toward better therapy and drug treatment. Here, we discuss current methods to build context-specific integrative networks, focusing on tissue-specific networks. We highlight applications of these networks in predicting tissue-specific molecular response, identifying candidate disease genes, and increasing power by amplifying the disease signal in quantitative genetics data. Altogether, these exciting developments enable biomedical scientists to characterize disease from pathophysiology to cellular system and, finally, to specific gene alterations\u2014making significant strides toward the goal of precision medicine.<br>\n<br>\n<strong>NETWORKS AS MODELS OF HUMAN BIOLOGY<\/strong><br>\nTo realize the promise of precision medicine, we must elucidate the immense molecular complexity that forms the foundation of disease. Most human diseases are polygenic, perhaps even \u201comnigenic\u201d [1]. While decades of targeted disease research and the rise of large-scale quantitative genetics studies such as genome-wide association studies (GWAS) have been valuable in identifying genes and genetic variants that may be linked to a wide range of diseases and phenotypes, it is increasingly clear that there is a \u201cmissing heritability\u201d problem (i.e., even as the sample sizes in these studies continue to grow, only a small proportion of estimated heritability appears to be explained by the identified variants) [2, 3]. Understanding complex disease at the molecular level requires us to model specific molecular-level changes that lead to disease. These changes can happen through a variety of mechanisms, for example, regulatory abnormalities, modifications to protein interactions, or effects on signaling cascades. Modern genome-scale experimental techniques enable us to monitor and probe these molecular events by providing a wealth of data along multiple axes of cellular activity. Diverse high-throughput data vary in relevance depending on the biological process under study (e.g., a specific tissue, disease, or pathway), as both experimental technologies and perturbations capture different biological signals with varying degrees of success. Thus, integrative analysis of these data is paramount because (1) many diseases\/tissues of interest are interrogated by multiple data sets; (2) each data set holds a complex mixture of signals relevant not only to the biological question or disease under study, but also to many other biological events (e.g., cancer data sets have very strong immune signals, kidney disease data have strong inflammation signals); and (3) individual data sets are noisy, necessitating the identification of strong, recurring signals in relevant data sets. A powerful set of approaches has emerged for integrating diverse data into functional maps of human cellular biology. These network approaches use a variety of machine learning and statistical algorithms to integrate very large collections of noisy and heterogeneous human \"omics\" data into functional maps, or networks. Biological network models have typically represented general views of organismal biology, not resolved to specific tissues or cell types. However, tissue and cellular context is critical for interpreting the behavior of genes and pathways, as gene function and interactions can vary greatly between tissues and cell types, and dysregulation of tissue- and cell-lineage-specific processes underlies many diseases. For example, selective neuronal vulnerability is a key characteristic of neurodegenerative diseases such as Parkinson's disease, and the neuronal subtypes as well as affected brain regions tend to be strong determinants of their corresponding clinical phenotypes. In Parkinson's disease, the dopaminergic neurons in the substantia nigra pars compacta area of the brain are particularly susceptible to cell death, while highly similar dopaminergic neurons in the nearby ventral tegmental area are much less vulnerable. Thus, to fully capture the underlying biological processes relevant for a disease like Parkinson's disease, brain-region-specific networks are necessary. Below, we discuss approaches to construct tissue- and cell-type-specific networks from integrations of large collections of public functional genomic data, and how such networks can be applied to study the molecular basis of human disease.<br>\n<br><strong>Building tissue- and cell-type-specific functional networks:<\/strong> Methods that construct tissue-specific networks have historically been limited by the availability of experimental data for specific tissues and cell types, especially in humans. These direct approaches typically assemble available tissue-specific expression data into gene correlation networks or overlay those expression data on global (non-tissue-specific) protein-protein interaction networks. More sophisticated methods to construct context-specific regulatory networks by integrating context-specific (e.g., tissue- or cell-type specific) expression data with a non-context-specific network have also been recently developed. These approaches, while valuable, are applicable only to tissues and cell types which can be readily assayed and depend almost entirely on the quality of the available tissue- or cell-type-specific data. To address this challenge, recent work introduced a method that can simultaneously construct tissue-specific networks by integrating a large data compendium using a tissue-specific regularized Bayesian integration method. These integrative networks are used for a variety of applications, including predicting tissue-specific molecular response, identifying candidate disease genes, and amplifying the disease signal in quantitative genetics data. Overall, these developments have significant potential for advancing precision medicine by enabling a deeper understanding of complex human diseases and facilitating the development of more effective therapeutic strategies.",
                "DataExportTag": "AI265629",
                "QuestionID": "QID151",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Enabling Precis Integrative Netw\" A key challenge in precision medicine lies in understanding mo...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunology and Infection Response\": receptor, cell, activation, innate_immune, macrophage, cytokine, expression, toll_receptor, immune, mouse, infection, inflammation, immune_response, tlrs, signal"
                    },
                    "2": {
                        "Display": "\"Genomic Analysis and Disease Prediction\": cell, gene, image, variant, prediction, phenotype, snp, tissue, deep_learning, gene_expression, omic, genomic, disease, analysis, mutation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID179",
            "SecondaryAttribute": "\"Fluorescence Tomography Characterization for Sub-surface Imaging with Protoporphyrin IX\" Optical...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Fluorescence Tomography Characterization for Sub-surface Imaging with Protoporphyrin IX&quot;<\/strong><br \/>\n<br \/>\nOptical imaging of fluorescent objects embedded in a tissue-simulating medium was characterized using non-contact-based approaches to fluorescence remittance imaging (FRI) and sub-surface fluorescence diffuse optical tomography (FDOT). Using Protoporphyrin IX as a fluorescent agent, experiments were performed on tissue phantoms comprised of typical in-vivo tumor to normal tissue contrast ratios, ranging from 3.5:1 up to 10:1. It was found that tomographic imaging was able to recover interior inclusions with high contrast relative to the background; however, simple planar fluorescence imaging provided a superior contrast-to-noise ratio. Overall, FRI performed optimally when the object was located on or close to the surface, and, perhaps most importantly, FDOT was able to recover specific depth information about the location of embedded regions. The results indicate that an optimal system for localizing embedded fluorescent regions should combine fluorescence reflectance imaging for high sensitivity and sub-surface tomography for depth detection, thereby allowing more accurate localization in all three directions within the tissue.",
                "DataExportTag": "CAN968308",
                "QuestionID": "QID179",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Fluorescence Tomography Characterization for Sub-surface Imaging with Protoporphyrin IX\" Optical...",
                "Choices": {
                    "1": {
                        "Display": "\"Radiation Therapy and Dosimetry Planning\": radiotherapy, beam, planning, proton, imrt, irradiation, dosimetric, energy, dosimetry, intensity_modulate, photon, monte_carlo, absorb_dose, delivery, arc"
                    },
                    "2": {
                        "Display": "\"Medical Imaging and Cancer Treatment\": imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "LabelPosition": "SIDE"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID240",
            "SecondaryAttribute": "\"Fluorescence Tomography Characterization for Sub-surface Imaging with Protoporphyrin IX\" Optical...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Fluorescence Tomography Characterization for Sub-surface Imaging with Protoporphyrin IX&quot;<\/strong><br \/>\n<br \/>\nOptical imaging of fluorescent objects embedded in a tissue-simulating medium was characterized using non-contact-based approaches to fluorescence remittance imaging (FRI) and sub-surface fluorescence diffuse optical tomography (FDOT). Using Protoporphyrin IX as a fluorescent agent, experiments were performed on tissue phantoms comprised of typical in-vivo tumor to normal tissue contrast ratios, ranging from 3.5:1 up to 10:1. It was found that tomographic imaging was able to recover interior inclusions with high contrast relative to the background; however, simple planar fluorescence imaging provided a superior contrast-to-noise ratio. Overall, FRI performed optimally when the object was located on or close to the surface, and, perhaps most importantly, FDOT was able to recover specific depth information about the location of embedded regions. The results indicate that an optimal system for localizing embedded fluorescent regions should combine fluorescence reflectance imaging for high sensitivity and sub-surface tomography for depth detection, thereby allowing more accurate localization in all three directions within the tissue.",
                "DataExportTag": "CAN968308",
                "QuestionID": "QID240",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Fluorescence Tomography Characterization for Sub-surface Imaging with Protoporphyrin IX\" Optical...",
                "Choices": {
                    "1": {
                        "Display": "\"Radiation Therapy and Dosimetry Planning\": radiotherapy, beam, planning, proton, imrt, irradiation, dosimetric, energy, dosimetry, intensity_modulate, photon, monte_carlo, absorb_dose, delivery, arc"
                    },
                    "2": {
                        "Display": "\"Medical Imaging and Cancer Treatment\": imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID75",
            "SecondaryAttribute": "\"Functionalization of Living Cells with Organic Electronics: Smart Cells\" \"Smart Cells,\" living c...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Functionalization of Living Cells with Organic Electronics: Smart Cells&quot;<\/strong><br \/>\n<br \/>\n&quot;Smart Cells,&quot; living cells with electronic functionalities, will add entirely new capabilities to the field of bioelectronics and could potentially revolutionize biomedical research. This project represents a synergy between forefront bio-materials research and the rapidly developing field of organic electronics. Precisely, we will attach organic electronic devices onto living cells in order to probe their environments in living organisms. A multitude of applications are envisioned, such as recording interaction events of the cell with surrounding proteins, delivering drugs to specific locations inside a body, or targeting cancer cells with remotely controlled &quot;killer devices.&quot; The cell functions either as a transport medium to introduce the organic electronic device into specific environments inside the organism or actively interacts with the device through chemical or electronic\/ionic communication pathways. The electronic device triggers a cell function upon receiving an external stimulus, for example, in the form of electromagnetic radiation.<br \/>\n<br \/>\nThe technique relies on a method recently developed by the Rubner group at the Massachusetts Institute of Technology (USA), where the fellow will spend the outgoing phase of this project. They demonstrated the attachment of functional polymer backpacks onto living cells without modifying the cell&#39;s natural behavior. The fellow will combine this technique with his strong background in the fabrication and physics of organic electronic devices to design suitable electronic device backpacks for living cells. The technique will then be transferred to the European host, the Malliaras group in France, a key player in the field of organic bioelectronics. As a result of this work, the fellow will develop a comprehensive understanding of bioelectronics and immensely benefit from exposure to scientific perspectives in bio-materials research. The project will uniquely qualify him for his desired career in bioelectronic research.",
                "DataExportTag": "COR13966",
                "QuestionID": "QID75",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Functionalization of Living Cells with Organic Electronics: Smart Cells\" \"Smart Cells,\" living c...",
                "Choices": {
                    "1": {
                        "Display": "\"Advanced Imaging and Detection Techniques\": imaging, x_ray, image, microscopy, sensor, nuclear_magnetic_resonance, optic, resolution, measurement, detection, probe, sensitivity, detector, spectroscopy, molecule"
                    },
                    "2": {
                        "Display": "\"Nanotechnology and Molecular Chemistry\": molecule, electric, nanoparticle, single_molecule, chemical, self_assembly, surface, catalysis, deoxyribonucleic_acid, functional, protein, nanostructure, control, polymer, organic"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID93",
            "SecondaryAttribute": "\"Gene Vaccines\" Recent developments in immunology and molecular biology have permitted the develo...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Gene Vaccines&quot;<\/strong><br \/>\n<br \/>\nRecent developments in immunology and molecular biology have permitted the development of DNA and other gene-based vaccines. Unlike previous vaccines, many of which were designed to induce antibodies as the mechanism of protection against clinical disease caused by microorganisms, gene-based vaccines are designed to generate both cytotoxic cellular and antibody immune responses. An additional goal is to make vaccines that can be used to treat chronic diseases. Targeted diseases have expanded from infectious diseases to include cancer, allergies, and autoimmune diseases.<br \/>\n<br \/>\nThe immune response comprises both cellular and humoral (antibody) components. For certain infectious agents, the efficacy of a vaccine may be correlated with the induction of a particular level of circulating antibody in a vaccinated person. Until recently, much less was known about how to induce or measure clinically effective cellular immunity. To date, no known type or level of cellular response has been shown to correlate with the efficacy of a vaccine or therapy in humans. Moreover, whereas antibodies can directly attack microorganisms, T cells work by killing infected or cancerous cells. Thus, by definition, T cells cannot prevent infection; rather, they contain or eliminate it.<br \/>\n<br \/>\nInstead of directly attacking a pathogen, a cytolytic T cell is activated to kill when its receptor binds to certain molecules on another cell and recognizes foreign elements (from the pathogen or from novel cancer proteins). The receptor has dual specificity, meaning that it sees two molecules on the surface of the target cell. One molecule is the MHC class I antigen, which is specific to an individual person, and the other molecule is a peptide derived from proteins of the pathogen or cancer cell. Thus, the T cell recognizes and is activated by foreign or abnormal peptides when those peptides are associated with a person&#39;s own MHC class I molecules. In an infected or cancerous cell, some of the newly synthesized viral (or other) proteins are degraded into peptides and enter the endoplasmic reticulum of the cell, where they meet and bind to newly synthesized MHC class I molecules.<br \/>\n<br \/>\nWhen this MHC class I antigenic peptide complex is expressed on the surface of an antigen-presenting cell (along with co-stimulatory molecules, to which the T cell must also bind), T cells with receptors that specifically recognize this complex are activated and can kill infected cells.<br \/>\n<br \/>\nEfforts to specifically induce cytolytic T cell responses have greatly increased so that this component of the immune system might be used for vaccination. The challenge has been that if a protein is provided to an antigen-presenting cell, in general, that cell will take up the protein into the endolysosomal pathway and degrade it into peptides associated with MHC class II rather than class I proteins. The MHC class II proteins recycle from the cell surface through the endolysosomal pathway, where they encounter peptide epitopes. These MHC class II peptide complexes stimulate helper T cells that produce cytokines that, in turn, help activate cytolytic T cells and assist B cells to generate antibodies.<br \/>\n<br \/>\nFor a protein to enter the proteolytic pathway that generates the peptides that will bind to MHC class I molecules, it must be introduced into the cytoplasm of a cell. Thus, finding a means of introducing antigenic proteins from pathogens or cancer cells into this pathway has become one of the prime challenges in vaccine development.<br \/>\n<br \/>\nIntroduction of proteins directly into the cytoplasm from extracellular spaces has proven difficult; a more feasible method is to deliver a gene encoding the desired protein to the cell. If the cell then transcribes and translates the gene, the protein will be in the cytoplasm for proper processing and presentation. Thus, a method of delivering adequate amounts of the desired gene, resulting in adequate expression of the encoded protein, is needed. In addition, because T cells can be stimulated to respond only when the MHC antigen complex is on the surface of professional antigen-presenting cells that have other cell surface proteins that interact with the T cell, it was thought that the genes would have to be delivered specifically to antigen-presenting cells rather than to other cell types.<br \/>\n<br \/>\n<strong>Viral and Bacterial Vectors<\/strong><br \/>\n<br \/>\nViruses have highly evolved structures that enable them to bind to cells and deliver genes into the cells they infect. It may be feasible to use live virus vaccines to induce cytolytic T lymphocytes and thus treat a disease in which cellular immunity is critical. However, in some diseases (such as AIDS), concerns about the safety of a live attenuated vaccine are too great to make this a practical approach.<br \/>\n<br \/>\nIn addition, a stronger immune response and an improved safety profile may be obtained by using a vaccine that delivers only the genes that encode the desired antigens (that is, the specific antigen against which the immune response important for protection is directed) rather than all the genes for a given pathogen. Therefore, efforts have been made to use harmless viruses or ones that have been rendered incapable of reproducing for the delivery of genes derived from a different pathogen.<br \/>\n<br \/>\nIn other words, a vector is made in which all the genes or key genes of a virus are removed, or the virus is nonpathogenic or nonreplicative in humans. Genes encoding a protein antigen from the pathogen (that is, from a different virus or a tumor antigen) are then put in the virus particle. The virus acts as a transporter, in the manner of a Trojan horse, to deliver the gene encoding the antigen. The viral vector is itself antigenic, and certain viruses cause inflammatory responses. These responses may benefit the desired immune response, depending on their type, location, and extent, but they might also be drawbacks.<br \/>\n<br \/>\nPreexisting immunity due to previous infection (for example, adenovirus) or vaccination (for example, smallpox) may limit the potency of the vaccine by clearing the viral vector before it can infect cells to deliver its payload of antigen genes. Similarly, if the vector is reused for a booster immunization, the immune response against the virus itself may eliminate too many of the vector particles before they can deliver the genes. Much effort has been directed to making vectors from other strains of viruses, vectors that are not as immunogenic, and vectors that do not induce inflammatory responses.<br \/>\n<br \/>\nBacteria are also being developed as delivery systems for plasmid DNA vaccines and as heterologous expression systems in which the genes encoding antigens are incorporated into the genome of the bacteria. One attractive feature of bacterial vectors is their potential for oral administration, which would make use easier and may generate mucosal immune responses.<br \/>\n<br \/>\n<strong>DNA Vaccines<\/strong><br \/>\n<br \/>\nPlasmid DNA is a simple ring of double-stranded DNA that contains a gene encoding the desired protein (an antigen, in the case of vaccines) and elements needed for the gene to be expressed in whatever cell has taken up the DNA (that is, a promoter at the beginning of the gene and a terminator at the end). It had long been assumed that simple plasmid DNA, called naked DNA because neither a chemical formulation nor a viral coat or envelope structure surrounds it, would not be taken up by cells or could not reach the nucleus for transcription of genes.<br \/>\n<br \/>\nIn 1990, Wolff and colleagues provided compelling evidence that plasmid DNA can transfect cells in vivo. This finding opened new possibilities for delivering genes in vivo without the use of viral vectors. Plasmid DNA is considerably simpler to manipulate in terms of putting genes into the plasmid and producing and purifying large quantities of plasmid. Thus, plasmid DNA offers several potential advantages in delivering genes, whether as a vaccine or as a therapy.<br \/>\n<br \/>\nA major attribute of DNA vaccines is their ability to deliver genes into cells for the generation of MHC class I-restricted cytolytic T lymphocyte responses. These responses require that the antigens reach the cytosol of specific antigen-presenting cells, and it was previously not clear whether professional antigen-presenting cells could take up plasmid DNA or whether the protein antigen encoded by the DNA (taken up by other cells) would reach the correct antigen-presenting cells and subcellular locations.<br \/>\n<br \/>\nUlmer and colleagues demonstrated that immunization of mice by intramuscular injection of plasmid DNA encoding an influenza viral protein generated specific cytolytic T cells and protected the mice against subsequent challenge with live influenza virus. Morbidity and mortality from infection with a strain of virus that differed from the strain from which the vaccine was made were decreased.",
                "DataExportTag": "CAN767223",
                "QuestionID": "QID93",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Gene Vaccines\" Recent developments in immunology and molecular biology have permitted the develo...",
                "Choices": {
                    "1": {
                        "Display": "\"Cancer Treatment and Histone Deacetylase Inhibitors Research\": histone_deacetylase, bortezomib, atra_treatment, hdac_inhibitor, hdac, egcg, proteasome, proteasome_inhibitor, hdaci, vpa, retinoid, saha, tsa, myeloma, sfn"
                    },
                    "2": {
                        "Display": "\"Radiation Therapy and Cancer Treatment Research\": nf_kappab, curcumin, deoxyribonucleic_acid, radiotherapy, histone_deacetylase, nuclear, interventional_radiology, irradiation, damage, hdac_inhibitor, ppar\u03b3, saha, ionize_radiation, radiosensitivity, tumor_necrosis_factor"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID144",
            "SecondaryAttribute": "\"GUI BASED FUZZY LOGIC CONTROL SYSTEM\" The work presented in this paper addresses a generalized g...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"GUI BASED FUZZY LOGIC CONTROL SYSTEM\"<\/strong><br>\n<br>\nThe work presented in this paper addresses a generalized graphic user interface (GUI) based Fuzzy Logic Control System (FLCS) to design, edit, and execute a control system. The proposed system is user-friendly and easy to implement. It helps the user to create a fuzzy logic system in simple steps, inserting the names and membership functions details of all inputs and outputs defining the rule base for FLCS. Detailed structure of the system and design of modules, fuzzifier, inference engine, rule base, and defuzzifier were discussed. The results of the proposed model were compared with the results of MATLAB Simulink. This research work proposed to develop a novel control system to create and succeed FLCS in a few steps efficiently.<br>\n<br>\n<strong>INTRODUCTION<\/strong><br>\nLotfi A. Zadeh proposed Fuzzy logic in 1965. Fuzzy logic is based on multi-valued logic. This logic defines intermediate values in formal evaluation as true\/false, hot\/cold, good\/bad, yes\/no, and high\/low, etc. Fuzzy systems are the substitutes for traditional design of membership and logic that has its start in early Greek philosophy. Fuzzy Logic was introduced as a useful tool for driving system, control, weather forecasting, household and entertainment electronics, expert systems, and complex industrial mechanisms. In the last few decades, a lot of work was done in the industrial control domain using fuzzy logic. Solanano S. S. et al. [1] mentioned the program and design of electronic remote controls with the help of Fuzzy logic using storage space-based structure made up of memory space components. Mohammad Y. Hassan et al. [2] proposed the design of PID based Fuzzy Logic (FL) controller on a field programmable gate array (FPGA) tool. He also proposed fuzzy inference rules along with an efficient selection of rules procedure for the management of heat variety for expert programs-based heaters. Homaifar A. et al. [3] explained the concurrent design of rule sets and MF for FC with the help of inherited algorithms for a specific set of environment. Poorani S. et al. [4] suggested FLC for DC motor control for electric vehicles based on FPGA. Jyh-Shing Roger Jang et al. [5] explained the concepts of fuzzy logic control and modeling, the method of fuzzy reasoning, fuzzification, defuzzification, for multiple and single rule along with single and multiple accents. Mamdani, Tsukamoto, Sungeno, proposed a partition style for neuro fuzzy and fuzzy models. They explain their structures with the help of examples. Kim J. H.. [6] implemented the FLC for the refrigerator for industrial use to control the distribution of refrigerant for multi-type AC. Wang L.X. et al. [7] explained the technique for developing fuzzy rules form numeric data. Hollstein T. et al. (1996) [8] explained that the performance of fuzzy logic systems will be better if they implement on the hardware as compared to their performance when they implement in software on microcontrollers. They discussed the hardware structure of three different systems and provide computer-aided design (CAD) packages for FCS. Chang Jian et al [9] proposed the new hardware design for a fuzzy controller (FC) and presented a new technique to create rules of FLC using neural networks and fuzzy set. Fuzzy logic system for temperature control was designed with the help of FPGA. Bin-Da Liu et al. [10] proposed a tree-based algorithm for the development of FLC. Cirstea M.N. et al. [11] implemented and designed an intelligent control for model power generator and converter system. Poplawski, M. et al. [12] proposed FPGA-based FLC architecture for a wheelchair. Shuo Shen et al. [13] suggested a technique to find out the center of the location by keeping the track of moving targets in wireless sensor networks. Roushan, R.T. et al. [14] proposed the implementation of FPGA-based fuel control system using. Fujii, Wataru et al. [15] proposed FPGA-based hardware control module for the design of electronic applications and proposed a decentralized, independent management system for a single-phase uninterruptible power supply inverter. Yasin M. et al. [16] developed Fuzzy Logic Control Systems-FLCS of temperature control for industrial use with Field Programmable Gate Array. The results of simulation display design were examined successfully. In all cases of design, implementation, and development, prequalified techniques of systems required to know that how best judge requirements were needed using graphic user interfaces. In this paper, structures of the fuzzy logic control system along with the detail of the main Window\/Menu of GUI-Based Fuzzy Logic Tool are discussed.<br>\n<br>\n<strong>MATERIALS AND METHODS<br>\n<br>\nBASIC STRUCTURE OF FUZZY LOGIC CONTROL SYSTEM<\/strong><br>\nFuzzy Logic based decision mechanism uses linguistic variables and can have more than two values [0, 1] in the interval. They do not do the strict binary decision.<br>\n<strong>-&nbsp;Fuzzifier:&nbsp;<\/strong>Fuzzifier is used to convert the input crisp value into the linguistic fuzzy values. The output of the fuzzifier is linguistic values of fuzzy sets. Each fuzzifier consists of a subtractor, region comparator, multiplier, and divider. The fuzzifier accepts the 0-5V input values, and the multiplier converts it into the crisp values of a given range. Then the crisp values are compared with the given region values with the help of a comparator and decide the respective region with the help of the input variable. The subtractor helps to find the difference of crisp values from the final values of the selected region. The divider takes the results from the subtractor and divides it to map the input values. Another subtractor is used to find the second value of the other fuzzy set. It is done by subtracting the first fuzzy set value from 1. It must be remembered that at least two linguistic values are generated from one crisp value.<br>\n<strong>-&nbsp;Rule Base: <\/strong>The crisp input values are received by the rule base receiver, and then these values are checked according to the defined rules of the system. The output singleton value for each output variable is returned if they satisfy the defined system rule. Eight rules are required for three input variables to find the output singleton value in the region of each variable.<br>\n<strong>-&nbsp;Defuzzifier:<\/strong> The linguistic fuzzy values are converted into output crisp values by the defuzzifier. It accepts singleton values from the rule base and the input values from the inference engine. It takes a multiple of R and Singleton values and divides it.<br>\n<strong>- Inference Engine: <\/strong>The inference engine plays a vital role in the performance and execution of the FLCS. Two input values from each fuzzifier are required. Then the min-max operation is done on received inputs by the inference engine and R output values are obtained. The min-max operation applies min-AND operation on each received input. The minimum values are returned as a result of min-AND operation.",
                "DataExportTag": "AI845315",
                "QuestionID": "QID144",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"GUI BASED FUZZY LOGIC CONTROL SYSTEM\" The work presented in this paper addresses a generalized g...",
                "Choices": {
                    "1": {
                        "Display": "\"Interdisciplinary Studies in Cognitive Science and Theory\": cognitive, cognition, theory, human, psychology, mind, consciousness, psychological, social, mental, mathematics, philosophical, turing, thinking, metaphor"
                    },
                    "2": {
                        "Display": "\"Integrated Cognitive Science and Philosophy of Mind\": cognitive, human, theory, mind, consciousness, cognition, brain, agent, psychology, philosophy, social, mathematics, philosophical, concept, turing"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID85",
            "SecondaryAttribute": "\"Gulag Echoes in the \u201cmulticultural prison\u201d: Historical and Geographical Influences on the Ident",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Gulag Echoes in the \u201cmulticultural prison\u201d: Historical and Geographical Influences on the Identity and Politics of Ethnic Minority Prisoners in the Communist Successor States of Russia Europe<\/strong><br>\n<br>\nThe project will examine the impact of the system of penality developed in the Soviet gulag on the ethnic identification and political radicalization of prisoners in the Soviet Union and the communist successor states of Europe today. It is informed by the proposition that prisons are sites of ethnic identity construction but that the processes involved vary within and between states. In the project, the focus is on the extent to which particular \"prison-styles\" affect the social relationships, self-identification, and political association of ethnic minority prisoners.<br>\n<br>\nAfter the collapse of the Soviet Union, the communist successor states all set about reforming their prison systems to bring them into line with international and European norms. However, all to a lesser or greater extent still have legacies of the system gestated in the Soviet Gulag and exported to East-Central-Europe after WWII. These may include the internal organization of penal space, a collectivist approach to prisoner management, penal labor, and, as in the Russian case, a geographical distribution of the penal estate that results in prisoners being sent excessively long distances to serve their sentences. It is how these legacies, interacting with other forces (including official and popular discourses, formal policy, and individual life-histories) transform, confirm, and suppress the ethnic identification of prisoners that the project seeks to excavate.<br>\n<br>\nIt will use a mixed method approach to answer research questions, including interviews with ex-prisoners and prisoners' families, the use of archival and documentary sources, and social media. The research will use case studies to analyze the experiences of ethnic minority prisoners over time and through space. These provisionally will be Chechens, Tartars, Ukrainians, Estonians, migrant Uzbek and Tajik workers, and Roma, and the country case studies are the Russian Federation, Georgia, and Romania.",
                "DataExportTag": "COR1045",
                "QuestionID": "QID85",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Gulag Echoes in the \u201cmulticultural prison\u201d: Historical and Geographical Influences on the Identi...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\": child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "2": {
                        "Display": "\"Global History and Cultural Studies\": indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID89",
            "SecondaryAttribute": "\"Health Consequences of Noise Exposure from Road Traffic\" There is growing public concern about a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Health Consequences of Noise Exposure from Road Traffic&quot;<\/strong><br \/>\n<br \/>\nThere is growing public concern about adverse effects of traffic noise on health, as research has found that traffic noise increases the risk for cardiovascular diseases. Noise is thought to act as a stressor and disturbs sleep. Though this potentially could increase the risk for other major diseases, noise effects on other than cardiovascular diseases are virtually unexplored.<br \/>\n<br \/>\nThe main objective of this project is to investigate if long-term exposure to road traffic noise is detrimental to various health outcomes in susceptible groups, i.e., children and the elderly. Outcomes in children include low birth weight, infections, and cognitive performance, and in the elderly, outcomes include diabetes, cancer, cancer survival, health-related quality of life, and health behavior.<br \/>\n<br \/>\nThe basis of this proposal is two unique Danish cohorts, comprising 57,053 elderly and 101,042 children (a national birth cohort). Historic and present residential addresses for all cohort members will be obtained through linkage with the nationwide Central Population Registry, and exposure to road traffic noise and air pollution will be calculated by validated models at all addresses.<br \/>\n<br \/>\nThe health outcomes will be obtained from cohort interviews\/questionnaires or found through linkage with unique, nationwide, population-based health registers, such as the Danish National Hospital Registry, the Diabetes Registry, and the Cancer Registry. Data will be analyzed using a number of statistical analyses depending on the design and the character of the endpoint variable. All analyses will be adjusted for potential confounders such as air pollution, smoking, and education.<br \/>\n<br \/>\nWithin the EU, 30% of the population lives at locations where the 55dB WHO noise limit is exceeded. Knowledge of harmful effects of noise is, however, limited. The results of the proposed research have a high potential to influence the content and time schedule of noise action plans in the EU member states.",
                "DataExportTag": "COR26376",
                "QuestionID": "QID89",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Health Consequences of Noise Exposure from Road Traffic\" There is growing public concern about a...",
                "Choices": {
                    "1": {
                        "Display": "\"Infectious Disease Control and Vaccine Research\": vaccine, infection, antibiotic, tuberculosis, pathogen, bacterial, infectious_disease, vaccination, antigen, antimicrobial, virus, outbreak, antibiotic_resistance, antimicrobial_resistance, malaria"
                    },
                    "2": {
                        "Display": "\"Public Health and Preventive Medicine\": health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID88",
            "SecondaryAttribute": "\"IN-OVO Automation for Cancer Drug Discovery\" Currently, the in vivo step in the drug discovery p...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"IN-OVO Automation for Cancer Drug Discovery\"<br>\n<br><\/strong>Currently, the in vivo step in the drug discovery process is always performed using the mouse model. This step is expensive and time-consuming, with an average cost of 40k\u20ac per molecule tested and an average duration of 6-9 months. It represents a huge obstacle for the pharmaceutical industry, leading to fewer new drugs brought to market each year, and insufficient treatments for severe pathologies such as cancer or orphan diseases.\n\nINOVOTION can play a major role in solving this huge societal issue: we have the technology to revolutionize drug discovery by automating in vivo drug evaluation for efficacy and toxicity. INOVOTION is a spin-off of the University of Grenoble, founded in 2014 by Prof. Viallet. Our researchers have been working since 2009 on a new proprietary technology to perform \u201cin ovo\u201d assays that precede mouse testing, using chick embryos. Our tests allow researchers to eliminate low-value molecules at an early stage in the process, and to dedicate mouse testing for high-potential molecules only. Furthermore, our technology is reliable, reproducible, highly sensitive, fast, and automatable. The successful 150 studies we performed for big pharma, small biotechs, and academics (in the USA, France, UK, Japan, South Africa, Czech Republic, and the United Arab Emirates) have confirmed the market demand for our technology.\n\nThe worldwide market for in vivo testing is about 30 billion euros per year, mainly for cancer research, and INOVOTION has the technology to meet the market's needs. For each 1% in market share we cover, we could save the pharmaceutical industry up to 240 M\u20ac.\n\nThe OVOMATIC project aims to initiate an automated \u201cin ovo\u201d process including developing a robot tool head prototype and finalize the commercial and financial strategies of our approach. The long-range impact of the OVOMATIC project is to introduce this new tool into the pharmaceutical landscape, greatly increasing the productivity of our clients' anti-cancer drug discovery efforts.",
                "DataExportTag": "COR48252",
                "QuestionID": "QID88",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"IN-OVO Automation for Cancer Drug Discovery\" Currently, the in vivo step in the drug discovery p...",
                "Choices": {
                    "1": {
                        "Display": "\"Pharmaceutical Drug Delivery and Nanomedicine\": drug, delivery, cellular, therapeutic, nanoparticle, drug_delivery, drug_discovery, compound, toxicity, pharmaceutical, gene_therapy, nanomedicine, electric_vehicle, vivo, release"
                    },
                    "2": {
                        "Display": "\"Gene Therapy and Immune Response in Cardiac and Metabolic Diseases\": cellular, therapeutic, immune, genetic, inflammation, metabolic, gene_therapy, mechanism, liver, ra, cardiac, vivo, heart_failure, inflammatory, molecule"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID79",
            "SecondaryAttribute": "\"Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean:\" P...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\"Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean:\"\n\nPrimary production in the oceans is strongly limited by the availability of fixed nitrogen. In open ocean nutrient-impoverished areas, which make up ~50% of the global ocean surface, nitrogen is mainly provided through the process of biological atmospheric nitrogen (N2) fixation. N2 fixation is carried out by the so-called diazotrophs, marine microorganisms that may belong to the cyanobacteria, bacteria, or archaea.\n\nFor many years, autotrophic diazotrophs were thought to be the most abundant diazotrophs in the ocean. Autotrophic diazotrophs need light to fix carbon dioxide via photosynthesis and are, therefore, constrained to the sunlit layer of the ocean, which is generally less than 100 m deep. Recent investigations have revealed that heterotrophic diazotrophs, which cannot photosynthesize, are present in greater abundance than autotrophic diazotrophs in the world's oceans. Heterotrophic diazotrophs are not constrained by the availability of light and are, therefore, able to live in the dark ocean, the largest and less studied habitat on Earth. This discovery significantly expands the boundaries where N2 fixation was thought to be possible and theoretically increases the inputs of fixed nitrogen to the ocean, which remain unaccounted for.\n\nBecause they are not photosynthetic, heterotrophic diazotrophs need an external source of dissolved organic matter (DOM) for their nutrition. However, the nature of this DOM and how it influences their activity is largely unknown. This project aims to cover this gap by studying their relationship with DOM in the ocean. Through shipboard experiments and the use of cutting-edge analytical techniques, we will explore the spatial distribution of heterotrophic diazotrophs' abundance, diversity, and N2 fixation activity related to the in-situ concentration and composition of DOM. The results will provide unique insights into the ecology of heterotrophic diazotrophs and their role in the oceanic nitrogen cycle.",
                "DataExportTag": "COR7096",
                "QuestionID": "QID79",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean:\" P...",
                "Choices": {
                    "1": {
                        "Display": "\"Polymer and Composite Material Science\": polymer, plastic, composite, textile, packaging, coating, fibre, film, printing, ink, adhesive, resin, surface, composite_material, biodegradable"
                    },
                    "2": {
                        "Display": "\"Environmental Science and Climate Change\": soil, carbon_dioxide, microbial, ocean, carbon, bacterial, fate, sediment, flux, metabolic, atmospheric, microplastic, plant, climate, metal"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID268",
            "SecondaryAttribute": "\"IoT Based Smart Water Monitoring & Distribution System For An Apartments\" As we know water is so...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">\"IoT Based Smart Water Monitoring &amp; Distribution System For An Apartments\"<\/b><\/p><p dir=\"ltr\"><b><br><\/b><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">As we know water is so precious for human beings as well as for the complete nature without which it will not be possible to survive. Even though many efforts have been taken by the government through various schemes, it is becoming difficult day by day to save water for the future and make efficient utilization of it. In this proposed work, an IoT design for water monitoring and control approach which supports internet-based data collection on real time bases. This proposed system shall be implemented in highly populated residential buildings like hotels, lodge, hostels, dormitories, apartments, shopping malls etc. And also, this system can provide a complete survey and the usage of water by every individual room. This system addresses the flow rate measuring and scheming the supply of water in order to limit the water wastage and approach the water conservation and also this system can measure the quality and quantity of water distributed to every household by using ph and flow rate sensors. The system has been designed in such a way that it will monitor the available water level continuously. System has been implemented by using embedded systems and communication will take place.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><br>\n<b id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">INTRODUCTION<\/b><\/p><p dir=\"ltr\"><b><br><\/b><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">The internet of things (IoT) forms an important part of intelligent monitoring which connects people and devices using wireless sensor technology. It is a fast growing research area in the military, energy management, healthcare and many more. The concept of IoT was proposed by Kevin Ashton to demonstrate a set of interconnected devices. IoT makes it possible to transfer information between different electronic devices embedded with new technology. Energy management is possible using energy harvesting mechanisms, which is a method of collecting energy from natural sources such as light, vibration, pressure etc. The combination of technologies such as Wireless sensor network (WSN), Radio frequency identification (RFID), Energy harvesting (EH) and Artificial Intelligence (AI) helps IoT to flourish widely.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">Water distribution system (WDS) is a very important research area that affects the economic growth of our country. WDS mainly have two issues, first is the water loss due to leakage and the second is that it is prone to contamination. It is affecting the health and safety of the people. According to the report of world health organization (WHO) in 2017, around 2.1 billion people around the world lack safe drinking water. So there is a need to ensure the water quality and wastage by using Iot to reduce such issues. There are different traditional methods to collect water datasets to measure its quality, but managing and monitoring the data from WDS in real time is challenging as the data is heterogeneous, data collection is time consuming, energy required for processing, coverage and connectivity of the nodes in the network. By using IoT and combining technologies such as WSN, AI and EH can be used to ensure the water quality in real time and alerts the users to take remedial measures.&nbsp;<br>\n<br><b>\nLITERATURE REVIEW<\/b><\/span><\/p><p dir=\"ltr\"><span><b><br><\/b><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">While over 15 million American households rely upon private well sources for water [3], the remaining 110 million households are connected to public water supplies. Likewise, most commercial and industrial applications use public water supplies. Public and municipal water utilities must carefully monitor the water they provide for public safety, billing, and resource management. Over the last few decades, water utility companies have begun installing automated meter reading (AMR) systems to further simplify the process of meter reading, decrease manual labor, and reduce transcription errors within collected data [4]. These systems allow more frequent reporting of measured demand at the individual customers, while simultaneously reducing the manual effort of physically looking at each meter to record the volume measured. In 2018, the American Water Works Association reported 37% of utilities in North America have fully implemented AMR systems, and another 24% are in the process of doing so [5]. Many of the AMR systems support quarter-hourly reads, but battery limitations and data related costs constrain the data collection to hourly or daily reads. It is from these AMR systems that the data for our proposed algorithm comes. While there is little work in customer water flow clustering, other research explores clustering of energy customer\u2019s using smart meter data. Panapakid is et al. [6], [7] implement clustering of electric smart meter data. As opposed to creating models such as our algorithm, their work clusters the daily typical load profiles within a customer\u2019s dataset. Representatives of those clusters are used to complete the second stage clustering across the population of all customers. Their work illustrates the complex problem of identifying the optimal number of clusters in a diverse dataset. In contrast to the Panapakid is work, the clustering method presented here does not require a definition of an optimal number of clusters. Bose and Chen [8], [9] track changing cluster populations over time using fuzzy c-means algorithms. Their work focuses upon migratory patterns of cellular phone customers, for the purposes of tracking dynamic market demands and customer retention. Their data exhibit not only customers who migrate from one cluster within the data to another, but also the formation of new clusters and dissolution of others as new behavior patterns emerge within the population.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\"><b>PROPOSED EXPLANATIONS<\/b><br>\n<br>\nThis system addresses the flow rate measuring and scheming the supply of water in order to limit the water wastage and approach the water conservation and also this system can measure the quality and quantity of water distributed to every household by using ph and flow rate sensors. The system has been designed in such a way that it will monitor the available water level continuously.&nbsp;<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">Proposed smart management platform (hereinafter referred to as SmartWMP) is considering a supervisory and control of both water and energy flows to improve water and energy efficiency offering simultaneously the possibility of carrying out transactions directly between utilities and local renewable producers in order to provide a sustainable management of water supply systems.&nbsp;<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">SmartWMP integrates water and energy nexus usage related information from smart meters, data analysis (profiling, modeling, simulation, and optimization) using AI techniques, DR programme, and services for peer-to-peer (P2P) transactions on basis of smart contracts between the water utilities and local renewable producers (inside micro grids).&nbsp;<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">The integrated modules of software are the following: Integrated Water and Energy Database Management comprehensive analysis of integrated databases to describe different patterns to the level of pump stations (water and energy) and consumers (water). Water distribution system (WDS) is a very important research area that affects the economic growth of our country. WDS mainly have two issues first is the water loss due to leakage and the second is that it is prone to contamination. It is affecting the health and safety of the people.&nbsp;<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\">According to the report of world health organization (WHO) in 2017, around 2.1 billion people around the world lack safe drinking water. So there is a need to ensure the water quality and wastage by using Iot to reduce such issues. The DR programme identifies solutions for the water pumps which could participate in a DR scheme and how the control of power consumption improves the potential of participation in a DR scheme. Blockchain based ICT topology analyses the potential impact of Blockchain technology on the integrated water and power sectors and explores what opportunities it may hold for water utilities. A vision of a solution is presented. Integrated Profiling Concept to treat integrated profiling techniques to energy and water used in smart management based on the correlation between data provided by the smart metering system. AI based techniques for modelling and simulation develop the mathematical models and computing algorithms based on meta-heuristic approaches for optimization and expert systems for decision making in pumping water management.<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-5b361d54-7fff-abe9-aeab-aea4b935df7c\"><b>ARDUINO UNO: <\/b>The Arduino Integrated Development Environment (IDE) is a cross platform application (for Windows, macOS, Linux) that is written in functions from C and C++. It is used to write and upload programs to Arduino compatible boards, but also, with the help of third-party cores, other vendor development boards. The source code for the IDE is released under the GNU General Public License, version 2. The Arduino IDE supports the languages C and C++ using special rules of code structuring. The Arduino IDE supplies a software library from the Wiring project, which provides many common input and output procedures.<\/span><\/p>",
                "DataExportTag": "AI900548",
                "QuestionID": "QID268",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"IoT Based Smart Water Monitoring & Distribution System For An Apartments\" As we know water is so...",
                "Choices": {
                    "1": {
                        "Display": "\"Unmanned Aerial and Underwater Vehicles\": unmanned_aerial_vehicles, flight, mission, drone, aircraft, vehicle, unmanned_aerial, ship, autonomous, underwater, spacecraft, unmanned, vehicle_uav, landing, vessel"
                    },
                    "2": {
                        "Display": "\"Industrial Automation and Robotics\": robot, computer_vision, image, inspection, speech_recognition, industrial, remote, autonomous_robot, sensor, voice, autonomous, smart, microcontroller, raspberry_pi, wireless"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID143",
            "SecondaryAttribute": "\"Learning by DesignTM: Promoting Deep Science Learning Through A Design Approach\".\u00a0 This paper de..",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Learning by DesignTM: Promoting Deep Science Learning Through A Design Approach\".&nbsp;<\/strong><br>\n<br>\nThis paper describes a new method of using design activities to teach science methods and principles in middle school. We describe the history and cognitive foundations in addition to the curriculum design. Finally, we describe how written and performance assessments are used to determine the extent to which students acquire science content and methods in both test and comparison classes.<br>\n<br>\n<strong>Introduction<\/strong><br>\n<br>\nHands-on project experiences are becoming more and more common in education but often, while students do a fine job of completing a project and certainly enjoy the experience, learning tends to be superficial. Learning by DesignTM draws on a theoretical understanding of human cognition together with pragmatic classroom organization techniques to create an activity-based science curriculum organized by complex, real-world design problems. Case-based reasoning (CBR), a model of learning by analogy with prior experience, and Problem-based Learning (PBL), an approach to facilitating learning from problem-solving experiences, together provide guidelines for orchestrating and facilitating hands-on activities in classrooms in ways that promote deeper learning.<br>\n<br>\nDrawing on CBR and PBL, we've identified many of the affordances and potential affordances for learning that project and problem-solving activities provide, and we've designed classroom rituals and software tools that help teachers and students identify those affordances and exploit them. We've implemented our approach in several curriculum units covering aspects of both physical and earth science. These units have been piloted in a wide variety of classrooms in the greater Atlanta area, covering a full spectrum of student backgrounds and capabilities. Formal assessments show that our understanding of design as a vehicle for deep learning is sound. Formative evaluation of the software shows that it adds to the affordances for learning.<br>\n<br>\nLearning by DesignTM has been piloted and field tested with over 2000 students and two dozen teachers. We've learned many things about how to make a design approach work well in science learning. This paper describes the background and current structure of LBDTM as well as the results from our evaluation effort.<br>\n<br>\n<strong>Theoretical Framework<\/strong><br>\n<br>\nCase-based reasoning (Kolodner, 1993) was developed as a way of enhancing the reasoning capabilities of computers; it is a kind of analogical reasoning in which problems are solved by reference to previously experienced situations and the lessons learned from them. Experiences implementing computer systems that could reason and learn based on their experience have allowed the case-based reasoning community to extract principles about learning from experience \u2013 e.g., the kinds of interpretations of experience that are important to reach a reusable encoding of an experience, the kinds of interpretations of experience that promote accessibility, and triggers for generating learning, explanation goals, and revising previously-made encodings.<br>\n<br>\nThe basic premise underlying CBR is the preference to reason using the most specific and most cohesive applicable knowledge available. Inferences made using specific knowledge are relatively simple to make. Inferences made using cohesive knowledge structures, i.e., those that tie together several aspects of a situation, are relatively efficient. Cases, which describe situations, are both specific and cohesive. In addition, they record what is possible, providing a reasoner with more probability of moving forward in a workable way than is provided by using general knowledge that is merely plausible. Furthermore, reasoning based on previous experience seems natural in people; an understanding of how it is done well and effectively can provide guidelines for helping people to effectively use this natural reasoning process.<br>\n<br>\nLearning, in the CBR paradigm, means extending one's knowledge by interpreting new experiences and incorporating them into memory, by reinterpreting and re-indexing old experiences to make them more usable and accessible, and by abstracting out generalizations over a set of experiences. Interpreting an experience means creating an explanation that connects one\u2019s goals and actions with resulting outcomes. Such learning depends heavily on the reasoner\u2019s ability to create such explanations, suggesting that the ability and desire to explain are key to promoting learning.<br>\n<br>\nCBR thus gives failure a central role in promoting learning because failure promotes a need to explain. When the reasoner's expectations fail, it is alerted that its knowledge or reasoning is deficient. When some outcome or solution is unsuccessful, the reasoner is similarly alerted of a deficiency in his\/her knowledge. When such failures happen in the context of attempting to achieve a personally-meaningful goal, the reasoner wants to explain so that he\/she can be more successful. Crucial to recognizing and interpreting failure is useful feedback from the world. A reasoner that is connected to the world will be able to evaluate its solutions with respect to what results from them, allowing indexing that discriminates usability of old cases and allowing good judgments later about reuse.<br>\n<br>\nProblem-based learning (Barrows, 1985) is an approach to educational practice that has students learn by solving authentic, real-world problems and reflecting on their experiences. Because the problems are complex, students work in groups, where they pool their expertise and experience and together grapple with the complexities of the issues that must be considered. Coaches guide student reflection on their problem-solving experiences, asking students to articulate both the concepts and skills they are learning, and helping them identify the cognitive skills needed for problem solving, the full range of skills needed for collaboration and articulation, and the principles behind those skills. But students decide how to go about solving problems and what they need to learn, while coaches question students to force them to justify their approach and explain their conclusions.<br>\n<br>\nStudents learn the practices of the profession they are learning, the content professionals need to know, as well as skills needed for lifelong learning. PBL has been used substantially at medical and business schools for over 20 years. Research shows that students in problem-based curricula are indeed learning facts and concepts and the skills needed for critical problem solving and self-learning.<br>\n<br>\nProblem-based learning prescribes a highly-structured sequence of classroom practices. Students keep track, on a set of whiteboards, of facts they know, hypotheses and ideas they have about explaining and solving the problem, and issues they do not yet understand and need to learn more about (learning issues). The whiteboards organize and track the progress of an investigative cycle that continues until students are satisfied that they have solved the problem. The explanations from those discussions soon fill up the whiteboards with ideas, diagrams, and explanations. This in-process reflection helps students make connections between their problem-solving goals, the processes involved in achieving those goals, and the content they are learning.<br>\n<br>\nWhile CBR suggests many of the kinds of experiences students should have to learn deeply, PBL suggests means of coaching and scaffolding those experiences so that they are both student-centered and focused on relevant content and skills. Both suggest that students\u2019 experiences should be well-orchestrated so that early experiences help students identify what they need to learn, and later experiences engage students in investigations that allow such learning. CBR suggests, in addition, that later experiences be rich in application and feedback, iteratively looping through that cycle until students\u2019 understandings are complete or adequate.",
                "DataExportTag": "AI752556",
                "QuestionID": "QID143",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Learning by DesignTM: Promoting Deep Science Learning Through A Design Approach\".\u00a0 This paper de...",
                "Choices": {
                    "1": {
                        "Display": "\"Interdisciplinary Studies in Cognitive Science and Theory\": cognitive, cognition, theory, human, psychology, mind, consciousness, psychological, social, mental, mathematics, philosophical, turing, thinking, metaphor"
                    },
                    "2": {
                        "Display": "\"Integrated Cognitive Science and Philosophy of Mind\": cognitive, human, theory, mind, consciousness, cognition, brain, agent, psychology, philosophy, social, mathematics, philosophical, concept, turing"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID76",
            "SecondaryAttribute": "\"MIGOSA - Image Sensor for Low-Light Camera Applications\" Challenge: One of the biggest global ch...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;MIGOSA - Image Sensor for Low-Light Camera Applications&quot;<\/strong><br \/>\n<br \/>\n<strong>Challenge: <\/strong>One of the biggest global challenges in low-light camera applications, such as in videosurveillance, is the need to significantly improve image quality and identification in low light. Even software-enhanced low-light images remain blurred and are unsuited to automated image recognition, demonstrating the need to develop more efficient image sensors.<br \/>\n<br \/>\n<strong>Solution: <\/strong>Cameras with Pixpolar&#39;s low-light image sensor (MIG technology) capture both visible light and Near Infra-Red (NIR) light that cannot be seen with the naked eye. Cameras equipped with MIG image sensors extend 1.6 times the maximum detection range compared to existing technology. It allows superior identification of objects and is immune to interference. It also allows an 80% reduction in illumination cost or a 60% reduction in the number of cameras required for area surveillance at a cost lower than the cost of present technology. A prototype MIG pixel has been made, tested, and validated.<br \/>\n<br \/>\n<strong>Business opportunity: <\/strong>The MIG image sensor is a game-changing technology. The advantages of the MIG technology enable Pixpolar to enter the large global image sensor market (USD 15.2 billion by 2020). The MIG technology was demonstrated in Horizon 2020 Phase 1 to many camera producers and camera end users. They all saw great market opportunity in embedding MIG image sensors in their cameras. A solid business case can be secured by targeting the first 5-10 camera manufacturers, from which further business expansion can be nurtured. In February 2018, the business case was also endorsed by an investment of 2 million EUR from an international investor consortium.<br \/>\n<br \/>\n<strong>The purpose of the MIGOSA project: <\/strong>To accomplish test production of low-light MIG image sensors and to start the commercialization of the image sensors. This will open the large business potential in the fields of Security and Surveillance (S&amp;S), night vision, maritime, and scientific markets as well as later in the automotive, aviation, drone, industrial, and medical markets.",
                "DataExportTag": "COR61570",
                "QuestionID": "QID76",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"MIGOSA - Image Sensor for Low-Light Camera Applications\" Challenge: One of the biggest global ch...",
                "Choices": {
                    "1": {
                        "Display": "\"Advanced Imaging and Detection Techniques\": imaging, x_ray, image, microscopy, sensor, nuclear_magnetic_resonance, optic, resolution, measurement, detection, probe, sensitivity, detector, spectroscopy, molecule"
                    },
                    "2": {
                        "Display": "\"Nanotechnology and Molecular Chemistry\": molecule, electric, nanoparticle, single_molecule, chemical, self_assembly, surface, catalysis, deoxyribonucleic_acid, functional, protein, nanostructure, control, polymer, organic"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID398",
            "SecondaryAttribute": "\"Multipath and Explicit Rate Congestion Control on Data Networks\" Computer networks based on the...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<b>\"Multipath and Explicit Rate Congestion Control on Data Networks\"<br><\/b>\n<br>\nComputer networks based on the TCP\/IP (Transmission Control Protocol\/Internet Protocol) employ TCP congestion control and shortest path routing. However, TCP congestion control can result in under-utilization of link capacity, low session throughput, and unfairness in its throughput performance over impaired links. Conventional shortest path routing can lead to network congestion and under-utilized links due to uneven distribution of traffic in the network.<br>\n<br>\nTo address these problems, this thesis proposes multipath congestion control algorithms for data networks, which combine multipath routing with network congestion control.&nbsp;<br>\n<br>\nFirst, an efficient multipath route discovery algorithm is proposed to find multiple paths in the underlying network infrastructure. The multipath route discovery algorithm can find multipath routes with varying degrees of disjointedness.&nbsp;<br>\n<br>\nSecond, we develop a multipath traffic distribution algorithm to alleviate network congestion by exploiting multipath routes. The proposed \"congestion-triggered multipath protocol\" requires relatively minor upgrades to the existing Internet architecture.<br>\n<br>\nRecently, there have been proposals to introduce explicit rate signaling into the Internet. Explicit rate signaling has the potential to substantially improve network performance but requires routers that can support signaling on a per-flow basis. Along these lines, we propose an adaptive dynamic rate controller that computes the rate for flows in response to network status (e.g., network congestion, link underutilization) in order to minimize network congestion and fully utilize the link capacity. We evaluate its performance in conjunction with a rate-based transport protocol.<br>\n<br>\n<strong>Chapter 1: Introduction<\/strong><br>\n<br>\nThe Internet was originally designed to provide a single level of service (i.e., best-effort) in which the network does not provide any performance guarantees on data delivery or on the Quality-of-Service (QoS) level. Most computer networks are designed to provide best-effort service and employ shortest path routing protocols that provide a shortest path between source and destination pairs, and end-to-end congestion control mechanisms that react to congestion by adapting the sender transmission rate based on indirect network congestion indications, e.g., packet loss.<br>\n<br>\nThe route between source and destination is typically determined by means of shortest path routing protocols such as Routing Information Protocol (RIP), Open Shortest Path First (OSPF), and Border Gateway Protocol (BGP). In these routing protocols, the path length is typically taken as the hop count, i.e., the link cost is taken to be unity by default. Another link metric that is often used in conjunction with shortest path routing algorithms is link capacity. Under shortest path routing, all packets associated with a given source-destination pair generally traverse a single path of shortest length, even though other paths may be available. Consequently, shortest path routing can lead to network congestion due to excessive traffic routed on a single path, even though other paths may be under-utilized.<br>\n<br>\nMultipath routing has been proposed as a means to alleviate the limitations of shortest path routing algorithm. In multipath routing, packets belonging to a given source-destination pair may be transmitted over multiple paths. Some of the potential benefits of multipath routing include load balancing, higher network throughput, reduction of routing oscillation, the alleviation of congestion, and improved packet delivery reliability.<br>\n<br>\nIn prior work dating to the 1950\u2019s, the K shortest paths problem has been studied in a number of papers. The K shortest paths problem is to determine the K shortest paths (i.e., the shortest path, the second shortest path, . . ., the K-th shortest path) between the given pair of nodes. However, K shortest paths problem may not be ideally suited to controlling congestion over multiple paths.<br>\n<br>\nEqual-Cost MultiPath (ECMP) and Optimized Multipath (OMP) are protocols that have been proposed to implement multipath routing in the current Internet. ECMP finds equal cost multipath routes (i.e., multiple paths with an equal number of hops or equal metric) and distributes traffic equally over a multipath route. OMP is an improved version of ECMP, which allows unequal traffic distribution (i.e., distribution in inverse proportion to the utilizations of the constituent paths). The paths in the multipath route need not be of equal length. The set of paths in a multipath route includes the shortest path, obtained from the shortest path routing protocol, plus alternative paths derived from shortest paths from each of the neighbors of the source node to the destination node.<br>\n<br>\nTransmission Control Protocol (TCP) has been a dominant protocol for end-to-end congestion control in the Internet. The current implementation of TCP congestion control uses a strategy called Additive Increase Multiplicative Decrease (AIMD). The basic idea is that when a packet is acknowledged, the window size is increased by one, but when a packet is lost, the window is reduced to half of its previous size. However, TCP has often been found to be inefficient over links with large delays and\/or high packet loss rates. TCP throughput is inversely proportional to round trip time (RTT), which may lead to efficiency and fairness problems.<br>\n<br>\nAmong multiple connections with different RTTs sharing a link, connections with shorter RTT will obtain more bandwidth resources. Due to these problems in TCP, some applications use User Datagram Protocol (UDP) rather than TCP and are unresponsive to congestion indicators. However, excessive use of unresponsive applications can lead to congestion collapse of the computer network.<br>\n<br>\nTo solve the problems of TCP, protocols based on explicit congestion control have been proposed, such as eXplicit Control Protocol (XCP), Rate Control Protocol (RCP), and TCP-Explicit Rate (TCP-ER). In these protocols, congestion control is based on the feedback of an explicit sending rate determined by the network elements (e.g., routers or switches) along the path from source to destination. Rather than increase the window size by one during each RTT, the sender transmits data at an explicit rate signaled by the network. If congestion occurs in the network, the network feeds back an adjusted explicit rate to the sender in order to alleviate the congestion.<br>\n<br>\n<strong>1.1 Problem Statement<\/strong><br>\n<br>\nECMP and OMP provide multipath routing to solve the problem of shortest path routing. However, ECMP is not guaranteed to determine a multipath route for each source-destination pair because only equal cost paths are considered for a multipath route. Also, packets are forwarded in equal proportion, on a packet-by-packet basis, over the paths in the multipath route, without considering network congestion. As a result, packets may arrive out-of-order within a flow at the destination which leads to degraded TCP performance. In OMP, the characteristics (i.e., disjointedness\/partially disjointedness) of the multipath route are not taken into account. OMP is triggered by path utilization information so multiple nodes sharing common subpaths may simultaneously begin distributing traffic over multipath routes sharing common links. Although congestion may be avoided over the original path, other paths may become congested as an unwanted side effect of the OMP traffic distribution policy. This could lead to the triggering of OMP at further nodes, which may eventually result in network instability. In both ECMP and OMP, each path in a multipath route may have a different propagation delay.<br>\n<br>\nExplicit rate control protocols such as XCP, and RCP have been proposed to solve the problems of TCP. The sending rate is determined by the network and fed back to the sender as an explicit rate. Then, the sender transmits data at<br>\n<br>\n&nbsp;the given explicit rate. However, these protocols may underestimate or fail to take into account the effect of the feedback propagation delay between the network and sender. This may lead to congestion collapse due to the use of outdated rate information. Dynamic rate controllers proposed take into account this propagation delay in the explicit rate computation. These controllers are designed based on queue dynamics so that their mechanisms are optimized for relatively heavy traffic conditions, but not for light traffic conditions. Also, these controllers result in rate fluctuations even in steady-state conditions, that is, they do not provide a constant rate in steady state.<br>\n<br>\n<strong>1.2 Research Objective<\/strong><br>\n<br>\nThe primary goal of this research is to develop multipath and explicit rate congestion control algorithms to optimize network performance. An essential step towards this goal is the development of an algorithm to determine multipath routes, which can be implemented on top of conventional shortest path routing algorithms without requiring major changes to the underlying network infrastructure.<br>\n<br>\nWe consider a traffic distribution mechanism that can distribute traffic over a multipath route using the current Internet infrastructure. Then, we develop an implementation of a rate-based transport protocol to improve the throughput performance of the Internet via explicit rate signaling. Finally, we discuss a novel rate computation algorithm for determining the flow sending rates, which can minimize network congestion and enable high utilization of the network capacity.",
                "DataExportTag": "AI725498",
                "QuestionID": "QID398",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Multipath and Explicit Rate Congestion Control on Data Networks\" Computer networks based on the...",
                "Choices": {
                    "1": {
                        "Display": "\"Reinforcement Learning and Decision Making\": reinforcement_learning, policy, reward, action, agent, deep_reinforcement_learning, deep_learning, exploration, environment, mdp, actor_critic, decision_making, imitation, markov, dqn"
                    },
                    "3": {
                        "Display": "\"Wireless Sensor Network Optimization and Routing Protocols\": routing, wireless_sensor_network, energy, route, packet, clustering, routing_protocol, protocol, transmission, lifetime, wireless, genetic_algorithm, traffic, sensor, mobile"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "3"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 5,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID266",
            "SecondaryAttribute": "\"Multiple Agent Event Detection and Representation in Videos\" \u00a0 We propose a novel method to dete..",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<h3 dir=\"ltr\"><b id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">\"Multiple Agent Event Detection and Representation in Videos\"<\/b><\/h3>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">We propose a novel method to detect events involving multiple agents in a video and to learn their structure in terms of temporally related chain of sub-events. The proposed method has three significant contributions over existing frameworks. First, we present the concept of a video event graph , to learn the event structure from training videos. The video event graph is composed of temporally correlated sub-events, which is used to automatically encode the event correlation graph . The event correlation graph signifies the frequency of occurrence of conditionally dependent sub-events. Second, we pose the problem of event detection in novel videos as clustering the maximally correlated sub-events, and use normalized cuts to determine these clusters. The principal assumption made in this work is that the events are composed of highly correlated chains of sub-events, that have high weights (association) within the cluster and relatively low weights (disassociation) between clusters. Last, we recognize the importance of representing the variations (in the temporal order of sub-events) occurring in an event and encode the probabilities directly into our representation. We show results of our learning and detection of events for videos in the meeting, surveillance, and railroad monitoring domains.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Introduction&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">The world that we live in is a complex network of agents and their interactions which we term events. These interactions can be visualized in the form of a hierarchy of events and sub-events. An instance of an event is a composition of directly measurable low-level actions (which we term sub-events) having a temporal order. For example, a voting event is composed of a sequence of move, raise and lower hand sub-events. Also, the agents can act independently (e.g. voting) as well as collectively (e.g. touchdown in a football game) to perform certain events. Hence, in the enterprise of machine vision, the ability to detect and learn the observed events must be one of the ultimate goals.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">In literature, a variety of approaches have been proposed for the detection of events in video sequences. Most of these approaches can be arranged into three categories based on their approach to event detection.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">First, approaches where event models are pre-defined include force dynamics, stochastic context free grammars (Bobick and Ivanov 1998), state machines (Koller, Heinze, and Nagel 1991), and PNF Networks (Pinhanez and Bobick 1998). These approaches either manually encode the event models or provide constraints (grammar or rules) to detect events in novel videos.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Second, approaches that learn the event models such as Hidden Markov Models (HMMs) (Ivanov and Bobick 2000, Brand and Kettnaker 2000), Coupled HMMs (Oliver, Rosario, and Pentland 1999), and Dynamic Bayesian Networks (Friedman, Murphy, and Russell 1998) have been widely used in the area of activity recognition. The above learning methods either model single person activities or require prior knowledge about the number of people involved in the events and variation in data may require complete re-training, so as to modify the model structure and parameters to accommodate those variations. Similarly, there is no straight-forward method of expanding the domain to other events, once training has been completed.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Third, approaches that do not model the events, but utilize clustering methods for event detection include co-embedding prototypes (Zhong, Shi, Visontai 2004), and spatio-temporal derivatives (Zelnik-Manor and Irani 2001). These methods find event segments by spectral graph partitioning (e.g. normalized cut) of the weight (similarity) matrix. These methods assume maximum length of an event and are restricted to single person non-interactive event detection.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">What is missing in these approaches is the ability to model long complex events involving multiple agents performing multiple actions simultaneously. Can these approaches be used to automatically learn events involving unknown number of agents? Will the learnt event model still hold for a novel video, in case of interfering events from an independent agent? Can these approaches extend their abstract event model to representations related to human understanding of events? Can a human communicate his or her observation of an event to a computer or vice versa? These questions are addressed in this paper, where event models are learnt from training data, and are used for event detection in novel videos. Event learning is formulated in a probabilistic framework while event detection is treated as a graph theoretic clustering problem. The primary objective of this work is to detect and learn the complex interactions of the multiple agents performing multiple actions in the form of domain events, without prior knowledge about the number of agents involved in the interaction and length of the event.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Another objective is to present a coherent representation of these domain events, as a means to encode the relationships between agents and objects participating in a domain event. Formally, domain events are defined as a collection of actions performed by one or more agents. Also, we term these actions as video events , since they are directly measurable from the video (e.g. move, pick, enter, etc.). In this paper, events refer to domain events , and sub-events refer to video events, unless otherwise stated. Although CASE (Hakeem, Sheikh, Shah 2004) is an existing multiple agent event representation, the proposed method caters for three of its shortcomings.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Firstly, we automatically learn the domain event structure from training videos and encode the domain event ontology. This has a significant advantage, since the domain experts need not go through the tedious task of determining the structure of events by browsing all the videos in the domain.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Secondly, we recognize the importance of representing the variations in the temporal order of the sub-events occurring in a domain event and encode it directly into our representation. These variations in the temporal order of sub-events occur due to the style of execution of events for different agents.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">Finally, we present the concept of a video event graph (instead of event-tree) for event detection in videos. The reason for departing from the temporal event-tree representation of the video is that it fails to detect events when there are interfering sub-events from an independent agent, present in the tree structure of the novel video, which were not present in the actual event tree structure. Also, it fails to represent the complete temporal order between sub-events, which can easily be represented by video event graphs.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-05f6d364-7fff-fe8e-ef34-670bc7255e8f\">For learning the domain events from training videos, firstly, we introduce the notion of video event graph, which is a Directed Acyclic Graph(DAG) for representing the temporal relationship of sub-events in a video. In the video event graph each vertex represents a sub-event and each directed edge provides the temporal relationship between two sub-events. These temporal relationships are based on the interval algebra in (Allen and Ferguson 1994), which is a more descriptive model of relationships compared to the low level abstract relationship model of HMMs. Secondly, using the video event graph, we determine the event correlation.<\/span><\/p>",
                "DataExportTag": "AI1037345",
                "QuestionID": "QID266",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Multiple Agent Event Detection and Representation in Videos\" \u00a0 We propose a novel method to dete...",
                "Choices": {
                    "1": {
                        "Display": "\"Operational Management and Automated Manufacturing Planning\": management, planning, plan, decision_making, manufacturing, workflow, automation, production, business, planner, support, automate, military, operational, framework"
                    },
                    "2": {
                        "Display": "\"Healthcare Expert System and Patient Diagnosis\": expert, diagnosis, healthcare, fault, acquisition, es, inference, reasoning, maintenance, kbs, design, inference_engine, patient, expertise, user"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID94",
            "SecondaryAttribute": "\"Natural and Synthetic Retinoids in Prostate Cancer\" Prostate cancer (PCa) is the most common can...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Natural and Synthetic Retinoids in Prostate Cancer\"<\/strong><br>\n<br>\nProstate cancer (PCa) is the most common cancer for men in Europe, North America, and some parts of Africa. Initially, the growth of prostate cancer is usually androgen-dependent, but often it becomes androgen-independent after androgen-deprivation therapy. Managing hormone-refractory prostate carcinoma remains a difficult challenge for clinicians.<br>\n<br>\nRetinoids, vitamin A and its synthetic analogs, are one of the most studied classes of chemopreventive drugs for PCa. Retinoids play a key role in several vital functions such as vision and development, and also exert anti-proliferative actions. Anti-proliferative effects of retinoids rely on the regulation of many biological processes, including differentiation, cell proliferation, and apoptosis.<br>\n<br>\nRetinoid actions are mediated by two classes of nuclear proteins called retinoic acid (RARalpha, beta, and gamma) and retinoic alpha, beta, and gamma receptors, which are ligand-regulated transcription factors. Effects of both all-trans-retinoic acid (RA), the natural active derivative of vitamin A, and its synthetic derivatives, on the prostate gland or prostate cell lines implicate retinoids in the regulation of prostate growth and suppression of PCa development.<br>\n<br>\nDeficient retinoid availability and action at the cellular level because of either decreased content or altered metabolism in PCa cells can play a key role in abnormal cellular differentiation pathways and the loss of anti-proliferative effects.<br>\n<br>\nHere we review the in vitro and in vivo effects of retinoids in PCa.",
                "DataExportTag": "CAN873016",
                "QuestionID": "QID94",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Natural and Synthetic Retinoids in Prostate Cancer\" Prostate cancer (PCa) is the most common can...",
                "Choices": {
                    "1": {
                        "Display": "\"Cancer Treatment and Histone Deacetylase Inhibitors Research\": histone_deacetylase, bortezomib, atra_treatment, hdac_inhibitor, hdac, egcg, proteasome, proteasome_inhibitor, hdaci, vpa, retinoid, saha, tsa, myeloma, sfn"
                    },
                    "2": {
                        "Display": "\"Radiation Therapy and Cancer Treatment Research\": nf_kappab, curcumin, deoxyribonucleic_acid, radiotherapy, histone_deacetylase, nuclear, interventional_radiology, irradiation, damage, hdac_inhibitor, ppar\u03b3, saha, ionize_radiation, radiosensitivity, tumor_necrosis_factor"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID156",
            "SecondaryAttribute": "\"Online Learning: Outcomes and Satisfaction among Underprepared Students in an Upper-Level Psycho...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Online Learning: Outcomes and Satisfaction among Underprepared Students in an Upper-Level Psychology Course\"<\/strong><br>\n<br>\nOnline learning is on the rise, but research on outcomes and student satisfaction has produced conflicting results, and systematic, targeted research on underprepared college students is generally lacking. This study compared three sections (traditional, online, and 50% hybrid) of the same upper-level psychology course, taught with identical materials by the same instructor. Although exam scores were marginally higher in the traditional course, final grades and written assignments did not differ across sections, nor did student satisfaction. Student engagement predicted outcomes online. Taken together, these results suggest that outcomes and satisfaction are equivalent in online, hybrid, and traditional courses, and that a student's own diligence and drive might better predict success in online learning.<br>\n<br>\nIn the past decade, the educational system has witnessed tremendous growth in online learning. An increasing number of courses at American colleges and universities are offered in completely online or hybrid (part online) format, and in fact, students can earn nearly every undergraduate and graduate degree, somewhere in the country, without ever setting foot on a college campus. At the university where the authors of this study teach, the number of courses offered in either online or hybrid format has doubled since 2008 (from 3.59% to 7.68%), and the number of students taking at least one online or hybrid course in a semester has tripled in the same time frame (from 10.29% to 30.54%). A recent report issued by the Babson Survey Research Group and Quahog Research Group, LLC (Allen &amp; Seaman, 2013) suggests that, while the growth rate in online enrollment has slowed slightly over the past two years, as many as 32% of college students are enrolled in online courses. In the United States in 2011, nearly 7 million postsecondary students took at least one online course (Allen &amp; Seaman, 2013). Even in the K-12 sector, online charter schools continue to emerge and market themselves, often quite successfully, to a population that enjoys the convenience that online education has to offer.<br>\n<br>\nThe real question for consumers and educators is whether the quality of online learning is comparable to that offered in a traditional face-to-face classroom setting. Additionally, given a recent report by The College Board that nearly 43% of SAT test takers are not college-ready (The College Board, 2013), it is important to know how underprepared students perform in online-learning platforms, particularly in the more challenging, upper-level courses.<br>\n<br>\nResearch on online course outcomes, which has focused primarily on exam scores and final grades, has produced conflicting results. Comparing online to traditional (in class, face-to-face) courses, equivalent exam performance has been reported by many researchers (e.g., Elvers, Polzella &amp; Graetz, 2003; Hemmati &amp; Omrani, 2013; Hollister &amp; Berenson, 2009; Jensen, 2011; McGready &amp; Brookmeyer, 2013; Stowell &amp; Bennett, 2010; Summers, Eaigandt, &amp; Whittaker, 2005). In fact, the phenomenon of \"no significant difference\" has been well documented by Russell (2013). Mosalanejad and colleagues found that among first-year nursing students, although there was no difference in a practical (applied) exam, online students outperformed traditional students on a theoretical exam (Mosalanejad, Shahsavari, Sobhanian, &amp; Dastpak, 2012). Their findings suggest that while questions tapping rote memorization may be \"easier\" online (perhaps due to the ability to look up the answers), deep learning does not vary depending on the delivery method.<br>\n<br>\nSome studies, however, have produced evidence of differences in online and traditional testing results, typically favoring courses offering a traditional setting to some degree. Waschull (2001) found a trend toward a higher final exam score in traditional versus online students. Ashby, Sadera and McNary (2011) found the highest exam scores in the traditional class, followed by online, then hybrid. Terry (2007) reported that both traditional and hybrid exams were higher than online and Fillion, Limayem, Laferriere, and Mantha (2009) likewise reported that hybrid students outperformed online ones. In contrast, Lim, Kim, Chen, and Ryder (2008) reported higher exam scores in both online and hybrid courses, compared to traditional. Taking the findings on exam scores as a whole, the picture becomes very muddied, with research demonstrating every possible combination of findings.<br>\n<br>\nFinal grades are another academic outcome that has received attention, albeit less so, in the online learning research. Here the results are a little more clear. While some studies have reported no significant differences in final grades (Akyol &amp; Garrison, 2010; Kirtman, 2009), the research demonstrating group differences favors the traditional setting. Students taking traditional courses were more likely to pass (Jaggers, Edgecombe &amp; Stacey, 2013; Waschull, 2001) and complete courses (Ashby, Sadera, &amp; McNary, 2011; Terry, 2007, although see Waschull, 2001) compared to those students taking hybrid or online versions of the same course.<br>\n<br>\nThree factors could possibly conflate these results. First, perhaps the results can simply be attributed to the different demographics of the online students, as typically, the students self-selected into the traditional, hybrid, or online section. Online learning offers a flexibility that allows nontraditional degree seekers to attend college courses. Therefore it is not surprising that in higher education, students who choose to take courses online tend to be older than the traditional college student, employed full-time, and have children at home. They are also more likely to be white, from a higher socioeconomic status, and English-speaking (Ashby, Sadera, &amp; McNary, 2011; Jaggers, Edgecombe, &amp; Stacey, 2013). Edmonds (2006) found that traditional students received higher exam scores than online students, after controlling for SATs and High School GPA, but the other demographic variables have been largely unstudied. Within individual studies some researchers have reported no significant differences in their online vs. traditional samples (e.g., Waschull, 2001), but this may be attributed to a small sample size. More research on the interplay of demographics is needed.<br>\n<br>\nSecond, and of great concern to educators and colleges, is the possibility of cheating online. Hollister and Berenson (2009) conducted a thorough analysis to ascertain whether online students' test scores could be attributed to cheating, but found no evidence of cheating online. Further, the studies reviewed here do not show that online students overwhelmingly outperform traditional students on exams; on the contrary, most of the research finds that exam scores are either equivalent, or traditional students do better. These results imply that educators need not be too concerned about cheating online, but it is still an issue of concern, particularly among online-learning critics.<br>\n<br>\nThird, the format of an online course typically requires the student to be disciplined and self-motivated. Failure to access the online course regularly, coupled with procrastination, can easily result in poor outcomes. Elvers, Pozella, and Graetz (2003) found that in an online course (but not a traditional one), procrastination led to lower exam scores. Similarly, DeNeui and Dodge (2006) found a small but significant correlation between the amount of Blackboard Vista usage (an online course delivery system) and the students' final grades. This issue is germane to the topic of underprepared students, as they may not have learned healthy study habits that would allow them to succeed in a self-paced course.<br>\n<br>\nTaken together, these three factors may account for some of the contradictions in research findings. Another important outcome to consider is the students' level of satisfaction with the course. Some aspects of online learning may be perceived as extremely advantageous to students. For example, students who are afraid to raise their hands in front of a room full of their peers may be much more comfortable voicing their opinions on a web-based discussion board. In contrast, online lectures often fail to maintain student attention the same way that classroom-based lectures do, and some students are partial to the personal interaction afforded by traditional classes.<br>\n<br>\nThe importance of student satisfaction is not to be underestimated. In a climate of extreme market competition, colleges and universities need to be on top of student attrition, and faculty members are similarly concerned with their course evaluations for the purposes of promotion and tenure. As with the academic course outcomes, satisfaction outcomes have produced very conflicting results. While some studies have reported increased satisfaction in hybrid and online courses (Hemmati &amp; Omrani, 2013; Lim et al., 2008), others have demonstrated the opposite pattern (Summers, Waigandt, &amp; Whittaker, 2005; Terry, 2007). Gecer and Dag (2012), and Kirtman (2009), along with Yudko, Hirokawa and Chi (2008) found that online and hybrid courses received positive ratings overall, and Beqiri, Chase and Bishka (2010) found that online courses were most preferred by males, graduate students, married students, and commuters. However, Waschull (2001) found no difference in satisfaction between traditional and online courses. The satisfaction findings, unclear as they are, may also be attributed to extraneous factors.",
                "DataExportTag": "AI365371",
                "QuestionID": "QID156",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Online Learning: Outcomes and Satisfaction among Underprepared Students in an Upper-Level Psycho...",
                "Choices": {
                    "1": {
                        "Display": "\"Online Community Management and Hate Speech Detection\": social, social_medium, hate_speech, user, post, comment, facebook, forum, community, crowdsource, online, crowd, social_media, crowdsourcing, security_administration"
                    },
                    "2": {
                        "Display": "\"Chatbot Development and User Interaction\": chatbot, app, user, conversation, personality, developer, mobile, agent, human, dialogue, response, interface, answer, bug, student"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID152",
            "SecondaryAttribute": "\"Possible Involvement Of Proline-rich Tyrosine Kinase 2 (pyk2) In The Pathogenesis Of Kawasaki Di...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Possible Involvement Of Proline-rich Tyrosine Kinase 2 (pyk2) In The Pathogenesis Of Kawasaki Disease&quot;<\/strong><br \/>\n<br \/>\n<strong>Introduction:&nbsp;<\/strong>Although etiology of Kawasaki disease (KD) remains elusive, a line of recent experimental studies implies that some kinds of infectious stimuli are implicate in the vasculitis through uncontrolled innate immune systems such as pattern recognition receptor (PPR)-mediated inflammatory signaling. It has already known that Candida albicans water-soluble fraction (CAWS) inducing KD-like vasculitis in mice function through PRP. Furthermore, it is reported that proline-rich tyrosine kinase (Pyk2), which is molecule involved in the PRPs-dependent signaling pathways, plays an important role in activation of NF-&kappa;B. Therefore, we investigated a possible relevance of Pyk2 in the pathogenesis of KD. Methods: Pyk2-knock out (Pyk2-KO) and wild-type C57BL\/6 mice (WT) were administered CAWS to induce KD-like vasculitis. Extension of the experimental vasculitis was immunohistochemically determined with anti-MPO antibody. CAWS-stimulated NF-&kappa;B activation was evaluated by quantifying nuclear translocation of NF-&kappa;B p65 subunit in peritoneal macrophages isolated from PYK2-KO and wild-type mice in vitro. Cytokines and chemokines across each mice were compared by cytokine array.<br \/>\n<br \/>\n<strong>Results: <\/strong>Pyk2-KO mice didn&rsquo;t show any apparent defective phenotype. While marked inflammation was observed in the aortic root of CAWS-treated WT mice, such vasculitis was barely detected in CAWS-treated Pyk2-KO mice. CAWS-induced NF-&kappa;B activation was also less observed in macrophages from Pyk2-KO mice. There were differences in some cytokines and chemokines production between mice.<br \/>\n<br \/>\n<strong>Conclusion:<\/strong> We speculate that Pyk2 is involved in the pathogenesis of KD. Pyk2 might be a potential therapeutic target for KD.",
                "DataExportTag": "AI623850",
                "QuestionID": "QID152",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Possible Involvement Of Proline-rich Tyrosine Kinase 2 (pyk2) In The Pathogenesis Of Kawasaki Di...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunology and Infection Response\": receptor, cell, activation, innate_immune, macrophage, cytokine, expression, toll_receptor, immune, mouse, infection, inflammation, immune_response, tlrs, signal"
                    },
                    "2": {
                        "Display": "\"Genomic Analysis and Disease Prediction\": cell, gene, image, variant, prediction, phenotype, snp, tissue, deep_learning, gene_expression, omic, genomic, disease, analysis, mutation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID90",
            "SecondaryAttribute": "\"Progressing a Non-Antibiotic Antimicrobial Treatment for Bovine Mastitis Towards Market - PanaMa...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Progressing a Non-Antibiotic Antimicrobial Treatment for Bovine Mastitis Towards Market - PanaMast&quot;<\/strong><br \/>\n<br \/>\nThe world is facing an antibiotics crisis, with resistant bacteria rendering current treatments ineffective. Novel, effective solutions are urgently required for this $40 billion global market. Westway Health (WWH) is based in Galway, Ireland, and is on track to become a leading veterinary antimicrobials company in the EU. It has developed an innovative, non-antibiotic technology effective at killing all bacteria, including antibiotic-resistant bacteria like MRSA, without inducing resistance.<br \/>\n<br \/>\nWWH&#39;s lead product in development is PanaMastTM LC, a breakthrough treatment for bovine mastitis, the most economically important infectious disease affecting the dairy industry. PanaMastTM LC will represent a step change in the way that dairy farmers manage mastitis, which affects 30% of the herd annually. It will:<br \/>\n<br \/>\n1. Be the world&#39;s first non-antibiotic medicine for mastitis, offering superior clinical outcomes and significant financial gains to end-users (&euro;150\/case).<br \/>\n2. Reduce wastage of milk and culling of cows.<br \/>\n3. Improve profitability and environmental performance on European farms and dairy processing sites.<br \/>\n<br \/>\nThe innovation directly addresses the major global problem of antibiotic resistance while providing a sustainable animal health solution that supports the production of safe and high-quality food. The Phase 1 PanaMast project was successfully completed ahead of schedule and on budget. It allowed WWH to:<br \/>\n<br \/>\n1. Develop a regulatory roadmap detailing all steps required to progress from the current stage of development (TRL7) to the first sale of a commercial product (TRL8).<br \/>\n2. Develop a robust commercial exploitation plan for PanaMastTM LC in EU and global markets.<br \/>\n3. Establish key strategic relationships to assist the commercialization of PanaMastTM LC.<br \/>\n<br \/>\nThe Phase 2 activities (pre-commercialization\/product development\/product launch and commercialization) are designed to ensure the market launch and sale of PanaMastTM LC in the EU at TRL8 by 2020.",
                "DataExportTag": "COR28244",
                "QuestionID": "QID90",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Progressing a Non-Antibiotic Antimicrobial Treatment for Bovine Mastitis Towards Market - PanaMa...",
                "Choices": {
                    "1": {
                        "Display": "\"Infectious Disease Control and Vaccine Research\": vaccine, infection, antibiotic, tuberculosis, pathogen, bacterial, infectious_disease, vaccination, antigen, antimicrobial, virus, outbreak, antibiotic_resistance, antimicrobial_resistance, malaria"
                    },
                    "2": {
                        "Display": "\"Public Health and Preventive Medicine\": health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID399",
            "SecondaryAttribute": "\"Q-Functionals For Value-Based Continuous Control\" We present Q-functionals, an alternative archi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Q-Functionals For Value-Based Continuous Control&quot;<\/strong><br \/>\n<br \/>\nWe present Q-functionals, an alternative architecture for continuous control deep reinforcement learning. Instead of returning a single value for a state-action pair, our network transforms a state into a function that can be rapidly evaluated in parallel for many actions, allowing us to efficiently choose high-value actions through sampling. This contrasts with the typical architecture of off-policy continuous control, where a policy network is trained for the sole purpose of selecting actions from the Q-function. We represent our action-dependent Q-function as a weighted sum of basis functions (Fourier, Polynomial, etc) over the action space, where the weights are state-dependent and output by the Q-functional network. Fast sampling makes practical a variety of techniques that require Monte-Carlo integration over Q-functions and enables action-selection strategies besides simple value-maximization. We characterize our framework, describe various implementations of Q-functionals, and demonstrate strong performance on a suite of continuous control tasks.<br \/>\n<br \/>\n<strong>Introduction<\/strong><br \/>\n<br \/>\nThe ultimate product of a successful reinforcement learning system is a policy that performs well in interacting with the environment, as measured by expected cumulative discounted reward. A dominant paradigm in reinforcement learning is to derive policies from a Q-function (Watkins and Dayan 1992) that estimates the expected cumulative reward for taking a given action. When the number of actions available is finite, a strong policy can be derived by enumerating all action-values for a given state and choosing the one with the highest value. However, enumeration is impossible when there are large or infinite possible actions, for example when actions are drawn from a continuous vector space. This is the natural description of, for example, robotic locomotion and control; significant effort has therefore gone into alternative approaches for action-selection in these domains (Gu et al. 2016; Asadi et al. 2021; Lillicrap et al. 2015).<br \/>\n<br \/>\nA common framework for so-called continuous control problems is to train a separate policy network that selects actions according to some criteria of the Q-values (Fujimoto, van Hoof, and Meger 2018; Haarnoja et al. 2018a).<br \/>\n<br \/>\n<strong>Background<\/strong><br \/>\n<br \/>\nThis work examines sequential decision making problems represented as Markov Decision Processes (MDPs) denoted by &lang;S, A, T, R, P0, &gamma;&rang;, where S, A, and &gamma; are the state space, action space, and the discount factor, respectively (Sutton and Barto 2018). The transition and reward functions are given by T (s, a) and R(s, a) respectively, and P0(s) is the initial state distribution. We seek to learn an action-selection strategy, or policy &pi;: S &rarr; A, resulting in high cumulative discounted reward. For a given policy &pi;, the state-action value function, or Q-function, is defined as:<br \/>\n<br \/>\nQ(st, at) = E&pi;[R(st, at) + &gamma;V (st+1)],<br \/>\n<br \/>\nwhere V (s) is the expected value of following policy &pi; from state s:<br \/>\n<br \/>\nV (s) = Ea&sim;&pi;(s)[Q(s, a)].<br \/>\n<br \/>\nWe also investigate &tau; -Entropy-Regularized RL (ER-RL), in which case the value of a state is regularized by the entropy of the current policy (Schulman, Chen, and Abbeel 2017; Haarnoja et al. 2018b):<br \/>\n<br \/>\nV &pi; ENT(s) = Ea&sim;&pi;(s)[Q(s, a)&minus; &tau; log(&pi;(a|s))],<br \/>\n<br \/>\nwhere &tau; controls the degree of regularization. When we are interested in maximizing cumulative reward, we can derive a policy from a Q-function by choosing the maximum-valued action at every state:<br \/>\n<br \/>\n&pi;(s) = argmax a Q(s, a).<br \/>\n<br \/>\nA common method for iteratively improving a Q function parameterized by &theta; is through bootstrapping (Sutton 1988):<br \/>\n<br \/>\n&theta; &larr; &theta; + &alpha;&delta;\u2206&theta;Q\u0302(s, a; &theta;)<br \/>\n<br \/>\nwhere &delta; = r + &gamma;V &pi;(s&prime;)&minus; Q\u0302(s, a; &theta;), (1) where &alpha; is the step-size and (s, a, r, s&prime;) tuples are drawn from experience. The fixed point of this update equation is Q&lowast;, the Q-function that describes the optimal policy &pi;&lowast;:<br \/>\n<br \/>\nQ(st, at) = E&pi;&lowast; [R(st, at) + &gamma;V (st+1)].<br \/>\n<br \/>\nCentral to this process is deriving a policy from a Q-function through maximization, or related techniques. When A is a set of discrete actions, this maximization can be done easily by comparing the Q-value for each action (Mnih et al. 2015). Alternatively, this work focuses on the problem of continuous control, where actions are drawn from a continuous vector space.<br \/>\n<br \/>\n<strong>Related Work<\/strong><br \/>\n<br \/>\nContinuous-control is concerned with domains that have continuous action spaces; thus, instead of choosing from a finite set of actions, an agent must output a vector of continuous entries at every timestep. This difference necessitates characteristically different agent architectures from discrete-action agents such as DQN. Policy-gradient (PG) methods are a dominant paradigm of continuous control due to their expressive policies and end-to-end training scheme (Sutton et al. 1999). Modern PG methods represent policies as deep neural networks, that are trained to output high-valued actions using a gradient signal produced by the Q-function. Deterministic PG methods output a single action, while stochastic PG methods generally parameterize a simple distribution (such as Gaussian) that can be sampled during action-selection (Haarnoja et al. 2018a).<br \/>\n<br \/>\nThe output of a &ldquo;perfectly&rdquo; trained policy network would maximize the value (as calculated by the Q-function) at any given state. The chief observation motivating our work is that throughout training, this is far from the case. Standard PG methods require a single evaluation of the policy network to produce an action, and a single evaluation of the Q-network to compute the value of that action. A fruitful line of research involves incorporating sampling methods, which compute the value of multiple actions, for either improved action-selection or better value-estimation. Prior work utilizes the cross-entropy method (CEM), a zero-order sample-based optimization method, to choose actions that have higher value than the original action output by the policy network (Simmons-Edler et al. 2019). The Expected Policy Gradient (EPG) framework reduces variance in the value computation through Monte Carlo integration over the policy&rsquo;s outputs (Ciosek and Whiteson 2018). Evaluating actions from a distribution around the policy&rsquo;s output has been found to reduce overfitting of a policy network to a Q-function (Fujimoto, van Hoof, and Meger 2018).<br \/>\n<br \/>\nAll of these methods, however, are limited to evaluating small numbers of action-values in practice because every computed value requires an additional evaluation of the Q-function&rsquo;s neural network. Our method does away with the policy network altogether, putting it in the class of value-function-only continuous control. We list other examples from this class for completeness. &ldquo;Continuous Action Q-Learning&rdquo; (Mill&aacute;n, Posenato, and Dedieu 2002) uses mixed-integer programming to solve for maximal valued actions (though in practice runtime considerations stop this at approximately maximal) in piecewise-linear value functions (such as neural networks with rectified linear.",
                "DataExportTag": "AI1067313",
                "QuestionID": "QID399",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Q-Functionals For Value-Based Continuous Control\" We present Q-functionals, an alternative archi...",
                "Choices": {
                    "1": {
                        "Display": "\"Reinforcement Learning and Decision Making\": reinforcement_learning, policy, reward, action, agent, deep_reinforcement_learning, deep_learning, exploration, environment, mdp, actor_critic, decision_making, imitation, markov, dqn"
                    },
                    "3": {
                        "Display": "\"Wireless Sensor Network Optimization and Routing Protocols\": routing, wireless_sensor_network, energy, route, packet, clustering, routing_protocol, protocol, transmission, lifetime, wireless, genetic_algorithm, traffic, sensor, mobile"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "3"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "NumColumns": 2,
                    "LabelPosition": "BELOW"
                },
                "NextChoiceId": 5,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID92",
            "SecondaryAttribute": "\"Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening\" Cont...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening&quot;<br \/>\n<br \/>\nContext: <\/strong>Repeated digital mammography examinations expose women to ionizing radiation that can increase breast cancer risk.<br \/>\n<br \/>\n<strong>Contribution:<\/strong> This modeling study found that annual mammography screening of 100,000 women aged 40 to 74 years might induce 125 breast cancer cases and 16 deaths but avert 968 breast cancer deaths because of early detection. Factors associated with increased risk for radiation-induced cancer included large breasts requiring extra views, higher-than-average doses per view, beginning screening at younger ages, and annual screening.<br \/>\n<br \/>\n<strong>Caution: <\/strong>The model had several assumptions.<br \/>\n<br \/>\n<strong>Implication: <\/strong>Biennial mammography screening starting at age 50 years and use of the fewest number of views possible would decrease the risk for radiation-induced breast cancer. Exposure to ionizing radiation from repeated mammography examinations may increase breast cancer risk (1, 2). Radiation-induced breast cancer incidence and mortality associated with recommended screening strategies are suggested to be low relative to breast cancer deaths prevented (35). However, prior projected population risks were based on exposure from screening only and assumed only 4 standard views per screening examination at the mean radiation dose. Evaluations of screening programs should consider full episodes of care, including diagnostic work-up prompted by an abnormal screening result (6). False-positive recalls, breast biopsies, and short-interval follow-up examinations are relatively common in the United States and add radiation exposure from diagnostic mammography (7). Some subgroups of women, such as obese women and those with dense breasts, are more likely to have additional evaluations (79), which may increase their risk for radiation-induced cancer. When the risk for radiation-induced breast cancer is being evaluated, it may also be important to consider variation in radiation dose from a single examination. Examinations vary in the number of views performed and dose per view; therefore, some women receive more than the mean dose. The American College of Radiology Imaging Network DMIST (Digital Mammographic Imaging Screening Trial) found an average radiation dose of 1.86 mGy to the breast from a single digital mammography screening view (10), but dose per view varied from 0.15 to 13.4 mGy (Supplement), and 21% of digital screening examinations used more than 4 views (10). Radiation dose is strongly correlated with compressed breast thickness; thus, women with large breasts tend to receive greater doses per view and may require more than 4 views for complete examination (10, 11). Women with breast augmentation receive implant-displacement views in addition to standard screening views, which doubles their radiation dose (12). Women may have repeated views because of movement artifacts or improper breast positioning.<br \/>\n<br \/>\n<strong>Methods<\/strong><br \/>\n<br \/>\n<strong>Screening Strategies:<\/strong> We used 2 complementary stochastic modeling approaches to evaluate the following 8 strategies for screening with digital mammography: annual screening from age 40 to 74, 45 to 74, or 50 to 74 years; biennial screening from age 40 to 74, 45 to 74, or 50 to 74 years; or a hybrid strategy of annual screening from age 40 to 49 or 45 to 49 years followed by biennial screening from age 50 to 74 years. We included the hybrid strategies because more frequent screening has been advocated for younger and premenopausal women due to their greater prevalence of dense breasts and more aggressive tumors, resulting in a greater risk for interval cancer, than older women (1417). Outcomes were breast cancer deaths averted (benefits) and radiation-induced breast cancer incidence and mortality (harms) associated with a lifetime of mammography screening relative to no screening.<br \/>\n<br \/>\n<strong>Simulation-Modeling Approaches:&nbsp; <\/strong>We used 2 complementary stochastic modeling approaches to simulate mammography events associated with radiation exposure and outcomes for a population adherent with each of the 8 screening strategies. The first approach used the Microsimulation of Screening AnalysisFatal Diameter (MISCAN-Fadia) model (18), which is a detailed natural history model of breast cancer. This approach provided estimates of breast cancer incidence and mortality with and without screening to contextualize estimates of radiation-induced breast cancer cases. Although MISCAN-Fadia models the average effects of screening on a population level, it does not model correlation among repeated mammography results in individual women or the specific types of work-up after an abnormal screening result; thus, it cannot be used to estimate the distribution of cumulative radiation exposure from both screening mammography and subsequent diagnostic work-up among women. Therefore, we developed a new simulation model that provides woman-level exposure histories that were not available from the MISCAN-Fadia model. This new model captures exposure heterogeneity by simulating mammography results and subsequent work-up in each woman and allowing for variability in radiation exposure and breast size.<br \/>\n<br \/>\nEstimates of the number of screening examinations and false-positive results from the MISCAN-Fadia model were combined with the mean radiation dose from the radiation exposure model to estimate mean incidence of radiation-induced breast cancer. Estimates of the probability distribution of cumulative radiation dose at each age among women from the radiation exposure model were used to estimate the probability distribution of radiation-induced breast cancer incidence. Radiation-induced breast cancer incidence was combined with breast cancer survival estimates from the MISCAN-Fadia model to estimate radiation-induced breast cancer mortality.<br \/>\n<br \/>\n<strong>MISCAN-Fadia Model: T<\/strong>he MISCAN-Fadia model simulates individual life histories of women with and without breast cancer in the presence and absence of screening from birth to death from breast cancer or other causes. The model has been described in detail elsewhere (18), information about the model can be found online (http:\/\/cisnet.cancer.gov), and inputs and assumptions are described in our report for the draft U.S. Preventive Services Task Force recommendations (19). In brief, on the basis of BCSC data on sensitivity of digital mammography screening, cancer detection rates, and cancer stage at detection, we estimated thresholds at which tumors become screen-detectable. Screening sensitivity and specificity depended on age, breast density, and screening interval. Breast cancer risk depended on age and breast density. The effect of screening on breast cancer natural history was assessed by modeling continuous tumor growth, in which tumors detected before they",
                "DataExportTag": "CAN759421",
                "QuestionID": "QID92",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening\" Cont...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunohistochemical Analysis in Cancer Research\": staining, stain, immunohistochemical, tumour, immunoreactivity, antibody, ihc, cytoplasmic, canine, nuclear, paraffin_embed, mast, immunostaine, formalin_fix, nucleus"
                    },
                    "2": {
                        "Display": "\"Skin Cancer and Dermoscopy Analysis\": melanoma, cutaneous, melanocytic, melanocyte, dermoscopic, nevi, dermoscopy, nevus, pigment, melanocytic_nevi, dermoscopic_feature, pigmented, pigmentation, blue_nevus, lentigo_maligna"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID199",
            "SecondaryAttribute": "\"Reproductive toxicants have a threshold of adversity\" This paper surveys the scientific basis fo...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Reproductive toxicants have a threshold of adversity&quot;<\/strong><br \/>\n<br \/>\nThis paper surveys the scientific basis for the current threshold approach for reproductive hazard and risk assessment. In some regulatory areas it was recently suggested to consider reproductive toxicants under the stringent linear extrapolation risk assessment paradigm that was developed for genotoxic carcinogens.<br \/>\n<br \/>\nFirst, the current risk assessment paradigm for genotoxic carcinogens is addressed, followed by an overview of reproductive toxicology and its threshold dose approach for hazard and risk assessment, the testing procedures for assessing the reproductive toxicity of chemicals, and the derivation of conclusions on their risk assessment and Classification, Labelling and Packaging (CLP).<br \/>\n<br \/>\nRelevant details of testing methodologies are discussed, such as exposure time windows, parameters determined, and the coverage of the entire reproductive cycle. In addition, the dose-response relationship is considered, illustrated with several examples.<br \/>\n<br \/>\nIt is concluded that the current risk assessment methodology for genotoxic carcinogens is a debatable worst-case scenario and that for risk assessment of reproductive toxicants, the threshold dose approach remains valid.",
                "DataExportTag": "CAN810728",
                "QuestionID": "QID199",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Reproductive toxicants have a threshold of adversity\" This paper surveys the scientific basis fo...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Discovery and Prevention Technologies\": challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "2": {
                        "Display": "\"Childhood Illness and Family Psychosocial Experience\": child, parent, survivor, family, pediatric, woman, experience, adolescent, qualitative, life, illness, childhood, interview, psychosocial, mother"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID78",
            "SecondaryAttribute": "\"Revealing the contribution of liver macrophage populations to NASH in insulin resistance\" Non-al...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Revealing the contribution of liver macrophage populations to NASH in insulin resistance&quot;<\/strong><br \/>\n<br \/>\nNon-alcoholic steatohepatitis (NASH), the most common chronic liver disease worldwide, is an unmet medical need with no approved therapies and debilitating consequences for patients. Obesity-associated insulin resistance is a high-risk factor for the development of NASH. The prevailing paradigm is a multiple-hit process, whereby lipid accumulation in the liver of obese patients leads to oxidative stress and increased production of inflammatory cytokines by macrophages. However, my research group&#39;s comprehensive investigations in mice and humans have revealed that liver macrophages (LMs) contribute to insulin resistance and oxidative stress independently of their inflammatory status. I thereby propose that LMs predispose insulin-resistant patients to NASH independently of their inflammatory status.<br \/>\n<br \/>\nIn this ambitious multidisciplinary project, we will use a novel platform encompassing multiple single-cell and in situ omics technologies tailored by my research group to characterize the phenotype of LM populations in healthy individuals and insulin-resistant patients with or without NASH. We will strengthen this approach with functional validation in animal models as well as human liver organoids using a patented technology that I have developed to specifically manipulate gene expression in macrophages. We will then decipher how hepatic insulin resistance creates a spatiotemporal environment facilitating NASH. My group&#39;s unique access to patient material combined with cutting-edge methodologies to reveal the phenotype of single LMs provides an exceptional starting point from which to identify genes and pathways involved in the development of NASH in obese insulin-resistant patients. This project will set the stage for a paradigm-shift in studying and treating life-threatening liver diseases.",
                "DataExportTag": "COR2821",
                "QuestionID": "QID78",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Revealing the contribution of liver macrophage populations to NASH in insulin resistance\" Non-al...",
                "Choices": {
                    "1": {
                        "Display": "\"Cardiovascular and Metabolic Diseases Research\": cardiac, cvd, diabetes, heart_failure, cardiovascular, heart, vascular, atrial_fibrillation, cardiovascular_disease, liver, kidney, hf, lung, cellular, diabete"
                    },
                    "2": {
                        "Display": "\"Public Health and Preventive Medicine\": health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID132",
            "SecondaryAttribute": "\"RNA Sequence Analysis Reveals Macroscopic Somatic Clonal Expansion Across Normal Tissues\" Somati...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"RNA Sequence Analysis Reveals Macroscopic Somatic Clonal Expansion Across Normal Tissues\"<\/strong><br>\n<br>\n<strong>Somatic Mosaicism in Normal Tissues<\/strong><br>\n<br>\nSomatic cells can accumulate mutations over the course of an individual's lifetime. This generates cells that differ genetically at specific loci within the genome. To explore how this genetic diversity in individuals contributes to disease, Yizhak et al. developed a method to detect mutations from RNA sequencing data (see the Perspective by Tomasetti). Applying this method to Cancer Genome Atlas samples and normal samples from the Genotype-Tissue Expression (GTEx) project generated a tissue-specific study of mutation accumulation. Somatic mutations were detected in nearly all individuals and across many normal human tissues in genomic regions called cancer hotspots and in genes that play a role in cancer. Interestingly, the skin, lung, and esophagus exhibited the most mutations, suggesting that the environment generates many human mutations.&nbsp;<br>\n<br>\n<strong>Introduction<\/strong><br>\n<br>\nCancer genome studies have contributed to the analysis and discovery of somatic mutations that drive cancer growth. However, studying the genetic makeup of a tumor when it is already fully developed limits our ability to uncover how and which somatic mutations accumulate in normal tissues in the stages preceding cancer initiation. To address this challenge, recent studies performed deep sequencing in a limited number of tissue types and a small number of individuals, identifying a large number of microscopic clones carrying somatic mutations, some in known cancer genes. These findings emphasize the need to uncover the genomic events that occur in all normal tissues. Although efforts have begun to collect and analyze DNA from normal tissues, we still lack a comprehensive catalog of genetic events and clonal properties across a large number of tissues and individuals. By analyzing the information-rich content in RNA now available from recent advances in RNA sequencing methods, we may be able to substantially expand the scope and scale of these studies.<br>\n<br>\n<strong>Rationale<\/strong><br>\n<br>\nSome mutations found in the DNA can be detected in the corresponding RNA, depending on the mutation allele fraction and sequence coverage. We therefore hypothesized that a careful analysis of RNA sequences from normal bulk tissues could uncover somatic mutations reflecting macroscopic clones within the samples. In this work, we used the large collection of RNA sequences from the Genotype\u2013Tissue Expression (GTEx) project, representing more than 6,700 samples from ~500 individuals, spanning across 29 different normal tissues.<br>\n<br>\n<strong>Results<\/strong><br>\n<br>\nWe developed a new method, called RNA-MuTect, to identify somatic mutations using a tissue-derived RNA sample and its matched-normal DNA. We validated RNA-MuTect on both tumor-adjacent and cancer samples from The Cancer Genome Atlas (TCGA), wherein DNA and RNA were coextracted from the same samples. Focusing on mutations contained within sufficiently covered sequences, RNA-MuTect achieved high sensitivity and precision, enabling the discovery of most driver events and mutational processes from TCGA tumor RNA data.<br>\n<br>\nWhen applied to the GTEx dataset of normal tissues, multiple somatic mutations were detected in almost all individuals and tissues studied here, including in known cancer genes. The three tissues with the largest number of somatic mutations were sun-exposed skin, esophagus mucosa, and lung; this finding suggests that environmental exposure can promote somatic mosaicism. Both the individuals\u2019 age and tissue-specific proliferation rate were found to be associated with the number of detected mutations. A dN\/dS (ratio of nonsynonymous to synonymous substitutions) analysis suggested that some of the mutations identified in cancer genes may confer a selective advantage. In addition, allelic imbalance events at the chromosome arm level were detected in normal tissues.<br>\n<br>\n<strong>Conclusion<\/strong><br>\n<br>\nGenetic clones carrying somatic mutations are detected across normal tissues to different extents, and these differences depend on factors such as the tissue\u2019s exposure to environmental mutagens, natural architecture, proliferation rate, and the microenvironment. Some of these clones may be the result of genetic drift. Others, however, may develop as a result of positive selection driven by certain somatic events, thus potentially representing the earliest stages of tumorigenesis. Higher-resolution studies of normal tissues and precancerous lesions are required if we are to advance our understanding of both aging and early cancer development.<br><br><b>Somatic Clonal Expansions in Normal Human Tissues<br><\/b>\n<br>\nRNA sequences from 29 normal human tissues collected as part of the Genotype\u2013Tissue Expression (GTEx) project are analyzed using RNA-MuTect, a method developed for detecting somatic mutations in RNA-seq data. Macroscopic clonal expansions, characterized by shared somatic mutations, are detected in all tissues; skin, esophagus, and lung have the largest number of somatic mutations. How somatic mutations accumulate in normal cells is poorly understood. A comprehensive analysis of RNA sequencing data from ~6,700 samples across 29 normal tissues revealed multiple somatic variants, demonstrating that macroscopic clones can be found in many normal tissues. We found that sun-exposed skin, esophagus, and lung have a higher mutation burden than other tested tissues, which suggests that environmental factors can promote somatic mosaicism. Mutation burden was associated with both age and tissue-specific cell proliferation rate, highlighting that mutations accumulate over both time and number of cell divisions. Finally, normal tissues were found to harbor mutations in known cancer genes and hotspots. This study provides a broad view of macroscopic clonal expansion in human tissues, thus serving as a foundation for associating clonal expansion with environmental factors, aging, and risk of disease.",
                "DataExportTag": "CAN1083771",
                "QuestionID": "QID132",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"RNA Sequence Analysis Reveals Macroscopic Somatic Clonal Expansion Across Normal Tissues\" Somati...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetic Sequencing and Genomic Diversity\": genome, gene, sequence, exon, deletion, somatic_mutation, mtdna, somatic, clone, sequencing, deoxyribonucleic_acid, diversity, transcript, splicing, region"
                    },
                    "2": {
                        "Display": "\"Genetic Disorders and Chromosomal Abnormalities in Acute Myelogenous Leukemia\": chromosome, deletion, loh, region, acute_myelogenous_leukemia, loss, cytogenetic, chromosomal, amplification, gene, translocation, fish, loss_heterozygosity, fusion, rearrangement"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID226",
            "SecondaryAttribute": "\"Signaturit\" \u00a0We want to change the way we sign contracts and other documents, and in doing so, t..",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Signaturit\"<\/strong><br>\n<br>\n&nbsp;We want to change the way we sign contracts and other documents, and in doing so, the whole process that accompanies them. Currently, when documents need signatures, we use the post\/courier to send them and then wait, sometimes days, to get the signed copies back. This is a manual process, slow, costly, prone to human error, with a negative impact on the environment and on the bottom line.<br>\n<br>\nResearch shows that companies are not ready to switch to e-signature solutions for 3 reasons: (a) lack of security, (b) usability, and (c) add minimal value. Our e-signature solution is:<br>\n<br>\n(a) very secure \u2013 we use a unique, proprietary, multi-authentication process, including biometric data, encrypting and sending material using a secure channel and logging every action,<br>\n(b) easy to use from any device \u2013 the user can sign with a finger, mouse or stylus and send the signed material via regular email, with no special hardware or software installation, and<br>\n(c) provides significant value added through Smart contracts that take out the inefficient administrative processes around contract implementation.<br>\n<br>\nOur solution will finally allow companies to stop using a manual process, reducing paper use, improving companies\u2019 competitiveness, and boosting the economy. Signaturit is the first e-signature service that guarantees 100% legal validity in business digital transactions, minimizes the risk of cybercrimes, and drastically improves productivity. During this project, we aim to add unique advanced biometric and blockchain encryption tools that ensure an unreachable level of cyber security in e-mail transactions, while Smart contracts will ensure that productivity is optimized.<br>\n<br>\nWe are Signaturit Solutions S.L., an SME that recently obtained the SME Instr. Phase 1 funding. After completion of the SME Inst. Phase 2 project, we aim to aggressively market our innovation to 5 European countries, with projections to gain an annual net profit of \u20ac17M by 2024 while directly creating 30 new jobs.",
                "DataExportTag": "COR36983",
                "QuestionID": "QID226",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Signaturit\" \u00a0We want to change the way we sign contracts and other documents, and in doing so, t...",
                "Choices": {
                    "1": {
                        "Display": "\"Nuclear Security and Digital Infrastructure Management\": nuclear, cybersecurity, critical_infrastructure, interoperable, blockchain, certification, marketplace, toolbox, federation, digital_twin, factory, security_privacy, experimentation, deploy, cross"
                    },
                    "2": {
                        "Display": "\"Mobile Banking and E-commerce\": mobile, blockchain, payment, personal, store, bank, transaction, insurance, biometric, smartphone, retailer, purchase, retail, pay, e_commerce"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID114",
            "SecondaryAttribute": "\"Smoking and Deaths between 40 and 70 Years of Age in Women and Men\" Context: The best way to est...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Smoking and Deaths between 40 and 70 Years of Age in Women and Men\"<\/strong><br>\n<br>\n<strong>Context<\/strong>: The best way to estimate deaths from smoking is to observe people over many decades, as done in a 50-year study of male British physicians. No such study exists for women.<br>\n<br>\n<strong>Content:<\/strong> A 25-year study of residents of rural Norway included 24,505 women. Of these women, 2,333 women died at age 40 to 70 years (9.4% of never smokers and 18.7% of continuing smokers), mostly from cardiovascular disease and cancer. Continuing heavy smokers survived 1.4 fewer years than never smokers. Mortality rates in smokers and nonsmokers were lower in women than in men.<br>\n<br>\n<strong>Cautions: <\/strong>Participants were white and lived in rural areas.<br>\n<br>\n<strong>Implications<\/strong>: Active smoking has a large effect on women's longevity in middle age.<br>\n<br>\n<strong>The Editors<\/strong><br>\nAn early report on smoking and longevity appeared more than 60 years ago (1), and it suggested a clear survival advantage of nonsmokers over heavy smokers in midlife. More than a half century later, another report presented precise survival curves for men by smoking habits (2). In the time between these reports, the historic dimension of the health consequences of smoking was increasingly realized (3-9). The World Health Organization now recognizes tobacco use as the major preventable cause of adult death, and about 5 million deaths worldwide each year (8.8% of all deaths annually) are attributed to smoking (10). More than half of smoking-attributable deaths worldwide (56%) occur in people younger than 70 years of age and account for 13% of deaths in people 30 to 69 years of age (9). On a global scale, the death toll from tobacco use is increasing (9, 11), and accurately quantifying and updating the number of premature deaths due to smoking is an important task.<br>\n<br>\nEstimated numbers of deaths caused by smoking on a national, international, or global level have been calculated by combining relative risks for smokers versus nonsmokers for different causes of death from the second large prospective American Cancer Society Cancer Prevention Study (CPS-II) (12, 13) with direct or indirect (via lung cancer mortality rates) estimates of smoking histories and national mortality statistics (6, 9, 14-17). However, estimating the number of smoking-attributable deaths that occur in a population in a given year depends on the maturity of the smoking epidemic and will predict the lifetime mortality experience of smoking men and women only under certain conditions (18). To predict individual risks precisely, follow-up studies of individuals through their lifetime are needed. Such studies are rare and have not always estimated direct long-term risk for death. Among studies of survival and smoking (1, 19, 20), the British doctors' study (2, 21) has provided the longest follow-up (50 years) and has allowed for direct and precise calculation of survival of men through middle age and beyond. Using data from nearly 50,000 Norwegian adults who were born in the second quarter of the 20th century and were followed in the last quarter, we studied smoking, death in middle age (40 to 70 years of age), and causes of death in both women and men.<br>\n<br><b>\nMethods<\/b><br>\n<br><b>Study Sample<\/b><br>\nBetween 1974 and 1978, all men and women 35 to 49 years of age who were residing in the 3 rural Norwegian counties of Oppland, Sogn og Fjordane, and Finnmark were invited to a cardiovascular health screening examination. Excluding 783 men and 215 women who were temporarily absent from their residence, 91.4% of the men and 94.2% of the women gave a self-report of their past and current smoking habits and were screened for cardiovascular risk factors (22, 23). Thus, our report is based on mortality follow-up between 40 and 70 years of age (middle age) of 24,505 women and 25,034 men (who were born between 1925 and 1941). Approximately 92% of the respondents attended a second survey, and 65% attended a third survey about 5 years and 10 years after the first examination, respectively.<br>\n<br>\nWe grouped participants into never smokers (no report of smoking at any examination); former smokers; or continuing smokers of 1 to 9 cigarettes, 10 to 19 cigarettes, or 20 or more cigarettes per day (heavy smokers). As a supplementary analysis to facilitate comparison with the 40-year follow-up of the British doctors' study (2), we also studied men who reported smoking 25 or more cigarettes daily. We classified persons as continuing smokers on the basis of the information from the last of up to 3 examinations. On the basis of information given about time since smoking cessation and changes among the 3 examinations, we separated the former smokers into 3 groups on the basis of their age when they stopped smoking (&lt;40 years, 40 to 49 years, or 50 to 59 years).<br>\n<br><b>Mortality Follow-up<br><\/b>\nWe performed mortality follow-up by record linkage using the Norwegian 11-digit birth number (date of birth plus a 5-digit identifier), which is unique to each person residing in Norway, to obtain the date and underlying cause of death kept by Statistics Norway. Loss to follow-up was due only to emigration, and we censored 93 men and 99 women (0.4%) on their registered date of taking residence abroad. Our report is based on mortality follow-up through the year 2000.<br>\n<br>\nWe classified the 7,013 deaths in middle age into 7 groups on the basis of the underlying cause of death coded at Statistics Norway by using the 8th, 9th, or 10th revisions of the International Classification of Diseases (ICD) and European shortlist categories (24). The groups were deaths due to 1) lung cancer (151 deaths in women and 316 deaths in men); 2) other types of smoking-related cancer, including cancer of the lip, oral cavity, pharynx, larynx, esophagus, stomach, liver, pancreas, cervix uteri, kidney, bladder, and acute myelogenous leukemia (262 deaths in women and 386 deaths in men); 3) other types of cancer (796 deaths in women and 648 deaths in men); 4) cardiovascular disease, including sudden death (624 deaths in women and 2,252 deaths in men); 5) other medical causes (358 deaths in women and 505 deaths in men); 6) alcohol abuse and chronic liver disease (20 deaths in women and 115 deaths in men); and 7) accidents and violence (122 deaths in women and 458 deaths in men). Types of smoking-related cancer were those reported to be associated with smoking in the 2004 U.S. Surgeon General report (25).<br>\n<br><b>Statistical Analysis<br><\/b>\nWe estimated Kaplan-Meier survival curves from 40 to 70 years of age separately for men and women, and for the various smoking categories, using age as the time scale. We performed computations with the survfit function of S-PLUS, version 6.1 (Insightful Corp., Seattle, Washington). The limited number of individuals who were recruited before 40 years of age led to unstable survival estimates. For this reason, we calibrated all survival analyses to start at 100% at age 40 years.<br>\n<br>\nWe estimated mortality hazard (rate) ratios comparing the persons in various smoking categories with never smokers by using the Cox proportional hazards model with age as the time scale and calculated with and without adjustment for potential confounding. The confounders were the following variables, which were all registered at the first examination: marital status (not married or married), duration of education (0 to 9 years, 10 years, 11 to 12 years, or 13 years), county of residence, and physical activity in leisure time (sedentary, moderate, intermediate, or intensive). We also used the Cox model to estimate women-men mortality hazard ratios overall and with the different cause-of-death groups as outcomes. We performed these analyses for never smokers and for continuing smokers with additional adjustment for age when smokers began smoking and for the number of cigarettes smoked per day.<br>\n<br>\nWe performed tests for difference in regression coefficients between men and women and between smokers and nonsmokers (interaction tests), with the variance estimated as the sum of each coefficient's variance. We calculated adjusted survival first by predicting survival probabilities for each individual from the Cox model with confounders and then by averaging over all individuals in the sample (26, 27). We did this separately for women and men. We computed the average number of years of life lost between 40 and 70 years of age by summing person-time lost over the survival curves.<br>\n<br>\nTo compute cause-specific mortality probabilities for the interval of 40 to 70 years of age, we analyzed the different cause-of-death groups in a competing risk framework. Again, we performed the computations separately for men and women and for the different smoking categories. The Nelson-Aalen estimator is used rather than the Kaplan-Meier since cause-specific Kaplan-Meier estimates do not correctly account for deaths from other causes.<br>\n<br>\nWe estimated the probability of dying between 40 and 70 years of age as a function of age when smoking began. We calculated these probabilities separately for men and women and for the 3 smoking categories (1 to 9 cigarettes per day, 10 to 19 cigarettes per day, and 20 cigarettes per day). We obtained the estimates by fitting a Cox model with a P-spline function (pspline function of S-PLUS) (29) to show the effect of age when smoking began. The P-spline allows for a flexible effect estimation, avoiding the need to assume a prespecified functional relationship between exposure and outcome, such as linear or polynomial (30). We then estimated the predicted survival from 40 to 70 years of age by using the S-PLUS survfit function.",
                "DataExportTag": "CAN1207323",
                "QuestionID": "QID114",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Smoking and Deaths between 40 and 70 Years of Age in Women and Men\" Context: The best way to est...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\": mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "2": {
                        "Display": "\"Breast and Ovarian Cancer Screening and Hormone Therapy\": breast, woman, screening, ovarian_cancer, mammography, hormone, estrogen_receptor, postmenopausal_woman, postmenopausal, hrt, estrogen, mammogram, invasive, family_history, ht"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID224",
            "SecondaryAttribute": "\"TEchnology TRAnsfer for GrOwth with twinNing\" The Technology Transfer Office Circle, launched by...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"TEchnology TRAnsfer for GrOwth with twinNing\"<\/strong><br>\n<br>\nThe Technology Transfer Office Circle, launched by the EC in 2011, analyzed the Technology Transfer (TT) in Europe, concluding that: \"It is anticipated that the next decade will see profound changes in this landscape. Studies have identified the lack of scale as one of the major issues of technology transfer.\" So, Technology Transfer is a prevailing topic. Although it's a concept as old as ancient Greece (Archimedes could be the first Technology Broker in history), the reality is that it is still needed to overcome barriers and face common challenges to improve the Technology Transfer process.<br>\n<br>\nTETRAGON will use the Twinning Advanced methodology to enhance SMEs' innovation capacity by providing them with better innovation support in Technology Transfer. In other words, it aims to offer better support to SMEs to improve TT from public research to the market.<br>\n<br>\nDue to the expertise of the consortium and the needs identified in their regions, this project will be focused on the following models of Technology Transfer: 1) Fostering an entrepreneurial environment at universities and research centers to increase the creation of spin-offs and improve the exploitation of technology by existing companies. 2) Fostering demand-driven collaborative projects between public researchers and private SMEs. 3) Exploring innovative ways of licensing technology, including open source, open innovation, and user innovation.<br>\n<br>\nThe project will have the support of the Netherlands Enterprise Agency (RVO) in the Twinning Advanced Methodology. TETRAGON aims to place TT at the core of innovation, improve TT measures, help SMEs grow, and consequently, foster the growth of the European economy.<br>\n<br>\nTETRAGON aims to reach almost 300 innovation agencies all over Europe, through networks such as EURADA. It also aims to have 20% of these agencies adopt at least one of the new measures defined in the DOP, while establishing a Twinning Advanced ecosystem where agencies collaborate.",
                "DataExportTag": "COR30967",
                "QuestionID": "QID224",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"TEchnology TRAnsfer for GrOwth with twinNing\" The Technology Transfer Office Circle, launched by...",
                "Choices": {
                    "1": {
                        "Display": "\"Political Science and Social Studies\": labour, ethnography, peace, military, legitimacy, indigenous, election, domestic, feminist, violent, ethnic, electoral, protest, victim, populism"
                    },
                    "2": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\": farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID180",
            "SecondaryAttribute": "\"The 1D lateral dose response functions of photon-dosimetry detectors in magnetic fields\u2014measurem.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"The 1D lateral dose response functions of photon-dosimetry detectors in magnetic fields\u2014measurement and Monte-Carlo simulation\"<\/strong><br>\n<br>\nThe present study is concerned with clinical photon-beam dosimetry at radiotherapy units combined with magnetic resonance imaging devices. Due to the superposed constant magnetic field, the deflections of the secondary electron trajectories by the Lorentz force not only influence the 2D dose distribution, D(x, y), in water phantoms, but furthermore modify the secondary electron transport within the detectors and thereby the detectors' signal profiles, M(x, y), across the photon beams. This second effect can be represented by the lateral dose response function, K(x, y), the convolution kernel transforming D(x, y) into M(x, y) via the convolution M(x, y) = D(x, y) * K(x, y).<br>\n<br>\nThe 1D functions K(x) of a set of commercial gas-filled and solid-state photon-beam detectors were experimentally determined using a slit beam geometry together with a constant, homogeneous magnetic field of up to 1.42 T. As predicted by a recent Monte-Carlo study (Looe et al Phys. Med. Biol. 62 5131\u201348), the functions K(x) of these detectors are shown experimentally to be distorted in a magnetic field. For the larger Semiflex 3D 31021 chamber, the FWHM value of K(x) decreases from 4.9 mm for the field-free (0 T) case to 4.8 mm in 0.35 T and 4.1 mm in 1.42 T magnetic field, whereas the FWHM value of the smaller PinPoint 3D 31022 chamber decreases from 2.8 mm for the field-free case to 2.6 mm in 1.42 T magnetic field. The FWHM values of the semiconductor detectors are not modified in magnetic fields. Additionally, the symmetry of K(x) is shown to be distorted in a magnetic field. Using a 10 mm wide field as an example, the signal profiles, M(x), predicted by the measured and simulated K(x) by convolution with D(x) (measured with EBT3 film) agree within 3% of the maximum value to the measured M(x) for all detectors, except for the silicon diode detector if the measured K(x) was used, where deviations of around 5% were observed at the field border.",
                "DataExportTag": "CAN628122",
                "QuestionID": "QID180",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"The 1D lateral dose response functions of photon-dosimetry detectors in magnetic fields\u2014measurem...",
                "Choices": {
                    "1": {
                        "Display": "\"Radiation Therapy and Dosimetry Planning\": radiotherapy, beam, planning, proton, imrt, irradiation, dosimetric, energy, dosimetry, intensity_modulate, photon, monte_carlo, absorb_dose, delivery, arc"
                    },
                    "2": {
                        "Display": "\"Medical Imaging and Cancer Treatment\": imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "LabelPosition": "SIDE"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID241",
            "SecondaryAttribute": "\"The 1D lateral dose response functions of photon-dosimetry detectors in magnetic fields\u2014measurem.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<b>\"The 1D lateral dose response functions of photon-dosimetry detectors in magnetic fields\u2014measurement and Monte-Carlo simulation\"<br><\/b>\n<br>\nThe present study is concerned with clinical photon-beam dosimetry at radiotherapy units combined with magnetic resonance imaging devices. Due to the superposed constant magnetic field, the deflections of the secondary electron trajectories by the Lorentz force not only influence the 2D dose distribution, D(x, y), in water phantoms but furthermore modify the secondary electron transport within the detectors and thereby the detectors\u2019 signal profiles, M(x, y), across the photon beams. This second effect can be represented by the lateral dose response function, K(x, y), the convolution kernel transforming D(x, y) into M(x, y) via the convolution M(x, y) = D(x, y) * K(x, y).<br>\n<br>\nThe 1D functions K(x) of a set of commercial gas-filled and solid-state photon-beam detectors were experimentally determined using a slit beam geometry together with a constant, homogeneous magnetic field of up to 1.42 T. As predicted by a recent Monte-Carlo study (Looe et al Phys. Med. Biol. 62, 5131\u201348), the functions K(x) of these detectors are shown experimentally to be distorted in a magnetic field. For the larger Semiflex 3D 31021 chamber, the FWHM value of K(x) decreases from 4.9 mm for the field-free (0 T) case to 4.8 mm in 0.35 T and 4.1 mm in 1.42 T magnetic field, whereas the FWHM value of the smaller PinPoint 3D 31022 chamber decreases from 2.8 mm for the field-free case to 2.6 mm in 1.42 T magnetic field. The FWHM values of the semiconductor detectors are not modified in magnetic fields. Additionally, the symmetry of K(x) is shown to be distorted in a magnetic field.<br>\n<br>\nUsing a 10 mm wide field as an example, the signal profiles, M(x), predicted by the measured and simulated K(x) by convolution with D(x) (measured with EBT3 film) agree within 3% of the maximum value to the measured M(x) for all detectors, except for the silicon diode detector if the measured K(x) was used, where deviations of around 5% were observed at the field border.",
                "DataExportTag": "CAN628122",
                "QuestionID": "QID241",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"The 1D lateral dose response functions of photon-dosimetry detectors in magnetic fields\u2014measurem...",
                "Choices": {
                    "1": {
                        "Display": "\"Radiation Therapy and Dosimetry Planning\": radiotherapy, beam, planning, proton, imrt, irradiation, dosimetric, energy, dosimetry, intensity_modulate, photon, monte_carlo, absorb_dose, delivery, arc"
                    },
                    "2": {
                        "Display": "\"Medical Imaging and Cancer Treatment\": imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID196",
            "SecondaryAttribute": "\"The Genetics of Colorectal Cancer\" Colorectal cancer is a common disease in both men and women....",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"The Genetics of Colorectal Cancer\"<\/strong><br>\n<br>\nColorectal cancer is a common disease in both men and women. Because 5% of persons (1 in 20 persons) will develop colorectal cancer, this disease is an important public health issue. Colon cancer is usually observed in one of three specific patterns: sporadic, inherited, or familial. Sporadic disease, with no familial or inherited predisposition, accounts for approximately 70% of colorectal cancer in the population. Sporadic colon cancer is common in persons older than 50 years of age, probably as a result of dietary and environmental factors as well as normal aging. Fewer than 10% of patients have an inherited predisposition to colon cancer. The inherited syndromes include those in which colonic polyps are a major manifestation of disease and those in which they are not. The polyposis syndromes are subdivided into familial adenomatous polyposis and the hamartomatous polyposis syndromes. The nonpolyposis predominant syndromes include hereditary nonpolyposis colorectal cancer (HNPCC) (Lynch syndrome I) and the cancer family syndrome (Lynch syndrome II). Although uncommon, these syndromes provide insight into the biology of all types of colorectal cancer. The third and least understood pattern of colon cancer development is known as familial colon cancer. In affected families, colon cancer develops too frequently to be considered sporadic colon cancer but not in a pattern consistent with an inherited syndrome. Up to 25% of all cases of colon cancer may fall into this category.<br>\n<br>\n<strong>Basic Genetics<\/strong><br>\nThe complement of DNA in the genetic code is a cell's guide to differentiation and proliferation. All cells of an organism have DNA that is virtually identical to the DNA found in the zygote. A mutation at or before this point of development is therefore termed a germline mutation. A mutation of the ova or sperm is transmitted from the parent as an inherited defect and is responsible for hereditary cancer. When a mutation occurs spontaneously in the sperm, ova, or zygote, the affected person's parents will not manifest a cancer syndrome. Successive generations, however, can inherit this de novo mutation because the abnormality can be passed on to progeny. More commonly, a spontaneous mutation occurs in a cell during the growth and development of a particular tissue or organ. This somatic mutation results in clonal proliferation of the cell containing the mutated genetic material. Sporadic colorectal cancer results from the accumulation of multiple somatic mutations in a cell (1). Genes commonly mutated in human cancer belong to one of three different classes: oncogenes, tumor suppressor genes, and mismatch repair genes (2, 3). Oncogenes are normal genes responsible for the stimulation of controlled cellular proliferation (4, 5) (Figure 1). When these genes are mutated, they result in uncontrolled proliferation and, ultimately, cancer. Tumor suppressor genes were first described in Knudson's study of the epidemiology of childhood retinoblastoma (6, 7). Knudson used the term antioncogene because the gene was thought to produce cancer in a recessive fashion at the cellular level, meaning that one normal gene was adequate to control cellular growth. Subsequently termed suppressor genes, these are normal genes whose function is lost when both copies (alleles) of the gene are inactivated (Figure 1). When a tumor suppressor gene is inherited as a germline mutation, only the mutation of the remaining normal allele is required for the gene's loss of function (Figure 2, top) (5, 6). When both copies of the gene are normal, two mutation events, or hits, are required before the gene loses function (Figure 2, bottom). This two-hit hypothesis explains why inherited disease usually manifests at an earlier age than sporadic disease, as well as the concept of suppressor genes producing cancer in a recessive fashion at the cellular level (7). Enzymes that monitor newly formed DNA and correct replication errors are called DNA mismatch repair (MMR) systems (Figure 1) (3). Defective MMR genes are associated with the so-called mutator phenotype. Cells with MMR mutations in both gene copies accumulate DNA errors throughout the genome, affecting growth regulatory genes, such as the transforming growth factor receptor (TGF-RII) gene (8). These genes are mutated in HNPCC. Recently, subtle genetic changes that do not affect protein structure have been recognized as a possible cause of familial colon cancer. These subtle changes, termed polymorphisms if they occur frequently in the population and do not affect protein structure, are slight variations of nucleotide base sequences in genes. They are more common in populations than the hereditary cancer gene mutations but do not usually result in clinical disease. One polymorphism, now thought to be a mutation because it ultimately results in an abnormal protein structure, is found on codon 1307 of the APC gene and is known as the I1307K APC mutation (9). This mutation is found in 6% of all Ashkenazi Jewish persons and in 28% of Ashkenazi Jewish persons with both a personal and family history of colon cancer (Figure 3) (5, 9).<br>\n<br>\n<strong>Molecular Genetics<\/strong><br>\nFearon and Vogelstein (1) have described the molecular basis for sporadic colon cancer as a multistep model of carcinogenesis. This model describes an accumulation of genetic events, each conferring a selective growth advantage to an affected colon cell. These changes ultimately result in uninhibited cell growth, proliferation, and clonal tumor development. The cumulative effect of these somatic mutations is the cause of sporadic colon cancer. Four main conclusions are drawn from the proposed model of sporadic colon cancer pathogenesis: 1) Colorectal cancer results from the mutational activation of oncogenes and the inactivation of tumor suppressor genes; 2) somatic mutations in at least four or five genes of a cell are required for malignant transformation; 3) the accumulation of multiple genetic mutations rather than the sequence of mutations determines the biological behavior of the tumor, although APC mutations usually occur early in the process and mutations of the p53 suppressor gene usually occur late in the process; and 4) features of the tumorigenic process of colon cancer are applicable to other solid tumors, such as breast and pancreatic cancer (10). The most commonly inherited colon cancer syndromes are familial adenomatous polyposis and HNPCC. Each of these syndromes is the result of a specific germline mutation. In familial adenomatous polyposis, the germline mutation is always the APC gene, a tumor suppressor gene. In HNPCC, one of the MMR genes is mutated, most commonly hMLH1 or hMSH2. Several of the hamartomatous polyp syndromes have recently been associated with germline mutations. One example is the PeutzJeghers syndrome, which results from an abnormality of the STK11 tumor suppressor gene (11). Familial colon cancer in Ashkenazi Jewish persons is probably the result of an I1307K APC germline mutation, although the relative risk for tumor is much lower<br>\n<br>\n&nbsp;in a person with this mutation than in a person with one of the germline mutations noted previously (Figure 3). Unlike most germline mutations, which cause protein structure abnormalities, the I1307K APC germline mutation causes a predisposition to sporadic mutations at distant sites of the gene (which then cause protein structure abnormalities) at a later stage of development.<br>\n<br>\n<strong>Specific Mutations Oncogenes<\/strong><br>\nThe oncogene ras on chromosome 12 codes for a binding protein that acts as a one-way switch for the transmission of extracellular growth signals to the nucleus and regulates cellular signal transduction. Post-translational modification of the ras protein by farnesylation is necessary for activation. Mutations of ras are detected in up to 50% of cases of sporadic colorectal cancer and in large polyps. Activation of ras leads to constitutive activity of the protein, which results in a continuous growth stimulus that can be the basis of carcinogenesis. Recognition of ras mutations may be helpful in screening and early diagnosis of colorectal cancer (12). The usefulness of a sensitive assay for the detection of ras mutations in the stool of patients with curable colorectal tumors has been studied (13). Therapeutic potential also exists because clinical trials of farnesyl transferase inhibitors, which specifically inhibit ras-mediated signal transduction, have been initiated in patients with colorectal cancer exhibiting ras mutations. The src oncogene was first identified in Rous sarcoma virus. It encodes for a transforming protein that directly modifies the cytoskeleton. Disruption of the cytoskeleton may be an early event in the process of malignant transformation and carcinogenesis (14, 15).",
                "DataExportTag": "CAN880118",
                "QuestionID": "QID196",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"The Genetics of Colorectal Cancer\" Colorectal cancer is a common disease in both men and women....",
                "Choices": {
                    "1": {
                        "Display": "\"Cellular Biology and Aging Research\": mitosis, stress, microtubule, homeostasis, aurora, aging, centrosome, membrane, cellular, clock, regulation, polarity, intracellular, dynamic, ion_channel"
                    },
                    "2": {
                        "Display": "\"Epigenetic Regulation and Gene Modification\": pten, chromatin, histone, regulation, stress, methylation, epigenetic, autophagy, metabolic, enzyme, gene, modification, translation, acetylation, control"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID154",
            "SecondaryAttribute": "\"TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT\" In the development of intelligent co...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">\"TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT\"<\/b><\/p><p dir=\"ltr\"><b><br><\/b><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">In the development of intelligent computer-aided design systems, three important issues need to be considered. These issues are: how to support the generation of product concepts using evolutionary computation techniques; how to use intelligent databases and constraint management systems for detailed exploration of product embodiment; and how to integrate rapid prototyping facilities for product evaluation. In this paper, we present a brief review of knowledge-based design and evolutionary design and discuss ways of integrating both in the development of a generative design system. Based on this review, we present the model and its applications of a generative design system utilizing a number of AI and evolutionary computation techniques. This generative design model is intended to provide a generic computational framework for the development of intelligent design support systems.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Knowledge-based design<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Existing computer-based design systems provide little support for the evolution of design solution concepts that satisfy all physical constraints. They rely on precise geometric information to specify the model of an artifact. They cannot support early stage design tasks because the geometry of an artifact is not sufficiently definite at the early stage. The design of complex products containing many components requires a number of alternative solutions to be explored, each being modified by making changes to key design parameters until a satisfactory design solution is found. The generation and exploration of design concepts based on incomplete design requirements and the handling of a multitude of variables and constraints embedded within 3D design objects is a challenging task for designers.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Four main strategies may characterize the current developments in knowledge-based design systems:<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- The intelligent CAD approach;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- The building block approach;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- The prototype approach; and<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- The constraint-based approach.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">The intelligent CAD approach extends the capability of a CAD system by employing heuristic knowledge on top of geometric models of the artifacts. An intelligent CAD system plays an active rather than a passive role in the design process, by incorporating a significant amount of design knowledge into the system. Functional modeling, for example, is an approach that attempts to build intelligent CAD systems through a mapping from a function space to a conceptual solution space using functional instead of geometric components and their interfaces. This approach uses abstract knowledge to represent the function and behavior of elementary design components, parts, and their causal relationships. The building block approach formulates the design process as a sequence of tasks that can be tackled individually by different classes of CAD tools and AI methods. Chandrasekaran classified these tasks into three categories:<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">1. Class 1 design tasks in which the components of the artifact being designed are unknown.<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">2. Class 2 design tasks in which the components of the artifact being designed are known, but the design plans, i.e., the methods of how to design, are unavailable in a compiled form.<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">3. Class 3 design tasks where ways of decomposing a design problem are already known, and for which compiled design plans are available for each stage of the design process.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">In the prototype design approach, the design process is divided into three different activities: prototype refinement, prototype adaptation, and prototype creation based on a library of design prototypes. Gero classified design as:<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Routine design where the functions, expected behaviors, and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables.<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements.<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Creative design, in which an entirely new design problem space is to be defined.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">In this approach, design tasks under class 3 are considered routine designs that can be conveniently supported by automated computer programs. The others are more difficult and require significant creative inputs from human designers. The prototype design approach divides the design process into three different activities: prototype refinement, prototype adaptation and prototype creation based on a library of design prototypes. Gero classified design as: routine design where the functions, expected behaviors, and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables; innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements; and creative design, in which an entirely new design problem space is to be defined. Prototype refinement involves instantiating the variables in a design prototype and determining their values. Prototype adaptation becomes necessary when a design prototype is found to be inadequate in some way, and needs to be adapted to a new design problem, or made generally more useful for other design problem solving. Prototype creation is seen as the ultimate design endeavor in this approach. So far research based on this approach has mainly concerned the problems of prototype refinement and prototype adaptation. Little progress has been reported on the issue of prototype creation and it remains a difficult subject.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">The constraint-based approach models an engineering system as a constraint network or a hierarchical structure, i.e. collections of constraints that are interconnected by design variables. Instantiation of this structure, or part of it, forms a basic structure of a new design problem. Computer-based constraint-based reasoning techniques are used to derive the values of unknown design variables from some initial data provided by the designers to form the solutions to the new design problem.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Computational methods are used to detect constraint violation and to suggest alternative values for design variables. The application of design strategies or design methods is reflected in the way in which design variables are created, constrained, and manipulated.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">The original aim of AI and knowledge based design was to produce general purpose, domain independent AI tools that would automate design tasks requiring intelligence. The early promise of knowledge based design has not been fulfilled because these approaches:&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- did not scale within and across domains,&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- unable to learn and adapt to design contexts,&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- failed to handle complexity, consistency and unpredictability,&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- could not acquire knowledge satisfactorily, and&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- could not model and support creativity.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Evolutionary design&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Recent advancement in evolutionary computing provides new opportunities to re-examine the issue of intelligent design support. Genetic algorithms are widely accepted as powerful generative and adaptive techniques generally applicable to many design activities. Evolutionary design is an approach that utilizes different evolutionary computation techniques in different stages of the design process. Evolutionary design is capable of generating astonishing imaginative and innovative designs.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">The strength of evolutionary design comes from the factor that controlled evolution can be formulated as a general purpose problem solver with ability similar to human design intelligence but with a magnitude of speed and efficiency. Traditional AI methods such as rule-based reasoning have to model design intelligence explicitly in terms of knowledge both in representation and inference. These methods have serious drawbacks because the process of how human designers actually use this kind of knowledge is not necessarily fully understood.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Bentley classified evolutionary design into four categories. These are:&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Evolutionary design optimization that is concerned with optimizing existing designs by evolving the values of suitably constrained design parameters;&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Creative evolutionary design that generates entirely new designs from little abstract knowledge to satisfy functional requirements;&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Conceptual evolutionary design that deals with the production of high level conceptual frameworks of preliminary designs; and&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">- Generative evolutionary design that directly produces forms of designs contributing to the emergence of implicit design concepts.&nbsp;<\/span><\/p><p dir=\"ltr\"><span><br><\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">These evolutionary design approaches combine several vital aspects of design intelligence in an evolutionary process, including modeling design data and information, concept formation, idea generation, optimization, learning, and evaluation. Once a design problem is properly formulated in an evolutionary process, the computer is able to generate a&nbsp;<\/span>large number of candidate solutions before reaching an optimum one. The candidate solutions are unpredictable but the process and the final outcome are controllable by the designers. The evolutionary design approach has an excellent potential for developing more intelligent design support tools.<\/p>\n<br>For example, Frazer used genetic algorithms in his evolutionary architectural design to evolve unpredicted forms of architectures and their possible interactions with the environments. Chakrabarti developed a functional synthesis program that generates a large number of abstract design concepts from functional requirements and abstract building blocks of engineering elements. Thornton utilized genetic algorithms as constraint management tools in the process of embodiment design. However, the development of evolutionary design tools is still at its early stage. So far, many genetic algorithms have been used and tested only in design problems of small scale. A theoretical understanding of evolutionary design and its applications in the design process is necessary.",
                "DataExportTag": "AI418642",
                "QuestionID": "QID154",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT\" In the development of intelligent co...",
                "Choices": {
                    "1": {
                        "Display": "\"Edge Computing and Deployment of Lightweight DNNs\": dnn, deep_learning, pruning, energy, inference, gpu, dnns, deployment, mobile, automl, accelerator, deploy, latency, lightweight, edge"
                    },
                    "2": {
                        "Display": "\"High Performance Computing and GPU Optimization\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID159",
            "SecondaryAttribute": "\"TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT\" In the development of intelligent co...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">\"TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT\"<\/b><\/p><p dir=\"ltr\"><b><br><\/b><\/p>\n\n<p dir=\"ltr\">In the development of intelligent computer-aided design systems, three important issues need to be considered. These issues are: how to support the generation of product concepts using evolutionary computation techniques; how to use intelligent databases and constraint management systems for detailed exploration of product embodiment; and how to integrate rapid prototyping facilities for product evaluation. In this paper, we present a brief review of knowledge-based design and evolutionary design and discuss ways of integrating both in the development of a generative design system. Based on this review, we present the model and its applications of a generative design system utilizing a number of AI and evolutionary computation techniques. This generative design model is intended to provide a generic computational framework for the development of intelligent design support systems.&nbsp;<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Knowledge-based design<\/b><\/p>\n\n<p dir=\"ltr\">Existing computer-based design systems provide little support for the evolution of design solution concepts that satisfy all physical constraints. They rely on precise geometric information to specify the model of an artifact. They cannot support early stage design tasks because the geometry of an artifact is not sufficiently definite at the early stage. The design of complex products containing many components requires a number of alternative solutions to be explored, each being modified by making changes to key design parameters until a satisfactory design solution is found. The generation and exploration of design concepts based on incomplete design requirements and the handling of a multitude of variables and constraints embedded within 3D design objects is a challenging task for designers.<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\">Four main strategies may characterize the current developments in knowledge-based design systems:&nbsp;<\/p>\n\n<p dir=\"ltr\">- The intelligent CAD approach;<br>\n- The building block approach;<br>\n- The prototype approach; and<br>\n- The constraint-based approach.&nbsp;<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\">The intelligent CAD approach extends the capability of a CAD system by employing heuristic knowledge on top of geometric models of the artifacts. An intelligent CAD system plays an active rather than a passive role in the design process, by incorporating a significant amount of design knowledge into the system. Functional modeling, for example, is an approach that attempts to build intelligent CAD systems through a mapping from a function space to a conceptual solution space using functional instead of geometric components and their interfaces. This approach uses abstract knowledge to represent the function and behavior of elementary design components, parts, and their causal relationships. The building block approach formulates the design process as a sequence of tasks that can be tackled individually by different classes of CAD tools and AI methods. Chandrasekaran classified these tasks into three categories:<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\">1. Class 1 design tasks in which the components of the artifact being designed are unknown.<br>\n2. Class 2 design tasks in which the components of the artifact being designed are known, but the design plans, i.e., the methods of how to design, are unavailable in a compiled form.<br>\n3. Class 3 design tasks where ways of decomposing a design problem are already known, and for which compiled design plans are available for each stage of the design process.&nbsp;<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\">In the prototype design approach, the design process is divided into three different activities: prototype refinement, prototype adaptation, and prototype creation based on a library of design prototypes. Gero classified design as:<\/p><p dir=\"ltr\"><br><\/p>\n- Routine design where the functions, expected behaviors, and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables<br>\n- Innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements.<br>\n- Creative design, in which an entirely new design problem space is to be defined.<br>\n&nbsp;<br>\nIn this approach, design tasks under class 3 are considered routine designs that can be conveniently supported by automated computer programs. The others are more difficult and require significant creative inputs from human designers. The prototype design approach divides the design process into three different activities: prototype refinement, prototype adaptation and prototype creation based on a library of design prototypes. Gero classified design as: routine design where the functions, expected behaviors, and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables; innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements; and creative design, in which an entirely new design problem space is to be defined. Prototype refinement involves instantiating the variables in a design prototype and determining their values. Prototype adaptation becomes necessary when a design prototype is found to be inadequate in some way, and needs to be adapted to a new design problem, or made generally more useful for other design problem solving. Prototype creation is seen as the ultimate design endeavor in this approach. So far research based on this approach has mainly concerned the problems of prototype refinement and prototype adaptation. Little progress has been reported on the issue of prototype creation and it remains a difficult subject.&nbsp;<br>\n&nbsp;<br>\nThe constraint-based approach models an engineering system as a constraint network or a hierarchical structure, i.e. collections of constraints that are interconnected by design variables. Instantiation of this structure, or part of it, forms a basic structure of a new design problem. Computer-based constraint-based reasoning techniques are used to derive the values of unknown design variables from some initial data provided by the designers to form the solutions to the new design problem.&nbsp;\n<p dir=\"ltr\">Computational methods are used to detect constraint violation and to suggest alternative values for design variables. The application of design strategies or design methods is reflected in the way in which design variables are created, constrained, and manipulated.&nbsp;<\/p>\n\n<p dir=\"ltr\">The original aim of AI and knowledge based design was to produce general purpose, domain independent AI tools that would automate design tasks requiring intelligence. The early promise of knowledge based design has not been fulfilled because these approaches:&nbsp;<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\">- did not scale within and across domains,&nbsp;<br>\n- unable to learn and adapt to design contexts,&nbsp;<br>\n- failed to handle complexity, consistency and unpredictability,&nbsp;<br>\n- could not acquire knowledge satisfactorily, and&nbsp;<br>\n- could not model and support creativity.&nbsp;<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-ed87b298-7fff-2a94-4043-a45fd84b7d57\">Evolutionary design&nbsp;<\/b><\/p>\n\n<p dir=\"ltr\">Recent advancement in evolutionary computing provides new opportunities to re-examine the issue of intelligent design support. Genetic algorithms are widely accepted as powerful generative and adaptive techniques generally applicable to many design activities. Evolutionary design is an approach that utilizes different evolutionary computation techniques in different stages of the design process. Evolutionary design is capable of generating astonishing imaginative and innovative designs.&nbsp;<\/p><p dir=\"ltr\"><br><\/p>\n\n<p dir=\"ltr\">The strength of evolutionary design comes from the factor that controlled evolution can be formulated as a general purpose problem solver with ability similar to human design intelligence but with a magnitude of speed and efficiency. Traditional AI methods such as rule-based reasoning have to model design intelligence explicitly in terms of knowledge both in representation and inference. These methods have serious drawbacks because the process of how human designers actually use this kind of knowledge is not necessarily fully understood.&nbsp;<\/p>\n\n<p dir=\"ltr\">Bentley classified evolutionary design into four categories. These are:<br>\n<br>\n- Evolutionary design optimization that is concerned with optimizing existing designs by evolving the values of suitably constrained design parameters;&nbsp;<br>\n- Creative evolutionary design that generates entirely new designs from little abstract knowledge to satisfy functional requirements;&nbsp;<br>\n- Conceptual evolutionary design that deals with the production of high level conceptual frameworks of preliminary designs; and&nbsp;<br>\n- Generative evolutionary design that directly produces forms of designs contributing to the emergence of implicit design concepts.&nbsp;<\/p>\n\n<p dir=\"ltr\"><br><\/p><p dir=\"ltr\">These evolutionary design approaches combine several vital aspects of design intelligence in an evolutionary process, including modeling design data and information, concept formation, idea generation, optimization, learning, and evaluation. Once a design problem is properly formulated in an evolutionary process, the computer is able to generate a large number of candidate solutions before reaching an optimum one. The candidate solutions are unpredictable but the process and the final outcome are controllable by the designers. The evolutionary design approach has an excellent potential for developing more intelligent design support tools.<\/p><p dir=\"ltr\"><br><\/p>For example, Frazer used genetic algorithms in his evolutionary architectural design to evolve unpredicted forms of architectures and their possible interactions with the environments. Chakrabarti developed a functional synthesis program that generates a large number of abstract design concepts from functional requirements and abstract building blocks of engineering elements. Thornton utilized genetic algorithms as constraint management tools in the process of embodiment design. However, the development of evolutionary design tools is still at its early stage. So far, many genetic algorithms have been used and tested only in design problems of small scale. A theoretical understanding of evolutionary design and its applications in the design process is necessary.",
                "DataExportTag": "AI80272",
                "QuestionID": "QID159",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT\" In the development of intelligent co...",
                "Choices": {
                    "1": {
                        "Display": "\"Parallel Computing Ecosystem Accelerators\": parallel, gpu, processor, compiler, thread, cpu, distribute, speedup, parallelism, parallelization, execution, parallelize, hpc, heterogeneous, cuda"
                    },
                    "2": {
                        "Display": "\"Neural Network Hardware and Software Tuning\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID95",
            "SecondaryAttribute": "\"Ultrasonic Measurements of Breast Viscoelasticity\" In vivo measurements of the viscoelastic prop...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Ultrasonic Measurements of Breast Viscoelasticity&quot;<\/strong><br \/>\n<br \/>\nIn vivo measurements of the viscoelastic properties of breast tissue are described. Ultrasonic echo frames were recorded from volunteers at 5 frames per second (fps) while applying a uniaxial compressive force (1-20 N) within a 1-second ramp time and holding the force constant for up to 200 seconds. A time series of strain images was formed from the echo data, spatially averaged viscous creep curves were computed, and viscoelastic strain parameters were estimated by fitting creep curves to a second-order Voigt model. The useful strain bandwidth from this quasi-static ramp stimulus was 10^(-2) &le; &omega; &le; 10^(0) rad\/s (0.0016-0.16 Hz).<br \/>\n<br \/>\nThe stress-strain curves for normal glandular tissues are linear when the surface force applied is between 2 and 5 N. In this range, the creep response was characteristic of biphasic viscoelastic polymers, settling to a constant strain (arrheodictic) after 100 seconds. The average model-based retardance time constants for the viscoelastic response were 3.2 +\/- 0.8 and 42.0 +\/- 28 seconds. Also, the viscoelastic strain amplitude was approximately equal to that of the elastic strain.<br \/>\n<br \/>\nAbove 5 N of applied force, however, the response of glandular tissue became increasingly nonlinear and rheodictic, i.e., tissue creep never reached a plateau. Contrasting in vivo breast measurements with those in gelatin hydrogels, preliminary ideas regarding the mechanisms for viscoelastic contrast are emerging.",
                "DataExportTag": "CAN1183500",
                "QuestionID": "QID95",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Ultrasonic Measurements of Breast Viscoelasticity\" In vivo measurements of the viscoelastic prop...",
                "Choices": {
                    "1": {
                        "Display": "\"Statistical Modeling and Error Prediction\": simulation, error, prediction, bayesian, dynamic, modeling, estimator, uncertainty, mathematical_model, probability, measurement, noise, stochastic, equation, monte_carlo"
                    },
                    "2": {
                        "Display": "\"Medical Imaging and Cancer Treatment\": imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID86",
            "SecondaryAttribute": "\"Understanding Intergenerational Transmissions: A Cross-Disciplinary Approach\" This project combi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Understanding Intergenerational Transmissions: A Cross-Disciplinary Approach\"<\/strong><br>\n<br>\nThis project combines the best sociology and economics has to offer to establish a new understanding of intergenerational transmissions. We know that socioeconomic outcomes are correlated across generations, but we have only little understanding of the mechanisms and intergenerational transmissions which generate these correlations.<br>\n<br>\nIn this project, we propose to combine formal models of intergenerational transmissions in economics with substantive insights from sociology to develop new and improved models of intergenerational transmissions. Furthermore, we combine longitudinal data with state-of-the-art econometric methods to analyze intergenerational transmissions of cultural endowments and educational expectations, the role of the extended family in intergenerational transmissions, and finally the utility of educational decision making.<br>\n<br>\nThe project has the potential to significantly improve our understanding of the causes and consequences of intergenerational transmissions and, in doing so, to contribute new knowledge to inform policies to promote social mobility.",
                "DataExportTag": "COR58321",
                "QuestionID": "QID86",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Understanding Intergenerational Transmissions: A Cross-Disciplinary Approach\" This project combi...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\": child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "2": {
                        "Display": "\"Global History and Cultural Studies\": indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID113",
            "SecondaryAttribute": "\"Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developme...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>&quot;Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Development and Validation of a New Predictive Model&quot;<\/strong><br \/>\n<br \/>\n<strong>Context:<\/strong>&nbsp;Existing breast cancer prediction tools do not account for breast density, a strong risk factor for breast cancer and have been studied in white women only.<br \/>\n<br \/>\n<strong>Contribution:<\/strong>&nbsp;The authors developed a breast cancer risk prediction model that incorporates a measure of breast density routinely reported with mammography. Its predictions were accurate, but it had only modest ability to distinguish women who did not develop cancer from those who did, and it misclassified risk in some subgroups.<br \/>\n<br \/>\n<strong>Implication:&nbsp;<\/strong>The model requires validation in additional populations. A breast cancer prediction model that incorporates breast density does well in some but not all domains of predicting risk. Its accuracy should be better characterized before it is used clinically.<br \/>\n<br \/>\n<strong>The Editors<\/strong><br \/>\nIn 2007, breast cancer will have been diagnosed in more than 178000 women in the United States, and more than 40000 women will have died of breast cancer (1). Most of these women never had their risk for breast cancer assessed, and even fewer considered chemoprevention (25). Providing women with an estimate of their risk for breast cancer would provide an opportunity for them to consider options to decrease their risk. Women at low short-term risk for breast cancer may experience less anxiety about their health and would be less likely to benefit from prevention efforts. Women at very high risk may warrant additional screening tests, such as breast magnetic resonance imaging (6), and might benefit from chemoprevention of breast cancer with tamoxifen or raloxifene. The standard risk assessment model available to practitioners (the Gail model) (7) identifies only a minority of women who eventually develop breast cancer being at high risk (8). Better breast cancer risk prediction tools are needed (9).<br \/>\n<br \/>\nThe radiographic appearance of the breast has been consistently shown to be a major risk factor for breast cancer, whether it is defined by a qualitative assessment of the parenchymal pattern or a quantitative measure of percentage of density (1012). Women in whom more than 50% of total breast area is mammographically dense have high breast density and are at 3- to 5-fold greater risk for breast cancer than women in whom breast density is less than 25% (10, 1316). The increased risk for breast cancer associated with breast density is due in part to the lower sensitivity of mammography in dense breasts (1719), but the association remains strong after accounting for masking (20, 21). Mammographically dense breast tissue is rich in epithelium and stroma (10), and the association could represent activation of epithelial cells or fibroblasts (2225).<br \/>\n<br \/>\nRecently, several models have been published that incorporate breast density: One uses a continuous measure of breast density that is not available to clinicians and has not been validated (26), and the other predicts 1-year risk for breast cancer (27). We previously demonstrated that a simple model based on age, ethnicity, and a categorical measure of breast density had predictive accuracy similar to that of the Gail model in a multiethnic cohort of women receiving screening mammograms in northern California (28). We expand on that work by using data from more than 1 million ethnically diverse women throughout the United States to develop and validate a risk assessment tool that incorporates breast density and therefore might improve breast cancer screening and prevention efforts.<br \/>\n<br \/>\n<strong>Methods<\/strong><br \/>\nStudy Population We included 1095484 women age 35 years or older who had had at least 1 mammogram with breast density measured by using the Breast Imaging Reporting and Data System (BI-RADS) classification system in any of the 7 mammography registries participating in the National Cancer Institutefunded Breast Cancer Surveillance Consortium (BCSC) (available at breastscreening.cancer.gov) (29). The BCSC is a community-based, ethnically and geographically diverse sample that broadly represents the United States (30). We excluded women who had a diagnosis of breast cancer before their first eligible mammography examination. Because our goal was to develop a model of long-term risk for invasive breast cancer, we excluded women with cancer diagnosed in the first 6 months of follow-up to minimize the number of cases of cancer included in the model that were diagnosed on the basis of the mammogram used for risk assessment. Women were also excluded if they had breast implants. Women in whom ductal carcinoma in situ was diagnosed were censored at the time of diagnosis in the primary analysis. When women had several mammograms, we based our analysis on findings from the first mammogram.<br \/>\n<br \/>\nEach registry obtains annual approval from its institutional review board for consenting processes or a waiver of consent, enrollment of participants, and ongoing data linkage for research purposes. All registries have received a Certificate of Confidentiality from the federal government that protects the identities of research participants.<br \/>\n<br \/>\n<strong>Measurement of Risk Factors<\/strong><br \/>\nPatient information was obtained primarily from self-report at the time of mammography. We selected 2 risk factors in addition to breast density for inclusion in the model on the basis of simplicity (yes or no) and a high attributable risk: history of breast cancer in a first-degree relative and history of a breast biopsy. Body mass index was later considered for addition to the model, but it was excluded to maintain parsimony and because it had minimal effect on model discrimination (the increase in the concordance statistic [c-statistic] was only 0.003). For modeling and validation, missing data for relatives with breast cancer and number of breast biopsies were set to 0. The 5-year Gail risk was computed for each woman by using the algorithms provided by the National Cancer Institute to calculate the Gail model risk for individual women (31). For Gail model calculations, missing data were coded as specified by that model (age at menarche as 14 years, age at first live birth as &lt;20 years, number of breast biopsies as 0, and number of first-degree relatives as 0). Ethnicity was coded by using the expanded race and ethnicity definition currently used in the Surveillance, Epidemiology, and End Results (SEER) database and U.S. Vital Statistics (non-Hispanic White, non-Hispanic Black, Asian or Pacific Islander, Native American\/Alaskan Native, Hispanic, or other). We classified women who self-identified as mixed or other race with participants who did not report race and ethnicity.<br \/>\n<br \/>\n<strong>Breast Density<\/strong><br \/>\nCommunity radiologists at each site classified breast density on screening mammograms as part of routine clinical practice by using the American College of Radiology BI-RADS density categories (32): almost entirely fat (category 1), scattered fibroglandular densities (category 2), heterogeneously dense (category 3), and extremely dense (category 4). The BI-RADS category 2 was used as the reference group for breast density because it formed the largest group.<br \/>\n<br \/>\n<strong>Ascertainment of Breast Cancer Cases<\/strong><br \/>\nBreast cancer outcomes (invasive cancer and ductal carcinoma in situ) were obtained at each site through linkage with the regional population-based SEER program, state tumor registries, and pathology databases.<br \/>\n<br \/>\n<strong>Vital Status<\/strong><br \/>\nVital status was obtained through linkage to SEER registries, state tumor registries, and the individual state vital statistics or the National Death Index.<br \/>\n<br \/>\n<strong>Model Development<\/strong><br \/>\nWe used a proportional hazards model of invasive breast cancer to estimate the hazard ratios for each BI-RADS breast density category. Women entered the model 6 months after the index mammogram and were censored at the time of death, diagnosis of ductal carcinoma in situ, or the end of follow-up. All models were adjusted for age (in 5-year intervals) and race and ethnicity. The strength of the breast density association with breast cancer was greater for women younger than age 65 years (P for interaction &lt; 0.001). Thus, separate models were fitted for women younger than age 65 years and for women age 65 years or older. No other interaction terms were included in the final model.<br \/>\n<br \/>\nWe calculated similar estimates for first-degree relatives with breast cancer (yes or no) and a personal history of breast biopsy (yes or no) from the BCSC. All predictors met the proportional hazards assumption that was assessed by loglog plots and by including interaction terms with time for each predictor variable. We then developed an absolute risk model by using methods described in the Appendix Figure. The model primarily estimates predicted incidence of invasive breast cancer by using age, race or ethnicity, and breast density. These estimates are then adjusted for family history and biopsy history if available.<br \/>\n<br \/>\nWe based our estimates of breast cancer incidence on the SEER age- and ethnicity-specific risk for invasive breast cancer (1992 to 2002) (33). Age-specific incidence for each ethnic group was estimated by fitting a third-order polynomial model to the SEER data. Age-specific incidence rates for the Native American and Alaskan Native group were inconsistent in SEER, so we excluded this group from further analyses. We calculated the baseline risk for the model by adjusting SEER incidence for the population&#39;s attributable risk for each breast density subgroup. We estimated the age- and ethnicity-specific distribution of mammographic breast density needed for these calculations by using data from a larger set of 3343047 mammograms from the BCSC. The distribution of breast density varied statistically significantly by age and by race or ethnicity (P&lt; 0.001 for each comparison). The model used these variations by age and race to distribute the 5-year risk for invasive breast cancer across the 4 breast density subgroups.<br \/>\n<br \/>\nWe used the methods described by Gail and colleagues (7) to translate the hazard ratios and risk factor distributions into absolute risks. The age-, sex-, and ethnicity-specific competing risks for death for women were calcula",
                "DataExportTag": "CAN442469",
                "QuestionID": "QID113",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developme...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\": mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "2": {
                        "Display": "\"Breast and Ovarian Cancer Screening and Hormone Therapy\": breast, woman, screening, ovarian_cancer, mammography, hormone, estrogen_receptor, postmenopausal_woman, postmenopausal, hrt, estrogen, mammogram, invasive, family_history, ht"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID198",
            "SecondaryAttribute": "\"Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developme...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Development and Validation of a New Predictive Model\"<\/strong><br>\n<br>\n<strong>Context:<\/strong> Existing breast cancer prediction tools do not account for breast density, a strong risk factor for breast cancer and have been studied in white women only.<br>\n<br>\n<strong>Contribution:<\/strong> The authors developed a breast cancer risk prediction model that incorporates a measure of breast density routinely reported with mammography. Its predictions were accurate, but it had only modest ability to distinguish women who did not develop cancer from those who did, and it misclassified risk in some subgroups.<br>\n<br>\n<strong>Implication:&nbsp;<\/strong>The model requires validation in additional populations. A breast cancer prediction model that incorporates breast density does well in some but not all domains of predicting risk. Its accuracy should be better characterized before it is used clinically.<br>\n<br>\n<strong>The Editors<\/strong><br>\nIn 2007, breast cancer will have been diagnosed in more than 178000 women in the United States, and more than 40000 women will have died of breast cancer (1). Most of these women never had their risk for breast cancer assessed, and even fewer considered chemoprevention (25). Providing women with an estimate of their risk for breast cancer would provide an opportunity for them to consider options to decrease their risk. Women at low short-term risk for breast cancer may experience less anxiety about their health and would be less likely to benefit from prevention efforts. Women at very high risk may warrant additional screening tests, such as breast magnetic resonance imaging (6), and might benefit from chemoprevention of breast cancer with tamoxifen or raloxifene. The standard risk assessment model available to practitioners (the Gail model) (7) identifies only a minority of women who eventually develop breast cancer being at high risk (8). Better breast cancer risk prediction tools are needed (9).<br>\n<br>\nThe radiographic appearance of the breast has been consistently shown to be a major risk factor for breast cancer, whether it is defined by a qualitative assessment of the parenchymal pattern or a quantitative measure of percentage of density (1012). Women in whom more than 50% of total breast area is mammographically dense have high breast density and are at 3- to 5-fold greater risk for breast cancer than women in whom breast density is less than 25% (10, 1316). The increased risk for breast cancer associated with breast density is due in part to the lower sensitivity of mammography in dense breasts (1719), but the association remains strong after accounting for masking (20, 21). Mammographically dense breast tissue is rich in epithelium and stroma (10), and the association could represent activation of epithelial cells or fibroblasts (2225).<br>\n<br>\nRecently, several models have been published that incorporate breast density: One uses a continuous measure of breast density that is not available to clinicians and has not been validated (26), and the other predicts 1-year risk for breast cancer (27). We previously demonstrated that a simple model based on age, ethnicity, and a categorical measure of breast density had predictive accuracy similar to that of the Gail model in a multiethnic cohort of women receiving screening mammograms in northern California (28). We expand on that work by using data from more than 1 million ethnically diverse women throughout the United States to develop and validate a risk assessment tool that incorporates breast density and therefore might improve breast cancer screening and prevention efforts.<br>\n<br>\n<strong>Methods<\/strong><br>\nStudy Population We included 1095484 women age 35 years or older who had had at least 1 mammogram with breast density measured by using the Breast Imaging Reporting and Data System (BI-RADS) classification system in any of the 7 mammography registries participating in the National Cancer Institutefunded Breast Cancer Surveillance Consortium (BCSC) (available at breastscreening.cancer.gov) (29). The BCSC is a community-based, ethnically and geographically diverse sample that broadly represents the United States (30). We excluded women who had a diagnosis of breast cancer before their first eligible mammography examination. Because our goal was to develop a model of long-term risk for invasive breast cancer, we excluded women with cancer diagnosed in the first 6 months of follow-up to minimize the number of cases of cancer included in the model that were diagnosed on the basis of the mammogram used for risk assessment. Women were also excluded if they had breast implants. Women in whom ductal carcinoma in situ was diagnosed were censored at the time of diagnosis in the primary analysis. When women had several mammograms, we based our analysis on findings from the first mammogram.<br>\n<br>\nEach registry obtains annual approval from its institutional review board for consenting processes or a waiver of consent, enrollment of participants, and ongoing data linkage for research purposes. All registries have received a Certificate of Confidentiality from the federal government that protects the identities of research participants.<br>\n<br>\n<strong>Measurement of Risk Factors<\/strong><br>\nPatient information was obtained primarily from self-report at the time of mammography. We selected 2 risk factors in addition to breast density for inclusion in the model on the basis of simplicity (yes or no) and a high attributable risk: history of breast cancer in a first-degree relative and history of a breast biopsy. Body mass index was later considered for addition to the model, but it was excluded to maintain parsimony and because it had minimal effect on model discrimination (the increase in the concordance statistic [c-statistic] was only 0.003). For modeling and validation, missing data for relatives with breast cancer and number of breast biopsies were set to 0. The 5-year Gail risk was computed for each woman by using the algorithms provided by the National Cancer Institute to calculate the Gail model risk for individual women (31). For Gail model calculations, missing data were coded as specified by that model (age at menarche as 14 years, age at first live birth as &lt;20 years, number of breast biopsies as 0, and number of first-degree relatives as 0). Ethnicity was coded by using the expanded race and ethnicity definition currently used in the Surveillance, Epidemiology, and End Results (SEER) database and U.S. Vital Statistics (non-Hispanic White, non-Hispanic Black, Asian or Pacific Islander, Native American\/Alaskan Native, Hispanic, or other). We classified women who self-identified as mixed or other race with participants who did not report race and ethnicity.<br>\n<br>\n<strong>Breast Density<\/strong><br>\nCommunity radiologists at each site classified breast density on screening mammograms as part of routine clinical practice by using the American College of Radiology BI-RADS density categories (32): almost entirely fat (category 1), scattered fibroglandular densities (category 2), heterogeneously dense (category 3), and extremely dense (category 4). The BI-RADS category 2 was used as the reference group for breast density because it formed the largest group.<br>\n<br>\n<strong>Ascertainment of Breast Cancer Cases<\/strong><br>\nBreast cancer outcomes (invasive cancer and ductal carcinoma in situ) were obtained at each site through linkage with the regional population-based SEER program, state tumor registries, and pathology databases.<br>\n<br>\n<strong>Vital Status<\/strong><br>\nVital status was obtained through linkage to SEER registries, state tumor registries, and the individual state vital statistics or the National Death Index.<br>\n<br>\n<strong>Model Development<\/strong><br>\nWe used a proportional hazards model of invasive breast cancer to estimate the hazard ratios for each BI-RADS breast density category. Women entered the model 6 months after the index mammogram and were censored at the time of death, diagnosis of ductal carcinoma in situ, or the end of follow-up. All models were adjusted for age (in 5-year intervals) and race and ethnicity. The strength of the breast density association with breast cancer was greater for women younger than age 65 years (P for interaction &lt; 0.001). Thus, separate models were fitted for women younger than age 65 years and for women age 65 years or older. No other interaction terms were included in the final model.<br>\n<br>\nWe calculated similar estimates for first-degree relatives with breast cancer (yes or no) and a personal history of breast biopsy (yes or no) from the BCSC. All predictors met the proportional hazards assumption that was assessed by loglog plots and by including interaction terms with time for each predictor variable. We then developed an absolute risk model by using methods described in the Appendix Figure. The model primarily estimates predicted incidence of invasive breast cancer by using age, race or ethnicity, and breast density. These estimates are then adjusted for family history and biopsy history if available.<br>\n<br>\nWe based our estimates of breast cancer incidence on the SEER age- and ethnicity-specific risk for invasive breast cancer (1992 to 2002) (33). Age-specific incidence for each ethnic group was estimated by fitting a third-order polynomial model to the SEER data. Age-specific incidence rates for the Native American and Alaskan Native group were inconsistent in SEER, so we excluded this group from further analyses. We calculated the baseline risk for the model by adjusting SEER incidence for the population's attributable risk for each breast density subgroup. We estimated the age- and ethnicity-specific distribution of mammographic breast density needed for these calculations by using data from a larger set of 3343047 mammograms from the BCSC. The distribution of breast density varied statistically significantly by age and by race or ethnicity (P&lt; 0.001 for each comparison). The model used these variations by age and race to distribute the 5-year risk for invasive breast cancer across the 4 breast density subgroups.<br>\n<br>\nWe used the methods described by Gail and colleagues (7) to translate the hazard ratios and risk factor distributions into absolute risks. The age-, sex-, and ethnicity-specific competing risks for death for women were calcula",
                "DataExportTag": "CAN677383",
                "QuestionID": "QID198",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developme...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Discovery and Prevention Technologies\": challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "2": {
                        "Display": "\"Childhood Illness and Family Psychosocial Experience\": child, parent, survivor, family, pediatric, woman, experience, adolescent, qualitative, life, illness, childhood, interview, psychosocial, mother"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID84",
            "SecondaryAttribute": "\"Web Graph: Learning Models for Prediction and Evolution Monitoring\" Data-intensive systems flour...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<strong>\"Web Graph: Learning Models for Prediction and Evolution Monitoring\"<\/strong><br>\n<br>\nData-intensive systems flourish in the last decades with an ever-increasing rate of data production. A characteristic case is the Web graph. Given the dynamism of the Web, we aim to study the Web graph in terms of learning models and monitoring its evolution. The main problems we study are:<br>\n<br>\n- Identification of trends and patterns in the web graph, using the spectral properties of the evolving web adjacency matrix.<br>\n- Monitoring of web pages' ranking over time, and prediction of pages' web ranking.<br>\n- Learn models for the evolving web graph with statistical learning techniques.<br>\n<br>\nThe results of the proposed research will be a framework of approaches and algorithms that will enable effective and efficient:<br>\n<br>\n- Query-based top-k list predictions (future and historical ones)<br>\n- Prediction-based crawling: based on our ranking predictive modeling, crawling resources can be optimized while maintaining a satisfactory top-k quality.<br>\n<br>\nAll the above are profoundly beneficial for resource management in the context of large-scale Web search, and the added value of the above will be the potential use of these techniques by the Web search industry.",
                "DataExportTag": "COR13744",
                "QuestionID": "QID84",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\"Web Graph: Learning Models for Prediction and Evolution Monitoring\" Data-intensive systems flour...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\": computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "2": {
                        "Display": "\"Nuclear Transport and Digital Twin Experimentation\": nuclear, transport, interoperable, enabler, experimentation, agile, certification, toolbox, factory, testbed, bim, broad, federation, vertical, digital_twin"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID107",
            "SecondaryAttribute": "1.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "1.\n\n",
                "DataExportTag": "FIG",
                "QuestionID": "QID107",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "1.",
                "Choices": {
                    "1": {
                        "Display": "Preoperative abdominal MRI that demonstrates an 11-cm upper pole mass (dashed arrow) in the left moiety of a horseshoe kidney with a left renal vein thrombus (straight arrow) extending to the confluence of the inferior vena cava."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID392",
            "SecondaryAttribute": "1.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "1.\n\n",
                "DataExportTag": "Fig",
                "QuestionID": "QID392",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "1.",
                "Choices": {
                    "1": {
                        "Display": "Specificity of thyroid hormone signaling through plasma membrane integrins. The plasma membrane \u03b1v\u03b23-integrin has distinct binding sites for 3,5,3\u2032-triiodo-l-thyronine (T3) and l-thyroxine (T4). One binding site binds only T3 and ..."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID109",
            "SecondaryAttribute": "2.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "2.\n\n",
                "DataExportTag": "FIG-1",
                "QuestionID": "QID109",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "2.",
                "Choices": {
                    "1": {
                        "Display": "Intraoperative view that shows use of extra-large Weck clips to retract the renal vein thrombus (A) and to exclude a portion of the thrombus-free renal vein for resection (B)."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID111",
            "SecondaryAttribute": "3.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "3.\n\n",
                "DataExportTag": "FIG-2",
                "QuestionID": "QID111",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "3.",
                "Choices": {
                    "1": {
                        "Display": "Intact extracted heminephrectomy specimen with intraluminal renal vein thrombus exposed after removal of Hem-o-Lok clip (arrows). Venous resection margins were negative for cancer."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "QC",
            "PrimaryAttribute": "Survey Question Count",
            "SecondaryAttribute": "411",
            "TertiaryAttribute": null,
            "Payload": null
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID332",
            "SecondaryAttribute": "8 Awakening or awareness: are we being honest about the retrieval and revial of Australia's Abori...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "8 Awakening or awareness: are we being honest about the retrieval and revial of Australia's Aboriginal languages? This is a report on the process of language retrieval and revival for some Aboriginal languages in north Queensland in recent years. The writer challenges language workers, language centres and government funding bodies to be honest with language learners about their role in the process of language awakening and revitalisation and the anticipated language outcomes. Reference is made to the importance of doing language work on country, the practicalities of working to awaken a language there, the retrieval and revival process when preparing and conducting Aboriginal language awareness workshops, and the continuing language learning and revitalisation process through language programs on country. The development of the Warrgamay language program and the Gudjal language program is reviewed, noting some of the difficulties due to the lack of language resources. The writer acknowledges the vital need to work with and respect the position of Elders, and the essential training role embedded within all language learning activities. The conclusion draws attention to the limited funding available for retreival and revival language work and the narrow views held by many funding bodies in regard to their understanding of the second language learning process as it applies to these Aboriginal languages. Finally, a metaphor from traditional life at Yirrkala, Northern Territory is used to draw an analogy between fire and the process of language re-awakening through awareness, retrieval, revival and revitalisation. 1 Indigenous languages teacher and consultant. Language in communities 91 Awareness, retrieval, revival, revitalisation Which is the right word? None of these words helps to fully understand and appreciate the intense feelings of joy, empowerment and pride or the strong want and need in a language learner when first hearing and speaking their ancestral Aboriginal language; nor the enormous difficulty, challenges, dedication, frustration and time which will be involved in learning and using, what is in fact, a new second language. Australia as the modern world knows it has been here a mere 200-odd years, a blink in time for ancient Aboriginal Australia. But these 200 years have been unequally shared with non-Indigenous Australians, speaking a foreign language and living a very different culture. In this short time the Aboriginal languages of Australia, the languages of the land, have for the most part been silenced. The condition of Aboriginal languages varies from the treasured few in the far north and centre of Australia, which are still spoken right through, used for everyday communication and are being learnt by children as their first language; to those languages which are struggling, not being learnt by children anymore and often only being spoken or partially remembered by a few Elders; and then to those languages which are no longer spoken, where the knowledge of the Elders is scarce. These are the language remnants of Australia\u2019s Aboriginal language heritage of around 250 distinct languages (Senior Secondary Assessment Board of South Australia 1996, p. 7). In the past, colonial, commonwealth and state governments have shown little regard for the languages of Aboriginal people, with these languages depicted as inferior and simple, and the speakers shamed and punished. Survivors of the violence and introduced diseases were often forcibly moved off their lands, breaking the bond between country and language. Later government policies of stealing children, breaking up families and punishment for traditional beliefs and values prevented languages from being passed on. A violent colonial history and its overwhelming consequences have left many Aboriginal languages without speakers and learners, and many who would like to be speakers and learners have been left without languages, right across Australia. Today these Aboriginal languages, which have often been inadequately recorded and sometimes went unrecorded, are being revitalised by community-based Aboriginal language groups. The challenge is to work together to halt the decline of Aboriginal languages, to create an awareness of the language remaining in the community, to retrieve any language knowledge which has been recorded, to revive the language for the descendants of today and to revitalise the language for the children of tomorrow. A further challenge is to assist learners and funding bodies to discard the incorrect, preconceived idea inherited from our colonial past that these languages are simple and will be easy and quick to learn. Australian Aboriginal languages sound different when spoken and are constructed in different ways from the Aboriginal English or Standard Australian English being spoken as a first language by the majority of Aboriginal people today. Language learners will find that, even though it was their own ancestors who were 92 Re-awakening languages the last speakers, this will not make it easier for them to learn and speak these unique languages (see also Walsh, this volume). There appears to be a hopeful assumption that Aboriginal languages are still there, sleeping, recumbent, just waiting for a community of speakers to come and make a bit of wake-up noise. In reality the continuing process of culture change and language loss has had enormous effects on Aboriginal people and their languages. We cannot avoid the tough historical reality for Indigenous people arising from the loss of their lands and the denigration of their culture and languages for generations, which has resulted in many Aboriginal languages no longer being spoken in homes. Aboriginal languages in some cases may not have been spoken for a number of years, sometimes generations. As a consequence of this relentless attack on Aboriginal languages their re-awakening involves the processes of language retrieval, revival and revitalisation and will need many years of hard language work and research, and more years of dedicated practice for learners to make new sounds, learn many new words and their meanings, learn new ways to form words and sentences, and to master the different grammar of an Aboriginal language. Language revival, turning the language loss process around, and heading back on the road to language revitalisation, is indeed the process being undertaken but may well only result in partial success. All language learners learn something about their language but only a dedicated few will achieve a strong command of the new sounds and intonation and come to terms with the different grammar and sentence structures involved (see also N. Reid, this volume). Learning your Aboriginal language can make you feel really good about yourself; it can help you to feel comfortable about the world by understanding the effects of history on your language. Understanding and learning language can make you think differently about your identity and self concept, your place in your family and community. It makes you feel proud. These are tremendous social and emotional results from language work but we must be honest, both with ourselves and language learners, about what can be achieved in bringing back these languages through a revitalisation process. We need to be honest about the daunting and dedicated language work necessary and the long-term view required when anticipating a full return of spoken language. This view requires us to look towards generations of language learners, to a pool of future language speakers, writers and readers. It is a view which includes language-speaking communities with families who are maintaining and passing on their languages in the face of the dominance of English and its overpowering role in Australian society. The unavoidable, fundamental and most difficult feature in this language revitalisation process is the basic need to communicate, the need to use the language you are learning. There is a need for other people to converse with, for someone to share with, others to be in a language group with, friends to joke with or swear at, family to be serious with, to care for and to do all this, in your ancestral Aboriginal language. We know why this language loss has happened and we also know that even those few remaining, fully-spoken Aboriginal languages in Australia are still seriously Language in communities 93 threatened by loss due to social upheaval and change of government policy. What we need to know is how to turn this language situation around and find real ways to revitalise languages on country while remaining true to the spirit of the languages, the wishes of the Elders and the hopes of the learners. This leads us to an important question: What do Aboriginal people want to do with their re-awakened languages in the Australian society of today? Language awareness workshops In recent years I have worked with the Girringun Aboriginal Corporation and the North Queensland Regional Aboriginal Corporation Language Centre. By using and sharing my teaching skills and the knowledge gained from learning and speaking Gumatj (one of the Yol\u014bu Matha languages of north-east Arnhemland), I have been able to help in the continuing process of retrieval, revival and revitalisation of some Aboriginal languages in north Queensland. This is a community-based language movement which was started after consultation with Elders in an attempt to try and fulfil their wishes and dreams to re-awaken their languages and to hear them on country again. The languages chosen to work with in the wet tropics were the rainforest Dyirrbal languages of Djirrbal, Ngadjan and Girramay, the coastal rainforest languages of Warrgamay and Nyawaygi and, in the dry country around Charters Towers, the languages of Gudjal (and Gugu-Badhun). It is important that all the language work happens on country and this is a central element of our activities. On country language work allows us to show respect to the Elders, to the ancestral voices and to give a context to these languages of the\n\n",
                "DataExportTag": "AI557766",
                "QuestionID": "QID332",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "8 Awakening or awareness: are we being honest about the retrieval and revial of Australia's Abori...",
                "Choices": {
                    "1": {
                        "Display": "\"Neuroscience and Sensory Perception\""
                    },
                    "2": {
                        "Display": "spike, brain, neuronal, synaptic, animal, biological, neuroscience, sensory, cortical, stimulus, odor, synapsis, fire, hebbian, plasticity"
                    },
                    "3": {
                        "Display": "\"Neuroscience and Reinforcement Learning in Parkinson's Disease\""
                    },
                    "4": {
                        "Display": "sequence, reward, motor, reinforcement_learning, implicit, action, dopamine, trial, parkinson_disease, feedback, movement, basal_ganglia, participant, striatum, behavior"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID351",
            "SecondaryAttribute": "A Disruptive Router Platform for the Internet of Things \u2013 TrustNode The TrustNode project aims at.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "A Disruptive Router Platform for the Internet of Things \u2013 TrustNode The TrustNode project aims at prototyping, validation and demonstration of a disruptive routing platform in real worldconditions. It will enable the design of novel routing paradigms asking for local intelligence to evaluate packet streams to serve upcoming B2B applications for the Internet of Things. Routing functionality is expected to migrate from core towards network edge in the next years.Outcome of the project will be (1) assessment of market share and manufacturing costs (2) business plan and (3) figures achieved by a prototype against SoA.\n\n",
                "DataExportTag": "COR10094",
                "QuestionID": "QID351",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "A Disruptive Router Platform for the Internet of Things \u2013 TrustNode The TrustNode project aims at...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\""
                    },
                    "2": {
                        "Display": "computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and Cloud Infrastructure\""
                    },
                    "4": {
                        "Display": "computation, heterogeneous, hpc, code, hardware, cps, processor, programming, cryptography, storage, layer, machine_learning, cloud_computing, compute, exascale"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID314",
            "SecondaryAttribute": "A scintillator-based approach to monitor secondary neutron production during proton therapy. PURP...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "A scintillator-based approach to monitor secondary neutron production during proton therapy. PURPOSE\nThe primary objective of this work is to measure the secondary neutron field produced by an uncollimated proton pencil beam impinging on different tissue-equivalent phantom materials using organic scintillation detectors. Additionally, the Monte Carlo code mcnpx-PoliMi was used to simulate the detector response for comparison to the measured data. Comparison of the measured and simulated data will validate this approach for monitoring secondary neutron dose during proton therapy.\n\n",
                "DataExportTag": "CAN740428",
                "QuestionID": "QID314",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "A scintillator-based approach to monitor secondary neutron production during proton therapy. PURP...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID325",
            "SecondaryAttribute": "A Simple Measure of Hepatocellular Carcinoma Burden Predicts Tumor Recurrence After Liver Transpl...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "A Simple Measure of Hepatocellular Carcinoma Burden Predicts Tumor Recurrence After Liver Transplantation: The Recurrent Hepatocellular Carcinoma\u2013Initial, Maximum, Last Classification Risk of recurrent hepatocellular carcinoma (rHCC) after liver transplantation (LT) depends on the pre\u2010LT HCC burden, tumor behavior, and response to locoregional therapy (LRT). In December 2017, LT priority for HCC was expanded to select patients outside the Milan criteria who respond to LRT. Our aims were to develop a novel objective measure of pre\u2010LT HCC burden (model of recurrent hepatocellular carcinoma\u2013initial, maximum, last [RH\u2010IML]), incorporating tumor behavior over time, and to apply RH\u2010IML to model post\u2010LT rHCC. Using United Network for Organ Sharing data from between 2002\u20102014 (development) and 2015\u20102017 (validation), we identified adult LT recipients with HCC and assessed pre\u2010LT HCC tumor behavior and post\u2010LT rHCC. For each patient, HCC burden was measured at 3 points on the waiting list: initial (I), maximum (M) total tumor diameter, and last (L) exception petition. HCC burden at these 3 points were classified as (A) Milan to University of California, San Francisco (UCSF), and (D) >UCSF, resulting in each patient having a 3\u2010letter RH\u2010IML designation. Of 16,558 recipients with HCC, 1233 (7%) had any post\u2010LT rHCC. rHCC rates were highest in RH\u2010IML group CCC (15%) and DDD (18%). When M and L tumor burdens did not exceed Milan (class B or A), rHCC was low (\u226410%) as in AAA, ABA, ABB, BBA, BBB; rHCC was also low (\u226410%) with successful downstaging when L was A (\n\n",
                "DataExportTag": "CAN20546",
                "QuestionID": "QID325",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "A Simple Measure of Hepatocellular Carcinoma Burden Predicts Tumor Recurrence After Liver Transpl...",
                "Choices": {
                    "1": {
                        "Display": "\"Women's Reproductive Health and Gynecological Disorders\""
                    },
                    "2": {
                        "Display": "endometrial_cancer, uterine, endometriosis, estrogen, progesterone, progesterone_receptor, aromatase, uterus, epirubicin_cyclophosphamide, estradiol, endometrial_hyperplasia, hysterectomy, endometrioid, progestin, vaginal"
                    },
                    "3": {
                        "Display": "\"Liver Cancer Treatment and Management\""
                    },
                    "4": {
                        "Display": "hepatocellular_carcinoma, liver, transplant, tace, lt, recurrence, portal_vein, sorafenib, hepatectomy, resection, transarterial_chemoembolization, transcatheter_arterial, milan_criterion, chemoembolization, rfa"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID306",
            "SecondaryAttribute": "A Single Dose of the Anti-Resorptive Peptide Human Calcitonin Paradoxically Augments Particle- an...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "A Single Dose of the Anti-Resorptive Peptide Human Calcitonin Paradoxically Augments Particle- and Endotoxin-Mediated Pro-Inflammatory Cytokine Production In Vitro Abstract The peptide hormone calcitonin (CT) is known to inhibit bone resorption and has previously been shown also to prevent particle-induced osteolysis, the leading cause of revision arthroplasty. In the present study, the influence of human CT on the initial inflammatory response to particulate wear debris or bacterial endotoxins, ultimately leading to osteoclast-mediated bone resorption, was analysed in human THP-1 macrophage-like cells. The cells were activated with either ultra-high molecular weight polyethylene (UHMWPE) particles or bacterial lipopolysaccharides (LPS) in order to simulate an osteolysis-associated inflammatory response. The cells were simultaneously treated with human CT (10\u22129 M). Cytokine production of tumour necrosis factor (TNF)-\u03b1 was quantified on both RNA and protein levels while interleukins (IL)-1\u03b2 and IL-6 were measured as secreted protein only. Stimulation of the cells with either particles or LPS led to a dose- and time-dependent increase of TNF-\u03b1 mRNA production and protein secretion of TNF-\u03b1, IL-1\u03b2, and IL-6. Application of CT mostly enhanced cytokine production as elicited by UHMWPE particles while a pronounced transient inhibitory effect on LPS-induced inflammation became evident at 24\u2009h of incubation. Human CT displayed ambivalent effects on the wear- and LPS-induced production of pro-inflammatory cytokines. Thereby, the peptide primarily upregulated particle-induced inflammation while LPS-induced cytokine secretion was temporarily attenuated in a distinct manner. It needs to be evaluated whether the pro- or anti-inflammatory action of CT contributes to its known anti-resorptive effects. Thus, the therapeutic potential of the peptide in the treatment of either particle- or endotoxin-mediated bone resorption could be determined.\n\n",
                "DataExportTag": "CAN401045",
                "QuestionID": "QID306",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "A Single Dose of the Anti-Resorptive Peptide Human Calcitonin Paradoxically Augments Particle- an...",
                "Choices": {
                    "1": {
                        "Display": "\"Inflammatory Cytokines and Immune Response\""
                    },
                    "2": {
                        "Display": "tumor_necrosis_factor, cytokine, lps, necrosis_factor, nf_kappab, alpha, tnfalpha, tnf\u03b1, lipopolysaccharide, production, inflammatory, interleukin, inos, monocyte, macrophage"
                    },
                    "3": {
                        "Display": "\"Neurological Disorders and Pain Management\""
                    },
                    "4": {
                        "Display": "alzheimer_disease, cerebral, pain, microglia, rat, neuron, neuronal, neuropathic_pain, mouse, a\u03b2, neuroinflammation, hippocampus, microglial, nerve, spinal_cord"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID342",
            "SecondaryAttribute": "Abstract 69: Possible Involvement Of Proline-rich Tyrosine Kinase 2 (pyk2) In The Pathogenesis Of...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Abstract 69: Possible Involvement Of Proline-rich Tyrosine Kinase 2 (pyk2) In The Pathogenesis Of Kawasaki Disease Introduction: Although etiology of Kawasaki disease (KD) remains elusive, a line of recent experimental studies implies that some kinds of infectious stimuli are implicate in the vasculitis through uncontrolled innate immune systems such as pattern recognition receptor (PPR)-mediated inflammatory signaling. It has already known that Candida albicans water-soluble fraction (CAWS) inducing KD-like vasculitis in mice function through PRP. Furthermore, it is reported that proline-rich tyrosine kinase (Pyk2), which is molecule involved in the PRPs-dependent signaling pathways, plays an important role in activation of NF-\u03baB. Therefore, we investigated a possible relevance of Pyk2 in the pathogenesis of KD. Methods: Pyk2-knock out (Pyk2-KO) and wild-type C57BL\/6 mice (WT) were administered CAWS to induce KD-like vasculitis. Extension of the experimental vasculitis was immunohistochemically determined with anti-MPO antibody. CAWS-stimulated NF-\u03baB activation was evaluated by quantifying nuclear translocation of NF-\u03baB p65 subunit in peritoneal macrophages isolated from PYK2-KO and wild-type mice in vitro. Cytokines and chemokines across each mice were compared by cytokine array. Results: Pyk2-KO mice didn\u2019t show any apparent defective phenotype. While marked inflammation was observed in the aortic root of CAWS-treated WT mice, such vasculitis was barely detected in CAWS-treated Pyk2-KO mice. CAWS-induced NF-\u03baB activation was also less observed in macrophages from Pyk2-KO mice. There were differences in some cytokines and chemokines production between mice. Conclusion: We speculate that Pyk2 is involved in the pathogenesis of KD. Pyk2 might be a potential therapeutic target for KD.\n\n",
                "DataExportTag": "AI623850",
                "QuestionID": "QID342",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Abstract 69: Possible Involvement Of Proline-rich Tyrosine Kinase 2 (pyk2) In The Pathogenesis Of...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunology and Infection Response\""
                    },
                    "2": {
                        "Display": "receptor, cell, activation, innate_immune, macrophage, cytokine, expression, toll_receptor, immune, mouse, infection, inflammation, immune_response, tlrs, signal"
                    },
                    "3": {
                        "Display": "\"Genomic Analysis and Disease Prediction\""
                    },
                    "4": {
                        "Display": "cell, gene, image, variant, prediction, phenotype, snp, tissue, deep_learning, gene_expression, omic, genomic, disease, analysis, mutation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID135",
            "SecondaryAttribute": "AIMS The aim of this study was to identify prognostic factors associated with contralateral nodal...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "AIMS\nThe aim of this study was to identify prognostic factors associated with contralateral nodal metastasis in cases of buccal mucosa cancers.\n\n",
                "DataExportTag": "63",
                "QuestionID": "QID135",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "AIMS The aim of this study was to identify prognostic factors associated with contralateral nodal...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID323",
            "SecondaryAttribute": "Although the study is very interesting and carefully executed, there remain some loose ends. For...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Although the study is very interesting and carefully executed, there remain some loose ends. For example, the authors have not shown the levels of VEGFR-2 expression and if it is altered in the imatinib-treated tumor endothelial cells. It is also somewhat confusing what they mean by the internal activation of VEGFR. VEGFRs are receptor tyrosine kinases that undergo transactivation through autophosphorylation in their cytoplasmic region upon external ligand binding such as VEGF.11 The mechanism of VEGF binding to its receptor VEGFR-2 in the cytoplasm is confusing and needs further explanation. Furthermore, the authors need to demonstrate the specificity of inhibition of paracrine VEGF signaling via sFlt1 in their system. Since vitronectin is known to bind to other integrins in addition to integrin \u03b1v\u03b23,12 the authors need to conclusively show that integrin \u03b1v\u03b23 is the integrin responsible for the observed activation of NF\u03baB. In addition, the authors need to show if the level of collagen VI is altered in their system. Collagen VI has also been implicated to play an important role in pericyte-associated endothelial cell maturation and endothelial cell survival. Collagen VI null mice show a reduced number of matured pericytes associated with endothelial cells and exhibit leaky tumor blood vessels along with reduced tumor size.13\n\n",
                "DataExportTag": "41",
                "QuestionID": "QID323",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Although the study is very interesting and carefully executed, there remain some loose ends. For...",
                "Choices": {
                    "1": {
                        "Display": "The importance of the work of Franco et al. lies in understanding how the tumor vasculature is stabilized. It provides newer avenues for therapeutic intervention against cancer angiogenesis, macular degeneration and corneal angiogenesis using drugs targeting endothelial cells and pericytes. A variety of therapeutic strategies using anti-VEGFR drugs to disrupt the tumor vasculature have provided limited response.7 On the contrary, simultaneous targeting of multiple receptor tyrosine kinases such as FGFR, VEGFR and PDGFR resulted in reduced tumor angiogenesis and reduced expression of the survival protein survivin.14 Thus suggesting that in addition to VEGF\/Bcl-w axis, pericytes may also provide an additional layer of protection to tumor vasculature. More research is warranted to explore these possibilities in order to develop a more efficient treatment to control cancer angiogenesis."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "\"Immunology and Autoimmune Disease Research\""
                    },
                    "2": {
                        "Display": "lymphocyte, treg_cells, mouse, regulatory, memory, subset, tregs, autoimmune, thymus, tolerance, thymic, peripheral_blood, pbmc, thymocyte, subpopulation"
                    },
                    "3": {
                        "Display": "\"Viral Vectors and Disease Research\""
                    },
                    "4": {
                        "Display": "virus, hiv, vector, gene, epstein_barr_virus, strain, alzheimer_disease, kshv, transduction, immunodeficiency_virus, antiviral, adenovirus, kaposi_sarcoma, hcmv, latency"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID206",
            "SecondaryAttribute": "AN AUGMENTED REALITY UAV-GUIDED GROUND NAVIGATION INTERFACE IMPROVE HUMAN PERFORMANCE IN MULTI-RO...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "AN AUGMENTED REALITY UAV-GUIDED GROUND NAVIGATION INTERFACE IMPROVE HUMAN PERFORMANCE IN MULTI-ROBOT TELE-OPERATION This research proposes a human-multirobot system with semi-autonomous ground robots and UAV view for contaminant localization tasks. A novel Augmented Reality based operator interface has been developed. The interface uses an over-watch camera view of the robotic environment and allows the operator to direct each robot individually or in groups. It uses an A* path planning algorithm to ensure obstacles are avoided and frees the operator for higher-level tasks. It also displays sensor information from each individual robot directly on the robot in the video view. In addition, a combined sensor view can also be displayed which helps the user pin point source information. The sensors on each robot monitor the contaminant levels and a virtual display of the levels is given to the user and allows him to direct the multiple ground robots towards the hidden target. This paper reviews the user interface and describes several initial usability tests that were performed. This research demonstrates the development of a humanmultirobot interface that has the potential to improve cooperative robots for practical applications. INTRODUCTION Multi-robot systems can often deal with tasks that are difficult for single robot. For example, teams of robots may be able to complete tasks such as multipoint surveillance, cooperative transport, and explorations in hazardous environments more efficiently. Additionally, time-critical missions may require the use of multiple robots working simultaneously to efficiently accomplish the tasks. Controlling multiple robots is a challenging humanoperator task. In multi-robot scenarios, one of the main challenges for a human operator in search and detection missions is to remotely control the semi-autonomous robots [1]. Thus, there is a need to research and develop technologies that can enable an operator to control groups of semi-autonomous robots. Most human-robot interfaces for robot control have focused on providing users data collected by the robot and giving status messages about what the robot is doing. The conventional interface consists of several separate display windows to show information from the robot. [2] is an example of a conventional display from the Idaho National Laboratory (INL). The display may require the operator to integrate information, and this may increase the operator\u2019s workload. Another example of a conventional interface for multiple robots control was designed by Humphrey et al. [3]. Operators may have high workload from needing to simultaneously integrate each multiple status bar. An alternative to conventional interface is a 3D virtual environment display based on a robot simulation. In contrast to direct interfaces, a virtual environment provides an external perspective which allows the operator to see the environment and drive the robot from viewpoints generated by the interface. Nguyen et al. [4] describe several Virtual Reality (VR) based interfaces for exploration, one of which Proceedings of the 2011 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) An Augmented Reality UAV-Guided Ground Navigation Interface Improve Human Performance in Multi-Robot Tele-operation, Lee, et al. Page 2 of 7 is Viz. Viz has shown that VR interfaces can help the user understand and analyze the robot surroundings and improve the operator situational awareness. However, in virtual environments, the operator\u2019s attention is drawn away from the actual environment which can reduce situational awareness and dynamic situations not modeled in the VR world could pose significant problems. In contrast to the virtual environment display, Augmented Reality (AR) is an advanced visualization technology which allows computer generated virtual images to merge with physical objects in real time. Unlike VR, the user enters and interacts with computer-generated 3D environments. AR allows the user to interact with the virtual images using real objects [5]. Researchers in robotics are beginning to use AR techniques in robotics because it provides direct views of the scene combined with the advantages of virtual displays for human-robot collaboration [6-8]. Communicating to robots and human, on the other hand, touch-based input may allow users to perform complex tasks in an intuitive manner [9]. Micire et al. [10] studied the control of a single agent with a multi-touch table. Moreover, a multi-touch (DREAM) controller [11, 12] using a multitouch table is developed for multi-robot command and control [13, 14]. Figure 1: A group of semi-autonomous robots is controlled using the human-multirobot interface. We have developed a system to combine virtual and augmented reality interfaces capabilities with human supervisor\u2019s ability to control the robots [15]. The role of this human-multirobot interface is to allow an operator to control group of heterogeneous robots in real time and in collaborative way. This paper presents results from a user evaluation of the real multiple robot system in which three interface conditions were evaluated (i.e. Joystick, Point-andGo, and Path Planning). Results show that the novel multirobot control (Point-and-Go and Path Planning) reduced their mission completion times compared to the traditional joystick control for target detection performance. SYSTEM DESCRIPTION The system hardware configuration is shown in Figure 1. The human-multirobot interface (see Figure 2) is a top-down view from the stationary camera. We assumed that the topdown view could be taken from a manned or unmanned aerial vehicle. Figure 2: The human-multirobot interface is a top-down view from the stationary camera. We assumed that the topdown view could be taken from a manned or unmanned aerial vehicle. Hardware Four Mindstorms NXT robots were used as the remote robots. The NXT robot includes two NXT motors with encoders used for differential drive and a third passive caster wheel to maintain balance. An infrared sensor with a 240 degree view is attached on the NXT robot (see Figure 3) to search and detect infrared beacons. A marker on top of a NXT robot is used for position reading of the multi-robot system and for viewing robot status using AR software. The NXT robots are controlled through a Bluetooth connection. Proceedings of the 2011 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) An Augmented Reality UAV-Guided Ground Navigation Interface Improve Human Performance in Multi-Robot Tele-operation, Lee, et al. Page 3 of 7 A HiTechnic infrared electronic ball was used as a contaminant source. The infrared ball was hidden by one of the decoys. The testbed was equipped with a Logitech Webcam Pro 9000 with autofocus to obtain video frames at a resolution of 1280x1024 and at a refresh rate of 10 frames\/ sec. The video was displayed on a 17\u201d liquid crystal display (LCD) computer monitors. Figure 3: A marker is attached on the top of a NXT robot. The marker is detected by AR software to measure a robot position and to generate virtual images to merge with a robot in real time. An infrared sensor with a 240 degree view is attached on the NXT robot to search and detect infrared beacons. Software The ARToolKit augmented reality system is used to determine the position and orientation of each robot. A marker is attached to the top of each robot. Client\u2010server system for data communication has developed in the testbed. The robot server is programmed to communication with NXT robots using Bluetooth. AR client delivers localization data obtained from an overhead camera to ground robots, and it displays the synthesized views. Visual C++ in Visual Studio 2008 is the programming language used for AR and robot communication API software development. Not eXactly C (NXC) in Bricx Command Center is the programming language used for NXT robot to configure the infrared sensor and robot communication. Figure 4: AR client programs share robot state information and display the synthesized view. Robot server programs read robot sensor data and send movement commands. AR INTERFACE FOR MULTI-ROBOT CONTROL This section describes three features of the interface used for user evaluation. First, we describe the Point-and-Go algorithm developed for multi-robot control and the Path Planning implemented for obstacle avoidance. Then, we describe the joystick control of the multiple robots. Finally, we explain the sensor data and robot messages visualization. Point-and-Go Mode A point\u2010and\u2010go algorithm is developed for single human operator controlling multiple robots. The operator is able to select any ground robot using a mouse left click, and then designates a goal location in the video feed from an overhead camera (see Figure 5). A navigation algorithm is developed that allows the robot turns toward the desired goal location, drive straightly toward the goal, and then stops at the target. If a robot is stuck with an obstacle, the user is able to reverse the robot using mouse right click (see Figure 5). Figure 5: Point-and-Go is a high level instruction that allows an operator to control multiple semi-autonomous robots simultaneously. Path Planning Mode An A* algorithm path planning algorithm [16] is implemented to the system. The A* algorithm allows the multiple robots to traverse to the target location with obstacle avoidance and shortest path. Proceedings of the 2011 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) An Augmented Reality UAV-Guided Ground Navigation Interface Improve Human Performance in Multi-Robot Tele-operation, Lee, et al. Page 4 of 7 Figure 6: The path planning algorithm allows the multiple robots to traverse to the target location with obstacle avoidance and shortest path. Joystick Mode A joystick (ExtremeTM 3D Pro; Logitech, California) was used to manipulate the direction in which the robot moved when in joystick mode. The joystick push-pull axe was used to control the forward and back movements for translation of the robot, and the joystick rotati\n\n",
                "DataExportTag": "AI967037",
                "QuestionID": "QID206",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "AN AUGMENTED REALITY UAV-GUIDED GROUND NAVIGATION INTERFACE IMPROVE HUMAN PERFORMANCE IN MULTI-RO...",
                "Choices": {
                    "1": {
                        "Display": "\"Unmanned Aerial and Underwater Vehicles\""
                    },
                    "2": {
                        "Display": "unmanned_aerial_vehicles, flight, mission, drone, aircraft, vehicle, unmanned_aerial, ship, autonomous, underwater, spacecraft, unmanned, vehicle_uav, landing, vessel"
                    },
                    "3": {
                        "Display": "\"Industrial Automation and Robotics\""
                    },
                    "4": {
                        "Display": "robot, computer_vision, image, inspection, speech_recognition, industrial, remote, autonomous_robot, sensor, voice, autonomous, smart, microcontroller, raspberry_pi, wireless"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID390",
            "SecondaryAttribute": "An environment-dependent transcriptional network specifies human microglia identity Of mice and m...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "An environment-dependent transcriptional network specifies human microglia identity Of mice and men's microglia Microglia are immune system cells that function in protecting and maintaining the brain. Gosselin et al. examined the epigenetics and RNA transcripts from single microglial cells and observed consistent profiles among samples despite differences in age, sex, and diagnosis. Mouse and human microglia demonstrated similar microglia-specific gene expression profiles, as well as a shared environmental response among microglia collected either immediately after surgery (ex vivo) or after culturing (in vitro). Interestingly, those genes exhibiting differences in expression between humans and mice or after culturing were often implicated in neurodegenerative diseases. Science, this issue p. eaal3222 Single-cell sequencing of brain microglia reveals ex vivo and in vitro differences in transcription. INTRODUCTION Microglia play essential roles in central nervous system homeostasis and influence diverse aspects of neuronal function, including refinement of synaptic networks and elaboration of neuromodulatory factors for memory and motor learning. Many lines of evidence indicate that dysregulation of microglial functions contributes to the pathogenesis of neurodegenerative diseases, including Alzheimer\u2019s disease and Parkinson\u2019s disease. Emerging evidence from mouse and human studies also suggests that microglia influence neurodevelopmental and psychiatric disorders such as schizophrenia and depression. Most disease risk alleles associated with neurodegenerative diseases reside in noncoding regions of the genome, requiring the delineation of functional genomic elements in the relevant human cell types to establish mechanisms of causation. The recent observation that mouse brain environment strongly influences microglia-specific gene expression has implications for understanding pathogenic responses of microglia in diseases and disorders and modeling their phenotypes in vitro. RATIONALE Although dysregulation of microglial activity is genetically linked to neurodegenerative diseases and psychiatric disorders, no systematic evaluations of human microglia gene expression or regulatory landscapes are currently available. In addition, the extent to which mice provide suitable models for human microglia is unclear. The major goals of this study were to define the transcriptomes and DNA regulatory elements of human microglia ex vivo and in vitro in comparison to the mouse and to systematically relate these features to expression of genes associated with genome-wide association study (GWAS) risk alleles or exhibiting altered expression in neurodegenerative diseases and psychiatric disorders. RESULTS We used RNA sequencing, chromatin immunoprecipitation sequencing, and assay for transposase-accessible chromatin sequencing to characterize the transcriptomes and epigenetic landscapes of human microglia isolated from surgically resected brain tissue in excess of that needed for diagnosis. Although some effects of underlying disease cannot be excluded, the overall pattern of gene expression was markedly consistent. Microglia-enriched genes were found to overlap significantly with genes exhibiting altered expression in neurodegenerative diseases and psychiatric disorders and with genes associated with a wide spectrum of disease-specific risk alleles. Human microglia gene expression was well correlated with mouse microglia gene expression, but numerous species-specific differences were also observed that included genes linked to human disease. More than half of the genes associated with noncoding GWAS risk alleles for Alzheimer\u2019s disease are preferentially expressed in microglia. DNA recognition motifs enriched at active enhancers and expression of the corresponding lineage-determining transcription factors were very similar for human and mouse microglia. Transition of human and mouse microglia from the brain to tissue culture revealed remodeling of their respective enhancer landscapes and extensive down-regulation of genes that are induced in primitive mouse macrophages following migration into the fetal brain. Treatment of microglia in vitro with transforming growth factor \u03b21 (TGF-\u03b21) had relatively modest effects in maintaining the ex vivo pattern of gene expression. A significant subset of the genes up- or down-regulated in vitro exhibited altered expression in neurodegenerative diseases and psychiatric disorders. CONCLUSION These studies identify core features of human microglial transcriptomes and epigenetic landscapes. Intersection of the microglia-specific gene signature with GWAS and transcriptomic data supports roles of microglia as both responders and contributors to disease phenotypes. The identification of an environment-sensitive program of gene expression and corresponding regulatory elements enables inference of a conserved and dynamic transcription factor network that maintains microglia identity and function. The combinations of signaling factors in the brain necessary to maintain microglia phenotypes remain largely unknown. In concert, these findings will inform efforts to generate microglia-like cells in simple and complex culture systems and understand gene-environment interactions that influence homeostatic and pathogenic functions of microglia in the human brain. Brain environment specifies gene expression in microglia. Human microglia transcriptomes and enhancer landscapes were defined ex vivo following purification from surgically resected brain tissue and in vitro after transfer to a tissue culture environment. Dynamic changes in these features enabled delineation of transcription factors controlling an environment-dependent program of gene expression that overlaps with genes that are dysregulated in brain pathologies. Microglia play essential roles in central nervous system (CNS) homeostasis and influence diverse aspects of neuronal function. However, the transcriptional mechanisms that specify human microglia phenotypes are largely unknown. We examined the transcriptomes and epigenetic landscapes of human microglia isolated from surgically resected brain tissue ex vivo and after transition to an in vitro environment. Transfer to a tissue culture environment resulted in rapid and extensive down-regulation of microglia-specific genes that were induced in primitive mouse macrophages after migration into the fetal brain. Substantial subsets of these genes exhibited altered expression in neurodegenerative and behavioral diseases and were associated with noncoding risk variants. These findings reveal an environment-dependent transcriptional network specifying microglia-specific programs of gene expression and facilitate efforts to understand the roles of microglia in human brain diseases.\n\n",
                "DataExportTag": "CAN1272688",
                "QuestionID": "QID390",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "An environment-dependent transcriptional network specifies human microglia identity Of mice and m...",
                "Choices": {
                    "1": {
                        "Display": "\"Gene Transcription and Regulation\""
                    },
                    "2": {
                        "Display": "promoter, transcription, bind, transcriptional, messenger_rna, gene, element, binding, splicing, enhancer, translation, deoxyribonucleic_acid, creb, region, transactivation"
                    },
                    "3": {
                        "Display": "\"Epigenetic Regulation and Gene Modification\""
                    },
                    "4": {
                        "Display": "pten, chromatin, histone, regulation, stress, methylation, epigenetic, autophagy, metabolic, enzyme, gene, modification, translation, acetylation, control"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID397",
            "SecondaryAttribute": "ANALYSIS IN R 3 Table 1 An overview of text analysis operations , with the R packages used in thi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "ANALYSIS IN R 3 Table 1 An overview of text analysis operations , with the R packages used in this teacher \u2019 s corner Operation R packages example alternatives Data preparation importing text Computational text analysis has become an exciting research field with many applications in communication research. It can be a difficult method to apply, however, because it requires knowledge of various techniques, and the software required to perform most of these techniques is not readily available in common statistical software packages. In this teacher\u2019s corner, we address these barriers by providing an overview of general steps and operations in a computational text analysis project, and demonstrate how each step can be performed using the R statistical software. As a popular open-source platform, R has an extensive user community that develops and maintains a wide range of text analysis packages. We show that these packages make it easy to perform advanced text analytics. With the increasing importance of computational text analysis in communication research (Boumans & Trilling, 2016; Grimmer & Stewart, 2013), many researchers face the challenge of learning how to use advanced software that enables this type of analysis. Currently, one of the most popular environments for computational methods and the emerging field of \u201cdata science\u201d 1 is the R statistical software (R Core Team, 2017). However, for researchers that are not well-versed in programming, learning how to use R can be a challenge, and performing text analysis in particular can seem daunting. In this teacher\u2019s corner, we show that performing text analysis in R is not as hard as some might fear. We provide a step-bystep introduction into the use of common techniques, with the aim of helping researchers get acquainted with computational text analysis in general, as well as getting a start at performing advanced text analysis studies in R. R is a free, open-source, cross-platform programming environment. In contrast to most programming languages, R was specifically designed for statistical analysis, which makes it highly suitable for data science applications. Although the learning curve for programming with R can be steep, especially for people without prior programming experience, the tools now available for carrying out text analysis in R make it easy to perform powerful, cutting-edge text analytics using only a few simple commands. One of the keys to TEXT ANALYSIS IN R 2 R\u2019s explosive growth (Fox & Leanage, 2016; TIOBE, 2017) has been its densely populated collection of extension software libraries, known in R terminology as packages, supplied and maintained by R\u2019s extensive user community. Each package extends the functionality of the base R language and core packages, and in addition to functions and data must include documentation and examples, often in the form of vignettes demonstrating the use of the package. The best-known package repository, the Comprehensive R Archive Network (CRAN), currently has over 10,000 packages that are published, and which have gone through an extensive screening for procedural conformity and cross-platform compatibility before being accepted by the archive.2 R thus features a wide range of inter-compatible packages, maintained and continuously updated by scholars, practitioners, and projects such as RStudio and rOpenSci. Furthermore, these packages may be installed easily and safely from within the R environment using a single command. R thus provides a solid bridge for developers and users of new analysis tools to meet, making it a very suitable programming environment for scientific collaboration. Text analysis in particular has become well established in R. There is a vast collection of dedicated text processing and text analysis packages, from low-level string operations (Gagolewski, 2017) to advanced text modeling techniques such as fitting Latent Dirichlet Allocation models (Blei, Ng, & Jordan, 2003; Roberts et al., 2014)\u2014nearly 50 packages in total at our last count. Furthermore, there is an increasing effort among developers to cooperate and coordinate, such as the rOpenSci special interest group.3 One of the main advantages of performing text analysis in R is that it is often possible, and relatively easy, to switch between different packages or to combine them. Recent efforts among the R text analysis developers\u2019 community are designed to promote this interoperability to maximize flexibility and choice among users.4 As a result, learning the basics for text analysis in R provides access to a wide range of advanced text analysis features. Structure of this Teacher\u2019s Corner This teacher\u2019s corner covers the most common steps for performing text analysis in R, from data preparation to analysis, and provides easy to replicate example code to perform each step. The example code is also digitally available in our online appendix, which is updated over time.5 We focus primarily on bag-of-words text analysis approaches, meaning that only the frequencies of words per text are used and word positions are ignored. Although this drastically simplifies text content, research and many real-world applications show that word frequencies alone contain sufficient information for many types of analysis (Grimmer & Stewart, 2013). Table 1 presents an overview of the text analysis operations that we address, categorized in three sections. In the data preparation section we discuss five steps to prepare texts for analysis. The first step, importing text, covers the functions for reading texts from various types of file formats (e.g., txt, csv, pdf) into a raw text corpus in R. The steps string operations and preprocessing cover techniques for manipulating raw texts and processing them into tokens (i.e., units of text, such as words or word stems). The tokens are then used for creating the document-term matrix (DTM), which is a common format for representing a bag-of-words type corpus, that is used by many R text analysis packages. Other nonbag-of-words formats, such as the tokenlist, are briefly touched upon in the advanced topics section. Finally, it is a common step to filter and weight the terms in the DTM. These TEXT ANALYSIS IN R 3 Table 1 An overview of text analysis operations, with the R packages used in this teacher\u2019s corner Operation R packages example alternatives Data preparation importing text readtext jsonlite, XML, antiword, readxl, pdftools string operations stringi stringr preprocessing quanteda stringi, tokenizers, snowballC, tm, etc. document-term matrix (DTM) quanteda tm, tidytext, Matrix filtering and weighting quanteda tm, tidytext, Matrix Analysis dictionary quanteda tm, tidytext, koRpus, corpustools supervised machine learning quanteda RTextTools, kerasR, austin unsupervised machine learning topicmodels quanteda, stm, austin, text2vec text statistics quanteda koRpus, corpustools, textreuse Advanced topics advanced NLP spacyr coreNLP, cleanNLP, koRpus word positions and syntax corpustools quanteda, tidytext, koRpus Figure 1 . Order of text analysis operations for data preparation and analysis. dtm files web pages etc. R text corpus tokens tokenlist\n\n",
                "DataExportTag": "AI788741",
                "QuestionID": "QID397",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "ANALYSIS IN R 3 Table 1 An overview of text analysis operations , with the R packages used in thi...",
                "Choices": {
                    "1": {
                        "Display": "\"Natural Language Processing and Text Annotation\""
                    },
                    "2": {
                        "Display": "annotation, tag, corpus, annotate, annotator, tagging, tagger, metadata, labeling, treebank, text, genre, annotate_corpus, pos_tag, language"
                    },
                    "3": {
                        "Display": "\"Natural Language Processing and Semantic Analysis\""
                    },
                    "4": {
                        "Display": "word, sense, sense_disambiguation, semantic, verb, lexical, wordnet, wsd, meaning, dictionary, lexicon, language, noun, collocation, relation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID343",
            "SecondaryAttribute": "ANALYTIC FRAMEWORK FOR SOCIAL OPINION PREDICTION OF ONLINE NEWS BASED ON SENTIMENT ANALYSIS In to...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "ANALYTIC FRAMEWORK FOR SOCIAL OPINION PREDICTION OF ONLINE NEWS BASED ON SENTIMENT ANALYSIS In today\u2019s world everything available on the internet because of the digital era. With the fast development of web, peoples spent their most time on internet. Recently, in social platforms new function introduced in which user may select one emotion out of many choices, to express their opinion more precisely. For this tasks the social emotion mining technique is used. This is the most important and widely used in social platforms and businesses alike. In history, lots of work has been done of social emotion mining from author\u2019s perspectives. Recent research focuses on reader\u2019s emotions with news articles. In existing system Social Opinion Mining is developed which predict the reader\u2019s emotions of news based on the social opinion network. The construction of opinion network based on the semantic distance between two words. In existing system model for analysis of news the system do various comparisons which is time consuming. Therefore, in proposed system we are going to develop an Intellectual Sentiment Analytic Model for the social opinion prediction. Due to Sentiment analysis the Intellectual Sentiment Analytic Model reduced time to large extent and effective results constructed as compared to the state of the art model. IndexTerms Social Emotion Mining, Social opinion prediction, News Article, Preprocessing, Sentiment Analysis, Intellectual Sentiment Analytical model.  I.INTRODUCTION It is a fact that social media has tremendously changed the way people interact and carry on with their every-day lives. The internet has becoming a lifeblood in every peoples life, majority of people who are online spend most of their time on social media sites. One of the most important service of social media is News delivery. This motivate many social websites. The websites providing a way in which user will share their opinion after reading the news article. With the increased development of web, numerous documents are evoke with usergenerated emotions such as happiness, sadness and surprise. On research it has been found that emotional labels widely used in social web services. The Social Emotion Mining is used to understand the emotions of users. The Social Emotion Mining understand and predict the user\u201fs emotions towards online documents. In history many models and methods have been proposed and well discussed to deal with social emotion mining. In existing work the Social Opinion Mining model used for social opinion prediction. This model based on the construction of real-time opinion network. On studies, it is clear that the performance of social opinion mining model is more stable as compared to the previous models i.e. Emotion Term, Emotion Topic Model, Sentiment Topic Model, Affective Topic Model and Contextual sentiment topic model. In the existing system, the Social Opinion Mining Model for the analysis of News do the comparison of various vector conversions which is time-consuming. Therefore in a proposed system, we are going to develop an Intellectual Sentiment Analytic Model for the interceptive matching of the proposed news article with the existing dataset. The intellectual Sentiment Analytic Model constructed based on the sentiment analysis of news article, in which parsing, data cleaning and processing done over the news article. Due to sentimental analysis time will reduce to large extent and effective results will be constructed. The accuracy of the prediction of social opinion prediction is enriched as compared to the social opinion mining model. II. SOCIAL EMOTION MINING In Data\/Text Mining the term \"mining\" is an analogy to the resource extraction process of mining for rare minerals. The Social Emotion Mining is a process of understanding and predicting the hidden emotions of the social user towards online contents known as social emotion mining. It has become increasingly important to both social platform and businesses alike to better understand their user and leverage the learned knowledge to their advantage. In 2016 Facebook introduced the function, where the user may select one emotion out of many choices to express their opinion more precisely. The similar function allowed in Chinese news portal i.e. www.sina.com. This is one of the largest news websites in China. Sina websites contain breaking news, current events and useful information on life, culture, and travel in China. Large numbers of people concerned about a hot news online. In preliminary work there are many models have been studied and proposed for the Social emotion Mining. III. RELATED WORK There are two different perspectives to mined emotions from the text: From the perspective of writer and reader. Past work focused on the writer\u201fs perspectives i.e. accurately model the connections between word and emotions. Recent research focuses on reader\u201fs emotions with the News article. The sentiment analysis of reader\u201fs emotions is more meaningful as compared to classical sentiment analysis from writer\u201fs perspectives. Due to this prediction of reader\u201fs emotions has become a promising research area. Many models have been proposed and wellstudied in previous work to deal with social emotion mining. C. Strapparava et al. proposed a SWAT model [1] In SWAT in which a word emotion mapping dictionary has been constructed, then used this to detect emotions of unlabeled news headings. \u00a9 2018 JETIR May 2018, Volume 5, Issue 5 www.jetir.org (ISSN-2349-5162) JETIR1805577 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 951 Shenghua Bao et al. proposed a Joint Emotion-Topic Model [2], [3] in which first developed the Emotion \u2013Term model which is the wordlevel model used to find word emotion associations. In Emotion-Term model different senses can be evoked from the same word. To overcome this drawback Joint Emotion-Topic Model which is topic-level is proposed. The Joint Emotion-Topic model established additional emotion generation layer to Latent Dirichlet Allocation. Yanghui Rao et al. proposed two sentiment topic models [4], The Multi-label Supervised Topic Model (MSTM) find the association between latent topics with readers emotions. The Sentiment Latent Topic Model (SLTM) which invent purposeful latent topics from which social emotions produced. Yanghui Rao et al. proposed Affective Topic Model [5] (ATM) which used to develop a social emotion lexicon and to distinguish social emotions of documents. Yanghui Rao et al. proposed Contextual Sentiment Topic Model [7] (CSTM) The CSTM model solved the problem of adaptive social emotion classification by classifying reader\u201fs emotions across different context IV. EXISTING SYSTEM Xintong Li et al. proposed Social Opinion Mining (SOM) Model [8] which is constructed based on the social opinion network, which is used for social opinion prediction. Compared with the previous model the SOM model treats the news content and emotions distribution jointly in opinion network. Kim and Hovy [11] describe an opinion can represent with a quadruple i.e. Topic, Holder, Claim, and Sentiment. This kind of social opinion can be represented with a quadruple <event, f, s, t> where an event is a social event; f is the text feature set of the social event; s is the result of voting and t is the time when this social event occurred. The Social Opinion Mining model aims to predict s based on the social opinion quadruples. The following Figure shows the flowchart of the existing model i.e. Social Opinion Mining model. Figure 1. Social Opinion Mining Model 4.1 Working of Social Opinion Mining Model The Social opinion mining model predicts social opinions by measuring the semantic similarity between events. In this model, the opinion network is constructed based on the semantic distance. The Social Opinion Mining model treats the news content and emotion distributing as a whole opinion structure. The following are the steps of Social Opinion Mining Model. 1. Word Extraction The first step of social opinion model is word extraction. Here the raw feature (f) of social event is considered as the bag of words. For measuring the importance of a word in corpus the term frequency (TF) is used. TF (t) = (Number of times term t appears in a document) \/ (Total number of terms in the document). Start\n\n",
                "DataExportTag": "AI1061860",
                "QuestionID": "QID343",
                "QuestionType": "TE",
                "Selector": "SL",
                "QuestionDescription": "ANALYTIC FRAMEWORK FOR SOCIAL OPINION PREDICTION OF ONLINE NEWS BASED ON SENTIMENT ANALYSIS In to...",
                "Choices": {
                    "1": {
                        "Display": "\"Cybersecurity and Forensic Investigation\""
                    },
                    "2": {
                        "Display": "crime, authentication, forensic, criminal, biometric, password, police, deception, signature, fingerprint, transaction, fraud, security, fake, law_enforcement"
                    },
                    "3": {
                        "Display": "\"Child Violence and Artificial Intelligence in Spanish Language\""
                    },
                    "4": {
                        "Display": "la, el, para, child, en_el, los, violence, como, del, en, se, student, una, esta, inteligencia_artificial"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID219",
            "SecondaryAttribute": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comprehensive theory for how anticipatory thinking capabilities emerge from cognitive processes. Event cognition describes some human anticipatory thinking capabilities, but is not integrated with general theories of cognition. We use Event Segmentation Theory to motivate a theoretical account for how the Soar cognitive architecture and the Common Model of Cognition can be extended to support event cognition, and in turn account for anticipatory thinking processes and reasoning. Current cognitive architectures appear to require additional mechanisms to create computational models implementing this theoretical account. Anticipatory thinking (AT) is an emergent cognitive functionality. AT has been described as the ability to proactively guide attention and take preparatory action (Klein, Snowden, and Pin 2011). We propose the development of a cognitive theory of human AT functionality based on the combination of event cognition research and research on cognitive architecture. A general cognitive theory of human AT could predict how human AT changes as a result of specific training, experience, environments, and\/or access to different kinds of knowledge. Additionally, with a theory of how AT is realized in human cognition, AT can be implemented in artificial systems with similar computational structure. Event cognition research studies the human ability to perceive, understand, and remember everyday events (Radvansky and Zacks 2014). This research has the potential to provide insight into how AT is realized in human cognition. Event cognition research hypothesizes that humans simultaneously perceive and predict events to guide attention in real-time and also use the same mental representations both for guiding action and comprehending the actions of others (Richmond and Zacks 2017). We propose that these properties of human event cognition are also core aspects of AT. While there is a neuro-physiological account for some aspects of event cognition (Franklin et al. 2019), event cognition is not currently integrated with a general theory of cognition. Such an integration would allow an understanding of how additional cognitive processes enable the decision making and response preparation necessary for functional AT. Copyright c \u00a9 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0) Including mechanisms for human-like event cognition in cognitive architectures can provide such an integration to better understand how human AT functionality emerges from cognitive processes. Cognitive architectures are theories for the fixed computational mechanisms that underlie cognition. While many architectures initially made different and conflicting assumptions or described isolated aspects of cognition, over time a consensus has emerged. This consensus is formalized through the Common Model of Cognition, which is a theoretical specification of the computational processes underlying cognition (Laird, Lebiere, and Rosenbloom 2017). Extending the Common Model to include event cognition provides a model for how AT is realized in human-like cognition. Event Segmentation Theory With support from observations of human behavior (Eisenberg, Zacks, and Flores 2018), memory (Sargent et al. 2013), and brain activity (Baldassano et al. 2017), Event Segmentation Theory (EST) has become the dominant theory for event cognition. It provides the process model depicted in Figure 1. The theory is that humans understand their experience in terms of discrete segments of experience called events (Zacks and Swallow 2007). Similarly to AT as a form of sense making, the segmentation of experience into events is considered part of ongoing comprehension. The theory proposes that humans use mental models for events. These models are divided into event models describing specific situations and event schemas describing the Figure 1: The structure of the Event Segmentation Theory process model. (Reproduced from (Zacks et al. 2007).) commonalities for a given type or class of event (Radvansky and Zacks 2011). A mental model is an abstract representation of a situation used for reasoning. It is composed of individual elements (such as entities and relations) that can be rearranged and that are grounded to perceptual representations. An example of a mental model is representation of an animal in terms of an arrangement of body parts. An event model is a mental model for a specific event. Event models are entities and relations describing a particular span of space and time, but usually in a single location. Event models include labels, spatial relations, and relations that convey a temporal ordering. An event schema is a mental model for a class of event models, where multiple event model instances belong to the same event schema. As an example, a specific memory for having watched a film is an event model while an understanding for how a visit to the theater generally proceeds is an event schema. Event models are created by specializing event schemas to a set of observations. Both representations contain causal relations between changes. During everyday tasks, event models predict changes to the current situation and guide perceptual processing (Zacks et al. 2007). For example, predictive-looking describes the human behavior of looking to where changes are expected to occur. This ability is diminished near the boundaries between events (Eisenberg, Zacks, and Flores 2018). We use the term working event model to refer to event models used to describe the current situation (Radvansky 2012).1 As shown in Figure 1, when a prediction fails, a prediction error is detected and signals that the working event model does not match the situation. In this case, humans create a new event model to interpret the situation by retrieving an event schema that matches to recent sensory input and creating new expectations. The EST process model focuses on descriptions of ongoing perception for a directly-experienced event. Anticipatory thinking appears to require reasoning that includes expectations for future events beyond short-term expectations for the currently-experienced event, which motivates our account of event cognition using a cognitive architecture. Event Cognition in Soar Cognitive architectures are computational models for the fixed mechanisms and processes that underlie cognition. These architectures act as theories for the functionality provided by different memory systems and cognitive processes. They also can be used to implement artificial cognitive systems. However, these architectures do not currently exhibit the event cognition functionality found in humans. EST specifies representations of events, but does not describe how (together with other mental models) they are encoded, stored, or retrieved from memory systems, nor the reasoning processes that use them. Cognitive architectures can extend event cognition theory by including the memory systems and reasoning that EST lacks. An intriguing possiIn EST, event models are hierarchical. Thus, a single working event model describes the current situation, but it can contain nested sub-events that are event models for smaller space and\/or shorter segments of time. Figure 2: The structure of the Soar Cognitive Architecture. bility is to explore the integration of EST with the Common Model of Cognition. Unfortunately, due to its abstract nature, the Common Model does not provide the level of detail necessary for implementation of running computational models. Instead, we use Soar, an architecture consistent with the Common Model, as a model for how cognitive architectures (and, more abstractly, the Common Model) can realize event cognition functionality. Soar models cognition as a series of deliberate actions that perform reasoning steps, retrievals from long-term memories (episodic or semantic), or motor actions (Laird 2012). The actions are initiated by knowledge retrieved from procedural memory, based on the contents of working memory. Working memory contains a symbolic representation of the current situation (derived from perception and internal reasoning), current goals, and intended actions. A cognitive cycle, which consists of processing input, a deliberate decision, and output to the motor system, maps onto approximately 50 ms of human behavior. This low-latency perception and action cycle provides reactivity to both changes in perception and knowledge retrieved from long-term memory. Complex behavior arises from a sequence of cognitive cycles. Figure 2 shows Soar\u2019s structure. To theoretically model event cognition phenomena using Soar, we map the different mental representations specified by EST to Soar\u2019s memory systems. Both event schemas and event models contain relational information and lack perceptual detail. They contain entities and relations for describing an event, which are directly supported by the memory systems of Soar. We assume that event schemas and models have relations depicting changes over time, allowing for representation of future state using these relations. The association of event schemas and event models to the memory systems of Soar is depicted in Figure 3. The working event model is grounded to ongoing action and perception. It is also used in reasoning about the current situation. To provide this functionality, it must be composed of working memory structures and also representations of perception and action. Figure 3 depicts the working event model within working memory, but specifically as including the representations for perception and control. Working Event Model Retrieved Event Models Event Schemas Previous Event Models Reasoning Strategies Figure 3: Soar\u2019s memory systems populated with event cognition knowledge. In contrast, event models representing prior situations and event schemas require long-term storage and need to be stored within the long-term declarative memory systems in Soar. Even\n\n",
                "DataExportTag": "AI226167",
                "QuestionID": "QID219",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Cognitive Theory and Conceptual Modelling\""
                    },
                    "2": {
                        "Display": "cognitive, theory, concept, conceptual, human, interpretation, symbolic, structure, complex, abstraction, interaction, modelling, component, situation, description"
                    },
                    "3": {
                        "Display": "\"Explainable Artificial Intelligence and Automation Testing\""
                    },
                    "4": {
                        "Display": "explanation, testing, prediction, validation, user, trust, automate, developer, interpretability, safety, black_box, explainable, xai, case, automation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID280",
            "SecondaryAttribute": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comprehensive theory for how anticipatory thinking capabilities emerge from cognitive processes. Event cognition describes some human anticipatory thinking capabilities, but is not integrated with general theories of cognition. We use Event Segmentation Theory to motivate a theoretical account for how the Soar cognitive architecture and the Common Model of Cognition can be extended to support event cognition, and in turn account for anticipatory thinking processes and reasoning. Current cognitive architectures appear to require additional mechanisms to create computational models implementing this theoretical account. Anticipatory thinking (AT) is an emergent cognitive functionality. AT has been described as the ability to proactively guide attention and take preparatory action (Klein, Snowden, and Pin 2011). We propose the development of a cognitive theory of human AT functionality based on the combination of event cognition research and research on cognitive architecture. A general cognitive theory of human AT could predict how human AT changes as a result of specific training, experience, environments, and\/or access to different kinds of knowledge. Additionally, with a theory of how AT is realized in human cognition, AT can be implemented in artificial systems with similar computational structure. Event cognition research studies the human ability to perceive, understand, and remember everyday events (Radvansky and Zacks 2014). This research has the potential to provide insight into how AT is realized in human cognition. Event cognition research hypothesizes that humans simultaneously perceive and predict events to guide attention in real-time and also use the same mental representations both for guiding action and comprehending the actions of others (Richmond and Zacks 2017). We propose that these properties of human event cognition are also core aspects of AT. While there is a neuro-physiological account for some aspects of event cognition (Franklin et al. 2019), event cognition is not currently integrated with a general theory of cognition. Such an integration would allow an understanding of how additional cognitive processes enable the decision making and response preparation necessary for functional AT. Copyright c \u00a9 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0) Including mechanisms for human-like event cognition in cognitive architectures can provide such an integration to better understand how human AT functionality emerges from cognitive processes. Cognitive architectures are theories for the fixed computational mechanisms that underlie cognition. While many architectures initially made different and conflicting assumptions or described isolated aspects of cognition, over time a consensus has emerged. This consensus is formalized through the Common Model of Cognition, which is a theoretical specification of the computational processes underlying cognition (Laird, Lebiere, and Rosenbloom 2017). Extending the Common Model to include event cognition provides a model for how AT is realized in human-like cognition. Event Segmentation Theory With support from observations of human behavior (Eisenberg, Zacks, and Flores 2018), memory (Sargent et al. 2013), and brain activity (Baldassano et al. 2017), Event Segmentation Theory (EST) has become the dominant theory for event cognition. It provides the process model depicted in Figure 1. The theory is that humans understand their experience in terms of discrete segments of experience called events (Zacks and Swallow 2007). Similarly to AT as a form of sense making, the segmentation of experience into events is considered part of ongoing comprehension. The theory proposes that humans use mental models for events. These models are divided into event models describing specific situations and event schemas describing the Figure 1: The structure of the Event Segmentation Theory process model. (Reproduced from (Zacks et al. 2007).) commonalities for a given type or class of event (Radvansky and Zacks 2011). A mental model is an abstract representation of a situation used for reasoning. It is composed of individual elements (such as entities and relations) that can be rearranged and that are grounded to perceptual representations. An example of a mental model is representation of an animal in terms of an arrangement of body parts. An event model is a mental model for a specific event. Event models are entities and relations describing a particular span of space and time, but usually in a single location. Event models include labels, spatial relations, and relations that convey a temporal ordering. An event schema is a mental model for a class of event models, where multiple event model instances belong to the same event schema. As an example, a specific memory for having watched a film is an event model while an understanding for how a visit to the theater generally proceeds is an event schema. Event models are created by specializing event schemas to a set of observations. Both representations contain causal relations between changes. During everyday tasks, event models predict changes to the current situation and guide perceptual processing (Zacks et al. 2007). For example, predictive-looking describes the human behavior of looking to where changes are expected to occur. This ability is diminished near the boundaries between events (Eisenberg, Zacks, and Flores 2018). We use the term working event model to refer to event models used to describe the current situation (Radvansky 2012).1 As shown in Figure 1, when a prediction fails, a prediction error is detected and signals that the working event model does not match the situation. In this case, humans create a new event model to interpret the situation by retrieving an event schema that matches to recent sensory input and creating new expectations. The EST process model focuses on descriptions of ongoing perception for a directly-experienced event. Anticipatory thinking appears to require reasoning that includes expectations for future events beyond short-term expectations for the currently-experienced event, which motivates our account of event cognition using a cognitive architecture. Event Cognition in Soar Cognitive architectures are computational models for the fixed mechanisms and processes that underlie cognition. These architectures act as theories for the functionality provided by different memory systems and cognitive processes. They also can be used to implement artificial cognitive systems. However, these architectures do not currently exhibit the event cognition functionality found in humans. EST specifies representations of events, but does not describe how (together with other mental models) they are encoded, stored, or retrieved from memory systems, nor the reasoning processes that use them. Cognitive architectures can extend event cognition theory by including the memory systems and reasoning that EST lacks. An intriguing possiIn EST, event models are hierarchical. Thus, a single working event model describes the current situation, but it can contain nested sub-events that are event models for smaller space and\/or shorter segments of time. Figure 2: The structure of the Soar Cognitive Architecture. bility is to explore the integration of EST with the Common Model of Cognition. Unfortunately, due to its abstract nature, the Common Model does not provide the level of detail necessary for implementation of running computational models. Instead, we use Soar, an architecture consistent with the Common Model, as a model for how cognitive architectures (and, more abstractly, the Common Model) can realize event cognition functionality. Soar models cognition as a series of deliberate actions that perform reasoning steps, retrievals from long-term memories (episodic or semantic), or motor actions (Laird 2012). The actions are initiated by knowledge retrieved from procedural memory, based on the contents of working memory. Working memory contains a symbolic representation of the current situation (derived from perception and internal reasoning), current goals, and intended actions. A cognitive cycle, which consists of processing input, a deliberate decision, and output to the motor system, maps onto approximately 50 ms of human behavior. This low-latency perception and action cycle provides reactivity to both changes in perception and knowledge retrieved from long-term memory. Complex behavior arises from a sequence of cognitive cycles. Figure 2 shows Soar\u2019s structure. To theoretically model event cognition phenomena using Soar, we map the different mental representations specified by EST to Soar\u2019s memory systems. Both event schemas and event models contain relational information and lack perceptual detail. They contain entities and relations for describing an event, which are directly supported by the memory systems of Soar. We assume that event schemas and models have relations depicting changes over time, allowing for representation of future state using these relations. The association of event schemas and event models to the memory systems of Soar is depicted in Figure 3. The working event model is grounded to ongoing action and perception. It is also used in reasoning about the current situation. To provide this functionality, it must be composed of working memory structures and also representations of perception and action. Figure 3 depicts the working event model within working memory, but specifically as including the representations for perception and control. Working Event Model Retrieved Event Models Event Schemas Previous Event Models Reasoning Strategies Figure 3: Soar\u2019s memory systems populated with event cognition knowledge. In contrast, event models representing prior situations and event schemas require long-term storage and need to be stored within the long-term declarative memory systems in Soar. Even\n\n",
                "DataExportTag": "AI226167",
                "QuestionID": "QID280",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Cognitive Theory and Conceptual Modelling\""
                    },
                    "2": {
                        "Display": "cognitive, theory, concept, conceptual, human, interpretation, symbolic, structure, complex, abstraction, interaction, modelling, component, situation, description"
                    },
                    "3": {
                        "Display": "\"Explainable Artificial Intelligence and Automation Testing\""
                    },
                    "4": {
                        "Display": "explanation, testing, prediction, validation, user, trust, automate, developer, interpretability, safety, black_box, explainable, xai, case, automation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID409",
            "SecondaryAttribute": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comprehensive theory for how anticipatory thinking capabilities emerge from cognitive processes. Event cognition describes some human anticipatory thinking capabilities, but is not integrated with general theories of cognition. We use Event Segmentation Theory to motivate a theoretical account for how the Soar cognitive architecture and the Common Model of Cognition can be extended to support event cognition, and in turn account for anticipatory thinking processes and reasoning. Current cognitive architectures appear to require additional mechanisms to create computational models implementing this theoretical account. Anticipatory thinking (AT) is an emergent cognitive functionality. AT has been described as the ability to proactively guide attention and take preparatory action (Klein, Snowden, and Pin 2011). We propose the development of a cognitive theory of human AT functionality based on the combination of event cognition research and research on cognitive architecture. A general cognitive theory of human AT could predict how human AT changes as a result of specific training, experience, environments, and\/or access to different kinds of knowledge. Additionally, with a theory of how AT is realized in human cognition, AT can be implemented in artificial systems with similar computational structure. Event cognition research studies the human ability to perceive, understand, and remember everyday events (Radvansky and Zacks 2014). This research has the potential to provide insight into how AT is realized in human cognition. Event cognition research hypothesizes that humans simultaneously perceive and predict events to guide attention in real-time and also use the same mental representations both for guiding action and comprehending the actions of others (Richmond and Zacks 2017). We propose that these properties of human event cognition are also core aspects of AT. While there is a neuro-physiological account for some aspects of event cognition (Franklin et al. 2019), event cognition is not currently integrated with a general theory of cognition. Such an integration would allow an understanding of how additional cognitive processes enable the decision making and response preparation necessary for functional AT. Copyright c \u00a9 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0) Including mechanisms for human-like event cognition in cognitive architectures can provide such an integration to better understand how human AT functionality emerges from cognitive processes. Cognitive architectures are theories for the fixed computational mechanisms that underlie cognition. While many architectures initially made different and conflicting assumptions or described isolated aspects of cognition, over time a consensus has emerged. This consensus is formalized through the Common Model of Cognition, which is a theoretical specification of the computational processes underlying cognition (Laird, Lebiere, and Rosenbloom 2017). Extending the Common Model to include event cognition provides a model for how AT is realized in human-like cognition. Event Segmentation Theory With support from observations of human behavior (Eisenberg, Zacks, and Flores 2018), memory (Sargent et al. 2013), and brain activity (Baldassano et al. 2017), Event Segmentation Theory (EST) has become the dominant theory for event cognition. It provides the process model depicted in Figure 1. The theory is that humans understand their experience in terms of discrete segments of experience called events (Zacks and Swallow 2007). Similarly to AT as a form of sense making, the segmentation of experience into events is considered part of ongoing comprehension. The theory proposes that humans use mental models for events. These models are divided into event models describing specific situations and event schemas describing the Figure 1: The structure of the Event Segmentation Theory process model. (Reproduced from (Zacks et al. 2007).) commonalities for a given type or class of event (Radvansky and Zacks 2011). A mental model is an abstract representation of a situation used for reasoning. It is composed of individual elements (such as entities and relations) that can be rearranged and that are grounded to perceptual representations. An example of a mental model is representation of an animal in terms of an arrangement of body parts. An event model is a mental model for a specific event. Event models are entities and relations describing a particular span of space and time, but usually in a single location. Event models include labels, spatial relations, and relations that convey a temporal ordering. An event schema is a mental model for a class of event models, where multiple event model instances belong to the same event schema. As an example, a specific memory for having watched a film is an event model while an understanding for how a visit to the theater generally proceeds is an event schema. Event models are created by specializing event schemas to a set of observations. Both representations contain causal relations between changes. During everyday tasks, event models predict changes to the current situation and guide perceptual processing (Zacks et al. 2007). For example, predictive-looking describes the human behavior of looking to where changes are expected to occur. This ability is diminished near the boundaries between events (Eisenberg, Zacks, and Flores 2018). We use the term working event model to refer to event models used to describe the current situation (Radvansky 2012).1 As shown in Figure 1, when a prediction fails, a prediction error is detected and signals that the working event model does not match the situation. In this case, humans create a new event model to interpret the situation by retrieving an event schema that matches to recent sensory input and creating new expectations. The EST process model focuses on descriptions of ongoing perception for a directly-experienced event. Anticipatory thinking appears to require reasoning that includes expectations for future events beyond short-term expectations for the currently-experienced event, which motivates our account of event cognition using a cognitive architecture. Event Cognition in Soar Cognitive architectures are computational models for the fixed mechanisms and processes that underlie cognition. These architectures act as theories for the functionality provided by different memory systems and cognitive processes. They also can be used to implement artificial cognitive systems. However, these architectures do not currently exhibit the event cognition functionality found in humans. EST specifies representations of events, but does not describe how (together with other mental models) they are encoded, stored, or retrieved from memory systems, nor the reasoning processes that use them. Cognitive architectures can extend event cognition theory by including the memory systems and reasoning that EST lacks. An intriguing possiIn EST, event models are hierarchical. Thus, a single working event model describes the current situation, but it can contain nested sub-events that are event models for smaller space and\/or shorter segments of time. Figure 2: The structure of the Soar Cognitive Architecture. bility is to explore the integration of EST with the Common Model of Cognition. Unfortunately, due to its abstract nature, the Common Model does not provide the level of detail necessary for implementation of running computational models. Instead, we use Soar, an architecture consistent with the Common Model, as a model for how cognitive architectures (and, more abstractly, the Common Model) can realize event cognition functionality. Soar models cognition as a series of deliberate actions that perform reasoning steps, retrievals from long-term memories (episodic or semantic), or motor actions (Laird 2012). The actions are initiated by knowledge retrieved from procedural memory, based on the contents of working memory. Working memory contains a symbolic representation of the current situation (derived from perception and internal reasoning), current goals, and intended actions. A cognitive cycle, which consists of processing input, a deliberate decision, and output to the motor system, maps onto approximately 50 ms of human behavior. This low-latency perception and action cycle provides reactivity to both changes in perception and knowledge retrieved from long-term memory. Complex behavior arises from a sequence of cognitive cycles. Figure 2 shows Soar\u2019s structure. To theoretically model event cognition phenomena using Soar, we map the different mental representations specified by EST to Soar\u2019s memory systems. Both event schemas and event models contain relational information and lack perceptual detail. They contain entities and relations for describing an event, which are directly supported by the memory systems of Soar. We assume that event schemas and models have relations depicting changes over time, allowing for representation of future state using these relations. The association of event schemas and event models to the memory systems of Soar is depicted in Figure 3. The working event model is grounded to ongoing action and perception. It is also used in reasoning about the current situation. To provide this functionality, it must be composed of working memory structures and also representations of perception and action. Figure 3 depicts the working event model within working memory, but specifically as including the representations for perception and control. Working Event Model Retrieved Event Models Event Schemas Previous Event Models Reasoning Strategies Figure 3: Soar\u2019s memory systems populated with event cognition knowledge. In contrast, event models representing prior situations and event schemas require long-term storage and need to be stored within the long-term declarative memory systems in Soar. Even\n\n",
                "DataExportTag": "AI226167",
                "QuestionID": "QID409",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Anticipatory Thinking in Cognitive Architectures with Event Cognition Mechanisms There is no comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Cognitive Theory and Conceptual Modelling\""
                    },
                    "2": {
                        "Display": "cognitive, theory, concept, conceptual, human, interpretation, symbolic, structure, complex, abstraction, interaction, modelling, component, situation, description"
                    },
                    "3": {
                        "Display": "\"Explainable Artificial Intelligence and Automation Testing\""
                    },
                    "4": {
                        "Display": "explanation, testing, prediction, validation, user, trust, automate, developer, interpretability, safety, black_box, explainable, xai, case, automation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID149",
            "SecondaryAttribute": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,775s Excluded: 5,775 Full Text Review: 559 Articles Excluded: 539 Articles Not an included study design: 260 No intervention: 108 Outpatient setting: 71 Not stewardship: 57 Other: 43 Hand Search: 15 Articles Included: 35 Studies Evidence-based Synthesis Program (ESP) Results \u2022 Existing Cochrane Review (Davey 2013) \u2022 Included 89 studies published through 2009 \u2022 Focused on prescribing outcomes \u2022 Categorized interventions as: \u2022 Persuasive (education, reminders, audit and feedback) \u2022 Restrictive (order forms, formulary restriction) \u2022 Structural (EMR, decision support systems) Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Mortality \u2022 Interventions to increase guideline compliance for patients with community acquired pneumonia reduced mortality: \u2022 (4 studies, RR 0.89 [0.82, 0.97]) \u2022 Interventions to increase effective prescribing (3 studies) and interventions to decrease excessive prescribing (11 studies) were not significantly associated with mortality Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Other \u2022 Interventions to decrease excessive prescribing : \u2022 No effect length of stay \u2022 (6 studies, mean difference, -0.04 days [-0.34, 0.25]) \u2022 Associated with increased hospital readmissions \u2022 (5 studies, 1.26 [1.02, 1.57]) \u2022 Reduction in C. difficile infection: \u2022 15% to 65% at one month post-intervention \u2022 4 interrupted time series studies Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Prescribing Outcomes \u2022 Calculated median effect size with all prescribing outcomes; positive sign = change in intended direction \u2022 Persuasive, restrictive, and structural interventions: \u2022 Associated with improved prescribing \u2022 Median change in prescribing ranged from 4% to 46% across intervention types and study design types Evidence-based Synthesis Program (ESP) Results \u2022 VA-ESP Evidence o 9 RCTs, 4 CCTs, 2 CBAs, 20 ITS studies o Categorized studies by intervention type: \u2022 14 audit and feedback \u2022 5 formulary restriction and preauthorization \u2022 4 guideline implementation with feedback \u2022 4 guideline implementation with no feedback \u2022 4 computerized decision support \u2022 4 protocol or policy implementation Evidence-based Synthesis Program (ESP) ASP Intervention (# studies) Mortality Length of Stay Readmission CDI Prospective Audit and Feedback (14) + 1 study \u2248 9 studies \u2248 9 studies + 1 study \u2248 2 studies p=NR, 1 study Formulary Restriction and Preauthorization (5) \u2248 3 studies \u2248 2 studies NR + 1 study Guidelines with Feedback (4) \u2248 3 studies \u2248 3 studies NR + 2 studies Guidelines without Feedback (4) + 1 study \u2248 1study 1 study + 1 study \u2248 1study 1 study \u2248 1 study NR Computerized Decision Support (4) \u2248 3 studies + 1 study \u2248 2 studies \u2248 1 study + 1 study \u2248 1 study Protocols (4) + 1 study \u2248 2 studies + 2 studies \u2248 1 study \u2248 1 study NR KQ1 \u2013 Program Effectiveness: Clinical Outcomes Evidence-based Synthesis Program (ESP) KQ1 \u2013 Program Effectiveness: Prescribing Outcomes ASP Intervention (# studies) Use Selection Timing Duration Prospective Audit and Feedback (14) Decreased: + 8 studies Appropriate: + 1 study, \u2248 1 study + 1 study \u2248 1 study NR + 5 studies Formulary Restriction and Preauthorization (5) Decreased: + 4 studies NR NR + 1 study Guidelines with Feedback (4) Decreased: + 1 study Compliant\/appropriate: + 2 studies \u2248 1 study + 1 study \u2248 2 studies Guidelines without Feedback (4) Decreased: + 1 study Compliant\/ appropriate: + 2 studies, \u2248 1 study NR 1 study + 1 study \u2248 1 study Computerized Decision Support (4) Decreased: + 1 study, \u2248 1 study NR NR NR Protocols (4) Appropriate: \u2248 1 study NR \u2248 1 study + 2 studies Evidence-based Synthesis Program (ESP) KQ1 Program Effectiveness \u2022 Microbial Outcomes (9 studies) o 6 reported improvement \uf0d8 decreased infection or resistance o 3 reported no differences \u2022 Cost Outcomes (5 studies) o 4 reported decreased costs o 1 reported no difference Evidence-based Synthesis Program (ESP) KQ2 Results \u2013 Key Implementation Components \u2022 All author opinion (6 studies) o Consistent and persistent effort from qualified personnel o Effective communication skills o Support from electronic medical records or computerized decision support systems Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 Davey 2013 review o 9 from VA Medical Centers o Similar results (unchanged mortality [1], decreased CDI [2], mixed results prescribing [3], decreased infection [3]) \u2022 No recent studies from VA eligible \u2022 Most studies from University-affiliated hospitals Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 ICU programs (9 studies) o Similar to overall results \u2022 Most studies included \u201cany suspected infection\u201d \u2022 Respiratory infection (7 studies) o Similar to overall results Evidence-based Synthesis Program (ESP) KQ4 Results \u2013 Harms of Programs \u2022 2 studies reported possible harms (other than patient, prescribing, and microbial harms from KQ1) o Anecdotal evidence of inappropriate switch to narrow-spectrum antimicrobial o Termination of program speculated to be due to provider dissatisfaction with prescribing restrictions Evidence-based Synthesis Program (ESP) KQ5 Results \u2013 Barriers to Implementation, Sustainability, Scalability \u2022 Barriers (4 studies; 2 with interview\/survey data) o Lack of familiarity, experience, awareness o Disagreement with guidelines\/conflicting guidelines o Lack of communication between professionals o Organizational constraints \u2022 Sustainability (1 study) o Use and costs decreased over 5 years of study and increased when study was terminated \u2022 Scalability (No studies) Evidence-based Synthesis Program (ESP) Discussion \u2022 Antimicrobial stewardship strategies can decrease antimicrobial prescribing and limit costs, without substantial harms \u2022 Greatest body of recent evidence is from audit and feedback programs \u2022 Systematic review of earlier studies provided evidence of comparable effects for persuasive and restrictive interventions \u2022 Evidence base for KQ1 was substantial; much less substantial for KQ2-5 Evidence-based Synthesis Program (ESP) Discussion \u2022 Studies not designed to adequately assess impact on mortality or other clinical outcomes \u2022 Suggestions for improving adherence to ASPs: o Involvement of stakeholders and opinion leaders in guideline and program development o Addition of quality improvement cycles o Understanding the prescribing culture o Collaboration between physicians and pharmacists (mostly opinion rather than evidence-based) Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations: \u2022 Quality of evidence: Low \u2022 Few randomized controlled trials \u2022 Limited ability to control for secular trends or other confounding variables \u2022 Possibility of regression to the mean in C. difficile rates \u2022 Findings for specific interventions have not been replicated Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Prescribing improvements often not sustained or long-term follow-up not reported \u2022 Most studies done in academic medical centers; generalizability to other settings is difficult. Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Within academic centers, substantial variation in settings, structures, patient bases, culture o Strength: stewardship has impact in diverse settings o Weakness: Hard to be sure that each specific intervention works across all settings \u2022 Few studies reported on harms of stewardship interventions; most not designed to identify harms, and most would not have been adequately powered to recognize them Evidence-based Synthesis Program (ESP) Future Research Needs \u2022 Given the complexity of antimicrobial prescribing, conducting well-designed studies is difficult \u2022 Large healthcare organizations should consider organizing stewardship activities (that are likely on-going within the organization) to provide useful information on comparative effectiveness of different intervention types Evidence-based Synthesis Program (ESP) Overall Conclusions \u2022 Despite these many shortcomings, the overall evidence suggests that antimicrobial stewardship interventions are associated with improved intended outcomes, mostly antimicrobial usage, over short time periods (1-3 years) \u2022 The literature supports hospital and healthcare system implementation of sensible, practical antimicrobial stewardship interventions to improve outcomes Evidence-based Synthesis Program (ESP) Recommendations \u2022 Data on antimicrobial use by clinical unit, type of patients, provider groups, and by individual providers should be gathered to determine where antimicrobial use might be less than ideal or is in need of improvement Evidence-based Synthesis Program (ESP) Recommendations \u2022 Existing infection prevention programs, microbiology laboratories, pharmacy services, infectious disease physicians, electronic medical record systems, continuous improvement programs, and staff or trainee education and certification programs should be utilized to contribute to stewardship activities \u2022 Leadership should also be informed and involved in planning; leadership support is essential Evidence-based Synthesis Program (ESP) Recommendations \u2022 Formative evaluation should be used to identify effective stewardship programs and programs in need of modification Evidence-based Synthesis Program (ESP)\n\n",
                "DataExportTag": "AI707951",
                "QuestionID": "QID149",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
                "Choices": {
                    "1": {
                        "Display": "\"Academic Research and Bibliometric Analysis\""
                    },
                    "2": {
                        "Display": "community, discipline, big_data, academic, bibliometric, citation, article, progress, trend, industry, mathematics, cover, tutorial, advance, area"
                    },
                    "3": {
                        "Display": "\"Cognitive Psychology and Philosophy of Mind\""
                    },
                    "4": {
                        "Display": "cognitive, human, theory, mind, consciousness, cognition, brain, agent, psychology, philosophy, social, mathematics, philosophical, concept, turing"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID218",
            "SecondaryAttribute": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,775s Excluded: 5,775 Full Text Review: 559 Articles Excluded: 539 Articles Not an included study design: 260 No intervention: 108 Outpatient setting: 71 Not stewardship: 57 Other: 43 Hand Search: 15 Articles Included: 35 Studies Evidence-based Synthesis Program (ESP) Results \u2022 Existing Cochrane Review (Davey 2013) \u2022 Included 89 studies published through 2009 \u2022 Focused on prescribing outcomes \u2022 Categorized interventions as: \u2022 Persuasive (education, reminders, audit and feedback) \u2022 Restrictive (order forms, formulary restriction) \u2022 Structural (EMR, decision support systems) Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Mortality \u2022 Interventions to increase guideline compliance for patients with community acquired pneumonia reduced mortality: \u2022 (4 studies, RR 0.89 [0.82, 0.97]) \u2022 Interventions to increase effective prescribing (3 studies) and interventions to decrease excessive prescribing (11 studies) were not significantly associated with mortality Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Other \u2022 Interventions to decrease excessive prescribing : \u2022 No effect length of stay \u2022 (6 studies, mean difference, -0.04 days [-0.34, 0.25]) \u2022 Associated with increased hospital readmissions \u2022 (5 studies, 1.26 [1.02, 1.57]) \u2022 Reduction in C. difficile infection: \u2022 15% to 65% at one month post-intervention \u2022 4 interrupted time series studies Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Prescribing Outcomes \u2022 Calculated median effect size with all prescribing outcomes; positive sign = change in intended direction \u2022 Persuasive, restrictive, and structural interventions: \u2022 Associated with improved prescribing \u2022 Median change in prescribing ranged from 4% to 46% across intervention types and study design types Evidence-based Synthesis Program (ESP) Results \u2022 VA-ESP Evidence o 9 RCTs, 4 CCTs, 2 CBAs, 20 ITS studies o Categorized studies by intervention type: \u2022 14 audit and feedback \u2022 5 formulary restriction and preauthorization \u2022 4 guideline implementation with feedback \u2022 4 guideline implementation with no feedback \u2022 4 computerized decision support \u2022 4 protocol or policy implementation Evidence-based Synthesis Program (ESP) ASP Intervention (# studies) Mortality Length of Stay Readmission CDI Prospective Audit and Feedback (14) + 1 study \u2248 9 studies \u2248 9 studies + 1 study \u2248 2 studies p=NR, 1 study Formulary Restriction and Preauthorization (5) \u2248 3 studies \u2248 2 studies NR + 1 study Guidelines with Feedback (4) \u2248 3 studies \u2248 3 studies NR + 2 studies Guidelines without Feedback (4) + 1 study \u2248 1study 1 study + 1 study \u2248 1study 1 study \u2248 1 study NR Computerized Decision Support (4) \u2248 3 studies + 1 study \u2248 2 studies \u2248 1 study + 1 study \u2248 1 study Protocols (4) + 1 study \u2248 2 studies + 2 studies \u2248 1 study \u2248 1 study NR KQ1 \u2013 Program Effectiveness: Clinical Outcomes Evidence-based Synthesis Program (ESP) KQ1 \u2013 Program Effectiveness: Prescribing Outcomes ASP Intervention (# studies) Use Selection Timing Duration Prospective Audit and Feedback (14) Decreased: + 8 studies Appropriate: + 1 study, \u2248 1 study + 1 study \u2248 1 study NR + 5 studies Formulary Restriction and Preauthorization (5) Decreased: + 4 studies NR NR + 1 study Guidelines with Feedback (4) Decreased: + 1 study Compliant\/appropriate: + 2 studies \u2248 1 study + 1 study \u2248 2 studies Guidelines without Feedback (4) Decreased: + 1 study Compliant\/ appropriate: + 2 studies, \u2248 1 study NR 1 study + 1 study \u2248 1 study Computerized Decision Support (4) Decreased: + 1 study, \u2248 1 study NR NR NR Protocols (4) Appropriate: \u2248 1 study NR \u2248 1 study + 2 studies Evidence-based Synthesis Program (ESP) KQ1 Program Effectiveness \u2022 Microbial Outcomes (9 studies) o 6 reported improvement \uf0d8 decreased infection or resistance o 3 reported no differences \u2022 Cost Outcomes (5 studies) o 4 reported decreased costs o 1 reported no difference Evidence-based Synthesis Program (ESP) KQ2 Results \u2013 Key Implementation Components \u2022 All author opinion (6 studies) o Consistent and persistent effort from qualified personnel o Effective communication skills o Support from electronic medical records or computerized decision support systems Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 Davey 2013 review o 9 from VA Medical Centers o Similar results (unchanged mortality [1], decreased CDI [2], mixed results prescribing [3], decreased infection [3]) \u2022 No recent studies from VA eligible \u2022 Most studies from University-affiliated hospitals Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 ICU programs (9 studies) o Similar to overall results \u2022 Most studies included \u201cany suspected infection\u201d \u2022 Respiratory infection (7 studies) o Similar to overall results Evidence-based Synthesis Program (ESP) KQ4 Results \u2013 Harms of Programs \u2022 2 studies reported possible harms (other than patient, prescribing, and microbial harms from KQ1) o Anecdotal evidence of inappropriate switch to narrow-spectrum antimicrobial o Termination of program speculated to be due to provider dissatisfaction with prescribing restrictions Evidence-based Synthesis Program (ESP) KQ5 Results \u2013 Barriers to Implementation, Sustainability, Scalability \u2022 Barriers (4 studies; 2 with interview\/survey data) o Lack of familiarity, experience, awareness o Disagreement with guidelines\/conflicting guidelines o Lack of communication between professionals o Organizational constraints \u2022 Sustainability (1 study) o Use and costs decreased over 5 years of study and increased when study was terminated \u2022 Scalability (No studies) Evidence-based Synthesis Program (ESP) Discussion \u2022 Antimicrobial stewardship strategies can decrease antimicrobial prescribing and limit costs, without substantial harms \u2022 Greatest body of recent evidence is from audit and feedback programs \u2022 Systematic review of earlier studies provided evidence of comparable effects for persuasive and restrictive interventions \u2022 Evidence base for KQ1 was substantial; much less substantial for KQ2-5 Evidence-based Synthesis Program (ESP) Discussion \u2022 Studies not designed to adequately assess impact on mortality or other clinical outcomes \u2022 Suggestions for improving adherence to ASPs: o Involvement of stakeholders and opinion leaders in guideline and program development o Addition of quality improvement cycles o Understanding the prescribing culture o Collaboration between physicians and pharmacists (mostly opinion rather than evidence-based) Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations: \u2022 Quality of evidence: Low \u2022 Few randomized controlled trials \u2022 Limited ability to control for secular trends or other confounding variables \u2022 Possibility of regression to the mean in C. difficile rates \u2022 Findings for specific interventions have not been replicated Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Prescribing improvements often not sustained or long-term follow-up not reported \u2022 Most studies done in academic medical centers; generalizability to other settings is difficult. Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Within academic centers, substantial variation in settings, structures, patient bases, culture o Strength: stewardship has impact in diverse settings o Weakness: Hard to be sure that each specific intervention works across all settings \u2022 Few studies reported on harms of stewardship interventions; most not designed to identify harms, and most would not have been adequately powered to recognize them Evidence-based Synthesis Program (ESP) Future Research Needs \u2022 Given the complexity of antimicrobial prescribing, conducting well-designed studies is difficult \u2022 Large healthcare organizations should consider organizing stewardship activities (that are likely on-going within the organization) to provide useful information on comparative effectiveness of different intervention types Evidence-based Synthesis Program (ESP) Overall Conclusions \u2022 Despite these many shortcomings, the overall evidence suggests that antimicrobial stewardship interventions are associated with improved intended outcomes, mostly antimicrobial usage, over short time periods (1-3 years) \u2022 The literature supports hospital and healthcare system implementation of sensible, practical antimicrobial stewardship interventions to improve outcomes Evidence-based Synthesis Program (ESP) Recommendations \u2022 Data on antimicrobial use by clinical unit, type of patients, provider groups, and by individual providers should be gathered to determine where antimicrobial use might be less than ideal or is in need of improvement Evidence-based Synthesis Program (ESP) Recommendations \u2022 Existing infection prevention programs, microbiology laboratories, pharmacy services, infectious disease physicians, electronic medical record systems, continuous improvement programs, and staff or trainee education and certification programs should be utilized to contribute to stewardship activities \u2022 Leadership should also be informed and involved in planning; leadership support is essential Evidence-based Synthesis Program (ESP) Recommendations \u2022 Formative evaluation should be used to identify effective stewardship programs and programs in need of modification Evidence-based Synthesis Program (ESP)\n\n",
                "DataExportTag": "AI392307",
                "QuestionID": "QID218",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
                "Choices": {
                    "1": {
                        "Display": "\"Cognitive Theory and Conceptual Modelling\""
                    },
                    "2": {
                        "Display": "cognitive, theory, concept, conceptual, human, interpretation, symbolic, structure, complex, abstraction, interaction, modelling, component, situation, description"
                    },
                    "3": {
                        "Display": "\"Explainable Artificial Intelligence and Automation Testing\""
                    },
                    "4": {
                        "Display": "explanation, testing, prediction, validation, user, trust, automate, developer, interpretability, safety, black_box, explainable, xai, case, automation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID279",
            "SecondaryAttribute": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,775s Excluded: 5,775 Full Text Review: 559 Articles Excluded: 539 Articles Not an included study design: 260 No intervention: 108 Outpatient setting: 71 Not stewardship: 57 Other: 43 Hand Search: 15 Articles Included: 35 Studies Evidence-based Synthesis Program (ESP) Results \u2022 Existing Cochrane Review (Davey 2013) \u2022 Included 89 studies published through 2009 \u2022 Focused on prescribing outcomes \u2022 Categorized interventions as: \u2022 Persuasive (education, reminders, audit and feedback) \u2022 Restrictive (order forms, formulary restriction) \u2022 Structural (EMR, decision support systems) Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Mortality \u2022 Interventions to increase guideline compliance for patients with community acquired pneumonia reduced mortality: \u2022 (4 studies, RR 0.89 [0.82, 0.97]) \u2022 Interventions to increase effective prescribing (3 studies) and interventions to decrease excessive prescribing (11 studies) were not significantly associated with mortality Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Other \u2022 Interventions to decrease excessive prescribing : \u2022 No effect length of stay \u2022 (6 studies, mean difference, -0.04 days [-0.34, 0.25]) \u2022 Associated with increased hospital readmissions \u2022 (5 studies, 1.26 [1.02, 1.57]) \u2022 Reduction in C. difficile infection: \u2022 15% to 65% at one month post-intervention \u2022 4 interrupted time series studies Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Prescribing Outcomes \u2022 Calculated median effect size with all prescribing outcomes; positive sign = change in intended direction \u2022 Persuasive, restrictive, and structural interventions: \u2022 Associated with improved prescribing \u2022 Median change in prescribing ranged from 4% to 46% across intervention types and study design types Evidence-based Synthesis Program (ESP) Results \u2022 VA-ESP Evidence o 9 RCTs, 4 CCTs, 2 CBAs, 20 ITS studies o Categorized studies by intervention type: \u2022 14 audit and feedback \u2022 5 formulary restriction and preauthorization \u2022 4 guideline implementation with feedback \u2022 4 guideline implementation with no feedback \u2022 4 computerized decision support \u2022 4 protocol or policy implementation Evidence-based Synthesis Program (ESP) ASP Intervention (# studies) Mortality Length of Stay Readmission CDI Prospective Audit and Feedback (14) + 1 study \u2248 9 studies \u2248 9 studies + 1 study \u2248 2 studies p=NR, 1 study Formulary Restriction and Preauthorization (5) \u2248 3 studies \u2248 2 studies NR + 1 study Guidelines with Feedback (4) \u2248 3 studies \u2248 3 studies NR + 2 studies Guidelines without Feedback (4) + 1 study \u2248 1study 1 study + 1 study \u2248 1study 1 study \u2248 1 study NR Computerized Decision Support (4) \u2248 3 studies + 1 study \u2248 2 studies \u2248 1 study + 1 study \u2248 1 study Protocols (4) + 1 study \u2248 2 studies + 2 studies \u2248 1 study \u2248 1 study NR KQ1 \u2013 Program Effectiveness: Clinical Outcomes Evidence-based Synthesis Program (ESP) KQ1 \u2013 Program Effectiveness: Prescribing Outcomes ASP Intervention (# studies) Use Selection Timing Duration Prospective Audit and Feedback (14) Decreased: + 8 studies Appropriate: + 1 study, \u2248 1 study + 1 study \u2248 1 study NR + 5 studies Formulary Restriction and Preauthorization (5) Decreased: + 4 studies NR NR + 1 study Guidelines with Feedback (4) Decreased: + 1 study Compliant\/appropriate: + 2 studies \u2248 1 study + 1 study \u2248 2 studies Guidelines without Feedback (4) Decreased: + 1 study Compliant\/ appropriate: + 2 studies, \u2248 1 study NR 1 study + 1 study \u2248 1 study Computerized Decision Support (4) Decreased: + 1 study, \u2248 1 study NR NR NR Protocols (4) Appropriate: \u2248 1 study NR \u2248 1 study + 2 studies Evidence-based Synthesis Program (ESP) KQ1 Program Effectiveness \u2022 Microbial Outcomes (9 studies) o 6 reported improvement \uf0d8 decreased infection or resistance o 3 reported no differences \u2022 Cost Outcomes (5 studies) o 4 reported decreased costs o 1 reported no difference Evidence-based Synthesis Program (ESP) KQ2 Results \u2013 Key Implementation Components \u2022 All author opinion (6 studies) o Consistent and persistent effort from qualified personnel o Effective communication skills o Support from electronic medical records or computerized decision support systems Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 Davey 2013 review o 9 from VA Medical Centers o Similar results (unchanged mortality [1], decreased CDI [2], mixed results prescribing [3], decreased infection [3]) \u2022 No recent studies from VA eligible \u2022 Most studies from University-affiliated hospitals Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 ICU programs (9 studies) o Similar to overall results \u2022 Most studies included \u201cany suspected infection\u201d \u2022 Respiratory infection (7 studies) o Similar to overall results Evidence-based Synthesis Program (ESP) KQ4 Results \u2013 Harms of Programs \u2022 2 studies reported possible harms (other than patient, prescribing, and microbial harms from KQ1) o Anecdotal evidence of inappropriate switch to narrow-spectrum antimicrobial o Termination of program speculated to be due to provider dissatisfaction with prescribing restrictions Evidence-based Synthesis Program (ESP) KQ5 Results \u2013 Barriers to Implementation, Sustainability, Scalability \u2022 Barriers (4 studies; 2 with interview\/survey data) o Lack of familiarity, experience, awareness o Disagreement with guidelines\/conflicting guidelines o Lack of communication between professionals o Organizational constraints \u2022 Sustainability (1 study) o Use and costs decreased over 5 years of study and increased when study was terminated \u2022 Scalability (No studies) Evidence-based Synthesis Program (ESP) Discussion \u2022 Antimicrobial stewardship strategies can decrease antimicrobial prescribing and limit costs, without substantial harms \u2022 Greatest body of recent evidence is from audit and feedback programs \u2022 Systematic review of earlier studies provided evidence of comparable effects for persuasive and restrictive interventions \u2022 Evidence base for KQ1 was substantial; much less substantial for KQ2-5 Evidence-based Synthesis Program (ESP) Discussion \u2022 Studies not designed to adequately assess impact on mortality or other clinical outcomes \u2022 Suggestions for improving adherence to ASPs: o Involvement of stakeholders and opinion leaders in guideline and program development o Addition of quality improvement cycles o Understanding the prescribing culture o Collaboration between physicians and pharmacists (mostly opinion rather than evidence-based) Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations: \u2022 Quality of evidence: Low \u2022 Few randomized controlled trials \u2022 Limited ability to control for secular trends or other confounding variables \u2022 Possibility of regression to the mean in C. difficile rates \u2022 Findings for specific interventions have not been replicated Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Prescribing improvements often not sustained or long-term follow-up not reported \u2022 Most studies done in academic medical centers; generalizability to other settings is difficult. Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Within academic centers, substantial variation in settings, structures, patient bases, culture o Strength: stewardship has impact in diverse settings o Weakness: Hard to be sure that each specific intervention works across all settings \u2022 Few studies reported on harms of stewardship interventions; most not designed to identify harms, and most would not have been adequately powered to recognize them Evidence-based Synthesis Program (ESP) Future Research Needs \u2022 Given the complexity of antimicrobial prescribing, conducting well-designed studies is difficult \u2022 Large healthcare organizations should consider organizing stewardship activities (that are likely on-going within the organization) to provide useful information on comparative effectiveness of different intervention types Evidence-based Synthesis Program (ESP) Overall Conclusions \u2022 Despite these many shortcomings, the overall evidence suggests that antimicrobial stewardship interventions are associated with improved intended outcomes, mostly antimicrobial usage, over short time periods (1-3 years) \u2022 The literature supports hospital and healthcare system implementation of sensible, practical antimicrobial stewardship interventions to improve outcomes Evidence-based Synthesis Program (ESP) Recommendations \u2022 Data on antimicrobial use by clinical unit, type of patients, provider groups, and by individual providers should be gathered to determine where antimicrobial use might be less than ideal or is in need of improvement Evidence-based Synthesis Program (ESP) Recommendations \u2022 Existing infection prevention programs, microbiology laboratories, pharmacy services, infectious disease physicians, electronic medical record systems, continuous improvement programs, and staff or trainee education and certification programs should be utilized to contribute to stewardship activities \u2022 Leadership should also be informed and involved in planning; leadership support is essential Evidence-based Synthesis Program (ESP) Recommendations \u2022 Formative evaluation should be used to identify effective stewardship programs and programs in need of modification Evidence-based Synthesis Program (ESP)\n\n",
                "DataExportTag": "AI392307",
                "QuestionID": "QID279",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
                "Choices": {
                    "1": {
                        "Display": "\"Cognitive Theory and Conceptual Modelling\""
                    },
                    "2": {
                        "Display": "cognitive, theory, concept, conceptual, human, interpretation, symbolic, structure, complex, abstraction, interaction, modelling, component, situation, description"
                    },
                    "3": {
                        "Display": "\"Explainable Artificial Intelligence and Automation Testing\""
                    },
                    "4": {
                        "Display": "explanation, testing, prediction, validation, user, trust, automate, developer, interpretability, safety, black_box, explainable, xai, case, automation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 5,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID394",
            "SecondaryAttribute": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,775s Excluded: 5,775 Full Text Review: 559 Articles Excluded: 539 Articles Not an included study design: 260 No intervention: 108 Outpatient setting: 71 Not stewardship: 57 Other: 43 Hand Search: 15 Articles Included: 35 Studies Evidence-based Synthesis Program (ESP) Results \u2022 Existing Cochrane Review (Davey 2013) \u2022 Included 89 studies published through 2009 \u2022 Focused on prescribing outcomes \u2022 Categorized interventions as: \u2022 Persuasive (education, reminders, audit and feedback) \u2022 Restrictive (order forms, formulary restriction) \u2022 Structural (EMR, decision support systems) Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Mortality \u2022 Interventions to increase guideline compliance for patients with community acquired pneumonia reduced mortality: \u2022 (4 studies, RR 0.89 [0.82, 0.97]) \u2022 Interventions to increase effective prescribing (3 studies) and interventions to decrease excessive prescribing (11 studies) were not significantly associated with mortality Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Other \u2022 Interventions to decrease excessive prescribing : \u2022 No effect length of stay \u2022 (6 studies, mean difference, -0.04 days [-0.34, 0.25]) \u2022 Associated with increased hospital readmissions \u2022 (5 studies, 1.26 [1.02, 1.57]) \u2022 Reduction in C. difficile infection: \u2022 15% to 65% at one month post-intervention \u2022 4 interrupted time series studies Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Prescribing Outcomes \u2022 Calculated median effect size with all prescribing outcomes; positive sign = change in intended direction \u2022 Persuasive, restrictive, and structural interventions: \u2022 Associated with improved prescribing \u2022 Median change in prescribing ranged from 4% to 46% across intervention types and study design types Evidence-based Synthesis Program (ESP) Results \u2022 VA-ESP Evidence o 9 RCTs, 4 CCTs, 2 CBAs, 20 ITS studies o Categorized studies by intervention type: \u2022 14 audit and feedback \u2022 5 formulary restriction and preauthorization \u2022 4 guideline implementation with feedback \u2022 4 guideline implementation with no feedback \u2022 4 computerized decision support \u2022 4 protocol or policy implementation Evidence-based Synthesis Program (ESP) ASP Intervention (# studies) Mortality Length of Stay Readmission CDI Prospective Audit and Feedback (14) + 1 study \u2248 9 studies \u2248 9 studies + 1 study \u2248 2 studies p=NR, 1 study Formulary Restriction and Preauthorization (5) \u2248 3 studies \u2248 2 studies NR + 1 study Guidelines with Feedback (4) \u2248 3 studies \u2248 3 studies NR + 2 studies Guidelines without Feedback (4) + 1 study \u2248 1study 1 study + 1 study \u2248 1study 1 study \u2248 1 study NR Computerized Decision Support (4) \u2248 3 studies + 1 study \u2248 2 studies \u2248 1 study + 1 study \u2248 1 study Protocols (4) + 1 study \u2248 2 studies + 2 studies \u2248 1 study \u2248 1 study NR KQ1 \u2013 Program Effectiveness: Clinical Outcomes Evidence-based Synthesis Program (ESP) KQ1 \u2013 Program Effectiveness: Prescribing Outcomes ASP Intervention (# studies) Use Selection Timing Duration Prospective Audit and Feedback (14) Decreased: + 8 studies Appropriate: + 1 study, \u2248 1 study + 1 study \u2248 1 study NR + 5 studies Formulary Restriction and Preauthorization (5) Decreased: + 4 studies NR NR + 1 study Guidelines with Feedback (4) Decreased: + 1 study Compliant\/appropriate: + 2 studies \u2248 1 study + 1 study \u2248 2 studies Guidelines without Feedback (4) Decreased: + 1 study Compliant\/ appropriate: + 2 studies, \u2248 1 study NR 1 study + 1 study \u2248 1 study Computerized Decision Support (4) Decreased: + 1 study, \u2248 1 study NR NR NR Protocols (4) Appropriate: \u2248 1 study NR \u2248 1 study + 2 studies Evidence-based Synthesis Program (ESP) KQ1 Program Effectiveness \u2022 Microbial Outcomes (9 studies) o 6 reported improvement \uf0d8 decreased infection or resistance o 3 reported no differences \u2022 Cost Outcomes (5 studies) o 4 reported decreased costs o 1 reported no difference Evidence-based Synthesis Program (ESP) KQ2 Results \u2013 Key Implementation Components \u2022 All author opinion (6 studies) o Consistent and persistent effort from qualified personnel o Effective communication skills o Support from electronic medical records or computerized decision support systems Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 Davey 2013 review o 9 from VA Medical Centers o Similar results (unchanged mortality [1], decreased CDI [2], mixed results prescribing [3], decreased infection [3]) \u2022 No recent studies from VA eligible \u2022 Most studies from University-affiliated hospitals Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 ICU programs (9 studies) o Similar to overall results \u2022 Most studies included \u201cany suspected infection\u201d \u2022 Respiratory infection (7 studies) o Similar to overall results Evidence-based Synthesis Program (ESP) KQ4 Results \u2013 Harms of Programs \u2022 2 studies reported possible harms (other than patient, prescribing, and microbial harms from KQ1) o Anecdotal evidence of inappropriate switch to narrow-spectrum antimicrobial o Termination of program speculated to be due to provider dissatisfaction with prescribing restrictions Evidence-based Synthesis Program (ESP) KQ5 Results \u2013 Barriers to Implementation, Sustainability, Scalability \u2022 Barriers (4 studies; 2 with interview\/survey data) o Lack of familiarity, experience, awareness o Disagreement with guidelines\/conflicting guidelines o Lack of communication between professionals o Organizational constraints \u2022 Sustainability (1 study) o Use and costs decreased over 5 years of study and increased when study was terminated \u2022 Scalability (No studies) Evidence-based Synthesis Program (ESP) Discussion \u2022 Antimicrobial stewardship strategies can decrease antimicrobial prescribing and limit costs, without substantial harms \u2022 Greatest body of recent evidence is from audit and feedback programs \u2022 Systematic review of earlier studies provided evidence of comparable effects for persuasive and restrictive interventions \u2022 Evidence base for KQ1 was substantial; much less substantial for KQ2-5 Evidence-based Synthesis Program (ESP) Discussion \u2022 Studies not designed to adequately assess impact on mortality or other clinical outcomes \u2022 Suggestions for improving adherence to ASPs: o Involvement of stakeholders and opinion leaders in guideline and program development o Addition of quality improvement cycles o Understanding the prescribing culture o Collaboration between physicians and pharmacists (mostly opinion rather than evidence-based) Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations: \u2022 Quality of evidence: Low \u2022 Few randomized controlled trials \u2022 Limited ability to control for secular trends or other confounding variables \u2022 Possibility of regression to the mean in C. difficile rates \u2022 Findings for specific interventions have not been replicated Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Prescribing improvements often not sustained or long-term follow-up not reported \u2022 Most studies done in academic medical centers; generalizability to other settings is difficult. Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Within academic centers, substantial variation in settings, structures, patient bases, culture o Strength: stewardship has impact in diverse settings o Weakness: Hard to be sure that each specific intervention works across all settings \u2022 Few studies reported on harms of stewardship interventions; most not designed to identify harms, and most would not have been adequately powered to recognize them Evidence-based Synthesis Program (ESP) Future Research Needs \u2022 Given the complexity of antimicrobial prescribing, conducting well-designed studies is difficult \u2022 Large healthcare organizations should consider organizing stewardship activities (that are likely on-going within the organization) to provide useful information on comparative effectiveness of different intervention types Evidence-based Synthesis Program (ESP) Overall Conclusions \u2022 Despite these many shortcomings, the overall evidence suggests that antimicrobial stewardship interventions are associated with improved intended outcomes, mostly antimicrobial usage, over short time periods (1-3 years) \u2022 The literature supports hospital and healthcare system implementation of sensible, practical antimicrobial stewardship interventions to improve outcomes Evidence-based Synthesis Program (ESP) Recommendations \u2022 Data on antimicrobial use by clinical unit, type of patients, provider groups, and by individual providers should be gathered to determine where antimicrobial use might be less than ideal or is in need of improvement Evidence-based Synthesis Program (ESP) Recommendations \u2022 Existing infection prevention programs, microbiology laboratories, pharmacy services, infectious disease physicians, electronic medical record systems, continuous improvement programs, and staff or trainee education and certification programs should be utilized to contribute to stewardship activities \u2022 Leadership should also be informed and involved in planning; leadership support is essential Evidence-based Synthesis Program (ESP) Recommendations \u2022 Formative evaluation should be used to identify effective stewardship programs and programs in need of modification Evidence-based Synthesis Program (ESP)\n\n",
                "DataExportTag": "AI1035176",
                "QuestionID": "QID394",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
                "Choices": {
                    "1": {
                        "Display": "\"Smart Home Monitoring and Activity Recognition\""
                    },
                    "2": {
                        "Display": "activity, sensor, smartphone, monitoring, wearable, smart, human, har, smart_home, accelerometer, home, fall, remote, monitor, wheelchair"
                    },
                    "3": {
                        "Display": "\"Human Activity Recognition and Posture Classification\""
                    },
                    "4": {
                        "Display": "activity, human, speech_recognition, fall, classification, gait, body, posture, sensor, action, walk, motion, svm, wearable, sport"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID403",
            "SecondaryAttribute": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,775s Excluded: 5,775 Full Text Review: 559 Articles Excluded: 539 Articles Not an included study design: 260 No intervention: 108 Outpatient setting: 71 Not stewardship: 57 Other: 43 Hand Search: 15 Articles Included: 35 Studies Evidence-based Synthesis Program (ESP) Results \u2022 Existing Cochrane Review (Davey 2013) \u2022 Included 89 studies published through 2009 \u2022 Focused on prescribing outcomes \u2022 Categorized interventions as: \u2022 Persuasive (education, reminders, audit and feedback) \u2022 Restrictive (order forms, formulary restriction) \u2022 Structural (EMR, decision support systems) Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Mortality \u2022 Interventions to increase guideline compliance for patients with community acquired pneumonia reduced mortality: \u2022 (4 studies, RR 0.89 [0.82, 0.97]) \u2022 Interventions to increase effective prescribing (3 studies) and interventions to decrease excessive prescribing (11 studies) were not significantly associated with mortality Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Other \u2022 Interventions to decrease excessive prescribing : \u2022 No effect length of stay \u2022 (6 studies, mean difference, -0.04 days [-0.34, 0.25]) \u2022 Associated with increased hospital readmissions \u2022 (5 studies, 1.26 [1.02, 1.57]) \u2022 Reduction in C. difficile infection: \u2022 15% to 65% at one month post-intervention \u2022 4 interrupted time series studies Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Prescribing Outcomes \u2022 Calculated median effect size with all prescribing outcomes; positive sign = change in intended direction \u2022 Persuasive, restrictive, and structural interventions: \u2022 Associated with improved prescribing \u2022 Median change in prescribing ranged from 4% to 46% across intervention types and study design types Evidence-based Synthesis Program (ESP) Results \u2022 VA-ESP Evidence o 9 RCTs, 4 CCTs, 2 CBAs, 20 ITS studies o Categorized studies by intervention type: \u2022 14 audit and feedback \u2022 5 formulary restriction and preauthorization \u2022 4 guideline implementation with feedback \u2022 4 guideline implementation with no feedback \u2022 4 computerized decision support \u2022 4 protocol or policy implementation Evidence-based Synthesis Program (ESP) ASP Intervention (# studies) Mortality Length of Stay Readmission CDI Prospective Audit and Feedback (14) + 1 study \u2248 9 studies \u2248 9 studies + 1 study \u2248 2 studies p=NR, 1 study Formulary Restriction and Preauthorization (5) \u2248 3 studies \u2248 2 studies NR + 1 study Guidelines with Feedback (4) \u2248 3 studies \u2248 3 studies NR + 2 studies Guidelines without Feedback (4) + 1 study \u2248 1study 1 study + 1 study \u2248 1study 1 study \u2248 1 study NR Computerized Decision Support (4) \u2248 3 studies + 1 study \u2248 2 studies \u2248 1 study + 1 study \u2248 1 study Protocols (4) + 1 study \u2248 2 studies + 2 studies \u2248 1 study \u2248 1 study NR KQ1 \u2013 Program Effectiveness: Clinical Outcomes Evidence-based Synthesis Program (ESP) KQ1 \u2013 Program Effectiveness: Prescribing Outcomes ASP Intervention (# studies) Use Selection Timing Duration Prospective Audit and Feedback (14) Decreased: + 8 studies Appropriate: + 1 study, \u2248 1 study + 1 study \u2248 1 study NR + 5 studies Formulary Restriction and Preauthorization (5) Decreased: + 4 studies NR NR + 1 study Guidelines with Feedback (4) Decreased: + 1 study Compliant\/appropriate: + 2 studies \u2248 1 study + 1 study \u2248 2 studies Guidelines without Feedback (4) Decreased: + 1 study Compliant\/ appropriate: + 2 studies, \u2248 1 study NR 1 study + 1 study \u2248 1 study Computerized Decision Support (4) Decreased: + 1 study, \u2248 1 study NR NR NR Protocols (4) Appropriate: \u2248 1 study NR \u2248 1 study + 2 studies Evidence-based Synthesis Program (ESP) KQ1 Program Effectiveness \u2022 Microbial Outcomes (9 studies) o 6 reported improvement \uf0d8 decreased infection or resistance o 3 reported no differences \u2022 Cost Outcomes (5 studies) o 4 reported decreased costs o 1 reported no difference Evidence-based Synthesis Program (ESP) KQ2 Results \u2013 Key Implementation Components \u2022 All author opinion (6 studies) o Consistent and persistent effort from qualified personnel o Effective communication skills o Support from electronic medical records or computerized decision support systems Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 Davey 2013 review o 9 from VA Medical Centers o Similar results (unchanged mortality [1], decreased CDI [2], mixed results prescribing [3], decreased infection [3]) \u2022 No recent studies from VA eligible \u2022 Most studies from University-affiliated hospitals Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 ICU programs (9 studies) o Similar to overall results \u2022 Most studies included \u201cany suspected infection\u201d \u2022 Respiratory infection (7 studies) o Similar to overall results Evidence-based Synthesis Program (ESP) KQ4 Results \u2013 Harms of Programs \u2022 2 studies reported possible harms (other than patient, prescribing, and microbial harms from KQ1) o Anecdotal evidence of inappropriate switch to narrow-spectrum antimicrobial o Termination of program speculated to be due to provider dissatisfaction with prescribing restrictions Evidence-based Synthesis Program (ESP) KQ5 Results \u2013 Barriers to Implementation, Sustainability, Scalability \u2022 Barriers (4 studies; 2 with interview\/survey data) o Lack of familiarity, experience, awareness o Disagreement with guidelines\/conflicting guidelines o Lack of communication between professionals o Organizational constraints \u2022 Sustainability (1 study) o Use and costs decreased over 5 years of study and increased when study was terminated \u2022 Scalability (No studies) Evidence-based Synthesis Program (ESP) Discussion \u2022 Antimicrobial stewardship strategies can decrease antimicrobial prescribing and limit costs, without substantial harms \u2022 Greatest body of recent evidence is from audit and feedback programs \u2022 Systematic review of earlier studies provided evidence of comparable effects for persuasive and restrictive interventions \u2022 Evidence base for KQ1 was substantial; much less substantial for KQ2-5 Evidence-based Synthesis Program (ESP) Discussion \u2022 Studies not designed to adequately assess impact on mortality or other clinical outcomes \u2022 Suggestions for improving adherence to ASPs: o Involvement of stakeholders and opinion leaders in guideline and program development o Addition of quality improvement cycles o Understanding the prescribing culture o Collaboration between physicians and pharmacists (mostly opinion rather than evidence-based) Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations: \u2022 Quality of evidence: Low \u2022 Few randomized controlled trials \u2022 Limited ability to control for secular trends or other confounding variables \u2022 Possibility of regression to the mean in C. difficile rates \u2022 Findings for specific interventions have not been replicated Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Prescribing improvements often not sustained or long-term follow-up not reported \u2022 Most studies done in academic medical centers; generalizability to other settings is difficult. Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Within academic centers, substantial variation in settings, structures, patient bases, culture o Strength: stewardship has impact in diverse settings o Weakness: Hard to be sure that each specific intervention works across all settings \u2022 Few studies reported on harms of stewardship interventions; most not designed to identify harms, and most would not have been adequately powered to recognize them Evidence-based Synthesis Program (ESP) Future Research Needs \u2022 Given the complexity of antimicrobial prescribing, conducting well-designed studies is difficult \u2022 Large healthcare organizations should consider organizing stewardship activities (that are likely on-going within the organization) to provide useful information on comparative effectiveness of different intervention types Evidence-based Synthesis Program (ESP) Overall Conclusions \u2022 Despite these many shortcomings, the overall evidence suggests that antimicrobial stewardship interventions are associated with improved intended outcomes, mostly antimicrobial usage, over short time periods (1-3 years) \u2022 The literature supports hospital and healthcare system implementation of sensible, practical antimicrobial stewardship interventions to improve outcomes Evidence-based Synthesis Program (ESP) Recommendations \u2022 Data on antimicrobial use by clinical unit, type of patients, provider groups, and by individual providers should be gathered to determine where antimicrobial use might be less than ideal or is in need of improvement Evidence-based Synthesis Program (ESP) Recommendations \u2022 Existing infection prevention programs, microbiology laboratories, pharmacy services, infectious disease physicians, electronic medical record systems, continuous improvement programs, and staff or trainee education and certification programs should be utilized to contribute to stewardship activities \u2022 Leadership should also be informed and involved in planning; leadership support is essential Evidence-based Synthesis Program (ESP) Recommendations \u2022 Formative evaluation should be used to identify effective stewardship programs and programs in need of modification Evidence-based Synthesis Program (ESP)\n\n",
                "DataExportTag": "AI440126",
                "QuestionID": "QID403",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
                "Choices": {
                    "1": {
                        "Display": "\"Optimization Techniques in Routing and Pathfinding\""
                    },
                    "2": {
                        "Display": "genetic_algorithm, heuristic, route, routing, tsp, sa, path, travel_salesman, simulated_annealing, vehicle_routing, tabu_search, ant_colony, metaheuristic, simulate_annealing, placement"
                    },
                    "3": {
                        "Display": "\"Production Scheduling and Job Shop Optimization\""
                    },
                    "4": {
                        "Display": "scheduling, job, schedule, job_shop, genetic_algorithm, production, flow_shop, sequence, heuristic, flexible_job, manufacturing, shop_scheduling, planning, assembly, constraint"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID408",
            "SecondaryAttribute": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,775s Excluded: 5,775 Full Text Review: 559 Articles Excluded: 539 Articles Not an included study design: 260 No intervention: 108 Outpatient setting: 71 Not stewardship: 57 Other: 43 Hand Search: 15 Articles Included: 35 Studies Evidence-based Synthesis Program (ESP) Results \u2022 Existing Cochrane Review (Davey 2013) \u2022 Included 89 studies published through 2009 \u2022 Focused on prescribing outcomes \u2022 Categorized interventions as: \u2022 Persuasive (education, reminders, audit and feedback) \u2022 Restrictive (order forms, formulary restriction) \u2022 Structural (EMR, decision support systems) Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Mortality \u2022 Interventions to increase guideline compliance for patients with community acquired pneumonia reduced mortality: \u2022 (4 studies, RR 0.89 [0.82, 0.97]) \u2022 Interventions to increase effective prescribing (3 studies) and interventions to decrease excessive prescribing (11 studies) were not significantly associated with mortality Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Patient-Centered Outcomes Other \u2022 Interventions to decrease excessive prescribing : \u2022 No effect length of stay \u2022 (6 studies, mean difference, -0.04 days [-0.34, 0.25]) \u2022 Associated with increased hospital readmissions \u2022 (5 studies, 1.26 [1.02, 1.57]) \u2022 Reduction in C. difficile infection: \u2022 15% to 65% at one month post-intervention \u2022 4 interrupted time series studies Evidence-based Synthesis Program (ESP) Results (Davey 2013) \u2022 Prescribing Outcomes \u2022 Calculated median effect size with all prescribing outcomes; positive sign = change in intended direction \u2022 Persuasive, restrictive, and structural interventions: \u2022 Associated with improved prescribing \u2022 Median change in prescribing ranged from 4% to 46% across intervention types and study design types Evidence-based Synthesis Program (ESP) Results \u2022 VA-ESP Evidence o 9 RCTs, 4 CCTs, 2 CBAs, 20 ITS studies o Categorized studies by intervention type: \u2022 14 audit and feedback \u2022 5 formulary restriction and preauthorization \u2022 4 guideline implementation with feedback \u2022 4 guideline implementation with no feedback \u2022 4 computerized decision support \u2022 4 protocol or policy implementation Evidence-based Synthesis Program (ESP) ASP Intervention (# studies) Mortality Length of Stay Readmission CDI Prospective Audit and Feedback (14) + 1 study \u2248 9 studies \u2248 9 studies + 1 study \u2248 2 studies p=NR, 1 study Formulary Restriction and Preauthorization (5) \u2248 3 studies \u2248 2 studies NR + 1 study Guidelines with Feedback (4) \u2248 3 studies \u2248 3 studies NR + 2 studies Guidelines without Feedback (4) + 1 study \u2248 1study 1 study + 1 study \u2248 1study 1 study \u2248 1 study NR Computerized Decision Support (4) \u2248 3 studies + 1 study \u2248 2 studies \u2248 1 study + 1 study \u2248 1 study Protocols (4) + 1 study \u2248 2 studies + 2 studies \u2248 1 study \u2248 1 study NR KQ1 \u2013 Program Effectiveness: Clinical Outcomes Evidence-based Synthesis Program (ESP) KQ1 \u2013 Program Effectiveness: Prescribing Outcomes ASP Intervention (# studies) Use Selection Timing Duration Prospective Audit and Feedback (14) Decreased: + 8 studies Appropriate: + 1 study, \u2248 1 study + 1 study \u2248 1 study NR + 5 studies Formulary Restriction and Preauthorization (5) Decreased: + 4 studies NR NR + 1 study Guidelines with Feedback (4) Decreased: + 1 study Compliant\/appropriate: + 2 studies \u2248 1 study + 1 study \u2248 2 studies Guidelines without Feedback (4) Decreased: + 1 study Compliant\/ appropriate: + 2 studies, \u2248 1 study NR 1 study + 1 study \u2248 1 study Computerized Decision Support (4) Decreased: + 1 study, \u2248 1 study NR NR NR Protocols (4) Appropriate: \u2248 1 study NR \u2248 1 study + 2 studies Evidence-based Synthesis Program (ESP) KQ1 Program Effectiveness \u2022 Microbial Outcomes (9 studies) o 6 reported improvement \uf0d8 decreased infection or resistance o 3 reported no differences \u2022 Cost Outcomes (5 studies) o 4 reported decreased costs o 1 reported no difference Evidence-based Synthesis Program (ESP) KQ2 Results \u2013 Key Implementation Components \u2022 All author opinion (6 studies) o Consistent and persistent effort from qualified personnel o Effective communication skills o Support from electronic medical records or computerized decision support systems Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 Davey 2013 review o 9 from VA Medical Centers o Similar results (unchanged mortality [1], decreased CDI [2], mixed results prescribing [3], decreased infection [3]) \u2022 No recent studies from VA eligible \u2022 Most studies from University-affiliated hospitals Evidence-based Synthesis Program (ESP) KQ3 Results \u2013 Effectiveness in Different Settings or Different Suspected Conditions \u2022 ICU programs (9 studies) o Similar to overall results \u2022 Most studies included \u201cany suspected infection\u201d \u2022 Respiratory infection (7 studies) o Similar to overall results Evidence-based Synthesis Program (ESP) KQ4 Results \u2013 Harms of Programs \u2022 2 studies reported possible harms (other than patient, prescribing, and microbial harms from KQ1) o Anecdotal evidence of inappropriate switch to narrow-spectrum antimicrobial o Termination of program speculated to be due to provider dissatisfaction with prescribing restrictions Evidence-based Synthesis Program (ESP) KQ5 Results \u2013 Barriers to Implementation, Sustainability, Scalability \u2022 Barriers (4 studies; 2 with interview\/survey data) o Lack of familiarity, experience, awareness o Disagreement with guidelines\/conflicting guidelines o Lack of communication between professionals o Organizational constraints \u2022 Sustainability (1 study) o Use and costs decreased over 5 years of study and increased when study was terminated \u2022 Scalability (No studies) Evidence-based Synthesis Program (ESP) Discussion \u2022 Antimicrobial stewardship strategies can decrease antimicrobial prescribing and limit costs, without substantial harms \u2022 Greatest body of recent evidence is from audit and feedback programs \u2022 Systematic review of earlier studies provided evidence of comparable effects for persuasive and restrictive interventions \u2022 Evidence base for KQ1 was substantial; much less substantial for KQ2-5 Evidence-based Synthesis Program (ESP) Discussion \u2022 Studies not designed to adequately assess impact on mortality or other clinical outcomes \u2022 Suggestions for improving adherence to ASPs: o Involvement of stakeholders and opinion leaders in guideline and program development o Addition of quality improvement cycles o Understanding the prescribing culture o Collaboration between physicians and pharmacists (mostly opinion rather than evidence-based) Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations: \u2022 Quality of evidence: Low \u2022 Few randomized controlled trials \u2022 Limited ability to control for secular trends or other confounding variables \u2022 Possibility of regression to the mean in C. difficile rates \u2022 Findings for specific interventions have not been replicated Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Prescribing improvements often not sustained or long-term follow-up not reported \u2022 Most studies done in academic medical centers; generalizability to other settings is difficult. Evidence-based Synthesis Program (ESP) Discussion \u2022 Limitations (continued): \u2022 Within academic centers, substantial variation in settings, structures, patient bases, culture o Strength: stewardship has impact in diverse settings o Weakness: Hard to be sure that each specific intervention works across all settings \u2022 Few studies reported on harms of stewardship interventions; most not designed to identify harms, and most would not have been adequately powered to recognize them Evidence-based Synthesis Program (ESP) Future Research Needs \u2022 Given the complexity of antimicrobial prescribing, conducting well-designed studies is difficult \u2022 Large healthcare organizations should consider organizing stewardship activities (that are likely on-going within the organization) to provide useful information on comparative effectiveness of different intervention types Evidence-based Synthesis Program (ESP) Overall Conclusions \u2022 Despite these many shortcomings, the overall evidence suggests that antimicrobial stewardship interventions are associated with improved intended outcomes, mostly antimicrobial usage, over short time periods (1-3 years) \u2022 The literature supports hospital and healthcare system implementation of sensible, practical antimicrobial stewardship interventions to improve outcomes Evidence-based Synthesis Program (ESP) Recommendations \u2022 Data on antimicrobial use by clinical unit, type of patients, provider groups, and by individual providers should be gathered to determine where antimicrobial use might be less than ideal or is in need of improvement Evidence-based Synthesis Program (ESP) Recommendations \u2022 Existing infection prevention programs, microbiology laboratories, pharmacy services, infectious disease physicians, electronic medical record systems, continuous improvement programs, and staff or trainee education and certification programs should be utilized to contribute to stewardship activities \u2022 Leadership should also be informed and involved in planning; leadership support is essential Evidence-based Synthesis Program (ESP) Recommendations \u2022 Formative evaluation should be used to identify effective stewardship programs and programs in need of modification Evidence-based Synthesis Program (ESP)\n\n",
                "DataExportTag": "AI392307",
                "QuestionID": "QID408",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Antimicrobial Stewardship Programs in Inpatient Settings s Triaged: 6,334 Abstracts Excluded: 5,7...",
                "Choices": {
                    "1": {
                        "Display": "\"Cognitive Theory and Conceptual Modelling\""
                    },
                    "2": {
                        "Display": "cognitive, theory, concept, conceptual, human, interpretation, symbolic, structure, complex, abstraction, interaction, modelling, component, situation, description"
                    },
                    "3": {
                        "Display": "\"Explainable Artificial Intelligence and Automation Testing\""
                    },
                    "4": {
                        "Display": "explanation, testing, prediction, validation, user, trust, automate, developer, interpretability, safety, black_box, explainable, xai, case, automation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID328",
            "SecondaryAttribute": "Approaches for measuring signalling plasticity in the context of resistance to targeted cancer th...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Approaches for measuring signalling plasticity in the context of resistance to targeted cancer therapies. The ability of cells in multicellular organisms to respond to signals in their environment is critical for their survival, development and differentiation. Once differentiated and occupying their functional niche, cells need to maintain phenotypic stability while responding to diverse extracellular perturbations and environmental signals (such as nutrients, temperature, cytokines and hormones) in a co-ordinated manner. To achieve these requirements, cells have evolved numerous intracellular signalling mechanisms that confer on them the ability to resist, respond and adapt to external changes. Although fundamental to normal biological processes, as is evident from their evolutionary conservation, such mechanisms also allow cancer cells to evade targeted therapies, a problem of immediate clinical importance. In the present article, we discuss the role of signalling plasticity in the context of the mechanisms underlying both intrinsic and acquired resistance to targeted cancer therapies. We then examine the emerging analytical techniques and theoretical paradigms that are contributing to a greater understanding of signalling on a global and untargeted scale. We conclude with a discussion on how integrative approaches to the study of cell signalling have been used, and could be used in the future, to advance our understanding of resistance mechanisms to therapies that target the kinase signalling network.\n\n",
                "DataExportTag": "CAN844373",
                "QuestionID": "QID328",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Approaches for measuring signalling plasticity in the context of resistance to targeted cancer th...",
                "Choices": {
                    "1": {
                        "Display": "\"Cellular Biology and Aging Research\""
                    },
                    "2": {
                        "Display": "mitosis, stress, microtubule, homeostasis, aurora, aging, centrosome, membrane, cellular, clock, regulation, polarity, intracellular, dynamic, ion_channel"
                    },
                    "3": {
                        "Display": "\"Stem Cell Differentiation and Hematopoiesis\""
                    },
                    "4": {
                        "Display": "differentiation, hematopoietic_stem, progenitor, embryonic_stem, es, erythroid, mouse, lineage, notch, differentiate, hematopoiesis, esc, myeloid, hematopoietic_stem_cells, gene"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID310",
            "SecondaryAttribute": "Assessing the Implementation of American College of Surgeons Quality Indicators for Pancreatic Ca...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Assessing the Implementation of American College of Surgeons Quality Indicators for Pancreatic Cancer Across an Integrated Health System. PURPOSE\nThe American College of Surgeons (ACS) recently published quality assurance (QA) indicators for pancreatic cancer care. Implementing quality indicators in a newly formed health system may lead to better patient selection and standardized cancer care.\n\n",
                "DataExportTag": "CAN1088966",
                "QuestionID": "QID310",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Assessing the Implementation of American College of Surgeons Quality Indicators for Pancreatic Ca...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID121",
            "SecondaryAttribute": "AUTHORS' CONCLUSIONS There is low to very low quality evidence (according to GRADE criteria) that...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "AUTHORS' CONCLUSIONS\nThere is low to very low quality evidence (according to GRADE criteria) that image guided surgery using iMRI, 5-ALA or DTI-neuronavigation increases the proportion of patients with high grade glioma that have a complete tumour resection on post-operative MRI. There is a theoretical concern that maximising the extent of resection may lead to more frequent adverse events but this was poorly reported in the included studies. Effects of image guided surgery on survival and QoL are unclear. Further research, including studies of ultrasound guided surgery, is needed.\n\n",
                "DataExportTag": "49",
                "QuestionID": "QID121",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "AUTHORS' CONCLUSIONS There is low to very low quality evidence (according to GRADE criteria) that...",
                "Choices": {
                    "1": {
                        "Display": "\"Robotic Surgery and Urological Procedures\""
                    },
                    "2": {
                        "Display": "laparoscopic, rectal, robotic, renal, partial_nephrectomy, bladder, laparoscopy, robot_assist, pelvic, post_radical_prostatectomy, laparoscopic_radical, nephrectomy, prostatectomy, rectum, port"
                    },
                    "3": {
                        "Display": "\"Surgical Procedures and Complications in Thoracic and Gastrointestinal Medicine\""
                    },
                    "4": {
                        "Display": "rectal, resection, pancreatic, lung, esophagectomy, esophageal, vat, anastomosis, video_assist, lobectomy, anastomotic_leakage, total_mesorectal, mortality, gastric, thoracic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID204",
            "SecondaryAttribute": "Automated Diagnosis of Physical Systems \u201cIf anything can go wrong, it will.\u201d (Murphy\u2019s Law) Al",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Automated Diagnosis of Physical Systems \u201cIf anything can go wrong, it will.\u201d (Murphy\u2019s Law) Although deceptive in its simplicity, the above quote has proved to be a profound insight into how things happen. Things are likely to fail no matter how well the system is designed. When a high degree of reliability and safety is desired the effects of these failures must be mitigated, and control must be maintained under all fault scenarios. Faults need to be detected close to their onset so that quick action can be taken by resetting control parameters to compensate for the fault or by reconfiguring the system to minimize the effects of the fault and thus prevent damage. In this paper we provide a brief introduction to variety of automated techniques for diagnosing faults and then discuss in more detail one specific technology called HyDE. WHAT IS AUTOMATED DIAGNOSIS? Fault diagnosis involves the detection of anomalous system behavior and the identification of the cause for the deviant behavior. Automated diagnosis refers to the use of software technologies to assist in diagnosing faults in a system. This is to be contrasted with autonomous diagnosis which deals with software technologies that operate autonomously to detect, isolate and compensate for faults in a system. We would like to introduce some definitions (taken from IFAC Technical Committee SAFEPROCESS [1]) to set the context for the rest of the paper. Fault A fault is an unpermitted deviation of at least one characteristic property or parameter of the system from acceptable, usual or standard conditions. Fault Detection Fault detection is monitoring measured variables to determine if a fault has occurred in the plant. If a fault has occurred, it may be important to determine the time at which the fault occurred. Fault Isolation Fault Isolation is determining the type and location of a fault once it is known that a fault has occurred. It typically follows fault detection but the two processes are often combined for additive faults. Fault Identification Fault Identification is determining the size and time-variant behavior of a fault. It follows fault isolation. Fault may be of many types including: \u2022 Plant, actuator, sensor or controller faults \u2022 Additive or multiplicative faults \u2022 Abrupt or incipient faults \u2022 Persistent or intermittent faults Diagnosis is made harder by several factors including and not limited to: \u2022 Typical only a few sensors are placed leading to limited observability into system behavior. \u2022 The data from sensors are noisy due to inherent properties of the sensors \u2022 There are unknown inputs acting on the systems, due to lack of complete knowledge about conditions in which system operates. \u2022 The knowledge about how the system functions may be limited. \u2022 The effects of faults may be non-local and may take some time to manifest. In the rest of this paper we will introduce a sampling of automated diagnosis techniques that deal with some of the fault characteristics described above. We will briefly describe these techniques and then focus on one specific technique called HyDE. AUTOMATED DIAGNOSIS TECHNIQUES The diagnosis problem has been dealt with in several domains from several angles resulting in a wide variety of diagnosis approaches. Some of these include Expert Systems, Case-based reasoning, Data-driven techniques and Model-based reasoning. Expert System Diagnosis [2,3] Traditionally diagnosis was performed by human troubleshooting experts who built up diagnostic knowledge based on their expertise and experience. The natural extension to this was to encode the diagnostic knowledge in a machine storable structure. These structures took the form of associations between observed symptoms and probable fault occurrences. Tools were built to assist in the creation of these structures and then to use these structures for the diagnosis task. Once these structures are built, users and non-expert operators could troubleshoot the system in case of faults. Some of the more commonly used structures to encode expert diagnostic knowledge are rules and fault trees. A rule describes the action(s) that should be taken if a symptom is observed. A set of rules describing the symptoms of all the possible faults is incorporated into a rule-based reasoning system. The reasoning may use a backward-chaining algorithm which starts at the hypothesis (consequents of rules) and collects and verifies evidence (antecedents of rules) that supports the hypothesis. Alternately forward-chaining may be used where rules whose antecedents match observed symptoms are examined. When several rules match, a chain of rule firings (based on pre-defined rule priority) is used to establish the diagnosis. A fault tree (decision tree) encodes diagnostic knowledge as a sequence of questions that trace a path from the root of the tree to its leaf nodes which represent diagnoses. Advantages of expert systems are: \u2022 Diagnosis structure is \u201ccertified\u201d by experts and can be trusted to produce \u201ccorrect\u201d results. Proceedings of ICALEPCS07, Knoxville, Tennessee, USA FOAA01\n\n",
                "DataExportTag": "AI248278",
                "QuestionID": "QID204",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Automated Diagnosis of Physical Systems \u201cIf anything can go wrong, it will.\u201d (Murphy\u2019s Law) Altho...",
                "Choices": {
                    "1": {
                        "Display": "\"Operational Management and Automated Manufacturing Planning\""
                    },
                    "2": {
                        "Display": "management, planning, plan, decision_making, manufacturing, workflow, automation, production, business, planner, support, automate, military, operational, framework"
                    },
                    "3": {
                        "Display": "\"Healthcare Expert System and Patient Diagnosis\""
                    },
                    "4": {
                        "Display": "expert, diagnosis, healthcare, fault, acquisition, es, inference, reasoning, maintenance, kbs, design, inference_engine, patient, expertise, user"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID155",
            "SecondaryAttribute": "\u201cCUSTOM ASSISTANT PERSONAS\u201d \u00a0 A virtual, intelligent, or computational assistant (e.g., also re",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">\u201cCUSTOM ASSISTANT PERSONAS\u201d<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">A virtual, intelligent, or computational assistant (e.g., also referred to simply as an \u201cassistant\u201d) is described that is configured to have a customizable persona (e.g., attitude, voice, dialect, syntax, etc.). Assistants can have different personas when interacting with different users. For instance, an assistant may instantiate (e.g., use) one persona when interacting with a first user and another persona when interacting with a second user.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">DESCRIPTION&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">Assistants execute on counter-top devices, mobile phones, automobiles, and many other types of computing devices. Assistants output useful information, respond to users\u2019 needs, or otherwise perform certain operations to help users complete real-world and\/or virtual tasks. Some assistants may always use the same persona, regardless of with which users they are interacting. This persona may be selected by a publisher of the assistants to be generally acceptable to everyone, such as by using relatively neutral language and vocal tones.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">The example system shown in FIG. 1 provides an assistant that is configured to use different personas when interacting with different users. For example, the assistant may determine the identity of a user that is currently interacting with the assistant. The assistant may then select a persona based on the identity of the user and use the selected persona to interact with the user. The system of FIG. 1 includes one or more external systems and computing devices communicating across a network with each of the computing devices executing an assistant that performs operations involving groups of people. The network of FIG. 1 represents a combination of any one or more public or private communication networks, for instance, television broadcast networks, cable or satellite networks, cellular networks, Wi-Fi networks, broadband networks, and\/or other type of network for transmitting data (e.g., telecommunications and\/or media data) between various computing devices, systems, and other communications and media equipment.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">Computing devices represent any type of computing device, server, cloud computing environment, or other system that is configured to execute an assistant and communicate on a network. The external systems represent any type of server or other computing system that is configured to support the assistants executing at computing devices . Computing devices can be personal computing devices. In some examples, the external systems and\/or computing devices may be shared assets of multiple users. Examples of computing devices include mobile phones, tablet computers, wearable computing devices, countertop computing devices, home automation computing devices, laptop computers, desktop computers, televisions, stereos, automobiles, and any and all other types of mobile and non-mobile computing device that is configured to execute an assistant. For example, computing device A may be a countertop assistant device and computing device N may be a mobile phone or automobile infotainment system.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">A<\/span>n assistant executes across any combination of external systems one or more of computing devices to provide assistant services to users of computing devices . Examples of assistant services include: setting up reminders, creating calendar entries, booking travel, online ordering, sending messages or other communications, reading text aloud, controlling televisions, lights, thermostats, appliances, or other computing devices, providing navigational instructions, or any other conceivable task or operation that may be performed by an assistant.&nbsp;<\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">As a user interacts with the assistant, the assistant may obtain personal information about the user. Examples of personal information include: habits, voice samples, routines, preferences, notes, lists, contacts, communications, interests, assistant persona preferences, location histories, and other types of user information. After receiving explicit permission from the user, the assistant may store the personal information at user information data stores and in the course of providing assistant services, make use of the personal information stored at the user information data stores. The external systems and computing devices and the assistant treat the information stored at the information stores so that the information is protected, encrypted, or otherwise not susceptible to unauthorized use. The information stored at the information data stores may be stored locally at each of computing devices and\/or remotely (e.g., in a cloud computing environment provided by the external systems and which is accessible via the network of FIG. 1).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\" style=\"\">Further to the descriptions below, a user may be provided with controls allowing the user to make an election as to both if and when the assistant, the computing d<\/span>evice, or the computing systems described herein can collect or make use of supplemental data (e.g., user information or contextual information about a user\u2019s social network, social actions or activities, profession, a user\u2019s preferences, or a user\u2019s current location), and if and when the user is sent content or communications from a server. In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user\u2019s identity may be treated so that no personally identifiable information can be determined for the user, or a user\u2019s geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over what supplemental data is collected about the user, how that supplemental data is used, and what supplemental data is provided to the user.&nbsp;<\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">In operation, the assistant can change personas based on the identity of the user that is currently interacting with the assistant (i.e., the identity of the current user). For instance, an assistant may instantiate one persona when interacting with user A but instantiate a different persona when interacting with user B. The assistant may determine the identity of the current user using a variety of techniques. As one example, the assistant may use voice recognition to determine the identity of the current user. For instance, when a shared device (e.g., a countertop device) captures audio data provided by a user, the assistant may compare the audio data with voice prints of known users and determine the identity of the user that spoke the query. In particular, if the assistant determines that the captured audio data corresponds to a voice print of user A, the assistant may determine that the identity of the current user is user A and select a persona that is customized to user A when responding to the query. Similarly, if the assistant determines that the captured audio data corresponds to a voice print of user B, the assistant may determine that the identity of the current user is user B and select a persona that is customized to user B when responding to the query.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">As another example, when the assistant is being accessed via a non-shared device (e.g., a mobile phone of a particular user), the assistant may determine that the identity of the current user is the owner of the non-shared device (e.g., the particular user). For instance, if the nonshared device is owned (e.g., primarily used by, not necessarily owns in the property sense as the primary user of a mobile phone provided by their work or their parents may not actually legally own the mobile device) by user B, the assistant may select a persona that is customized to user B when interacting via the non-shared device.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-6f2e233e-7fff-cdb5-229e-e1d8597a98f5\">As stated above, the assistant can interact with users using various personas. Different personas may have different attitudes, voices, dialect selections, grammar, intelligence, etc. In some examples, a persona may be dictated by values of one or more attributes (e.g., humor, charm, sensuality, courage, tenacity, empathy, curiosity, imagination, decisiveness, patience, humility, meekness, coordination, candor, general intelligence, vivacity, etc.). The assistant personas may be customized to the users in a variety of ways. As one example, users may expressly customize the persona used by the assistant to interact with them. For instance, user A (or user A\u2019s parents) may adjust the values of one or more attributes of the assistant persona. As another example, the assistant may automatically customize the personas used to interact with users. For instance, the assistant may adjust one or more aspects of the persona used to interact with user A based on demographical data of user A (e.g., use more familiar, more colloquial, more basic vocabulary when interacting with a child than when interacting with an adult, e.g., \u201cpick out\u201d for \u201cselect\u201d) and\/or based on interactions with user A. By performing using a persona customized to a user when interacting with the user, the assistant may allow for more natural and productive interactions with the user.<\/span><\/p>",
                "DataExportTag": "AI18574",
                "QuestionID": "QID155",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\u201cCUSTOM ASSISTANT PERSONAS\u201d \u00a0 A virtual, intelligent, or computational assistant (e.g., also refe...",
                "Choices": {
                    "1": {
                        "Display": "\"Online Community Management and Hate Speech Detection\": social, social_medium, hate_speech, user, post, comment, facebook, forum, community, crowdsource, online, crowd, social_media, crowdsourcing, security_administration"
                    },
                    "2": {
                        "Display": "\"Chatbot Development and User Interaction\": chatbot, app, user, conversation, personality, developer, mobile, agent, human, dialogue, response, interface, answer, bug, student"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID153",
            "SecondaryAttribute": "\u201cMINING IN HYPERMEDIA CASE LIBRARIES\u201d \u00a0 The idea of case-based reasoning (CBR), came as a welco",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\"><b>\u201cMINING IN HYPERMEDIA CASE LIBRARIES\u201d<\/b><\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">The idea of case-based reasoning (CBR), came as a welcome alternative to the rule-base paradigm, which was dominating knowledge-based design during the 80s. Attribute-value tables and object-oriented representations were the dominating approaches to case representation. Hypermedia added to the case representation a collection of \"natural\" descriptions represented as text in free or table format and other multimedia data, such as images, video, sound, etc. However, reasoning algorithms didn't take advantage of this representation. In this paper, we present the idea of employing data mining techniques for extracting additional information from hypermedia cases and incorporating the knowledge into the case-based model.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\"><b>Case-based design using informal representations to manage complexity&nbsp;<\/b><\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Since the early seventies considerable research in artificial intelligence (AI) has been focused on the implementation of various knowledge-based computing models, which was inspired by a common initial idea, namely, to formalize and represent human expertise in machine-readable form, and employ it in computer support of human problem solving activities. Rule-based descriptions of human expertise, although difficult to obtain, and rule-based inference techniques were dominating the area during the 80s. Rule-based systems tended to be used to represent generalized expertise, or \u201crules-of-thumb\u201d. The idea of using analogy, recalling previous experience, came as a welcome alternative to the rule-base paradigm. Case-based reasoning (CBR) is a technique which implements this idea. A \"case\" represents the experience accumulated during the solution of relevant problems in the past (Aamodt and Plaza, 1994). The collection of data forms the case base or case library. Past experience can be described in a variety of forms, which span from a row or a number of rows in a database table to a number of volumes of documentation.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Considering computer implementations of the CBR paradigm, the representation of cases requires an abstraction of the experience into a form that can be manipulated by the reasoner, where the reasoner comprises procedural or heuristic modules for retrieving and selecting relevant cases and for adapting a selected case for a new problem. Sometimes the reasoner is assumed to be the user, rather than a computational process. What exactly is denoted by a case and how it is represented are major structural issues in CBR.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">The application of case-based reasoning to structural design (Maher et al., 1995), has shown that in the early systems, attribute-value pairs and object-oriented representations were the dominating approaches to case representation. Case models based on hypermedia representations are alternatives to the strict format of object-oriented and attribute-value representations. The hypermedia representations comprise a collection of \"natural\" descriptions represented as text in free or table format and other multimedia data, such as images, video, sound, etc. Another characteristic of hypermedia is the use of links, where the links can connect information within a case, between different cases, or links to data that lies outside the case library.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Case-based reasoning, as a design process, is illustrated in Figure 1. A case library provides several examples of designs and the basis for finding relevant designs to a new design problem. A new design problem provides some information that serves as the basis for recalling one or more design cases. A selected design case can then be adapted to be a new design. The resulting new design can be added to the case library, allowing the library to grow with use. This accumulation of experience is considered as the machine learning part of the case-based design computing model.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Case-based design, as an information rich process, readily made the shift towards hypermedia case representations. However, reasoning algorithms are based on attribute-value representations. Therefore, hypermedia design cases include an additional structured layer, and case indexing and selection is still based on the comparison of attribute values.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Following this paradigm, we have developed a hypermedia case library of buildings that focus on structural design. The library is referred to as SAM, for its use in teaching Structures And Materials to undergraduate architecture students. In developing SAM, we consider the issues raised by the need to organize the material within a multimedia case library of structural designs, while presenting the material using multimedia. Specifically, we consider:&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">- the need to represent and manage complex design cases,&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">- the need to formalize a typically informal body of knowledge or experiences.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">CASE LIBRARY&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Design in any domain usually involves the development and understanding of complex systems. The complex representations needed to adequately capture a design case have introduced challenges to CBR systems. As mentioned earlier, the CBR paradigm assumes that there is a concept of \"a case\", but in most design domains this concept accommodates a set of experiences and decisions resulting in a complex system. Three approaches to addressing complexity are:&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">1. A case is a hierarchy of concepts, or subcases&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">2. A case is represented by different views&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">3. A case is presented as hypermedia.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">A general approach to addressing domain complexity is the representation and reuse of parts of cases, typically organized as hierarchies of \u201csubcases.\u201d This supports case-based reasoning because subdividing designs in this way allows reasoning to focus only on the relevant parts of a design. By processing only some of the knowledge associated with a case, reasoning can become more efficient.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">The development of a case-base that has a hierarchical structure usually requires defining a typical decomposition of a design experience. The use of different views of a design case recognises that a design can be understood from different perspectives. In this approach, a single, complex design project is represented as multiple cases. The use of multimedia can make it easier to understand complex systems icons, images, sketches, etc. and can highlight and illustrate corresponding text or tabular information.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">The lack of formal knowledge in design affects both the ability to define a formal and consistent representation of design cases and the role of adaptation as a human-centered activity or an automated process. The development of CBR for design domains in which there is little formal or theoretical knowledge has been pursued by either formalizing knowledge that previously was not formalized, eg by using an object-oriented representation, or by identifying a representation of the design cases to support human reasoning rather than automated reasoning, eg by creating a multimedia presentation.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">Resolving the issue of \u201cwhat is in a design case?\u201d is done in many different ways. Contrary to the initial observation that case acquisition should be straightforward, most design stories told by designers or found in design documents are not easily formed into cases that can be indexed and classified for reuse. A systematic approach is needed to identify a uniform representation and to parse design stories into these formats.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">The representation of design cases in SAM follows the structural design principals that are taught in the Structures and Materials course. The overall organization of design information falls into three categories:&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">1. Project information,&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">2. Functional decomposition of the structural design, and&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-dfcab6e2-7fff-83da-c945-3e7740388f8a\">3. Structural system types.&nbsp;<\/span><\/p>",
                "DataExportTag": "AI44150",
                "QuestionID": "QID153",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\u201cMINING IN HYPERMEDIA CASE LIBRARIES\u201d \u00a0 The idea of case-based reasoning (CBR), came as a welcome...",
                "Choices": {
                    "1": {
                        "Display": "\"Edge Computing and Deployment of Lightweight DNNs\": dnn, deep_learning, pruning, energy, inference, gpu, dnns, deployment, mobile, automl, accelerator, deploy, latency, lightweight, edge"
                    },
                    "2": {
                        "Display": "\"High Performance Computing and GPU Optimization\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID158",
            "SecondaryAttribute": "\u201cTowards Web 3.0: A Unifying Architecture for Next Generation Web Applications\u201d \u00a0 While Web 2.0",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">&ldquo;Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications&rdquo;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">While Web 2.0 term is used to describe the current trend in the use of web technologies, Web 3.0 term is used to describe the next generation web, which will combine Semantic Web technologies, Web 2.0 principles and artificial intelligence. Towards this perspective, in this work we introduce a 3-tier architecture for web applications that will fit into the Web 3.0 definition. We present the fundamental features of this architecture, its components and their interaction, as well as the current technological limitations. Furthermore, some indicative application scenarios are outlined in order to illustrate the features of the proposed architecture. The aim of this architecture is to be a step towards supporting the development of intelligent semantic web applications of the near future as well as supporting the user collaboration and community-driven evolution of these applications.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">INTRODUCTION<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">Current trends in Web research and development seem to revolve around two major technological pillars: Social-driven applications, a main component in the Web 2.0 domain, and the Semantic Web. It is our firm belief that web semantics and Web 2.0 are complementary visions about the near future of the Web, rather than in competition: surely they can learn from each other in order to overcome their drawbacks, in a way that enables forthcoming web applications to combine Web 2.0 principles, especially those that focus on usability, community and collaboration, with the powerful Semantic Web infrastructure, which facilitates the information sharing among applications.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">Recently, the term Web 3.0 is used to describe the long-term future of the web (Lassila, 2007; Hendler, 2008). Web 3.0 will surely incorporate semantic web and Web 2.0 principles, but researchers believe that it will also include some more sophisticated concepts like artificial intelligence on the web. Towards this direction, in this work we propose a 3-tier architecture for web applications that will fit into the Web 3.0, the next generation web. At the lower layer of the architecture, we introduce and describe an advanced semantic knowledge base infrastructure that can support integration of multiple disparate data sources, without requiring a concrete underlying semantic structure. In addition, the upper layers of the architecture provide greater flexibility in the user interactions with the underlying ontological data model. As a result, it supports user collaboration and community-driven evolution of the next generation web applications. This architecture gives the developers the ability to build complicated web applications which combine the philosophy of Web 2.0 applications, and the powerful technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles on the Web. Furthermore, this architecture is well suited for supporting enhanced Knowledge Systems with advanced knowledge discovery characteristics, towards the future implementation of an Internet-scale Knowledge System. For example, the proposed architecture could be used to enrich current wiki applications towards next generation semantic wiki platforms that will mash-up scattered data sources and provide intelligent search capabilities. The following text is organized in five sections.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">In section 2 we start by providing some broad definitions and discussing the concepts of Semantic Web and Web 2.0. Furthermore, we discuss related work and the theoretical background of the research area. In section 3, we describe in detail the proposed architecture, its components, its fundamental features and the current technological limitations. In section 4, we outline some indicative application scenarios in order to illustrate the features of the proposed architecture and prove that it can be applied today and support modern web applications. Finally, we discuss future work and summarize our conclusions.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">BACKGROUND&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">As Semantic Web and Web 2.0 were firstly introduced separately by groups with completely contrary beliefs on the evolution of the World Wide Web, and even targeting different audiences, there has been a common perception that both are competing approaches for organizing and emerging the Web. The Semantic Web, outlined by Berners-Lee (2001), becomes a revolutionary technological approach for organizing and exchanging information in a cross-application dimension. Strongly supported by World Wide Web Consortium and powered by heavy academic and enterprise research, Semantic Web can demonstrate standardized and well-defined approaches in language description, such as RDF (Manola, 2004), RDF(S) (Brickley, 2004) and Web Ontology Language OWL (Smith, 2004), as well as research background in ontology engineering and modeling tools, from SHOE (Heflin, 1998) to Prot&eacute;g&eacute; (Knublauch, 2004). Semantic Web is powered by a strong AI background through its foundation on the Description Logics (DL) formalism (Baader, 2007). DL languages have become in recent years a well-studied formalism, originating from Semantic Networks and Frames and, as such, they have been extensively used in formal Semantic Web specifications and tools. These languages are of variable expressive strength which comes with the cost of increased computational complexity. Therefore, current research in this area is focused on efficient and advanced algorithms and procedures that would provide intelligent querying capabilities for the real word Web, based on DL descriptions and possibly subsets of and reductions from them that may exhibit more satisfying computational properties (Grau, 2008). One main reason for transforming the current Web to a Semantic Web is the ability to deduce new, unexpressed information that is only implied by existing descriptions. If the Web is to be considered as a huge, distributed knowledge base, then well-known AI techniques, at least for the part with sound foundations in logic, can be utilized in order to form the basis for intelligent negotiation and discovery on the Semantic Web. Such techniques may include for example deductive query answering and inference-based reasoning (Luke, 1996; Berners-Lee, 2001).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">On the other hand, the Web 2.0 term, introduced by Tim O&rsquo;Reilly (2005), represents a widely spread trend of adopting certain technologies and approaches in web development, targeting more flexible and user friendly applications, and easier distributed collaboration. The usability aspect is met by Rich Internet Applications (RIA) (Loosley, 2006) and especially Asynchronous JavaScript and XML (AJAX), which support the creation of responsive user interfaces as well as more interactive browsing experience. Collaboration conveniences come through the creation of virtual online communities of users that contribute effort and data to a common cause, achieving better results than each individual could do on his own. Finally there is a greater flexibility in data handling, enabling the development of hybrid web applications, called Mash-ups, which combine discrete data sources and services from different sites in order to provide a unified and enriched result. Therefore, the Semantic Web can provide a rich and powerful technical infrastructure for any kind of web application, while the paradigm of Web 2.0 applications can be used to provide useful guidelines, focusing on usability and collaboration. Thus, the Semantic Web and Web 2.0 principles can be combined as complementary approaches to provide more efficient web applications. Such applications could be thought to be part of the next generation&#39;s web and seem to fall under the term Web 3.0 (Hendler, 2008), which lately is sort of &ldquo;talk of the town&rdquo; (Lassila, 2007). In this context, there are several approaches; from developing AJAX tools for the Semantic Web (Oren, 2006) and studying the combination of ontologies and taxonomies (Mika, 2005), up to the proposition of sophisticated hybrid architectures, combining both of these technologies (Ankolekar, 2007). All of the above are of great use in any data-handling web application, and where there is need for a knowledge system. Especially for next generation knowledge systems that try to benefit from Web 2.0 approaches and collaborative development in order to build, or more precisely grow, Internet-scale knowledge systems (Tenenbaum, 2006).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">PROPOSED ARCHITECTURE&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">In this section we propose an architecture for web applications, which provides developers the ability to structure complicated web applications, which combine the vision of Web 2.0 and the rich technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles. Such applications could be next generation semantic wikis, intelligent mash-ups, semantic portals and in general any data-handling web application that intends to provide semantic information combined with advanced intelligent querying capabilities. The information of these applications could be delivered by two main ways:&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\">&nbsp;<\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">1. Directly to end users through the web-based interface of a stand-alone application&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">2. To other programs or services, that act as intermediaries with third-party web applications, by interacting with the API of our semantic infrastructure to retrieve precisely the information they need.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">A conformant implementation may follow the traditional 3-tier model, which lately (Hendler, 2008) is commonly used to support web 3.0 applications, with an important variation: Where a database server would be typically used, we now use a knowledge base system, since a traditional DBMS lacks the necessary features and functions for managing and utilizing ontological knowledge.&nbsp;<\/span><\/p>",
                "DataExportTag": "AI836231",
                "QuestionID": "QID158",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\u201cTowards Web 3.0: A Unifying Architecture for Next Generation Web Applications\u201d \u00a0 While Web 2.0 t...",
                "Choices": {
                    "1": {
                        "Display": "\"Cloud and Edge Computing Management\": cloud, cloud_computing, server, edge, workload, job, scheduling, edge_computing, latency, distribute, deployment, vm, mobile, storage, workflow"
                    },
                    "2": {
                        "Display": "\"High Performance Computing and GPU Optimization\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID274",
            "SecondaryAttribute": "\u201cTowards Web 3.0: A Unifying Architecture for Next Generation Web Applications\u201d \u00a0 While Web 2.0",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">&ldquo;Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications&rdquo;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">While Web 2.0 term is used to describe the current trend in the use of web technologies, Web 3.0 term is used to describe the next generation web, which will combine Semantic Web technologies, Web 2.0 principles and artificial intelligence. Towards this perspective, in this work we introduce a 3-tier architecture for web applications that will fit into the Web 3.0 definition. We present the fundamental features of this architecture, its components and their interaction, as well as the current technological limitations. Furthermore, some indicative application scenarios are outlined in order to illustrate the features of the proposed architecture. The aim of this architecture is to be a step towards supporting the development of intelligent semantic web applications of the near future as well as supporting the user collaboration and community-driven evolution of these applications.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">INTRODUCTION<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">Current trends in Web research and development seem to revolve around two major technological pillars: Social-driven applications, a main component in the Web 2.0 domain, and the Semantic Web. It is our firm belief that web semantics and Web 2.0 are complementary visions about the near future of the Web, rather than in competition: surely they can learn from each other in order to overcome their drawbacks, in a way that enables forthcoming web applications to combine Web 2.0 principles, especially those that focus on usability, community and collaboration, with the powerful Semantic Web infrastructure, which facilitates the information sharing among applications.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">Recently, the term Web 3.0 is used to describe the long-term future of the web (Lassila, 2007; Hendler, 2008). Web 3.0 will surely incorporate semantic web and Web 2.0 principles, but researchers believe that it will also include some more sophisticated concepts like artificial intelligence on the web. Towards this direction, in this work we propose a 3-tier architecture for web applications that will fit into the Web 3.0, the next generation web. At the lower layer of the architecture, we introduce and describe an advanced semantic knowledge base infrastructure that can support integration of multiple disparate data sources, without requiring a concrete underlying semantic structure. In addition, the upper layers of the architecture provide greater flexibility in the user interactions with the underlying ontological data model. As a result, it supports user collaboration and community-driven evolution of the next generation web applications. This architecture gives the developers the ability to build complicated web applications which combine the philosophy of Web 2.0 applications, and the powerful technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles on the Web. Furthermore, this architecture is well suited for supporting enhanced Knowledge Systems with advanced knowledge discovery characteristics, towards the future implementation of an Internet-scale Knowledge System. For example, the proposed architecture could be used to enrich current wiki applications towards next generation semantic wiki platforms that will mash-up scattered data sources and provide intelligent search capabilities. The following text is organized in five sections.&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">In section 2 we start by providing some broad definitions and discussing the concepts of Semantic Web and Web 2.0. Furthermore, we discuss related work and the theoretical background of the research area. In section 3, we describe in detail the proposed architecture, its components, its fundamental features and the current technological limitations. In section 4, we outline some indicative application scenarios in order to illustrate the features of the proposed architecture and prove that it can be applied today and support modern web applications. Finally, we discuss future work and summarize our conclusions.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">BACKGROUND&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">As Semantic Web and Web 2.0 were firstly introduced separately by groups with completely contrary beliefs on the evolution of the World Wide Web, and even targeting different audiences, there has been a common perception that both are competing approaches for organizing and emerging the Web. The Semantic Web, outlined by Berners-Lee (2001), becomes a revolutionary technological approach for organizing and exchanging information in a cross-application dimension. Strongly supported by World Wide Web Consortium and powered by heavy academic and enterprise research, Semantic Web can demonstrate standardized and well-defined approaches in language description, such as RDF (Manola, 2004), RDF(S) (Brickley, 2004) and Web Ontology Language OWL (Smith, 2004), as well as research background in ontology engineering and modeling tools, from SHOE (Heflin, 1998) to Prot&eacute;g&eacute; (Knublauch, 2004). Semantic Web is powered by a strong AI background through its foundation on the Description Logics (DL) formalism (Baader, 2007). DL languages have become in recent years a well-studied formalism, originating from Semantic Networks and Frames and, as such, they have been extensively used in formal Semantic Web specifications and tools. These languages are of variable expressive strength which comes with the cost of increased computational complexity. Therefore, current research in this area is focused on efficient and advanced algorithms and procedures that would provide intelligent querying capabilities for the real word Web, based on DL descriptions and possibly subsets of and reductions from them that may exhibit more satisfying computational properties (Grau, 2008). One main reason for transforming the current Web to a Semantic Web is the ability to deduce new, unexpressed information that is only implied by existing descriptions. If the Web is to be considered as a huge, distributed knowledge base, then well-known AI techniques, at least for the part with sound foundations in logic, can be utilized in order to form the basis for intelligent negotiation and discovery on the Semantic Web. Such techniques may include for example deductive query answering and inference-based reasoning (Luke, 1996; Berners-Lee, 2001).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">On the other hand, the Web 2.0 term, introduced by Tim O&rsquo;Reilly (2005), represents a widely spread trend of adopting certain technologies and approaches in web development, targeting more flexible and user friendly applications, and easier distributed collaboration. The usability aspect is met by Rich Internet Applications (RIA) (Loosley, 2006) and especially Asynchronous JavaScript and XML (AJAX), which support the creation of responsive user interfaces as well as more interactive browsing experience. Collaboration conveniences come through the creation of virtual online communities of users that contribute effort and data to a common cause, achieving better results than each individual could do on his own. Finally there is a greater flexibility in data handling, enabling the development of hybrid web applications, called Mash-ups, which combine discrete data sources and services from different sites in order to provide a unified and enriched result. Therefore, the Semantic Web can provide a rich and powerful technical infrastructure for any kind of web application, while the paradigm of Web 2.0 applications can be used to provide useful guidelines, focusing on usability and collaboration. Thus, the Semantic Web and Web 2.0 principles can be combined as complementary approaches to provide more efficient web applications. Such applications could be thought to be part of the next generation&#39;s web and seem to fall under the term Web 3.0 (Hendler, 2008), which lately is sort of &ldquo;talk of the town&rdquo; (Lassila, 2007). In this context, there are several approaches; from developing AJAX tools for the Semantic Web (Oren, 2006) and studying the combination of ontologies and taxonomies (Mika, 2005), up to the proposition of sophisticated hybrid architectures, combining both of these technologies (Ankolekar, 2007). All of the above are of great use in any data-handling web application, and where there is need for a knowledge system. Especially for next generation knowledge systems that try to benefit from Web 2.0 approaches and collaborative development in order to build, or more precisely grow, Internet-scale knowledge systems (Tenenbaum, 2006).&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><b id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">PROPOSED ARCHITECTURE&nbsp;<\/b><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">In this section we propose an architecture for web applications, which provides developers the ability to structure complicated web applications, which combine the vision of Web 2.0 and the rich technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles. Such applications could be next generation semantic wikis, intelligent mash-ups, semantic portals and in general any data-handling web application that intends to provide semantic information combined with advanced intelligent querying capabilities. The information of these applications could be delivered by two main ways:&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\">&nbsp;<\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">1. Directly to end users through the web-based interface of a stand-alone application&nbsp;<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">2. To other programs or services, that act as intermediaries with third-party web applications, by interacting with the API of our semantic infrastructure to retrieve precisely the information they need.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-28a8bde7-7fff-f480-a87d-b3ecd2a8a4f5\">A conformant implementation may follow the traditional 3-tier model, which lately (Hendler, 2008) is commonly used to support web 3.0 applications, with an important variation: Where a database server would be typically used, we now use a knowledge base system, since a traditional DBMS lacks the necessary features and functions for managing and utilizing ontological knowledge.&nbsp;<\/span><\/p>",
                "DataExportTag": "AI836231",
                "QuestionID": "QID274",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\u201cTowards Web 3.0: A Unifying Architecture for Next Generation Web Applications\u201d \u00a0 While Web 2.0 t...",
                "Choices": {
                    "1": {
                        "Display": "\"Cloud and Edge Computing Management\": cloud, cloud_computing, server, edge, workload, job, scheduling, edge_computing, latency, distribute, deployment, vm, mobile, storage, workflow"
                    },
                    "2": {
                        "Display": "\"High Performance Computing and GPU Optimization\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID157",
            "SecondaryAttribute": "\u201cTrack Degradation Prediction Models, Using Markov Chain, Artificial Neural and Neuro-Fuzzy Netwo.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">&ldquo;Track Degradation Prediction Models, Using Markov Chain, Artificial Neural and Neuro-Fuzzy Network&rdquo;<\/b><br \/>\n&nbsp;<\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Track condition is one of the most important parameters affecting the track maintenance management. In order to obtain a good track maintenance management system, it is necessary to predict track condition through the time. In this study, the track&rsquo;s state will be defined in terms of the Combine Track Record index (CTR) rating which can vary from 0 to 100 where 100 denotes the best possible track condition and the states are defined as five intervals of CTR, similar to those used in Iranian Railways.<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Four models are built and tested for the prediction of track quality, one conventional model suggested by ORE, and three new models, using markov chain, artificial neural network and neurofuzzy network. The data for our empirical application was collected from the Iranian Railways network. Comparisons of the models show that all three proposed new models predict track deterioration better than the ORE model.&nbsp;<\/span>&nbsp;<\/p>\n\n<p dir=\"ltr\"><br \/>\n<b id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Introduction&nbsp;<\/b><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">The railway is a branch of the transportation system that is very expensive to construct but it has a long life and low operating costs. Therefore, the asset value is very high, which also leads to the possibility that maintenance might be expensive. Like other infrastructure with investment costs in the construction phase, maintenance plays a crucial role in the long-term cost effectiveness, and so maintenance management is one of the most important parts of the railway systems.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Track condition is one of the most important parameters affecting the track maintenance management. In order to obtain a good track maintenance management system, it is necessary to predict track condition through the time. The cost of maintaining a track depends directly on its condition or &quot;state&quot;. Maintenance and rehabilitation funds are often allocated to tracks that are in the worst state or that exhibit an accelerating rate of deterioration.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Some of factors that affect the rate of deterioration of tracks include: traffic loads, weather, and construction materials. The track conditions vary considerably even when such contributing factors are similar. Therefore, it is important that a procedure that can accommodate such randomness be incorporated into a track management system.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Several approaches and methods for predicting railway conditions have been proposed, and based on these, a considerable number of maintenance planning tools have been developed for railways systems in North America and Europe. A nonlinear regression model based on a product of the power functions has been proposed by the Office for Research and Experiments (ORE) of the International Union of Railways to predict track deterioration. Having track degradation models, operations research techniques are commonly used to optimize track maintenance activity.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\" style=\"\">Such approaches have been described by Esveld and Zarembski. Zhang has proposed an Integrated Track Degradation Model (ITDM). ITDM simulates track degradation based on the interaction between different track components under varying traffic. It also considers several mechanistic characteristics, including train speed and axle load. Based on the ITDM, Simson has developed a Track Maintenance Planning Model (TMPM). It aims to deal with track maintenance planning in the medium to long term. TMPM outputs the net present value of the financial benefits of undertaking a given maintenance strategy c<\/span>ompared with a base-case maintenance scenario. The Total Right-Of-Way Analysis and Costing System (TRACS) has been described by Martland. It is a system (software) developed by the Association of American Railroads (AAR) and Massachusetts Institute of Technology (MIT), in the USA. It is a computer-based tool developed to assist rail management to address change in the infrastructure. By combining engineering-based deterioration models with lifecycle costing techniques the model estimates track maintenance and renewal costs as a function of route geometry, track components, track condition, as well as traffic mix and volume. TRACS has been used by North American railroads as a tool for technology assessment costing in support of actions such as pricing, budgeting, and line consolidation. Both the ITDM and the TRACS models are based on an incremental approach where each event such as rail grinding, relining, and track renewal can be included.&nbsp;<\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">In recent years the application of soft computing techniques has been paid attention to predicting the future track conditions. Shafahi and Rasooli have considered neural-networks to predict future track conditions. A neuro-fuzzy decision support system for rail track maintenance planning has been described by Dell&#39;Orco. During the 1990s, the International Railway Union (UIC) in conjunction with the European Rail Research Institute (ERRI) developed an expert system for track maintenance and renewal (ECOTRACK). This model builds on the fact that rules can be specified for certain maintenance activities under certain conditions. A historical database containing infrastructure information on components and current condition is also a prerequisite to use this model. ECOTRACK solves the planning problem given the rules specified and points out the activities needed at a section at a certain time. Recent work on ECOTRACK at ERRI has developed the model further in order to improve its functionality. To describe the condition of the track, several indexes and criteria have been defined and used in different railway systems around the world. Commonly, the track quality index (TQI) is used to define the track quality. TQI is normally determined by track geometry parameters (TGP).<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">The term track deterioration is used to describe any changes in track geometry. Track deteriorations are classified as: unevenness, twist, alignment, and gauge. TQI is a function of these four parameters.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">In this study, the track&rsquo;s state will be defined in terms of the Combined Track Record index (CTR index) rating which can vary from 0 to 100 where 100 denotes the best possible track condition and the states are defined as five intervals of CTR index. It is supposed that the track began its life at some time in the past in near-perfect condition. It was then subject to a sequence of duty cycles that caused its condition to deteriorate. The duty cycle for the track in this study will be assumed to consist of one year&rsquo;s weather and traffic load. These discrete state and discrete time unit definitions let us to express the deterioration process as a Markov chain.&nbsp;<\/span><\/p>",
                "DataExportTag": "AI21960",
                "QuestionID": "QID157",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\u201cTrack Degradation Prediction Models, Using Markov Chain, Artificial Neural and Neuro-Fuzzy Netwo...",
                "Choices": {
                    "1": {
                        "Display": "\"Cloud and Edge Computing Management\": cloud, cloud_computing, server, edge, workload, job, scheduling, edge_computing, latency, distribute, deployment, vm, mobile, storage, workflow"
                    },
                    "2": {
                        "Display": "\"High Performance Computing and GPU Optimization\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID273",
            "SecondaryAttribute": "\u201cTrack Degradation Prediction Models, Using Markov Chain, Artificial Neural and Neuro-Fuzzy Netwo.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p dir=\"ltr\"><b id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">\u201cTrack Degradation Prediction Models, Using Markov Chain, Artificial Neural and Neuro-Fuzzy Network\u201d<\/b><\/p><p dir=\"ltr\"><b><br><\/b><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Track condition is one of the most important parameters affecting the track maintenance management. In order to obtain a good track maintenance management system, it is necessary to predict track condition through the time. In this study, the track\u2019s state will be defined in terms of the Combine Track Record index (CTR) rating which can vary from 0 to 100 where 100 denotes the best possible track condition and the states are defined as five intervals of CTR, similar to those used in Iranian Railways.<\/span><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Four models are built and tested for the prediction of track quality, one conventional model suggested by ORE, and three new models, using markov chain, artificial neural network and neurofuzzy network. The data for our empirical application was collected from the Iranian Railways network. Comparisons of the models show that all three proposed new models predict track deterioration better than the ORE model.&nbsp;<\/span>&nbsp;<\/p>\n\n<p dir=\"ltr\"><br>\n<b id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Introduction&nbsp;<\/b><\/p>\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">The railway is a branch of the transportation system that is very expensive to construct but it has a long life and low operating costs. Therefore, the asset value is very high, which also leads to the possibility that maintenance might be expensive. Like other infrastructure with investment costs in the construction phase, maintenance plays a crucial role in the long-term cost effectiveness, and so maintenance management is one of the most important parts of the railway systems.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Track condition is one of the most important parameters affecting the track maintenance management. In order to obtain a good track maintenance management system, it is necessary to predict track condition through the time. The cost of maintaining a track depends directly on its condition or \"state\". Maintenance and rehabilitation funds are often allocated to tracks that are in the worst state or that exhibit an accelerating rate of deterioration.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Some of factors that affect the rate of deterioration of tracks include: traffic loads, weather, and construction materials. The track conditions vary considerably even when such contributing factors are similar. Therefore, it is important that a procedure that can accommodate such randomness be incorporated into a track management system.<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">Several approaches and methods for predicting railway conditions have been proposed, and based on these, a considerable number of maintenance planning tools have been developed for railways systems in North America and Europe. A nonlinear regression model based on a product of the power functions has been proposed by the Office for Research and Experiments (ORE) of the International Union of Railways to predict track deterioration. Having track degradation models, operations research techniques are commonly used to optimize track maintenance activity.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\" style=\"\">Such approaches have been described by Esveld and Zarembski. Zhang has proposed an Integrated Track Degradation Model (ITDM). ITDM simulates track degradation based on the interaction between different track components under varying traffic. It also considers several mechanistic characteristics, including train speed and axle load. Based on the ITDM, Simson has developed a Track Maintenance Planning Model (TMPM). It aims to deal with track maintenance planning in the medium to long term. TMPM outputs the net present value of the financial benefits of undertaking a given maintenance strategy c<\/span>ompared with a base-case maintenance scenario. The Total Right-Of-Way Analysis and Costing System (TRACS) has been described by Martland. It is a system (software) developed by the Association of American Railroads (AAR) and Massachusetts Institute of Technology (MIT), in the USA. It is a computer-based tool developed to assist rail management to address change in the infrastructure. By combining engineering-based deterioration models with lifecycle costing techniques the model estimates track maintenance and renewal costs as a function of route geometry, track components, track condition, as well as traffic mix and volume. TRACS has been used by North American railroads as a tool for technology assessment costing in support of actions such as pricing, budgeting, and line consolidation. Both the ITDM and the TRACS models are based on an incremental approach where each event such as rail grinding, relining, and track renewal can be included.&nbsp;<\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">In recent years the application of soft computing techniques has been paid attention to predicting the future track conditions. Shafahi and Rasooli have considered neural-networks to predict future track conditions. A neuro-fuzzy decision support system for rail track maintenance planning has been described by Dell'Orco. During the 1990s, the International Railway Union (UIC) in conjunction with the European Rail Research Institute (ERRI) developed an expert system for track maintenance and renewal (ECOTRACK). This model builds on the fact that rules can be specified for certain maintenance activities under certain conditions. A historical database containing infrastructure information on components and current condition is also a prerequisite to use this model. ECOTRACK solves the planning problem given the rules specified and points out the activities needed at a section at a certain time. Recent work on ECOTRACK at ERRI has developed the model further in order to improve its functionality. To describe the condition of the track, several indexes and criteria have been defined and used in different railway systems around the world. Commonly, the track quality index (TQI) is used to define the track quality. TQI is normally determined by track geometry parameters (TGP).<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">The term track deterioration is used to describe any changes in track geometry. Track deteriorations are classified as: unevenness, twist, alignment, and gauge. TQI is a function of these four parameters.&nbsp;<\/span><\/p>\n&nbsp;\n\n<p dir=\"ltr\"><span id=\"docs-internal-guid-9ee39839-7fff-ce58-87cb-e2f57e6ecbfe\">In this study, the track\u2019s state will be defined in terms of the Combined Track Record index (CTR index) rating which can vary from 0 to 100 where 100 denotes the best possible track condition and the states are defined as five intervals of CTR index. It is supposed that the track began its life at some time in the past in near-perfect condition. It was then subject to a sequence of duty cycles that caused its condition to deteriorate. The duty cycle for the track in this study will be assumed to consist of one year\u2019s weather and traffic load. These discrete state and discrete time unit definitions let us to express the deterioration process as a Markov chain.&nbsp;<\/span>\u200b\u200b\u200b\u200b\u200b\u200b\u200b<\/p>",
                "DataExportTag": "AI21960",
                "QuestionID": "QID273",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "\u201cTrack Degradation Prediction Models, Using Markov Chain, Artificial Neural and Neuro-Fuzzy Netwo...",
                "Choices": {
                    "1": {
                        "Display": "\"Cloud and Edge Computing Management\": cloud, cloud_computing, server, edge, workload, job, scheduling, edge_computing, latency, distribute, deployment, vm, mobile, storage, workflow"
                    },
                    "2": {
                        "Display": "\"High Performance Computing and GPU Optimization\": deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID319",
            "SecondaryAttribute": "Benefits and Harms of Computed Tomography Lung Cancer Screening Strategies: A Comparative Modelin...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Benefits and Harms of Computed Tomography Lung Cancer Screening Strategies: A Comparative Modeling Study for the U.S. Preventive Services Task Force The burden of lung cancer in the world remains extremely high: The International Agency for Research on Cancer estimated 1.6 million new diagnoses in 2008 (12.7% of total cases of cancer) and 1.4 million deaths (18.2% of total cancer mortality) (1). In the United States and Canada, incidence (per 100000) is 48.5 for men and 35.8 for women; mortality (per 100000) is 37.9 and 24.2, respectively; and cumulative risk (to age 74 years) of dying of lung cancer is 3% in women and 4.6% in men. In the United States, 228000 new cases of lung cancer and about 160000 deaths are estimated for 2013 (2). Despite substantial reductions in smoking prevalence in the United States, which translated into an approximately 32% reduction in lung cancer mortality between 1975 and 2000 at the population level (3), lung cancer remains the leading cause of cancer death. Recently, the National Lung Screening Trial (NLST) demonstrated that in a volunteer population of current and former smokers who were aged 55 to 74 years at entry, had at least 30 pack-years of cigarette smoking history, and had quit no more than 15 years previously (for former smokers), 3 annual computed tomography (CT) screening examinations reduced lung cancerspecific mortality by 20% relative to 3 annual chest radiography screening examinations at a median follow-up of 6.5 years (4). This trial did not directly address the effects of additional rounds of screening, long-term benefits or harms, or multiple alternative screening policies with different screening intervals and different eligibility criteria. Moreover, long-term outcomes must be quantified to understand the tradeoffs between benefits and potential harms involved with alternative screening strategies (5). In this study, we estimate future harms and benefits of lung cancer screening and identify a set of possible efficient lung cancer screening policies by using 5 separately developed microsimulation models calibrated to the 2 largest randomized, controlled trials on lung cancer screening. This work was initiated by the U.S. Preventive Services Task Force (USPSTF) to inform its recommendations on lung cancer screening. Methods Calibration of 5 Models to Deidentified Lung Cancer Screening Data From the NLST and Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial We used 5 models calibrated to individual-level, de-identified data from the NLST (6) and the PLCO (Prostate, Lung, Colorectal, and Ovarian Cancer Screening) trial (7). The NLST enrolled 53452 persons at high risk for lung cancer at 33 U.S. centers from August 2002 through April 2004. Participants were randomly assigned to undergo 3 annual screening examinations with low-dose CT (26722 participants) or single-view posterioranterior chest radiography (26730 participants). The PLCO trial randomly assigned 154901 participants aged 55 through 74 years at entry, 77445 of whom were assigned to annual chest radiography and 77456 to usual care between November 1993 and July 2001. There was no eligibility requirement concerning smoking. Although the PLCO trial compared chest radiography with no screening, it provided information on the natural history of lung cancer. Groups of investigators at the following 5 institutions independently developed the models: Erasmus Medical Center in Rotterdam, the Netherlands (model E); Fred Hutchinson Cancer Research Center in Seattle, Washington (model F); the Massachusetts General Hospital in Boston, Massachusetts (model M); Stanford University in Stanford, California (model S); and the University of Michigan in Ann Arbor, Michigan (model U). Each model estimates screening effectiveness on the basis of a different set of assumptions that are key in predicting the effects of earlier treatment, and each model uses different mathematical formalisms and model structures. In essence, all the models account for the individual's age-specific smoking-related risk for lung cancer, date and stage of lung cancer diagnosis, the corresponding lung cancer mortality, and the individual's life expectancy in the presence and absence of screening (Appendix Figure 1). Appendix Figure 1. Diagram of how earlier detection (followed by treatment) may have an effect on reducing serious consequences of the disease and\/or increasing life expectancy. All models account for the individual's age-specific smoking-related risk for lung cancer, the date and stage of lung cancer diagnosis, the corresponding lung cancer mortality, and the individual's life expectancy in the presence and absence of screening. By replicating trial detection, models estimate key parameters of the screening-detectable period and\/or sensitivity and can subsequently estimate cancer detected in the screening scenarios. In essence, when a model incorporates the exact demographic characteristics of participants and the design of a trial, it should be able to reproduce cumulative incidence of lung cancer (by stage, histologic features, sex, age, type of detection, and round) and lung cancer mortality in both groups as closely as possible. The best fit is often defined as the lowest deviance between observed and model-expected numbers. For correct extrapolation of different possible screening scenarios, one must obtain the best estimates on several key parameters, including the duration of the screening-detectable preclinical period (by test, age, histologic characteristics, and sex), sensitivity (by test, age, and sex), and improvement in prognosis by earlier detection and treatment. All models were first set to mimic the design of both trials (for example, setting the numbers of screening examinations and screening method, ages at screening, smoking history and sex of enrollees, and screening intervals). The models are validly calibrated when the key parameterswhich may differ by modelcan be estimated or adjusted to replicate the trial data closely. After calibration, the models reproduce the observed cumulative incidence of lung cancer (by stage, histologic characteristics, sex, age, type of detection, and round of screening) and lung cancer mortality in both groups of the trials. Close calibration to the 19% (95% CI, 7% to 25%) lung cancer mortality difference between groups of the NLST at 6 years of follow-up was prioritized (see Appendix Figure 2 and Appendix Table, for key similarities and differences among the 5 models in calibration targets.) Appendix Figure 2. Percentage and 95% CI of lung cancer mortality in chest radiography group compared with computed tomography group in the NLST, by follow-up duration and comparison with 5 model group results. As stated in the Methods section, close calibration to difference in lung cancer mortality between groups of the NLST at 6 years' follow-up was prioritized, but not the slope before year 6. This was done on purpose because mortality differences in the first years of trials are subject to chance and small numbers. E = Erasmus Medical Center; F = Fred Hutchinson Cancer Research Center; M = Massachusetts General Hospital; NLST = National Lung Screening Trial; S = Stanford University; U = University of Michigan. Appendix Table. Key Similarities and Differences Between the Models in Estimating Effects on Life Expectancy With an Effective Lung Cancer Screening Test Choosing Screening Programs and Expressing Harms and Benefits The modeling groups standardized input data on smoking histories and nonlung cancer mortality to simulate life histories of the U.S. cohort born in 1950 by using an updated version of the National Cancer Institute's Smoking History Generator (811). All models included other-cause mortality to differ by sex, age, smoking status, and smoking intensity. A set of 576 programs that varied frequency of CT screening for lung cancer (1-, 2-, or 3-year intervals), ages of starting (45, 50, 55, or 60 years) and stopping (75, 80, or 85 years) screening (assuming that a last screening examination is included at this age), and eligibility based on smoking history (10, 20, 30, or 40 pack-years; having quit smoking 10, 15, 20, or 25 years previously) was examined, with analyses run separately for men and women. In all scenarios, perfect screening adherence was assumed. Once a person's characteristics did not satisfy the eligibility criteria (such as passing the limit of years since smoking cessation), he or she would not be invited for future screenings. Potential benefits are expressed as lung cancer deaths averted and life-years gained. Potential harms are expressed as the number of screening examinations plus follow-up imaging examinations, number of false-positive results (including findings on surgery and biopsy), number of overdiagnosed lung cancer cases, and number of radiation-related lung cancer deaths. Follow-up procedures were assumed to be consistent with the observed rate of examinations per positive screening examination in the NLST; 2 models used explicit follow-up algorithms based on nodule size thresholds. False-positive results were estimated as a direct proportion to the number of CT screening examinations, as based on the average in 3 rounds of the NLST; we assumed that a false-positive result in a given round did not influence the probability of a false-positive result in subsequent rounds. Overdiagnosed cases are the additional number of lung cancer cases detected in the screening scenarios compared with the estimated number of cases diagnosed in the absence of screening (12). All models simulate the underlying natural history of lung cancer (separately by histologic type) in individuals and include doseresponse modules that relate a detailed cigarette smoking history over time to lung cancer risk. Each comparison is based on an identical underlying simulated cohort of individuals with the same smoking histories, sex composition, and potential times of other-cause death. Another scenario that reflects overdiagnosis is a person who has lung cancer that is expected to be clinically detected after\n\n",
                "DataExportTag": "CAN240809",
                "QuestionID": "QID319",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Benefits and Harms of Computed Tomography Lung Cancer Screening Strategies: A Comparative Modelin...",
                "Choices": {
                    "1": {
                        "Display": "\"Survival Analysis and Prognosis in Medical Studies\""
                    },
                    "2": {
                        "Display": "overall_survival, heart_rate, prognosis, dfs, hazard_ratio, survival, progression_free_survival, rfs, kaplan_meier, css, log_rank, cox_proportional, independent_prognostic, multivariate_analysis, dss"
                    },
                    "3": {
                        "Display": "\"Cancer Treatment and Survival Analysis\""
                    },
                    "4": {
                        "Display": "overall_survival, lung, rectal, radiotherapy, chemotherapy, non_small_cell_lung_cancer, heart_rate, adenocarcinoma, dfs, adjuvant_chemotherapy, adjuvant, survival, neoadjuvant, squamous_cell_carcinoma, ac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID161",
            "SecondaryAttribute": "BIG DATA FOR SMART SOCIETY The general objective of GATE is to establish Centre of Excellence on...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "BIG DATA FOR SMART SOCIETY The general objective of GATE is to establish Centre of Excellence on \u201cBig Data for Smart Society \u2013 GATE\u201d that will fulfill the vision of Open Innovation through building a sustainable University-Government-Industry-Society ecosystem. The CoE will be established as joint initiative between Sofia University \u2013 the most prestigious educational and scientific hub in Bulgaria and Chalmers University of Technology, Sweden \u2013 leading European institution with extensive experience in research, education and innovation. CoE main research objective is to advance the state-of-the-art in the whole Big Data Value Chain, including development of advanced methods and tools for data collection from variety of structured and unstructured sources, data consistency checking and cleaning, data aggregation and linking, data processing, modeling and analysis, data delivery by providing both accessibility and proper visualization. Following challenges of the Programme Horizon 2020 and the Bulgarian Innovation Strategy for Smart Specialization 2014-2020, the project team selected the most promising Data Driven Innovation Pillars: Data Driven Government (Public Services based on Open Data); Data Driven Industry (Manufacturing and Production); Data Driven Society (Smart Cities); and Data Driven Science.The specific objectives of Teaming 1 phase are to establish a solid background for the creation and sustainability of a CoE at a national, regional and EU level through (1) Elaboration of a detailed and robust business plan with a long term vision for setting-up and sustaining of the CoE; (2) Strengthening the research capacity and potential in Big Data; (3) Establishment of an international collaborative network of Big Data and related fields researchers; (4) Increasing quality of education and training and offering measures for motivation and involvement of the next-generation Early-Stage Researchers; and (5) Wide dissemination and promotion of project aims, activities and expected outputs.\n\n",
                "DataExportTag": "COR51178",
                "QuestionID": "QID161",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "BIG DATA FOR SMART SOCIETY The general objective of GATE is to establish Centre of Excellence on...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\""
                    },
                    "2": {
                        "Display": "computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "3": {
                        "Display": "\"Nuclear Transport and Digital Twin Experimentation\""
                    },
                    "4": {
                        "Display": "nuclear, transport, interoperable, enabler, experimentation, agile, certification, toolbox, factory, testbed, bim, broad, federation, vertical, digital_twin"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID222",
            "SecondaryAttribute": "BIG DATA FOR SMART SOCIETY The general objective of GATE is to establish Centre of Excellence on...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "BIG DATA FOR SMART SOCIETY The general objective of GATE is to establish Centre of Excellence on \u201cBig Data for Smart Society \u2013 GATE\u201d that will fulfill the vision of Open Innovation through building a sustainable University-Government-Industry-Society ecosystem. The CoE will be established as joint initiative between Sofia University \u2013 the most prestigious educational and scientific hub in Bulgaria and Chalmers University of Technology, Sweden \u2013 leading European institution with extensive experience in research, education and innovation. CoE main research objective is to advance the state-of-the-art in the whole Big Data Value Chain, including development of advanced methods and tools for data collection from variety of structured and unstructured sources, data consistency checking and cleaning, data aggregation and linking, data processing, modeling and analysis, data delivery by providing both accessibility and proper visualization. Following challenges of the Programme Horizon 2020 and the Bulgarian Innovation Strategy for Smart Specialization 2014-2020, the project team selected the most promising Data Driven Innovation Pillars: Data Driven Government (Public Services based on Open Data); Data Driven Industry (Manufacturing and Production); Data Driven Society (Smart Cities); and Data Driven Science.The specific objectives of Teaming 1 phase are to establish a solid background for the creation and sustainability of a CoE at a national, regional and EU level through (1) Elaboration of a detailed and robust business plan with a long term vision for setting-up and sustaining of the CoE; (2) Strengthening the research capacity and potential in Big Data; (3) Establishment of an international collaborative network of Big Data and related fields researchers; (4) Increasing quality of education and training and offering measures for motivation and involvement of the next-generation Early-Stage Researchers; and (5) Wide dissemination and promotion of project aims, activities and expected outputs.\n\n",
                "DataExportTag": "COR51178",
                "QuestionID": "QID222",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "BIG DATA FOR SMART SOCIETY The general objective of GATE is to establish Centre of Excellence on...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\": computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "2": {
                        "Display": "\"Nuclear Transport and Digital Twin Experimentation\": nuclear, transport, interoperable, enabler, experimentation, agile, certification, toolbox, factory, testbed, bim, broad, federation, vertical, digital_twin"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID327",
            "SecondaryAttribute": "BMP4 induces primitive endoderm but not trophectoderm in monkey embryonic stem cells. Monkey embr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "BMP4 induces primitive endoderm but not trophectoderm in monkey embryonic stem cells. Monkey embryonic stem (ES) cells share similar characteristics to human ES cells and provide a primate model of allotransplantation, which allows to validate efficacy and safety of cell transplantation therapy in regenerative medicine. Bone morphogenetic protein 4 (BMP4) is known to promote trophoblast differentiation in human ES cells in contrast to mouse ES cells where BMP4 synergistically maintains self-renewal with leukemia inhibitory factor (LIF), which represents a significant difference in signal transduction of self-renewal and differentiation between murine and human ES cells. As the similarity of the differentiation mechanism between monkey and human ES cells is of critical importance for their use as a primate model system, we investigated whether BMP4 induces trophoblast differentiation in monkey ES cells. Interestingly, BMP4 did not induce trophoblast differentiation, but instead induced primitive endoderm differentiation. Prominent downregulation of Sox2, which plays a pivotal role not only in pluripotency but also placenta development, was observed in cells treated with BMP4. In addition, upregulation of Hand1, Cdx2, and chorionic gonadotropin beta (CG-beta), which are markers of trophoblast, was not observed. In contrast, BMP4 induced significant upregulation of Gata6, Gata4, and LamininB1, suggesting differentiation into the primitive endoderm, visceral endoderm, and parietal endoderm, respectively. The threshold of BMP4 activity was estimated as about 10 ng\/mL. These findings suggest that BMP4 induced differentiation into the primitive endoderm lineage but not into trophoblast in monkey ES cells.\n\n",
                "DataExportTag": "CAN1427312",
                "QuestionID": "QID327",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "BMP4 induces primitive endoderm but not trophectoderm in monkey embryonic stem cells. Monkey embr...",
                "Choices": {
                    "1": {
                        "Display": "\"Cellular Biology and Aging Research\""
                    },
                    "2": {
                        "Display": "mitosis, stress, microtubule, homeostasis, aurora, aging, centrosome, membrane, cellular, clock, regulation, polarity, intracellular, dynamic, ion_channel"
                    },
                    "3": {
                        "Display": "\"Stem Cell Differentiation and Hematopoiesis\""
                    },
                    "4": {
                        "Display": "differentiation, hematopoietic_stem, progenitor, embryonic_stem, es, erythroid, mouse, lineage, notch, differentiate, hematopoiesis, esc, myeloid, hematopoietic_stem_cells, gene"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID334",
            "SecondaryAttribute": "Brahms-Simulating Practice 1 BRAHMS : SIMULATING PRACTICE FOR WORK SYSTEMS DESIGN A continuing pr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Brahms-Simulating Practice 1 BRAHMS : SIMULATING PRACTICE FOR WORK SYSTEMS DESIGN A continuing problem in business today is the design of human-computer systems that respect how work actually gets done. The overarching context of work consists of activities, which people conceive as ways of organizing their daily life and especially their interactions with each other. Activities include reading mail, going to workshops, meeting with colleagues over lunch, answering phone calls, and so on. Brahms is a multiagent simulation tool for modeling the activities of groups in different locations and the physical environment consisting of objects and documents, including especially computer systems. A Brahms model of work practice reveals circumstantial, interactional influences on how work actually gets done, especially how people involve each other in their work. In particular, a model of practice reveals how people accomplish a collaboration through multiple and alternative means of communication, such as meetings, computer tools, and written documents. Choices of what and how to communicate are dependent upon social beliefs and behaviors\u2014 what people know about each other\u2019s activities, intentions, and capabilities and their understanding of the norms of the group. As a result, Brahms models can help human-computer system designers to understand how tasks and information actually flow between people and machines, what work is required to synchronize individual contributions, and how tools hinder or help this process. In particular, workflow diagrams generated by Brahms are the emergent product of local interactions between agents and representational artifacts, not pre-ordained, end-to-end paths built in by a modeler. Clancey, et al.: Brahms-Simulating Practice 2 We developed Brahms as a tool to support the design of work by illuminating how formal flow descriptions relate to the social systems of work; we accomplish this by incorporating multiple views\u2014 relating people, information, systems, and geography\u2014 in one tool. Applications of Brahms could also include system requirements analysis, instruction, implementing software agents, and a workbench for relating cognitive and social theories of human behavior. Clancey, et al.: Brahms-Simulating Practice for Work Systems Design 3 OVERVIEW OF OBJECTIVE, THEORETICAL STANCE, AND CONTRIBUTION Brahms is a multi-agent simulation framework (Tokoro, 1996) for modeling work practice, incorporating state-of-the-art methods from artificial intelligence research and insights about work and learning from the social sciences. A Brahms model is a kind of theatrical play, intended to provoke conversation and stimulate insights in groups of people seeking to analyze or redesign their work. Rather than modeling technical knowledge in detail, Brahms models focus on the conventions by which people choose to use particular tools and interact with each other, such as how they communicate. The quality, methods, and evaluation criteria of technical problem solving\u2014 the focus of most computer systems design\u2014 are constrained by this social-interactional context (Sachs, 1995; Schon, 1983; Weickert, 1995; Zuboff, 1987). We hypothesize that multiple, complementary views\u2014 cognitive, social, physical\u2014 integrated into one model provide a better basis for understanding organizations than cognitive task models, which are disembodied and oriented around individuals, or business process models, which are overly abstract, and hence decontextualized. More generally, we are interested in how organizations change themselves, and thus how to design a workplace so that people will dynamically reconfigure their processes, use of tools, and collaboration to creatively affect how a job gets done (Nonaka and Takeuchi, 1995). In Brahms we apply and extend knowledge-based techniques in a way that seeks to understand how information and workflow actually happens. Our approach demonstrates how symbolic cognitive modeling, traditional business process modeling, and situated cognition theories may be brought together in a coherent approach to the design of human-computer systems. This introduction provides an overview of our objectives, theoretical stance, and contribution to human-computer system design. Subsequent sections in this paper describe the relation of Brahms to situated cognition and workflow modeling, the methodology, provide examples, present results, and analyze broader implications. Practice, work systems design, and modeling work Broadly speaking, work practice may be contrasted with work process; practice concerns how people actually behave within a physical and social environment, as distinct from the functions they accomplish. For example, a description of work practice might include who picks up a fax, where it is delivered (to a desk? to a group of mailboxes?), and when this is done. In contrast, a typical description of work process would only show that an order, for example, is sent from one organization to an agent who processes it. The faxing process and how it is carried out might not be mentioned at all. In short, a model of practice is oriented around agents\u2014 how they interact with their environment and what they do during the course of a day. A model of work process is typically oriented around work products (such as orders)\u2014 how they are transformed and flow Clancey, et al.: Brahms-Simulating Practice for Work Systems Design 4 from one transformation to the next. A key finding of our work is that a representation of work process, such as work flow diagrams, can be derived from the result of a simulation of practice. The notion of \u201cpractice\u201d is central to work systems design, which has its roots in the design of socio-technical systems, a method developed in the 1950s by Eric Trist and Fred Emery (1959, 1960). Socio-technical systems design sought to analyze the relationship of the social system and the technical system, such as manufacturing machinery, and then design a \u201csocio-technical system\u201d that leveraged the advantages of each. Work design (Ehn, 1989; Greenbaum and Kyng, 1991; Pasmore, 1994; Weisbord, 1987 (see Chapter 16)) extends this tradition by focusing on both the formal features of work (explicit, intentional) and the informal features of work (as it is actually carried out \u201cin practice,\u201d analyzed with the use of ethnographic techniques). The aim of analyzing both the formal and the informal work practices is two-fold: to understand what it takes to actually accomplish a business function in order to use those insights in design, and to ensure that new designs of work can be effectively implemented. Socio-technical and work systems design aim at producing both a productive workplace and a positive work environment, which fosters human development by providing dignity, meaning and community. By contrast, business process reengineering focuses on the structuring of key business process, eliminating duplication and unnecessary steps, formulating processes and procedures and using information technology extensively to improve work processing. Consequently, business process reengineering focuses more exclusively on tasks, does not take into account the informal nature of work, and does not hold as important the dignity of work or human development. In short, work design includes a focus on practice, while business process design exclusively focuses on process (Sachs, 1993). The methods of business process reengineering (BPR) and work system design differ considerably. BPR tends to be conducted by external consultants who analyze the flow of work products in terms of business functions, and not how product get from one place to another. Work design is more typically carried out by people who actually do the work (both workers and managers), and emphasizes not only what flows, but how and why work products manage to get from one place to another. For example, managers, office workers, crafts people, and researcher-facilitators collaborate to understand an existing work setting (such as new order processing for a telecommunications company) in order to develop a comprehensive design for the business organization, work process, computer tools, documents, facilities (such as seating arrangements), training, performance metrics and incentives, etc. Both approaches may also be contrasted with an economic or business strategy analysis, which invents the products and services that both business process reengineering and work system design seek to deliver. Clancey, et al.: Brahms-Simulating Practice for Work Systems Design 5 The two approaches differ in their view of how information technology should be used. In BPR, models of technical problem-solving, for example, tend to result in business process designs in which information technology is seen as an opportunity to \u201cdo the work\u201d (e.g., to automate as much of the work as possible). In work systems design, information technology is seen in terms of augmenting and supporting human work practices. These outlooks have profoundly different consequences for the design of software systems. We emphasize that in work design the design process attempts to treat everyday work as the result of a combination of interacting conceptual and physical influences and that practices will develop over time through learning and worker invention. Because work systems are not simply \u201cimplemented,\u201d but develop and grow through the learning of communities, workers and designers (such as software engineers) must collaborate in the design process (Greenbaum and Kyng, 1991). The differences in these two approaches reflect theoretical underpinnings about the nature of knowledge and learning (what does \u201cknowledge of work\u201d mean when an individual is part of the work, or not part of it all at? See discussion below on Situated Cognition). Finding ways to effectively \u201csee\u201d process and practice at work has been a key challenge for us. We have developed Brahms because we think it provides a step forward in visualizing, analyzing, and thinking about the mul\n\n",
                "DataExportTag": "AI307913",
                "QuestionID": "QID334",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Brahms-Simulating Practice 1 BRAHMS : SIMULATING PRACTICE FOR WORK SYSTEMS DESIGN A continuing pr...",
                "Choices": {
                    "1": {
                        "Display": "\"Parallel Computing and High Performance Computing\""
                    },
                    "2": {
                        "Display": "parallel, gpu, processor, compiler, thread, cpu, distribute, speedup, parallelism, parallelization, execution, parallelize, hpc, heterogeneous, cuda"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and GPU Optimization\""
                    },
                    "4": {
                        "Display": "deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID359",
            "SecondaryAttribute": "Breakthroughs in Quantitative Magnetic resonance ImagiNg for improved DEtection of brain Diseases...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Breakthroughs in Quantitative Magnetic resonance ImagiNg for improved DEtection of brain Diseases Magnetic resonance imaging (MRI) is one of the most useful and rapidly growing neuroimaging tools. Unfortunately, signal intensities in conventional MRI images are expressed in relative units that depend on scanner hardware and acquisition protocols. While this does not hinder visual inspection of anatomy, it hampers quantitative comparison of tissue properties within a scan, between successive scans, and between subjects. In contrast, advanced quantitative MRI (Q-MRI) methods like MR relaxometry or diffusion MRI do enable absolute quantification of biophysical tissue characteristics. Evidence is growing that Q-MRI techniques detect subtle microscopic damage, enabling more accurate and early diagnosis of neurodegenerative diseases. However, due to the long scan time required for Q-MRI, causing discomfort for patients and limiting the throughput, Q-MRI methods have not entered clinical practice yet. B-Q MINDED aims to overcome the current barriers by developing widely-applicable post-processing breakthroughs for accelerating Q-MRI. The originality of B-Q MINDED lies in its ambition to replace the conventional rigid multi-step processing pipeline with an integrated single-step parameter estimation framework. This approach will unlock a wealth of options for optimization of Q-MRI. To accomplish this goal, B-Q MINDED proposes a collaborative cross-disciplinary approach (from basic MR physics to clinical applications) with strong involvement of industry (2 MRI vendors and 3 MRI-software SMEs). B-Q MINDED proposes a unique training platform that enables young European researchers to develop a holistic view on Q-MRI research and development. The fellows enrolled in B-Q MINDED will have access to a variety of network-wide training events and will gain essential transferable skills that will positively affect their employability in academia and industry. By combining research, innovation, and education, B-Q MINDED will pave the way for introducing Q-MRI into the clinic.\n\n",
                "DataExportTag": "COR10367",
                "QuestionID": "QID359",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Breakthroughs in Quantitative Magnetic resonance ImagiNg for improved DEtection of brain Diseases...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Disease Diagnosis in Therapeutic Medicine\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Medical Imaging and Diagnosis\""
                    },
                    "4": {
                        "Display": "image, imaging, nuclear_magnetic_resonance, positron_emission, tomography, computed_tomography, diagnosis, ultrasound, tissue, optic, non_invasive, eye, vivo, breast, brain"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID396",
            "SecondaryAttribute": "Bringing Intelligence to Translation Memory Technology In this paper we argue that Translation In...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Bringing Intelligence to Translation Memory Technology In this paper we argue that Translation Intelligence is the next generation of translation memory technology that supersedes current state-of-the-art translation memory systems, as it is based on real self-learning and intelligent reuse of human translation knowledge, instead of simply attempting to recycle strings of characters, as traditional systems do. We maintain that Translation Intelligence is the only cost-effective method for professional computer-aided translation that is usable by both professional translators and other professionals with frequent translation needs besides their main tasks. It has been shown that due to drawbacks in their techniques, current translation memories have only been able to reach a mere fragment of these wide customer groups. 1. Translation Market Today: Some Figures Estimates on the size of today\u2019s worldwide translation and localization services market vary from USD 4 billion to 15 billion, with the US and Europe as the main areas of business (approximately 40 % market share each). However, the market size of computer-aided translation (CAT) tools, especially translation memories, is noticeably lower. Estimates vary considerably, from USD 22 million to 700 million, but nevertheless, the figures illustrate that the lion\u2019s share of translations is still done without any real computerized means. As the translation need is expected to grow steadily, there is gigantic market potential for the provider of the right kind of translation tool that can be taken into use quickly, with very little tailoring or none at all. Clearly, today\u2019s providers of traditional translation memory systems have not been able to meet this need that professional translators and other professionals translating beside their main work tasks have. These user groups are big, in Finland alone there 1 According to the Localization Industry Primer (2003, 2 Edition) by LISA, www.lisa.org 2 According to Common Sense Advisory (2005), www.commonsenseadvisory.com 3 According to MultiCorpora R&D Inc. (2002), www.multicorpora.com 4 According to Globalization Insider XII\/1.5 (2003), www.localization.org are 4,000 professional translators, and the number of other professionals with translation needs can be counted in the tens of thousands. 2. The Translation Memory Pitfall Traditional translation memories are based on the presupposition that sentences recur from one text to another, either as such or with slight variation (in a mathematical, or fuzzy, sense). As this might be true for repetitive texts\u2014such as new versions of previously translated documents\u2014the statement does not hold for new, unrestricted texts where sentence repetition is in fact as low as 1 %. Consequently, sentence repetition is the biggest obstacle for translation memories to reach new customer groups. It is quite frankly no surprise that only a fraction of the global, yearly translation volume is produced with the aid of translation memories. Clearly, a technology which actually only recycles sentences rules out all those customers who work with new texts of new fields from day to day or who produce so small translation volumes each year that the long-term benefit of a translation memory database is overshadowed by the cost of taking such a tool into use. But this user group\u2014professionals translating beside their main work tasks\u2014is enormous. It includes communication officers, secretaries, law firms, bankers, marketing experts, technical writers etc. There is obviously a niche for a CAT tool that is easy and fast to take into use regardless of the text type to be translated. 3. Moving From Translation Memories to Translation Intelligence With a view to overcome the drawbacks of traditional translation memories and create a translation tool suitable for both translation agencies and translators in general, the Finland-based translation technology company Master\u2019s Innovations Ltd invented a completely new method for computer-aided translation: Translation Intelligence. In contrast to traditional translation memories, tools based on Translation Intelligence can be used for translating different types of both repetitive and less repetitive texts, and the time-to-market is up to ten times shorter, thanks to the unparalleled selflearning capability of the technology. 3.1. Flexible Segmenting vs. Static Segmenting Translation Intelligence introduces flexible segmenting as opposed to the static sentence segmenting conducted by traditional translation memories. By using artificial intelligence and previous human translation knowledge, a tool using Translation Intelligence will segment the source sentence at hand into smaller parts and translate these parts in turns instead of the full sentence in one go. By operating on the sub-sentence level, where there is in all text types much more repetition than on the sentence level, Translation Intelligence is guaranteed to 5 According to proprietary research conducted by Master's Innovations Ltd studying 10,000 Finnish newspaper sentences. outperform traditional methods used in translation memories and lead to significant translation cost reductions. 3.2. Three Strategies Ensure Better Recall Translation Intelligence features three different strategies when suggesting translations of the flexibly-sized segments. The effect is that a translation tool based on Translation Intelligence is able to provide the user with a translation suggestion in 99 % of the cases, even when faced with a completely new text. The primary translation strategy is translation recycling, i.e. the flexibly-sized segments and their human-made translations are just reused. This is what a traditional translation memory system would also do. Example on translation recycling: If a human-made translation for \u201cthe issue is not discussed\u201d already exists, and the same segment is to be translated again, the system will primarily reuse the existing translation. The secondary strategy is translation generation, i.e. the system tries to reuse the flexibly-sized segments and their human-made translations as grammatical translation patterns whenever possible and generate translations based on such a pre-translated example that has a similar grammatical structure. If several equally suitable grammatical patterns exist, the system picks the best match, primarily using semantics and secondarily on the basis of use frequency or domain information. Translation Intelligence uses its built-in lexicon and its word-form generator to generate a correct translation suggestion in the target language. Example on translation generation: When aiming at translating \u201cthe house is not sold\u201d, the system will recognize that the grammatical pattern of this segment is similar to the pattern of the previously translated \u201cthe issue is not discussed\u201d (\u201cthe\u201d + \u201cNounNominative-Singular\u201d + \u201cis not\u201d + \u201cVerb-Past Participle\u201d). Therefore the system will use the human-made translation of \u201cthe issue is not discussed\u201d to generate a translation for \u201cthe house is not sold\u201d, using the same target language pattern. It will also be able to translate correctly \u201cthe boy is not bullied\u201d, \u201cthe car is not stolen\u201d, and countless other similar phrases. For a traditional TM tool to reach the same level of coverage, each and every surface level clause would need to be stored separately. Translation generation gives tremendous potential to a tool using Translation Intelligence; it does not merely reuse surface level strings, but actually learns logical or grammatical translation patterns from the user. Where a traditional translation memory would need to store every surface level segment with its translation separately in its database, a tool using Translation Intelligence needs only one translation pattern in its Knowledge Base to be able to translate innumerable similarly translatable segments! A rough estimate is that the same translation coverage can be achieved with a Knowledge Base of 50,000 translation patterns as with a conventional translation memory database of 1,000,000 translation units. The last strategy applied is translation heuristics, during which a pure machine translation component takes over. This strategy is used when the memory-based strategies are unsuccessful, and it simply ensures wide translation coverage. The user always gets some translation suggestion even if it will require manual editing. 3.3. Initial Phase Is Cut Thanks to a Standard Knowledge Base Delivered to All As Translation Intelligence uses flexible segmenting and handles translation units as grammatical translation patterns, the knowledge base used by the system is not as text-type dependent as a translation memory database is. Example of text-type independence: The translation pattern \u201cturn\u201d + \u201coff\u201d+ \u201cthe\u201d + \u201cNoun-NominativeSingular\u201d can be found in many different kinds of texts, but only in the manual of a kitchen appliance will you find a full sentence like \u201cTurn off the dishwasher before opening the hatch\u201d. This means that a ready-made knowledge base can be produced, delivered to and used by all customers, largely regardless of what types of texts they translate. The gain here is that the annoying and labour-intensive initial phase of the tool is cut down to one tenth, as the user is provided with some high-frequent translation knowledge to start with. 3.4. Learning a Great Deal More To sum up, a translation tool using Translation Intelligence learns from the human translator at an incredible pace as opposed to translation memories that merely recycle static sentences. This adaptiveness means that a tool using Translation Intelligence starts speeding up the translation process and cutting costs much faster than do traditional translation memory programs. We are talking months, not years! Translation Intelligence currently supports translation between Finnish and English in both directions. A Finnish-Swedish-Finnish version is well under way, and Master's Innovations Ltd has the competence to develop transla\n\n",
                "DataExportTag": "AI177752",
                "QuestionID": "QID396",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Bringing Intelligence to Translation Memory Technology In this paper we argue that Translation In...",
                "Choices": {
                    "1": {
                        "Display": "\"Natural Language Processing and Text Annotation\""
                    },
                    "2": {
                        "Display": "annotation, tag, corpus, annotate, annotator, tagging, tagger, metadata, labeling, treebank, text, genre, annotate_corpus, pos_tag, language"
                    },
                    "3": {
                        "Display": "\"Natural Language Processing and Semantic Analysis\""
                    },
                    "4": {
                        "Display": "word, sense, sense_disambiguation, semantic, verb, lexical, wordnet, wsd, meaning, dictionary, lexicon, language, noun, collocation, relation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID101",
            "SecondaryAttribute": "Cancer survivorship care plans: Processes, effective strategies, and challenges in a Northern Pla...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Cancer survivorship care plans: Processes, effective strategies, and challenges in a Northern Plains rural state OBJECTIVES\nHealth systems face resource and time barriers to developing and implementing cancer survivorship care plans (SCPs) when active cancer treatment is completed. To address this problem, the South Dakota (SD) Department of Health partnered with two of SD's largest health systems to create the SD Survivorship Program. The purpose of this program evaluation study was to describe and compare SCP development and implementation at the two health systems.\n\n",
                "DataExportTag": "CAN241598",
                "QuestionID": "QID101",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Cancer survivorship care plans: Processes, effective strategies, and challenges in a Northern Pla...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID362",
            "SecondaryAttribute": "Capillary systems for advanced point-of-care diagnostics \"The worldwide in vitro diagnostic (IVD)...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Capillary systems for advanced point-of-care diagnostics \"The worldwide in vitro diagnostic (IVD) is continually growing. At least 36% of the IVD market comprises point-of-care (POC)tests, which are performed by healthcare professional in various settings or by patients themselves. A large number ofcompanies entered the POC testing market in the recent years while bringing significant technological improvements andnovel concepts to this market. Yet, many important needs for immunodiagnostics are unmet: POC tests still require relativelylarge volumes of samples, cannot readily be multiplexed, are not always accurate, and often involve simple liquiddisplacement \"\"from point A to point B\"\" with minimal control over the distribution of reagents and kinetics of reactions.Here, we propose a synergistic project aiming at developing simple-to-use and high-performance microfluidic capillarysystems (\"\"CAPSYS\"\") for POC testing. Each part of the project will correspond to a complete PhD work with specific skills,methodology, and know-how from the academic and industrial PIs. The common theme for both projects will be the use ofcapillary forces for the autonomous filling of microfluidics by samples and liquids and the integration of reagents andreceptors for analyte detection inside the microfluidic chips. Those microfluidic chips will allow 1-step assays, i.e. assayswherein a non-technical user only needs to load a sample to the chip for performing a biological test.CAPSYS will lead to a new state of the art in terms of POC testing with significant relevance in medicine and diagnostics.CAPSYS will therefore be ideal for early stage researchers (ESRs) in terms of learning various disciplines in depth, creatingnumerous opportunities for publishing, and working on problem-oriented research that can have an impact in the \"\"realworld\"\".\"\n\n",
                "DataExportTag": "COR57449",
                "QuestionID": "QID362",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Capillary systems for advanced point-of-care diagnostics \"The worldwide in vitro diagnostic (IVD)...",
                "Choices": {
                    "1": {
                        "Display": "\"Medical Diagnosis and Healthcare Screening\""
                    },
                    "2": {
                        "Display": "diagnosis, detection, biomarker, point_care, patient, blood, hospital, sample, assay, medical_device, medical, detect, healthcare, screening, biosensor"
                    },
                    "3": {
                        "Display": "\"Public Health and Preventive Medicine\""
                    },
                    "4": {
                        "Display": "health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID366",
            "SecondaryAttribute": "Causes of Death among Persons with AIDS in the Era of Highly Active Antiretroviral Therapy: New Y...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Causes of Death among Persons with AIDS in the Era of Highly Active Antiretroviral Therapy: New York City Context As HIV treatment becomes more effective, AIDS-related deaths are decreasing and HIV-infected patients are dying of other causes. Better information about these other causes will help to determine appropriate health care for this population. Contribution The authors used death certificates to identify the causes of death in 68669 residents of New York City reported with AIDS. The percentage of deaths from nonHIV-related causes increased from 19.8% to 26.3% between 1999 and 2004. The principal causes of nonHIV-related deaths were cardiovascular disease, substance abuse, and nonAIDS-defining cancer. Cautions Death certificates are an imperfect way to identify cause of death. Implications Health care for HIV-infected patients must include prevention and management of common diseases as well as HIV-focused care. The Editors Over the past 20 years, AIDS has been transformed from a disease that was almost inevitably fatal to a chronic condition that is manageable for many people in the United States (1). The evolution began modestly in the early 1990s with prophylaxis against common opportunistic illnesses and accelerated in the mid-1990s with the introduction of protease inhibitors and highly active antiretroviral therapy (HAART). Between 1996 and 1998, HIV-related morbidity and mortality decreased by 60% in the United States (24). Along with increases in survival, the spectrum of underlying causes of death among persons with AIDS has gradually shifted. Between 1987 and 1999, the proportion of deaths due to nonHIV-related causes increased from 10.6% to 22.9% in 2 U.S. metropolitan areas (5). The most common nonHIV-related causes of death reported in the literature are alcohol and drug dependence, cardiovascular disease, and nonHIV-related cancer (69). The distribution of these causes varies with the sociodemographic characteristics of the persons studied, notably the prevalence of injection drug use (1012). In recognition of the increasing importance of nonHIV-related causes of death, the Infectious Diseases Society of America (IDSA) has argued that health care for people with HIV infection should expand from a primary focus on HIV-related illnesses to include preventable conditions that account for an increasing proportion of deaths (13). Thus, analyses that contribute to a fuller understanding of the underlying causes of death in subpopulations of persons with AIDS are needed. Many previous analyses are limited by small sample size, lack of generalizability, a focus on specific causes of death, and a failure to distinguish between deaths of persons with AIDS and deaths of persons with HIV infection (non-AIDS) (8, 1419). New York City is the single largest HIV\/AIDS-reporting jurisdiction in the United States, accounting for 15.3% of AIDS cases and 16.4% of deaths among persons with AIDS (20). Thus, we had a unique opportunity to conduct a population-based analysis of the spectrum of underlying causes of death in a large and heterogeneous population. The data are drawn from 2 population-based registries, the New York City HIV\/AIDS Reporting System and Vital Statistics Registry, and cover the period of 1999 through 2004. Methods Population The population was made up of persons 13 years of age or older who received; a diagnosis of AIDS; were alive at any time between 1999 and 2004; were reported to the New York City HIV\/AIDS Reporting System as of 30 September 2005; were residents of New York City at the time of diagnosis; and, among those who died, had a known underlying cause of death (98.2% of all deaths). Data Sources The New York City HIV\/AIDS Reporting System is a population-based registry of persons who received a diagnosis of AIDS (beginning in 1981), as defined by the Centers for Disease Control and Prevention (CDC), or HIV infection (non-AIDS) (beginning in 2000) (21). The current AIDS case definition includes a positive test result for HIV plus 1 or more of 26 opportunistic illnesses or a CD4+ lymphocyte count less than 0.200109 cells\/L or less than 14% of total lymphocytes. The New York City HIV\/AIDS Reporting System receives reports of possible AIDS diagnoses through an electronic laboratory reporting system or physician reports and investigates them by chart review. Reporting of AIDS in New York City is estimated to be 95% complete (22). The vital status of persons with AIDS is ascertained by semiannual matches between the HIV\/AIDS Reporting System and the Vital Statistics Registry. The underlying cause of death is coded at the New York City Department of Health and Mental Hygiene (DOHMH) Office of Vital Statistics by a nosologist who is certified by the National Center for Health Statistics. The nosologist codes the cause of death using the International Statistical Classification of Diseases and Related Health Problems, 10th Revision (ICD-10) (23). We classified persons as living in an area of poverty if they lived in a ZIP codetabulation area with more than 20% of the population below the 1999 federal poverty level or if they were homeless (24). All other variables were patient-level and were collected as part of routine surveillance. We derived demographic data from medical record reviews and provider reports and computed age at the end of 2004 or at the time of death for persons who died. We classified race or ethnicity as Hispanic, black (non-Hispanic), white (non-Hispanic), or other or unknown. The HIV transmission categories were injection drug use, men who have sex with men, and high-risk heterosexual sex. The high-risk heterosexual category included heterosexual sex with a partner who had HIV infection, with an injection drug user, or with a bisexual man. We classified men who were injection drug users and who had sex with men as injection drug users. Otherwise, when more than 1 risk factor was reported, we classified persons on the basis of the CDC hierarchy of transmission categories (25). We defined borough as the borough of residence at the time of AIDS diagnosis. We grouped the year of the AIDS diagnosis into 3 periods: pre-HAART (before 1996), early HAART (19961998), and late HAART (19992004). We obtained CD4+ lymphocyte counts primarily through an electronic laboratory reporting system. The CD4+ lymphocyte count used in the analysis was the lowest count in the second half of 2004 or within 6 months of death. Outcome The outcome was the underlying cause of death. Persons with an unknown underlying cause of death (n= 233 [1.8%]) were excluded from cause-specific analyses. HIV-Related Underlying Causes of Death We classified deaths as HIV-related if the ICD-10 code for the underlying cause of death was between B20 and B24 (HIV disease) or if the ICD-10 code was for an opportunistic illness in the CDC case definition. The latter criterion ensured that we did not misclassify deaths of people with AIDS as nonHIV-related because HIV was not mentioned on the death certificate (26). We did not further categorize these deaths in the main analysis because 70.9% of deaths were assigned a nonspecific underlying cause, for example, HIV disease resulting in other specified conditions (ICD-10 code B23.8) (Appendix Table 1). Appendix Table 1. Categories of Underlying Causes of HIV-Related Deaths in Persons with AIDS in New York City, 19992004* NonHIV-Related Underlying Causes of Death We classified deaths with a known underlying cause that did not meet the criteria described earlier as nonHIV-related. We further classified underlying causes into 9 major categories based on those used by the New York City DOHMH Office of Vital Statistics (27). Appendix Table 2 shows these categories and their associated ICD-10 codes. The substance abuse category included heterogeneous conditions that were associated with alcohol and drug abuse, including drug dependence (that is, overdose), alcoholic liver disease, cirrhosis, hepatitis C, and liver cancer (2732). The cardiovascular disease category comprised all ICD-10 codes between I00 and I78, except cardiac arrest codes. The cancer category comprised malignant types of cancer, except liver cancer and neoplasms that are part of the CDC case definition. We further classified nonHIV-related causes into 16 specific subcategories to better characterize the cause of death. Appendix Table 2. Codes for Major Categories of NonHIV-Related Causes of Death and Selected Specific Causes within Categories* Statistical Analysis We calculated the age-adjusted mortality rates per 10000 persons with AIDS for each year from 1999 to 2004 and for the entire time period. Mortality rates were age-standardized to the U.S. Census population in New York City in 2000 (33). We tested trends in rates of HIV-related deaths, nonHIV-related deaths, and specific nonHIV-related causes by using linear regression models. The model that tested trends in HIV-related and nonHIV-related deaths pooled all deaths to allow for differential trends and an explicit statistical test of whether they differed. We compared crude and age-standardized mortality rates by using methods developed for mortality vital statistics (34). We tested the association between time to death and patient characteristics in separate Cox proportional hazards regression models for HIV-related and nonHIV-related deaths. Independent variables in the model were age, sex, race or ethnicity, HIV transmission category, borough, residence in an area of poverty, year of AIDS diagnosis, and lowest CD4+ lymphocyte count. Date of cohort entry was 1 January 1999 or the date of AIDS diagnosis if diagnosis was after this date. We followed cases until death or we censored cases on 31 December 2004 if patients were still alive on that date. Those who died of a nonHIV-related cause were censored on the date of death in the model that assessed time to HIV-related death. Similarly, those who died of an HIV-related cause were censored at death in the model that assessed time to nonHIV-related death. We verified the proportional hazards assumption by\n\n",
                "DataExportTag": "CAN477920",
                "QuestionID": "QID366",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Causes of Death among Persons with AIDS in the Era of Highly Active Antiretroviral Therapy: New Y...",
                "Choices": {
                    "1": {
                        "Display": "\"Cancer Drug Research and Development\""
                    },
                    "2": {
                        "Display": "mtor, xenograft, kinase, metformin, combination, rapamycin, antitumor, drug, resistance, celecoxib, target_rapamycin, akt_mtor, synergistic, inhibitor, anticancer"
                    },
                    "3": {
                        "Display": "\"Cancer Cell Research and Treatment\""
                    },
                    "4": {
                        "Display": "bax_activation, proliferation, apoptotic, flow_cytometry, gastric, resveratrol, viability, mtt_assay, cervical, phase, annexin_v, arrest, cycle_arrest, quercetin, western_blot"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID367",
            "SecondaryAttribute": "CEA \u2013 A Predictor for Pathologic Complete Response After Neoadjuvant Therapy for Rectal Cancer BA.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CEA \u2013 A Predictor for Pathologic Complete Response After Neoadjuvant Therapy for Rectal Cancer BACKGROUND: Preoperative chemoradiation therapy in patients with rectal cancer results in pathologic complete response in approximately 10% to 30% of patients. Accurate predictive factors for obtaining pathologic complete response would likely influence the selection of patients best treated by chemoradiation therapy as the primary treatment without radical surgery. OBJECTIVE: The aim of this study was to evaluate the impact of tumor size, stage, location, circumferential extent, patient characteristics, and pretreatment CEA levels on the development of pathologic complete response after chemoradiation therapy. DESIGN: This study is a retrospective review. SETTINGS AND PATIENTS: Five hundred thirty patients treated with preoperative chemoradiation therapy and radical surgery for rectal adenocarcinoma between 1998 and 2011 were identified. A total of 469 patients remained after excluding patients with a history of pelvic radiation (n = 2), previous transanal endoscopic microsurgery or polypectomy of the primary lesion (n = 15), concurrent malignant tumor (n = 14), and no information about pre- or posttreatment T stage in the chart (n = 30). Preoperative CEA levels were available for 267 patients (57%). INTERVENTIONS: Preoperative chemoradiation therapy and total mesorectal excision were performed in patients with rectal cancer. MAIN OUTCOME: The primary outcome measured was pathologic complete response. RESULTS: Ninety-six patients (20%) were found to have a pathologic complete response in the operative specimen. Low pretreatment CEA (3.4 vs 9.6 ng\/mL; p = 0.008) and smaller mean tumor size (4.2 vs 4.7 cm; p = 0.02) were significantly associated with pathologic complete response. Low CEA levels and interruption in chemoradiation therapy were significant predictors of pathologic complete response in the multivariate analysis. When stratifying for smoking status, low CEA level was significantly associated with pathologic complete response only in the group of nonsmokers (p = 0.02). LIMITATIONS: This study was limited by its retrospective design, missing CEA values, and lack of tumor regression grade assessment. CONCLUSIONS: We demonstrated an association between low pretreatment CEA levels, interruption in chemoradiation therapy, and pathologic complete response in patients treated with neoadjuvant chemoradiation therapy for locally advanced rectal cancer. The predictive value of CEA in smokers can be limited, and further studies are needed to evaluate the impact of smoking on the predictive value of CEA levels for pathologic complete response in rectal cancer.\n\n",
                "DataExportTag": "CAN1324068",
                "QuestionID": "QID367",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CEA \u2013 A Predictor for Pathologic Complete Response After Neoadjuvant Therapy for Rectal Cancer BA...",
                "Choices": {
                    "1": {
                        "Display": "\"Gallbladder Cancer Prognosis and Treatment\""
                    },
                    "2": {
                        "Display": "recurrence, prognosis, metastasis, gallbladder, staging, survival, preoperative, adjuvant, resection, adjuvant_chemotherapy, invasion, gbc, lr, local_recurrence, lymph_node"
                    },
                    "3": {
                        "Display": "\"Cancer Treatment and Survival Rate Analysis\""
                    },
                    "4": {
                        "Display": "overall_survival, lung, rectal, radiotherapy, chemotherapy, non_small_cell_lung_cancer, heart_rate, adenocarcinoma, dfs, adjuvant_chemotherapy, adjuvant, survival, neoadjuvant, squamous_cell_carcinoma, ac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID383",
            "SecondaryAttribute": "Changes in DNA Methylation in Neoplasia: Pathophysiology and Therapeutic Implications Our increas...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Changes in DNA Methylation in Neoplasia: Pathophysiology and Therapeutic Implications Our increasing knowledge of the molecular pathophysiology of cancer is beginning to find applications in the diagnosis and treatment of various neoplastic diseases. In particular, new therapeutic approaches such as targeted agents, differentiation therapy, and immunotherapy promise to yield substantial clinical benefits with relatively few side effects. Recently, aberrant methylation of the cytosine base within the regulatory area of selected genes was shown to be a very common event in neoplasia; it is thought to contribute to the molecular pathogenesis of the disease through inactivation of tumor suppressor genes (1, 2). This finding has increased interest in use of drugs that can inhibit the process of DNA methylation and restore tumor suppressor gene function as a potential strategy to treat various malignant diseases. Hematopoietic neoplasms in particular have a high degree of aberrant methylation (3), and clinical trials have demonstrated significant activity for hypomethylating drugs in this setting. We discuss the importance and prevalence of DNA hypermethylation in cancer and review the potential value of hypomethylating agents in the treatment of human neoplasms. DNA Methylation The presence of 5-methylcytosine in human DNA (4) has genetic and epigenetic effects on cellular development, differentiation, and neoplastic transformation. 5-Methylcytosine differs from cytosine by the presence of a methyl group at the 5 position of the pyrimidine ring (Figure 1). Methylcytosine is formed after replication by addition of a methyl group to a cytosine already present in the DNA strand. Dramatic changes in overall methylation of DNA occur at different periods of embryogenesis, development, and differentiation to adult cells (5). A wave of demethylation initially erases preset methylation patterns in the first days of embryogenesis. This is followed by several waves of de novo methylation that eventually establish adult patterns of gene methylation. In differentiated cells, methylation patterns change relatively little and are perpetuated after DNA replication through the high affinity of DNA methyltransferase for hemimethylated DNA (6) (Figure 2). Unlike cytosine, 5-methylcytosine is a relatively unstable base because its spontaneous deamination leads to uracil. Through evolution, such mutations have resulted in a relative depletion of 5-methylcytosine in human DNA, and they are a major cause of germ-line mutations in inherited disease and of somatic mutations in neoplasia (7). Figure 1. Structure of cytosine, 5-methylcytosine, and hypomethylating 5-methylcytidine analogues. Figure 2. The maintenance methylation process. Top. Middle. Mtase Bottom. left 5-Aza right The functions of DNA methylation in mammalian cells remain poorly defined. Early speculation that attributed a global transcriptional regulation role to cytosine methylation (8) has not yet been confirmed experimentally. In bacteria, methylation plays a role in defense against genomic invasion by foreign DNA sequences (9). In mammalian cells, most normal methylation takes place within highly repeated transposable elements, and it has been proposed that such methylation also plays a role in genome defense by suppressing the potentially harmful effects of expression at these sites (10). This hypothesis was questioned recently (11). Regardless of its global functions, one unequivocal role for DNA methylation is in irreversible gene inactivation in selected cases, such as imprinted genes (12) and genes on the inactivated X chromosome (13). CpG Island Methylation and Gene Silencing In mammalian DNA, normal methylation is restricted to cytosine followed by guanosine (the CpG dinucleotide). These CpG sites are rarer in the human genome than their predicted frequency, presumably because they are eliminated during evolution through C to T mutations of methylcytosine (14). The human genome, however, also contains small regions of DNA called CpG islands, in which the frequency of CpG is normal or higher than expected (14). About half of all human genes (including most housekeeping genes) have CpG islands in their 5-promoter regions. Of note, the promoter regions containing CpG islands are in fact usually unmethylated in normal tissues, regardless of the transcriptional status of the gene. CpG island methylation is associated with changes in chromatin organization and consequent repression of gene transcription (1). In normal tissues, CpG island methylation is limited to exceptional situations, such as imprinted alleles (12) and genes on the inactive X chromosome [13]. These well-studied exceptions to the rule of absent methylation at CpG islands suggest that, once established, gene silencing by CpG island methylation is physiologically irreversible during the lifetime of affected cells. A direct correlation between CpG island methylation and inhibited gene transcription is supported by the facts that 1) cells in which silencing occurs are usually transcriptionally competent for the affected genes [as demonstrated by normal expression of the unmethylated alleles and exogenously inserted unmethylated promoters], 2) demethylation by pharmacologic (15) or genetic [16] manipulation results in reactivation of gene expression, and 3) in vitro methylation substantially reduces gene expression in reporter experiments (1). The mechanism of CpG islandassociated gene silencing appears to involve binding of specific methylated DNA binding proteins, followed by recruitment of a silencing complex that includes histone deacetylases (Figure 3) (17, 18). Figure 3. Effects of methylation and histone deacetylation on gene expression and silencing. top black boxes ovals arrows (left m MBP bottom HDAC right top Aberrant CpG Island Methylation in Cancer Neoplastic cells often have simultaneous global DNA hypomethylation, localized hypermethylation that involves CpG islands, and increased levels of DNA methyltransferase activity (1). Hypomethylation was initially postulated to play a role in carcinogenesis through activation of oncogenes (19), but this hypothesis has not been experimentally confirmed. Hypomethylation has been linked to chromosomal instability in vitro (20), and it may play such a role in neoplasia. Aberrant CpG island hypermethylation in cancer, in contrast, is clearly associated with transcriptional silencing of gene expression, and increasing experimental data suggest that it plays an important role as an alternate mechanism by which tumor suppressor genes are inactivated in cancer (1, 2). Aberrant CpG island methylation in cancer was initially described for the calcitonin (21) and MyoD (22) genes. These two genes are not thought to play a tumor suppressive role in cancer, but these findings prompted additional investigations into the process. The first tumor suppressor gene shown to be inactivated by hypermethylation was the RB1 gene, in which methylation appeared to be a clear alternate to mutations and deletions for eliminating expression of functional protein (23). Several additional tumor suppressor genes have since been shown to be similarly inactivated in some cancers, including VHL (24), P16 (25), E-cadherin (26), and hMLH1 (27). For most of these genes, hypermethylation appears to provide a similar selective advantage as genetic inactivation and is usually associated with absence of coding region or promoter mutations of involved alleles. The list of genes that display hypermethylation-associated inactivation in some sporadic cancers has grown long (Table 1). Multiple cellular systems can be affected by this process, including cell growth and differentiation, cell cycle control, and DNA repair, as well as angiogenesis and invasion. However, hypermethylation in cancer is not invariably associated with repressed transcription. In some cases, the involved CpG island is not in the promoter of the genes (28). In other cases, methylation involves genes that are not normally expressed in the diseased tissues (29). In still others, methylation is relatively sparse, and although it can easily be detected experimentally, it does not lead to substantial decreases in gene expression. Aberrant methylation in cancer therefore functions as a mechanism of generating molecular diversity in neoplasia. In a manner analogous to mismatch repair defects in cancer, methylation defects affect many different loci, only some of which are pathophysiologically relevant to the neoplastic process (30). Table 1. Genes That Are Hypermethylated in Sporadic Cancers The causes of aberrant methylation in cancer remain poorly defined. Both hypomethylation and methyltransferase activation can occur in cells that are induced to proliferate (31), and it is not clear whether the observed changes in malignant cells simply reflect cell cycle deregulation. De novo CpG island methylation, however, is not a feature of proliferating cells, and it appears to represent a true pathologic event in neoplasia. For many genes, hypermethylation begins in normal tissues during the process of aging (32), which may partially explain the dramatic increase in cancer incidence associated with aging. Other genes are methylated exclusively in malignant cells and are presumed to arise from rare chance events that lead to gene inactivation and a selective advantage for affected cells (1, 2). Recent data in multiple neoplasms suggest that some cancers have a high degree of de novo methylation compared with others (33), and specific genetic defects or exposure events may explain these differences. In particular, acute and chronic leukemias have a high degree of aberrant CpG island methylation; genes involved include the cell-cycle regulator p15 (34), the p53 homologue p73 (35), the drug-resistance gene MDR1 (36), ER (37), and HIC1 (38). Therefore, hematologic malignant conditions present unique opportunities for studying the clinical implications of aberrant methylation. Rationale for Use of Methylation Inhibitors in Neop\n\n",
                "DataExportTag": "CAN260204",
                "QuestionID": "QID383",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Changes in DNA Methylation in Neoplasia: Pathophysiology and Therapeutic Implications Our increas...",
                "Choices": {
                    "1": {
                        "Display": "\"Cancer Metastasis and Cell Migration\""
                    },
                    "2": {
                        "Display": "integrin, metastasis, matrix_metalloproteinase_inhibitors, adhesion, invasion, migration, extracellular_matrix, fak, focal_adhesion, motility, upa_activator, lung, matrix, upar, fibronectin"
                    },
                    "3": {
                        "Display": "\"Breast Cancer and Brain Tumor Research\""
                    },
                    "4": {
                        "Display": "breast, glioma, glioblastoma, triple_negative_breast_cancer, estrogen_receptor, cerebral, malondialdehyde, estrogen, estrogen_receptor_alpha, mammary, tamoxifen, gsc, resistance, astrocytoma, epidermal_growth_factor"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID371",
            "SecondaryAttribute": "Changes in ocular aquaporin expression following optic nerve crush Purpose Changes in the express...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Changes in ocular aquaporin expression following optic nerve crush Purpose Changes in the expression of water channels (aquaporins; AQP) have been reported in several diseases. However, such changes and mechanisms remain to be evaluated for retinal injury after optic nerve crush (ONC). This study was designed to analyze changes in the expression of AQP4 (water selective channel) and AQP9 (water and lactate channel) following ONC in the rat. Methods Rat retinal ganglion cells (RGCs) were retrogradely labeled by applying FluoroGold onto the left superior colliculus 1 week before ONC. Retinal injuries were induced by ONC unilaterally. Real-time PCR was used to measure changes in AQP4, AQP9, thy-1, Kir4.1 (K+ channel), and \u03b2-actin messages. Changes in AQP4, AQP9, Kir4.1, B cell lymphoma-x (bcl-xl), and glial fibrillary acidic protein (GFAP) expression were measured in total retinal extracts using western blotting. Results The number of RGCs labeled retrogradely from the superior colliculus was 2,090\u00b185 cells\/mm2 in rats without any treatment, which decreased to 1,091\u00b178 (47% loss) and 497\u00b187 cells\/mm2 (76% loss) on days 7 and 14, respectively. AQP4, Kir4.1, and thy-1 protein levels decreased at days 2, 7, and 14, which paralleled a similar reduction in mRNA levels, with the exception of Kir4.1 mRNA at day 2 showing an apparent upregulation. In contrast, AQP9 mRNA and protein levels showed opposite changes to those observed for the latter targets. Whereas AQP9 mRNA increased at days 2 and 14, protein levels decreased at both time points. AQP9 mRNA decreased at day 7, while protein levels increased. GFAP (a marker of astrogliosis) remained upregulated at days 2, 7, and 14, while bcl-xl (anti-apoptotic) decreased. Conclusions The reduced expression of AQP4 and Kir4.1 suggests dysfunctional ion coupling in retina following ONC and likely impaired retinal function. The sustained increase in GFAP indicates astrogliosis, while the decreased bcl-xl protein level suggests a commitment to cellular death, as clearly shown by the reduction in the RGC population and decreased thy-1 expression. Changes in AQP9 expression suggest a contribution of the channel to retinal ganglion cell death and response of distinct amacrine cells known to express AQP9 following traumatic injuries.\n\n",
                "DataExportTag": "CAN1339921",
                "QuestionID": "QID371",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Changes in ocular aquaporin expression following optic nerve crush Purpose Changes in the express...",
                "Choices": {
                    "1": {
                        "Display": "\"Inflammatory Response and Immune System Regulation\""
                    },
                    "2": {
                        "Display": "macrophage, neutrophil, monocyte, inflammation, chemokine, inflammatory, mouse, leukocyte, endothelial, cytokine, inflammasome, pmn, tlr, toll_receptor, e_selectin"
                    },
                    "3": {
                        "Display": "\"Neurological Injury and Treatment Research\""
                    },
                    "4": {
                        "Display": "cerebral, injury, rat, central_nervous_system, microglia, spinal_cord, astrocyte, neuronal, eae, stroke, retinal, tbi, cerebral_ischemia, transplant, mass_spectrometry"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID305",
            "SecondaryAttribute": "Chemotherapy Use among Medicare Beneficiaries at the End of Life Context Some worry that physicia...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Chemotherapy Use among Medicare Beneficiaries at the End of Life Context Some worry that physicians prescribe chemotherapy for patients with cancer at the end of life even when treatment is unlikely to prolong life or palliate symptoms. Contribution Among the study sample of Medicare beneficiaries who died of cancer in 1996, the proportions that received chemotherapy were about 30%, 20%, and 10% in the last 6, 3, and 1 months of life. Chemotherapy use was similar for types of cancer that usually respond to chemotherapy and those that do not. Implications During the last 6 months of life, many Medicare beneficiaries with cancer receive chemotherapy, regardless of the type of cancer they have. Unfortunately, this study does not tell us why. The Editors Many people are concerned that patients dying of cancer are frequently overtreated with chemotherapy (1-3). Indeed, some critics contend that oncologists prey on their patients' vulnerability, imply that chemotherapy is the vehicle of hope, and press patients to try it before reconciling themselves to death (4). As one managed care executive claimed: My case managers are coming to me and saying that about half my patients are dying within 2 weeks of their last chemotherapy course. So where was the oncologist saying, it's time for palliative care? Instead [patients] continue to get treated, and treated, and treated (5). Oncologists respond that they use chemotherapy prudently only when it is likely to extend life or relieve symptoms and that it is terminally ill patients and their families who demand treatment. Although there are some data on treatment of patients with metastatic cancer, data on how frequently patients with cancer receive chemotherapy in the months before death are lacking (6). Furthermore, there are no data on whether chemotherapy is used appropriately for cancer that responds to chemotherapy or for cancer for which chemotherapy is proven to relieve symptoms but not cancer unresponsive to chemotherapy. To provide such data, we examined the use of chemotherapy among Massachusetts and California Medicare beneficiaries who died of cancer. Methods Data Sources We obtained Medicare denominator (Medicare Provider Analysis and Review), outpatient, and carrier files from the Centers for Medicare & Medicaid Services (CMS); death certificate information from Massachusetts; and a random 5% sample of Medicare beneficiaries in California. Identifying the Study Sample We studied fully entitled, fee-for-service Medicare beneficiaries in Massachusetts and California who died in 1996, were at least 66 years of age at death, and were not enrolled in Medicare's End Stage Renal Disease program. We merged CMS's denominator files with each state's 1996 death certificate files and selected decedents whose underlying cause of death was cancer. Among 42 452 decedents in Massachusetts, 7919 died of cancer and met all other eligibility criteria. In California, Medicare's 5% research sample encompassed 4715 decedents, including 956 study-eligible cancer decedents. Identifying Chemotherapy Use Chemotherapy was identified from claims in the inpatient, outpatient, or carrier Medicare files. Identifying codes were intravenous chemotherapy agents (Healthcare Common Procedure Coding System [HCPCS] codes 964xx, 965xx, and J9000 to 9999); chemotherapy administration (International Classification of Diseases, Ninth Revision [ICD-9], code 99.25, HCPCS codes Q0083 to Q0085); and medical evaluation for chemotherapy (ICD-9 codes V58.1, V66.2, and V67.2). Thus, we captured both hospital- and nonhospital-based chemotherapy administration, but could not capture (nonreimbursed) oral chemotherapeutic agents. We identified chemotherapy use for decedents in the six 30-day periods preceding death. Statistical Analysis Results are stated as percentages or means and standard deviations within groups with a reported denominator. Differences discussed are all statistically significant at a P value less than 0.05. Similarity statements are subjective assessments regarding clinical importance of magnitudes, not statistical tests. Statistical analyses were conducted by using SAS software, version 6.12 (SAS Institute, Inc., Cary, North Carolina). Role of the Funding Source This project was funded by the Department of Clinical Bioethics, Clinical Center, and National Institute of Aging, National Institutes of Health, which have no financial or other interests in the outcome. One author from the Department of Clinical Bioethics contributed to study design, conduct, and reporting. Results Frequency of Chemotherapy in the Last Months of Life In 1996, 33% of Medicare cancer decedents in Massachusetts received chemotherapy in the last 6 months of life, 23% in the last 3 months of life, and 9% in the last month of life (Table 1). In California, the percentages were 26%, 20%, and 9%, respectively. Although chemotherapy use during the last 6 and 3 months is more common in Massachusetts than in California (33% vs. 26% [P < 0.001]; 23% vs. 20% [P = 0.036], respectively), chemotherapy use in the last month of life is similar for the two states (9% vs. 9%; P > 0.2). Table 1. Characteristics of Cancer Decedents in Massachusetts and California by Receipt of Chemotherapy in the Last 6 Months of Life Table 1 shows the proportion of patients with each type of cancer who received chemotherapy in the last months of life. Patients who died of hematologic malignant conditions received chemotherapy most frequently. In both states, patients with breast, colon, or ovarian cancer, which tend to be more responsive to chemotherapy, received chemotherapy about as often as patients with less responsive tumors, such as pancreatic, hepatocellular, or renal-cell cancer or melanoma. None of the patients dying of melanoma or renal-cell cancer in Massachusetts received interferon- in the last 3 months of life. In both states, chemotherapy use decreases significantly with increasing age (Table 1). The use of chemotherapy is two to three times greater for Massachusetts patients 65 to 74 years of age than for patients 85 years of age and older and more than four times higher for the youngest versus oldest age groups in California (Massachusetts: 44% [65 to 74 years of age], 31% [75 to 84 years of age], 16% [ 85 years of age] [P < 0.001 for trend]; California: 39% [65 to 74 years of age], 25% [75 to 84 years of age], 8% [ 85 years of age] [P < 0.001 for trend]). This decrease in chemotherapy use by age was seen across cancer types and for both men and women (Appendix Table). Duration of Chemotherapy in the Last Months of Life Among Massachusetts patients receiving chemotherapy in the last 6 months of life, 41% received no more than 1 month of chemotherapy, 36% received 1 to 3 months of chemotherapy, and 23% received more than 3 months of chemotherapy (Table 2). Mean duration of chemotherapy use was somewhat shorter for men than women (2.2 months for men vs. 2.5 months for women; P = 0.003), and decreased significantly with age (2.5 months for patients 65 to 74 years of age, 2.3 months for patients 75 to 84 years of age, and 1.9 months for patients 85 years of age; P < 0.001 for trend). Only about one third of patients with breast, colon, or ovarian cancer received chemotherapy for 1 month or less; in contrast, half or more of patients with pancreatic, hepatocellular, or renal-cell cancer or melanoma received chemotherapy for 1 month or less (Table 2). Table 2. Massachusetts Cancer Decedents Receiving Chemotherapy in the Last 6 Months of Life and Duration of Chemotherapy Discussion This study examined chemotherapy use for Medicare cancer decedents in Massachusetts and California in their final months of life. Overall, more than one quarter of patients received chemotherapy in the last 6 months of life and more than 20% received chemotherapy in the last 3 months of life. Furthermore, because of the significantly greater use of chemotherapy in the last months of life at younger ages, chemotherapy use is likely to be even higher among all cancer decedents. Nevertheless, these data suggest that the claim that half of patients dying of cancer received chemotherapy within the last few weeks of life is exaggerated (5). Even among patients with hematologic malignant conditions, fewer than 20% received chemotherapy in their last month of life. Patients with pancreatic, hepatocellular, or renal-cell cancer or melanoma, which tend to be unresponsive to chemotherapy, were just as likely to receive chemotherapy in the last 6 months of life as patients with breast, colon, or ovarian cancer, which tend to be responsive to chemotherapy (7, 8). However, decedents with cancer unresponsive to chemotherapy were somewhat more likely to have only a short course of therapy. These findings suggest some selectivity in chemotherapy use at the end of life on the basis of the cancer's responsiveness to chemotherapy. It is possible that after one therapy cycle, many patients and oncologists are convinced by ineffectiveness or the side effects to stop treatment for cancer unresponsive to chemotherapy. Nevertheless, many patients receiving chemotherapy for unresponsive cancer, such as pancreatic and hepatocellular cancer, received chemotherapy during 4 or more of the final 6 months of life. Although phase II data showing greater than 20% tumor responsiveness to chemotherapy can be cited for almost any cancer, most studies to date with single-agent or combination chemotherapy have shown little impact on quality of life or survival of the tumors typically classified as unresponsive (7, 8). Many reasons may explain chemotherapy use at the end of life. Desperate patients and families may demand a trial to see whether chemotherapy might shrink the tumor; indeed, patients with cancer are often willing to endure substantial side effects for small prolongations in life (9, 10). Lacking an established relationship and confronting an emotionally distraught patient and family, oncologists may provide one cycle of chemotherapy while the patient and\n\n",
                "DataExportTag": "CAN674368",
                "QuestionID": "QID305",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Chemotherapy Use among Medicare Beneficiaries at the End of Life Context Some worry that physicia...",
                "Choices": {
                    "1": {
                        "Display": "\"Inflammatory Cytokines and Immune Response\""
                    },
                    "2": {
                        "Display": "tumor_necrosis_factor, cytokine, lps, necrosis_factor, nf_kappab, alpha, tnfalpha, tnf\u03b1, lipopolysaccharide, production, inflammatory, interleukin, inos, monocyte, macrophage"
                    },
                    "3": {
                        "Display": "\"Neurological Disorders and Pain Management\""
                    },
                    "4": {
                        "Display": "alzheimer_disease, cerebral, pain, microglia, rat, neuron, neuronal, neuropathic_pain, mouse, a\u03b2, neuroinflammation, hippocampus, microglial, nerve, spinal_cord"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID298",
            "SecondaryAttribute": "Co-creation of service innovation in Europe There is growing consensus that public services can b...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Co-creation of service innovation in Europe There is growing consensus that public services can be improved through experiments which bring together service providers and their users. This proposal is for H2020-SC6-Co-Creation-2016-217: Applied co-creation to deliver public services. The CoSIE project contributes to democratic dimensions and social inclusion through co-creating public services by engaging diverse citizen groups and stakeholders. Utilizing blended data sources (open data, social media) with innovative deployment of ICT (data-analytics, Living Lab, Community reporting) in nine pilots, the project introduces the culture of experiments that encompasses various stakeholders for co-creating service innovations. The CoSIE project has two overarching aims: i) advance the active shaping of service priorities by end users and their informal support networks, ii) engage citizens, especially groups often called \u2018hard to reach\u2019, in the collaborative design of public services. The aims are divided into six objectives: 1) develop practical resources grounded in asset based knowledge to support new ways for public service actors to re-define operational processes, 2) produce and deliver nine real-life pilots to co-create a set of relational public services with various combinations of public sector, civil society and commercial actors, 3) draw together cross-cutting lessons from pilots and utilise innovative visualisation methods to share and validate new ideas and models of good governance, 4) apply innovative approaches appropriate to local contexts and user groups to gather the necessary user insight to co-create services, 5) ensure sustainability by establishing local trainers for animating dialogue and collating user voice, embedded in community networks, 6) mobilise new knowledge from piloting and validating by creating an accessible, user friendly roadmap to co-creation for service providers and their partners.The project will be implemented as a joint venture with 24 partners from 10 EU countries.\n\n",
                "DataExportTag": "COR52830",
                "QuestionID": "QID298",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Co-creation of service innovation in Europe There is growing consensus that public services can b...",
                "Choices": {
                    "1": {
                        "Display": "\"Rural Health and Sustainable Agriculture Management\""
                    },
                    "2": {
                        "Display": "health, farming, agriculture, multi_actor, transdisciplinary, rural_area, empowerment, systemic, consultation, collective, pathway, inclusion, ce, tailor, pandemic"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID297",
            "SecondaryAttribute": "Cold War Europe Beyond Borders. A Transnational History of Cross-Border Practices in the Alps-Adr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Cold War Europe Beyond Borders. A Transnational History of Cross-Border Practices in the Alps-Adriatic area from World War II to the present. \"This project aims to rethink the history of Cold War Europe by examining the development of transnational cross-border cooperation from the end of World War II to the present. Overcoming traditional narratives of a clear-cut European separation symbolised by the Berlin Wall, a decentralised analysis of recent European history will show us that the question of a divided continent should be reframed. The final objective is to challenge a dichotomous vision of two separate Europes, \u00e2\u20ac\u0153East\u00e2\u20ac\u009d and \u00e2\u20ac\u0153West\u00e2\u20ac\u009d, from a new, border perspective. To this end, a highly qualified team of senior and junior scholars under my guidance will focus on the Alps-Adriatic region, a historical area that is now shared by Austria, Italy, Slovenia and Croatia. This case involves a relatively narrow geographical area but an unusually broad typological range of subjects. During the Cold War it was divided among socialist but non-aligned Yugoslavia, capitalist but neutral Austria, and NATO and EEC member Italy. Its development from the \"\"southern end\"\" of the Iron Curtain in 1946 to the \"\"most open border\"\" during the Cold War and a precursor to present-day Schengen Europe, represents a paradigmatic case to study an alternative attitude towards borders, frontiers and boundaries. Drawing on Cold War and borderland studies, social history and the history of European integration, which up till now have not found common ground, our innovative conceptual elaboration will demonstrate the interplay between top-down politics and bottom-up initiatives, thus offering a new, and more nuanced history of Cold War Europe from the border perspective. Reconsidering the European past from this transnational angle, both in terms of geographic and methodological perspectives, will allow us to rediscover the human face of European integration and will offer us a new platform for contemporary discussions on sovereignty, territoriality and belonging and on the future role of borders in Europe and in the world.\"\n\n",
                "DataExportTag": "COR1505",
                "QuestionID": "QID297",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Cold War Europe Beyond Borders. A Transnational History of Cross-Border Practices in the Alps-Adr...",
                "Choices": {
                    "1": {
                        "Display": "\"Rural Health and Sustainable Agriculture Management\""
                    },
                    "2": {
                        "Display": "health, farming, agriculture, multi_actor, transdisciplinary, rural_area, empowerment, systemic, consultation, collective, pathway, inclusion, ce, tailor, pandemic"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID164",
            "SecondaryAttribute": "Collective Responsibility towards Nature and Future Generations This research proposal aims at re...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Collective Responsibility towards Nature and Future Generations This research proposal aims at rethinking a collective and proactive concept of responsibility towards nature and future generations, on the basis of the understanding of nature developed by German Idealism in the early 19th century and the theory of responsibility first advanced by Hans Jonas in the 20th century. I will articulate my research into two steps, which aim at two objectives: 1) I will first work on the rational foundation of collective proactive responsibility towards the world in which we live, that is the duty to take care of it. This is the \u00e2\u20ac\u0153objective side\u00e2\u20ac\u009d of my research, in the sense that it grounds on a consideration of human beings as rational and responsible agents in themselves, in a Kantian sense. To reach this first objective (which is the most challenging one), a further step is needed: the elaboration of a concept of nature and its relation to human being able to take into account (1.1) the ontological continuity between nature and human being and with this the value of nature and life in themselves; (1.2) the primary responsibility of humans. All this will be possible analyzing and revitalizing the concept of nature in German Idealism and studying its ethical implications. 2) Second, I will investigate the role of human motivation, the context and the consequences of human action as essential part of any theory of responsibility. This is the \u00e2\u20ac\u0153subjective side\u00e2\u20ac\u009d of my research, that aims at providing the \u00e2\u20ac\u0153bridge\u00e2\u20ac\u009d between the formal foundation of the concept of collective responsibility and its application. In this part, my research will deal with the key concepts of \u00e2\u20ac\u0153respect\u00e2\u20ac\u009d, \u00e2\u20ac\u0153care\u00e2\u20ac\u009d towards vulnerable nature and future human beings, and \u00e2\u20ac\u0153fear\u00e2\u20ac\u009d of a collapse of nature and our future lives. The concept of collective responsibility generated by this project might show its fertility also with reference to contemporary debates (like those related to environmental issues).\n\n",
                "DataExportTag": "COR40200",
                "QuestionID": "QID164",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Collective Responsibility towards Nature and Future Generations This research proposal aims at re...",
                "Choices": {
                    "1": {
                        "Display": "\"Political Science and Social Studies\""
                    },
                    "2": {
                        "Display": "labour, ethnography, peace, military, legitimacy, indigenous, election, domestic, feminist, violent, ethnic, electoral, protest, victim, populism"
                    },
                    "3": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\""
                    },
                    "4": {
                        "Display": "farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID313",
            "SecondaryAttribute": "CONCLUSION The implementation of ACS quality indicators at our main hospital was feasible and eff...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSION\nThe implementation of ACS quality indicators at our main hospital was feasible and effective for several measures, without delaying treatment. Instituting these indicators at lower-volume affiliates was more challenging.\n\n",
                "DataExportTag": "31",
                "QuestionID": "QID313",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSION The implementation of ACS quality indicators at our main hospital was feasible and eff...",
                "Choices": {
                    "1": {
                        "Display": "\"Oncology Healthcare and Patient Management\""
                    },
                    "2": {
                        "Display": "physician, cam, hospital, oncologist, consultation, gps, laboratory, consensus, specialist, oncology, audit, screening, clinic, multidisciplinary, centre"
                    },
                    "3": {
                        "Display": "\"Oncology Nursing Training and Communication\""
                    },
                    "4": {
                        "Display": "nurse, training, oncology, community, staff, communication, student, provider, professional, app, team, nursing, barrier, oncology_nurse, resident"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID105",
            "SecondaryAttribute": "CONCLUSION This evaluation suggests that partnerships between state health departments and local...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSION\nThis evaluation suggests that partnerships between state health departments and local health systems could be key for meeting the nation-wide goal of universal SCP implementation. Particularly, other low-population rural states like SD can use the findings to help build their SCP programs.\n\n",
                "DataExportTag": "33",
                "QuestionID": "QID105",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSION This evaluation suggests that partnerships between state health departments and local...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Development and Prevention Strategies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Oncology Nursing Education and Communication\""
                    },
                    "4": {
                        "Display": "nurse, training, oncology, community, staff, communication, student, provider, professional, app, team, nursing, barrier, oncology_nurse, resident"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID128",
            "SecondaryAttribute": "CONCLUSIONS AND RELEVANCE Among patients with likely perforated diverticulitis and undergoing eme...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS AND RELEVANCE\nAmong patients with likely perforated diverticulitis and undergoing emergency surgery, the use of laparoscopic lavage vs primary resection did not reduce severe postoperative complications and led to worse outcomes in secondary end points. These findings do not support laparoscopic lavage for treatment of perforated diverticulitis.\n\n",
                "DataExportTag": "56",
                "QuestionID": "QID128",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS AND RELEVANCE Among patients with likely perforated diverticulitis and undergoing eme...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID140",
            "SecondaryAttribute": "CONCLUSIONS Contralateral nodal metastasis in the absence of ipsilateral nodal metastasis is very...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS\nContralateral nodal metastasis in the absence of ipsilateral nodal metastasis is very rare and frozen section of ipsilateral neck dissection specimen can be an important pointer for addressing contralateral neck.\n\n",
                "DataExportTag": "68",
                "QuestionID": "QID140",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS Contralateral nodal metastasis in the absence of ipsilateral nodal metastasis is very...",
                "Choices": {
                    "1": {
                        "Display": "\"Gastrointestinal and Salivary Gland Tumors Diagnosis and Treatment\""
                    },
                    "2": {
                        "Display": "gastrointestinal_stromal_tumour, salivary_gland, gastrointestinal_stromal, acc, spindle, parotid_gland, kit_kinase_inhibitor, c_kit, smooth_muscle, pleomorphic_adenoma, parotid, adenoid_cystic, vimentin, ameloblastoma, immunohistochemical"
                    },
                    "3": {
                        "Display": "\"Skin Cancer and Dermatology\""
                    },
                    "4": {
                        "Display": "cutaneous, squamous_cell_carcinoma, basal_cell_carcinoma, basal, sebaceous, merkel, eyelid, mcc, adnexal, rare, ak, bccs, dfsp, epidermal, vulva"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID389",
            "SecondaryAttribute": "CONCLUSIONS Liver-kidney transplants are worthy options in patients with hepatic and renal end fa...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS\nLiver-kidney transplants are worthy options in patients with hepatic and renal end failure. Acute rejection seems to have fewer incidences in simultaneous liver-kidney transplantation.\n\n",
                "DataExportTag": "43",
                "QuestionID": "QID389",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS Liver-kidney transplants are worthy options in patients with hepatic and renal end fa...",
                "Choices": {
                    "1": {
                        "Display": "\"Organ Transplantation and Graft Rejection\""
                    },
                    "2": {
                        "Display": "transplant, graft_vs_host_disease, donor, recipient, graft_versus, hsct, rejection, allogeneic_hematopoietic, allograft, engraftment, allogeneic, bmt, hct, csa, kidney"
                    },
                    "3": {
                        "Display": "\"Leukemia Research and Treatment\""
                    },
                    "4": {
                        "Display": "acute_myelogenous_leukemia, chronic_myeloid_leukemia, imatinib_treatment, mrd, cytogenetic, myeloid_leukemia, bcr_abl_protein, relapse, gene, blast, acute_lymphoblastic, minimal_residual, bcr, receptor_tyrosine_kinase, molecular"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID380",
            "SecondaryAttribute": "CONCLUSIONS Patients with lymph node tumor involvement following radical cystectomy may be strati...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS\nPatients with lymph node tumor involvement following radical cystectomy may be stratified into high risk groups based on the primary bladder tumor, pathological subgroup, number of lymph nodes removed and total number of lymph nodes involved. Lymph node density, which is a novel prognostic indicator, may better stratify lymph node positive cases because this concept collectively accounts for the total number of positive lymph nodes (tumor burden) and the total number of lymph nodes removed (extent of lymphadenectomy). Future staging systems and the application of adjuvant therapies in clinical trials should consider applying lymph node density to help standardize this high risk group of patients following radical cystectomy.\n\n",
                "DataExportTag": "34",
                "QuestionID": "QID380",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS Patients with lymph node tumor involvement following radical cystectomy may be strati...",
                "Choices": {
                    "1": {
                        "Display": "\"Gallbladder Cancer Prognosis and Treatment\""
                    },
                    "2": {
                        "Display": "recurrence, prognosis, metastasis, gallbladder, staging, survival, preoperative, adjuvant, resection, adjuvant_chemotherapy, invasion, gbc, lr, local_recurrence, lymph_node"
                    },
                    "3": {
                        "Display": "\"Cancer Progression and Prognosis\""
                    },
                    "4": {
                        "Display": "metastasis, lymph_node, invasion, cervical, colorectal_cancer, liver, lnm, endometrial_cancer, involvement, lymphatic, papillary_thyroid_carcinoma, recurrence, depth, peritoneal, prognosis"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID317",
            "SecondaryAttribute": "CONCLUSIONS The authors have demonstrated a method of monitoring the neutron component of the sec...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS\nThe authors have demonstrated a method of monitoring the neutron component of the secondary radiation field produced by therapeutic protons. The method relies on direct detection of secondary neutrons and gamma rays using organic scintillation detectors. These detectors are sensitive over the full range of biologically relevant neutron energies above 0.5 MeV and allow effective discrimination between neutron and photon dose. Because the detector system is portable, the described system could be used in the future to evaluate secondary neutron and gamma doses on various clinical beam lines for commissioning and prospective data collection in pediatric patients treated with proton therapy.\n\n",
                "DataExportTag": "35",
                "QuestionID": "QID317",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS The authors have demonstrated a method of monitoring the neutron component of the sec...",
                "Choices": {
                    "1": {
                        "Display": "\"Medical Imaging and Therapeutic Techniques\""
                    },
                    "2": {
                        "Display": "imaging, temperature, ultrasound, optical, oct, tissue, hifu, vivo, heating, optical_coherence, probe, hyperthermia, phantom, real_time, laser"
                    },
                    "3": {
                        "Display": "\"Radiation Therapy and Dosimetry Simulation\""
                    },
                    "4": {
                        "Display": "beam, energy, monte_carlo, radiotherapy, dosimetry, phantom, proton, photon, electron, irradiation, detector, absorb_dose, film, simulation, neutron"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID193",
            "SecondaryAttribute": "CONCLUSIONS The efficacy of endoscopic incision is favorable in the short term. However, retreatm...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS\nThe efficacy of endoscopic incision is favorable in the short term. However, retreatment is needed to maintain the long-term lumen patency for parts of the patients.\n\n",
                "DataExportTag": "33",
                "QuestionID": "QID193",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS The efficacy of endoscopic incision is favorable in the short term. However, retreatm...",
                "Choices": {
                    "1": {
                        "Display": "\"Endoscopic Procedures and Esophageal Disorders\""
                    },
                    "2": {
                        "Display": "endoscopic, esophageal, esd, endoscopy, endoscopic_submucosal, perforation, dysphagia, emr, tube, airway, resection, bleeding, anastomosis, bleed, stricture"
                    },
                    "3": {
                        "Display": "\"Surgical Procedures and Complications in Thoracic and Gastrointestinal Medicine\""
                    },
                    "4": {
                        "Display": "rectal, resection, pancreatic, lung, esophagectomy, esophageal, vat, anastomosis, video_assist, lobectomy, anastomotic_leakage, total_mesorectal, mortality, gastric, thoracic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID254",
            "SecondaryAttribute": "CONCLUSIONS The efficacy of endoscopic incision is favorable in the short term. However, retreatm...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CONCLUSIONS\nThe efficacy of endoscopic incision is favorable in the short term. However, retreatment is needed to maintain the long-term lumen patency for parts of the patients.\n\n",
                "DataExportTag": "33",
                "QuestionID": "QID254",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CONCLUSIONS The efficacy of endoscopic incision is favorable in the short term. However, retreatm...",
                "Choices": {
                    "1": {
                        "Display": "\"Endoscopic Procedures and Esophageal Disorders\""
                    },
                    "2": {
                        "Display": "endoscopic, esophageal, esd, endoscopy, endoscopic_submucosal, perforation, dysphagia, emr, tube, airway, resection, bleeding, anastomosis, bleed, stricture"
                    },
                    "3": {
                        "Display": "\"Surgical Procedures and Complications in Thoracic and Gastrointestinal Medicine\""
                    },
                    "4": {
                        "Display": "rectal, resection, pancreatic, lung, esophagectomy, esophageal, vat, anastomosis, video_assist, lobectomy, anastomotic_leakage, total_mesorectal, mortality, gastric, thoracic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID258",
            "SecondaryAttribute": "Conformational states dynamically populated by a kinase determine its function A moving target Ab...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Conformational states dynamically populated by a kinase determine its function A moving target Abl kinase is an important signaling protein that is dysregulated in leukemia and other cancers and is the target of inhibitors such as imatinib. Like other kinases, Abl kinase is dynamic, and regulating conformational dynamics is key to regulating activity. Xie et al. used nuclear magnetic resonance to show that the Abl kinase domain interconverts between one active and two inactive states. Imatinib stabilizes an inactive conformation, and several resistance mutations act by destabilizing this conformation. In a construct that includes the regulatory domain, depending on the relative arrangement of the kinase and regulatory domains, the kinase domain is stabilized in either the active state or one of the inhibited states. Understanding the conformational dynamics of kinases can be leveraged to design selective drugs. Science, this issue p. eabc2754 Nuclear magnetic resonance data allow dissection of regulatory and drug-resistance mechanisms in Abl kinase. INTRODUCTION Protein kinases mediate many cell signaling processes. Central to their physiological function is the regulation of their binding and enzymatic activities, which is typically achieved by conformational transitions between active and inactive states. Dysregulation of kinase activity by deletions or mutations often results in disease. Protein kinases are dynamic molecules that intrinsically sample a number of conformational states. However, it has been challenging to experimentally access their conformational ensemble and structurally characterize the discrete conformations associated with distinct activities. Such information could advance our understanding of activation and inhibition mechanisms in this protein family and aid in the development of selective inhibitors. RATIONALE We used nuclear magnetic resonance spectroscopy to monitor in atomic-level detail how Abl kinase transitions between distinct conformational states and to elucidate how the conformational ensemble is exploited by mutants, ligands, posttranslational modifications, and inhibitors to regulate the kinase activity and function. We combined structural and energetic approaches to quantitate the contribution of key structural elements such as the activation loop, the Asp-Phe-Gly (DFG) motif, the regulatory spine, and the gatekeeper residue to kinase regulation and provide the mechanistic basis for drug resistance. RESULTS We found that the Abl kinase domain interconverts between an active and two, transiently populated, conformational states that adopt discrete inactive structures. There are extensive differences in key structural elements between the conformational states that reveal multiple intrinsic regulatory mechanisms. The small energy difference between active and inactive states allows oncogenic mutations in the regulatory spine or the gatekeeper position to counteract inhibitory mechanisms and constitutively activate the kinase. By capturing and structurally characterizing the conformational state to which the cancer drug imatinib selectively binds, we explain a number of drug-resistance variants isolated in patients. These mutants confer resistance by depleting, through various mechanisms, the conformation to which imatinib binds. To determine the basis for allosteric regulation, we studied a construct that includes the kinase domain and the regulatory domains and that can adopt an assembled and an extended conformation. In the assembled conformation, in which the regulatory domains dock onto the back of the kinase domain, one of the inactive states is selectively stabilized, thereby suppressing catalytic activity. In the extended conformation, wherein the regulatory domains dock on top of the N-lobe, the inactive state is eliminated, thus explaining the increased leukemogenic activity associated with this conformational state. Only one of the detected inactive states appears to be physiologically relevant. The inactive state with no apparent biological function can nevertheless be leveraged for the design of selective inhibitors. Targeting nonphysiological conformational states may be an effective strategy in the design of drugs with increased selectivity and reduced selection pressure for the occurrence of drug-resistance mutations. Although the structure of inactive states can, in principle, vary considerably among kinases, structural comparison of the Abl inactive states with those previously determined for other kinases reveals that there may be a limited number of structurally divergent inactive states intrinsic to kinases. CONCLUSION Our data demonstrate that the detection and structural characterization of the distinct conformational states populated by a kinase, coupled to the energetic dissection of the contribution of key structural elements to the selective stability of these states, are essential to advance our understanding of the mechanisms underpinning kinase regulation and function. The approaches presented here can be extended to other kinases to characterize transiently populated conformational states, with the goal of revealing the full repertoire of regulatory and drug-resistance mechanisms in the kinome. Transitions of Abl kinase between conformational states. The Abl kinase domain adopts predominantly (~90%) an active state in solution, but it transiently switches between two low-populated (~5%) states that adopt distinct inactive conformations. Key structural elements that rearrange in the various states are highlighted. The conformational equilibrium is exploited by physiological and pathological stimuli to alter the function of Abl. Protein kinases intrinsically sample a number of conformational states with distinct catalytic and binding activities. We used nuclear magnetic resonance spectroscopy to describe in atomic-level detail how Abl kinase interconverts between an active and two discrete inactive structures. Extensive differences in key structural elements between the conformational states give rise to multiple intrinsic regulatory mechanisms. The findings explain how oncogenic mutants can counteract inhibitory mechanisms to constitutively activate the kinase. Energetic dissection revealed the contributions of the activation loop, the Asp-Phe-Gly (DFG) motif, the regulatory spine, and the gatekeeper residue to kinase regulation. Characterization of the transient conformation to which the drug imatinib binds enabled the elucidation of drug-resistance mechanisms. Structural insight into inactive states highlights how they can be leveraged for the design of selective inhibitors.\n\n",
                "DataExportTag": "CAN552842",
                "QuestionID": "QID258",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Conformational states dynamically populated by a kinase determine its function A moving target Ab...",
                "Choices": {
                    "1": {
                        "Display": "\"Cellular Biology and Aging Research\""
                    },
                    "2": {
                        "Display": "mitosis, stress, microtubule, homeostasis, aurora, aging, centrosome, membrane, cellular, clock, regulation, polarity, intracellular, dynamic, ion_channel"
                    },
                    "3": {
                        "Display": "\"Epigenetic Regulation and Gene Modification\""
                    },
                    "4": {
                        "Display": "pten, chromatin, histone, regulation, stress, methylation, epigenetic, autophagy, metabolic, enzyme, gene, modification, translation, acetylation, control"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID173",
            "SecondaryAttribute": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework P...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework Programme under the Energy Theme C-ENERGY Connecting Energy NCPs has the overall objective to reinforce the network of National Contact Points for the 7 Framework Programme of the Energy Theme (Energy NCPs). C-ENERGY is a 1 year coordination action that will lay the foundation for a 4-year action (1 year C-ENERGY and 3 years C-ENERGY +). The practices identified and activities carried out will be further exploited in C-ENERGY + to build all the necessary services\/activities to reach the expected long term result: a uniform high-level Energy NCP service across Europe. C-ENERGY is based on 3 main problems: (i) lack of uniform high-level Energy NCPs services across Europe (ii) new NCPs with little or no experience due to the rapid expansion in recent years of FP associated countries (iii) lack of connection among Energy NCPs through transnational exchange of experiences and dissemination of good practices. The specific objectives of C-ENERGY are: 1 Identifying and sharing good practices 2 Promoting trans-national cooperation. In order to reach these objectives C-ENERGY will identify needs and good practices of Energy NCPs. 1 Training session in Brussels and 10 working visits of less experienced Energy NCPs to more experienced Energy NCPs will be organised. Furthermore, trans-national cooperation will be promoted by implementing a common partner search methodology and through an FP7 take-up measure focused on SMEs and industries. 1 international Brokerage Event will be organised for EU researchers in energy sector. C-ENERGY will involve 27 Energy NCPs as partner from 27 different countries plus 13 Energy NCPs as associated partners from 10 different countries, fully involved in activities and budget. 3 Energy NCPs from 3 different Third Countries will receive a grant for travel costs to attend International Brokerage Event. The expected impacts are: (i) tailor made training\/ information\/ awareness raising mechanisms identified as a sound basis for C-ENERGY + (ii) a strengthened cooperation among NCPs\n\n",
                "DataExportTag": "COR65266",
                "QuestionID": "QID173",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework P...",
                "Choices": {
                    "1": {
                        "Display": "\"Seismic Activity and Earthquake Mechanics\""
                    },
                    "2": {
                        "Display": "fluid, earth, flow, earthquake, seismic, deformation, turbulence, multiscale, mantle, mechanical, microstructure, wave, rock, fracture, mechanic"
                    },
                    "3": {
                        "Display": "\"Nanotechnology and Solar Cell Fabrication\""
                    },
                    "4": {
                        "Display": "electric, graphene, solar_cell, semiconductor, magnetic, spin, organic, layer, fabrication, thin_film, silicon, spintronic, nanostructure, oxide, perovskite"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID234",
            "SecondaryAttribute": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework P...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework Programme under the Energy Theme C-ENERGY Connecting Energy NCPs has the overall objective to reinforce the network of National Contact Points for the 7 Framework Programme of the Energy Theme (Energy NCPs). C-ENERGY is a 1 year coordination action that will lay the foundation for a 4-year action (1 year C-ENERGY and 3 years C-ENERGY +). The practices identified and activities carried out will be further exploited in C-ENERGY + to build all the necessary services\/activities to reach the expected long term result: a uniform high-level Energy NCP service across Europe. C-ENERGY is based on 3 main problems: (i) lack of uniform high-level Energy NCPs services across Europe (ii) new NCPs with little or no experience due to the rapid expansion in recent years of FP associated countries (iii) lack of connection among Energy NCPs through transnational exchange of experiences and dissemination of good practices. The specific objectives of C-ENERGY are: 1 Identifying and sharing good practices 2 Promoting trans-national cooperation. In order to reach these objectives C-ENERGY will identify needs and good practices of Energy NCPs. 1 Training session in Brussels and 10 working visits of less experienced Energy NCPs to more experienced Energy NCPs will be organised. Furthermore, trans-national cooperation will be promoted by implementing a common partner search methodology and through an FP7 take-up measure focused on SMEs and industries. 1 international Brokerage Event will be organised for EU researchers in energy sector. C-ENERGY will involve 27 Energy NCPs as partner from 27 different countries plus 13 Energy NCPs as associated partners from 10 different countries, fully involved in activities and budget. 3 Energy NCPs from 3 different Third Countries will receive a grant for travel costs to attend International Brokerage Event. The expected impacts are: (i) tailor made training\/ information\/ awareness raising mechanisms identified as a sound basis for C-ENERGY + (ii) a strengthened cooperation among NCPs\n\n",
                "DataExportTag": "COR65266",
                "QuestionID": "QID234",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework P...",
                "Choices": {
                    "1": {
                        "Display": "\"Seismic Activity and Earthquake Mechanics\""
                    },
                    "2": {
                        "Display": "fluid, earth, flow, earthquake, seismic, deformation, turbulence, multiscale, mantle, mechanical, microstructure, wave, rock, fracture, mechanic"
                    },
                    "3": {
                        "Display": "\"Nanotechnology and Solar Cell Fabrication\""
                    },
                    "4": {
                        "Display": "electric, graphene, solar_cell, semiconductor, magnetic, spin, organic, layer, fabrication, thin_film, silicon, spintronic, nanostructure, oxide, perovskite"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID299",
            "SecondaryAttribute": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework P...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework Programme under the Energy Theme C-ENERGY Connecting Energy NCPs has the overall objective to reinforce the network of National Contact Points for the 7 Framework Programme of the Energy Theme (Energy NCPs). C-ENERGY is a 1 year coordination action that will lay the foundation for a 4-year action (1 year C-ENERGY and 3 years C-ENERGY +). The practices identified and activities carried out will be further exploited in C-ENERGY + to build all the necessary services\/activities to reach the expected long term result: a uniform high-level Energy NCP service across Europe. C-ENERGY is based on 3 main problems: (i) lack of uniform high-level Energy NCPs services across Europe (ii) new NCPs with little or no experience due to the rapid expansion in recent years of FP associated countries (iii) lack of connection among Energy NCPs through transnational exchange of experiences and dissemination of good practices. The specific objectives of C-ENERGY are: 1 Identifying and sharing good practices 2 Promoting trans-national cooperation. In order to reach these objectives C-ENERGY will identify needs and good practices of Energy NCPs. 1 Training session in Brussels and 10 working visits of less experienced Energy NCPs to more experienced Energy NCPs will be organised. Furthermore, trans-national cooperation will be promoted by implementing a common partner search methodology and through an FP7 take-up measure focused on SMEs and industries. 1 international Brokerage Event will be organised for EU researchers in energy sector. C-ENERGY will involve 27 Energy NCPs as partner from 27 different countries plus 13 Energy NCPs as associated partners from 10 different countries, fully involved in activities and budget. 3 Energy NCPs from 3 different Third Countries will receive a grant for travel costs to attend International Brokerage Event. The expected impacts are: (i) tailor made training\/ information\/ awareness raising mechanisms identified as a sound basis for C-ENERGY + (ii) a strengthened cooperation among NCPs\n\n",
                "DataExportTag": "COR65266",
                "QuestionID": "QID299",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Connecting Energy NCPs A Pro-Active Network of National Contact Points in the Seventh Framework P...",
                "Choices": {
                    "1": {
                        "Display": "\"Seismic Activity and Earthquake Mechanics\""
                    },
                    "2": {
                        "Display": "fluid, earth, flow, earthquake, seismic, deformation, turbulence, multiscale, mantle, mechanical, microstructure, wave, rock, fracture, mechanic"
                    },
                    "3": {
                        "Display": "\"Nanotechnology and Solar Cell Fabrication\""
                    },
                    "4": {
                        "Display": "electric, graphene, solar_cell, semiconductor, magnetic, spin, organic, layer, fabrication, thin_film, silicon, spintronic, nanostructure, oxide, perovskite"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID290",
            "SecondaryAttribute": "Coordination action to improve trans-national co-operation of NCPs REGIONAL The TRANS REG NCP pro...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Coordination action to improve trans-national co-operation of NCPs REGIONAL The TRANS REG NCP project is aimed to create trans-national network of National Contact Points (NCP) for the Seventh Framework Programme under Regions of Knowledge through reinforcing the NCPs and promoting trans-national co-operation. The objective will be reached using the following measures: transfer of knowledge, exchange of experiences and good practices among Regional NCPs, and development and implementation of tools and methodologies for trans-national cooperation. Special attention will be given to helping less experienced NCPs which are requiring the know-how accumulated in other countries. Specific objectives are as follows: - improvement of NCPs expertise concerning EU developments in identification, stimulation and support to research driven clusters. This expertise will be built in a first place on existing results stemming out for example from Regions of Knowledge pilot actions, Innovating Regions of Europe Network, Mutual Learning Platform, Europe INNOVA initiative, etc. - improvement of quality of services provided. Good practices in NCP services will be implemented. Regional NCP Network will learn from experience of the former FP projects such as for example: Trainet Future and Transtrac and from more advanced NCP\u00e2\u20ac\u2122s through trainings and transfer of knowledge, - development of common methodology for trans-national activities and networking between NCP\u00e2\u20ac\u2122s, - increase of the NCP visibility on regional level and stimulation of regional stakeholders participation in FP7.\n\n",
                "DataExportTag": "COR9382",
                "QuestionID": "QID290",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Coordination action to improve trans-national co-operation of NCPs REGIONAL The TRANS REG NCP pro...",
                "Choices": {
                    "1": {
                        "Display": "\"Research Funding and University Administration\""
                    },
                    "2": {
                        "Display": "cofund, national_contact, ncps, preparatory_phase, university, coordinator, administrative, smart_specialization, health, critical_mass, centres, website, strategic_partnership, preparation, increase_visibility"
                    },
                    "3": {
                        "Display": "\"Philosophy, Ethics and Psychology in Politics and Judiciary\""
                    },
                    "4": {
                        "Display": "philosophy, ethic, belief, logic, election, criminal, emotion, psychological, electoral, epistemic, opinion, responsibility, truth, news, judicial"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID355",
            "SecondaryAttribute": "Cosmological Tests of Gravity Einstein\u2019s theory of General Relativity (GR) is tested accurately w.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Cosmological Tests of Gravity Einstein\u2019s theory of General Relativity (GR) is tested accurately within the local universe i.e., the solar system, but this leaves open the possibility that it is not a good description at the largest scales in the Universe. The standard model of cosmology assumes GR as a theory to describe gravity on all scales. In 1998, astronomers made a surprising discovery that the expansion of the Universe is accelerating, not slowing down. This late-time acceleration of the Universe has become the most challenging problem in theoretical physics. Within the framework of GR, the acceleration would originate from an unknown \u201cdark energy.\u201d Alternatively, it could be that there is no dark energy and GR itself is in error on cosmological scales. The standard model of cosmology is based on a huge extrapolation of our limited knowledge of gravity. This discovery of the late time acceleration of the Universe may require us to revise the theory of gravity and the standard model of cosmology based on GR. The main objective of my project is to develop cosmological tests of gravity and seek solutions to the origin of the observed accelerated expansion of the Universe by challenging conventional GR. Upcoming surveys will make cosmological tests of gravity a reality in the next five years. There are remaining issues in developing theoretical frameworks for probing gravitational physics on cosmological scales. We construct modified gravity theories as an alternative to dark energy and analyse \u201cscreening mechanisms\u201d to restore GR on scales where it is well tested. We then develop better theoretical frameworks to perform cosmological tests of gravity that include non-linear scales by exploiting our theoretical knowledge of the models and our state-of-the-art simulations.This grant will exploit and develop the world-leading position of the group initiated by Kazuya Koyama at the University of Portsmouth funded by the ERC starting grant (2008-2013).\n\n",
                "DataExportTag": "COR30874",
                "QuestionID": "QID355",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Cosmological Tests of Gravity Einstein\u2019s theory of General Relativity (GR) is tested accurately w...",
                "Choices": {
                    "1": {
                        "Display": "\"Imaging Techniques and Molecular Detection\""
                    },
                    "2": {
                        "Display": "imaging, x_ray, image, microscopy, sensor, nuclear_magnetic_resonance, optic, resolution, measurement, detection, probe, sensitivity, detector, spectroscopy, molecule"
                    },
                    "3": {
                        "Display": "\"Astronomy and Cosmology\""
                    },
                    "4": {
                        "Display": "galaxy, star, universe, observation, black_hole, stellar, gravitational_wave, observational, star_formation, cosmic, telescope, dust, cosmology, cosmological, dark_matter"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID166",
            "SecondaryAttribute": "Cyber-security Excellence Hub in Estonia and South Moravia The proposed Cyber-security Excellence...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Cyber-security Excellence Hub in Estonia and South Moravia The proposed Cyber-security Excellence Hub in Estonia and South Moravia (CHESS) will integrate leading cybersecurity institutions and capitalize on the strengths of both regions to address important Europe-wide challenges. South Moravia is a major ICT industry & education powerhouse of the Czech Republic, with a very focused and coherent smart specialization strategy targeting cybersecurity. Estonia is among the most advanced digital societies globally, with exceptional e-government deployment \u00e2\u20ac\u201c which, however, makes it vulnerable to various cyber threats. CHESS will directly follow the strategies and roadmaps of the European Cybersecurity Competence Pilots and build on the experience of CHESS partners involved in all four of these pilots, contributing to safe transition of the EU to full-scale digital society. The CHESS Hub will conduct a thorough needs analysis of the two regions and develop a joint cross-border R&I strategy for cybersecurity. The strategy development will be aided by implementation of pilot R&I projects that will reinforce the cross-regional collaboration, engage regional innovation ecosystems and build evidence for future projects. Gaps in skills and expertise identified in the regions will be removed by training and knowledge transfer. Finally, dedicated task forces will ensure sustainability of CHESS by integration with regional, national, and EU-level strategies and funding programmes. To exploit the project outputs, especially the pilot project results, CHESS will aid with market potential assessment and link researchers and innovators with entrepreneurship training and business consultancy services available in the regions. The strategizing, skills-building and pilot R&I will cover the totality of the cybersecurity field, with special attention to 6 Challenge Areas: Internet of Secure Things; Security Certification; Verification of Trustworthy Software; Blockchain; Post-Quantum Cryptography; and Human-centric Aspects of Cybersecurity.\n\n",
                "DataExportTag": "COR12334",
                "QuestionID": "QID166",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Cyber-security Excellence Hub in Estonia and South Moravia The proposed Cyber-security Excellence...",
                "Choices": {
                    "1": {
                        "Display": "\"Nuclear Security and Digital Infrastructure Management\""
                    },
                    "2": {
                        "Display": "nuclear, cybersecurity, critical_infrastructure, interoperable, blockchain, certification, marketplace, toolbox, federation, digital_twin, factory, security_privacy, experimentation, deploy, cross"
                    },
                    "3": {
                        "Display": "\"Mobile Banking and E-commerce\""
                    },
                    "4": {
                        "Display": "mobile, blockchain, payment, personal, store, bank, transaction, insurance, biometric, smartphone, retailer, purchase, retail, pay, e_commerce"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID376",
            "SecondaryAttribute": "CytoSorb, a Novel Therapeutic Approach for Patients with Septic Shock: A Case Report Introduction...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "CytoSorb, a Novel Therapeutic Approach for Patients with Septic Shock: A Case Report Introduction Hemoadsorption using CytoSorb has gained attention as a potential immunotherapy to control systemic inflammation and sepsis. We report on a patient with septic shock, successfully treated with CytoSorb therapy. Methods A 72-year-old male with periodically recurring infectious episodes was admitted with the suspicion of urosepsis. In the following hours his hemodynamic situation deteriorated markedly, exhibiting respiratory-metabolic acidosis, elevated inflammatory marker plasma levels, a severely disturbed coagulation, increased retention parameters, liver dysfunction, and confirmation of bacteria and leucocytes in urine. After admission to the ICU in a state of septic shock the patient received renal support with additional hemoadsorption using CytoSorb. Three CytoSorb sessions were run during the following days. Results The first and consecutive second session resulted in a reduction of procalcitonin, C-reactive protein and bilirubin and a markedly reduced need for vasopressors while hemodynamics improved significantly (i.e., cardiac index, extravascular lung water). Due to a recurring inflammatory \u201csecond hit\u201d episode, another session with CytoSorb was run, resulting in a marked decrease in leukocytosis and liver (dys)function parameters. Conclusions The rapid hemodynamic stabilization with reduction of vasopressor needs within hours and reduction of the capillary leakage as well as a quick reduction in infection markers were the main conclusions drawn from the use of CytoSorb in this patient. Additionally, treatment appeared to be safe and was well tolerated. Despite the promising results of CytoSorb application in this patient, further studies are necessary to elucidate to what extent these favorable consequences are attributable to the adsorber itself.\n\n",
                "DataExportTag": "CAN572955",
                "QuestionID": "QID376",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "CytoSorb, a Novel Therapeutic Approach for Patients with Septic Shock: A Case Report Introduction...",
                "Choices": {
                    "1": {
                        "Display": "\"Exercise and Stress Response Monitoring\""
                    },
                    "2": {
                        "Display": "crp_levels, exercise, serum, plasma, concentration, blood, reactive, hs_crp, urine, cpb, lactate, hscrp, cortisol, exercise_training, training"
                    },
                    "3": {
                        "Display": "\"Cardiovascular Diseases and Heart Failure Treatment\""
                    },
                    "4": {
                        "Display": "heart_failure, left_ventricular, atrial_fibrillation, left_ventricle, cardiovascular, myocardial, hf, chf, leave_ventricular, ejection_fraction, heart, anthracycline, arrhythmia, leave_ventricle, doxorubicin"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID119",
            "SecondaryAttribute": "DATA COLLECTION AND ANALYSIS Two review authors independently assessed the search results for rel...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DATA COLLECTION AND ANALYSIS\nTwo review authors independently assessed the search results for relevance, undertook critical appraisal according to known guidelines, and extracted data using a pre-specified pro forma.\n\n",
                "DataExportTag": "47",
                "QuestionID": "QID119",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DATA COLLECTION AND ANALYSIS Two review authors independently assessed the search results for rel...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID360",
            "SecondaryAttribute": "Data-driven models for Progression Of Neurological Disease EuroPOND will develop a data-driven st...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Data-driven models for Progression Of Neurological Disease EuroPOND will develop a data-driven statistical and computational modeling framework for neurological disease progression. This will enable major advances in differential and personalized diagnosis, prognosis, monitoring, and treatment and care decisions, positioning Europe as world leaders in one of the biggest societal challenges of 21st century healthcare. The inherent complexity of neurological disease, the overlap of symptoms and pathologies, and the high comorbidity rate suggests a systems medicine approach, which matches the specific challenge of this call. We take a uniquely holistic approach that, in the spirit of systems medicine, integrates a variety of clinical and biomedical research data including risk factors, biomarkers, and interactions. Our consortium has a multidisciplinary balance of essential expertise in mathematical\/statistical\/computational modelling; clinical, biomedical and epidemiological expertise; and access to a diverse range of datasets for sporadic and well-phenotyped disease types.The project will devise and implement, as open-source software tools, advanced statistical and computational techniques for reconstructing long-term temporal evolution of disease markers from cross-sectional or short-term longitudinal data. We will apply the techniques to generate new and uniquely detailed pictures of a range of important diseases. This will support the development of new evidence-based treatments in Europe through deeper disease understanding, better patient stratification for clinical trials, and improved accuracy of diagnosis and prognosis. For example, Alzheimer\u2019s disease alone costs European citizens around \u20ac200B every year in care and loss of productivity. No disease modifying treatments are yet available. Clinical trials repeatedly fail because disease heterogeneity prevents bulk response. Our models enable fine stratification into phenotypes enabling more focussed analysis to identify subgroups that respond to putative treatments.\n\n",
                "DataExportTag": "COR14396",
                "QuestionID": "QID360",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Data-driven models for Progression Of Neurological Disease EuroPOND will develop a data-driven st...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Disease Diagnosis in Therapeutic Medicine\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Medical Imaging and Diagnosis\""
                    },
                    "4": {
                        "Display": "image, imaging, nuclear_magnetic_resonance, positron_emission, tomography, computed_tomography, diagnosis, ultrasound, tissue, optic, non_invasive, eye, vivo, breast, brain"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "RS",
            "PrimaryAttribute": "RS_b8D5fo6RPBMAvyu",
            "SecondaryAttribute": "Default Response Set",
            "TertiaryAttribute": null,
            "Payload": null
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID142",
            "SecondaryAttribute": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images are needed for many applications like medical imaging for diagnosis and treatment. Denoising and super-resolution of medical images is proposed in this paper. Construct a database of high and low resolution image patch pairs and with the help of this database estimate a high resolution image from a single noisy low resolution image. In order to find out a high resolution version from the given input low resolution version, get the non-negative sparse linear representation of the input patch over the low resolution patches from the database. Non-negative quadratic programming approach is used for the sparse process. For the low resolution and noisy images, it is a widely adequate method. Edges of the super resolved image can be enhanced by using the blind deconvolution algorithm. INTRODUCTION Number of pixels in an image represents the resolution of an image. The total number of pixels in the image and the width and height of the image represents the image resolution. It is the detail an image occupies. Resolution of image applies to a number of images like raster digital images, film images etc. An image with high resolution is the one which contain more image detail. A number of ways exist to measure the image resolution. Resolution measures how close lines can be to each other. It is the ability of the sensor to observe or measure the smallest object clearly with distinct boundaries. There is a distinction between the resolution and a pixel. A pixel is in fact a unit of the digital image. Resolution depends upon the pixel size. If we consider a lens with smaller the size of the pixel, higher the resolution will be and the clearer the object will be in the image. Images with smaller pixels might consist of more pixels. The amount of information within the image and the number of pixels is correlated. The aim of this work is to estimate a high resolution image from a single noisy low resolution image. This technique is known as super-resolution. Smoothing and interpolation techniques for noise reduction have been commonly used in image processing as simple resolution enhancement techniques. Gaussian, Wiener, and median filters are the spatial filters which are generally used for smoothing. Bicubic interpolation and cubic spline interpolation are the commonly used interpolation techniques. Compared to simple smoothing method, interpolation methods give improved performance. These methods smooth edges as well as regions with slight variations creating blurring problems. Optical images could be captured efficiently by an array of solid state detectors by the invention of charge coupled piece of equipment. Image resolution is determined by the detector size and its number. Most imaging areas require high resolution images. Improving detector array resolution is not always a feasible approach to increase resolution .There is a tendency to decrease signal-to-noise ratio (SNR) and light sensitivity with the use of higher resolution image sensor devices. For most legacy imaging systems it is very difficult to change detectors due to practical cost and physical restrictions. Image processing area is developing a set of algorithms known as superresolution as a solution to this problem generating high resolution image from systems having low-resolution imaging detectors [2]. Medical imaging is one of the applications which are using high resolution images. Medical imaging is one area which uses high resolution images for treatment. Images with high resolution have applications in several fields. Applications like surveillance, forensic and satellite need zooming of particular region of concern that is the reason for high resolution images becomes vital in such fields. A high resolution image is created in two methods. One way is to create a high resolution image from many lower resolution images or make a high resolution image from a single low resolution image with the help of a database that learns relationship between low and high resolution images. High resolution images are able to provide images with high pixel density and more details about the original. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [26] High resolution images have use in many areas. Medical imaging is one of the significant area which are using high resolution images for treatment and analysis. Doctors need very clear images for the identification of infirmity. It has applications in satellite, observation and forensic which need some specific part for processing. To generate a high resolution image, there exist two methods. One way is to create a high resolution image from a single low resolution image and another way is to create a high resolution image from many low resolution images. Characteristics existing with high resolution images are images with high pixel density and more information\u2019s. High resolution images are not always accessible. Because of the shortcomings of the sensor and optics building methods, images with high resolution is not always feasible and it is somewhat expensive. Super resolution is an answer for this difficulty, it uses some image processing algorithms. The merits of using super resolution is that the existing low resolution method can still be utilized and it is inexpensive. Early fast and precise detection of imaging biomarkers of the onset and progression of diseases is of great significance to the medical community since early detection and intervention often results in optimal treatment and recovery [2]. The advent of novel imaging systems has for the first time enabled clinicians and medical researchers to visualize the anatomical substructures, pathology, and functional features in vivo. However, earlier biomarkers of disease onset are often critically smaller or weaker in contrast compared to their corresponding features in the advanced stages of disease. Therefore, medical imaging community strives for inventing higherresolution\/contrast imaging systems. Super-resolution can be beneficial in improving the image quality of many medical imaging systems without the need for significant hardware alternation [2]. For optimal treatment and recovery, early detection of diseases is of great importance in the medical field [2]. It is possible to visualize the anatomical structures and other features by the advent of novel imaging systems. In the advanced stages of the disease if we consider the earlier biomarkers of the disease, it will be smaller or weaker. So high resolution or high contrast images are needed for medical imaging community. With no need for significant hardware alternation, there is a technique called super-resolution to improve the quality of image [2]. Super resolution methods are categorized into multi-image super-resolution and single image super-resolution methods. Single image super resolution methods are also called example learning based methods. Multi-image super-resolution techniques take a number of low resolution images to make a high resolution image as result. Registration is one of the complicated task in a multi-image super-resolution technique, the other two steps are deblurring and fusion. Motion estimation in case of multiple blurred and noisy low resolution images is a very difficult step in the image resolution. This is the reason for preferring single image super-resolution. From a single low resolution image example based super-resolution or single-image super-resolution methods generate a high resolution image [1]. This technique does not need many low resolution images of the same view and the registration procedure. In this approach, the correlation between low resolution images and corresponding high resolution images is learnt from a database of known low and high resolution image pairs.&nbsp;",
                "DataExportTag": "AI257573",
                "QuestionID": "QID142",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
                "Choices": {
                    "1": {
                        "Display": "\"Climate Monitoring and Weather Forecasting\": satellite, climate, cloud, ocean, weather, precipitation, temperature, ice, atmospheric, meteorological, snow, resolution, radar, forecast, sea_ice"
                    },
                    "2": {
                        "Display": "\"Satellite Imagery and Land Classification\": image, urban, land, map, classification, mapping, resolution, forest, satellite, imagery, vegetation, landsat, wetland, spectral, spatial"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID148",
            "SecondaryAttribute": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images are needed for many applications like medical imaging for diagnosis and treatment. Denoising and super-resolution of medical images is proposed in this paper. Construct a database of high and low resolution image patch pairs and with the help of this database estimate a high resolution image from a single noisy low resolution image. In order to find out a high resolution version from the given input low resolution version, get the non-negative sparse linear representation of the input patch over the low resolution patches from the database. Non-negative quadratic programming approach is used for the sparse process. For the low resolution and noisy images, it is a widely adequate method. Edges of the super resolved image can be enhanced by using the blind deconvolution algorithm. INTRODUCTION Number of pixels in an image represents the resolution of an image. The total number of pixels in the image and the width and height of the image represents the image resolution. It is the detail an image occupies. Resolution of image applies to a number of images like raster digital images, film images etc. An image with high resolution is the one which contain more image detail. A number of ways exist to measure the image resolution. Resolution measures how close lines can be to each other. It is the ability of the sensor to observe or measure the smallest object clearly with distinct boundaries. There is a distinction between the resolution and a pixel. A pixel is in fact a unit of the digital image. Resolution depends upon the pixel size. If we consider a lens with smaller the size of the pixel, higher the resolution will be and the clearer the object will be in the image. Images with smaller pixels might consist of more pixels. The amount of information within the image and the number of pixels is correlated. The aim of this work is to estimate a high resolution image from a single noisy low resolution image. This technique is known as super-resolution. Smoothing and interpolation techniques for noise reduction have been commonly used in image processing as simple resolution enhancement techniques. Gaussian, Wiener, and median filters are the spatial filters which are generally used for smoothing. Bicubic interpolation and cubic spline interpolation are the commonly used interpolation techniques. Compared to simple smoothing method, interpolation methods give improved performance. These methods smooth edges as well as regions with slight variations creating blurring problems. Optical images could be captured efficiently by an array of solid state detectors by the invention of charge coupled piece of equipment. Image resolution is determined by the detector size and its number. Most imaging areas require high resolution images. Improving detector array resolution is not always a feasible approach to increase resolution .There is a tendency to decrease signal-to-noise ratio (SNR) and light sensitivity with the use of higher resolution image sensor devices. For most legacy imaging systems it is very difficult to change detectors due to practical cost and physical restrictions. Image processing area is developing a set of algorithms known as superresolution as a solution to this problem generating high resolution image from systems having low-resolution imaging detectors [2]. Medical imaging is one of the applications which are using high resolution images. Medical imaging is one area which uses high resolution images for treatment. Images with high resolution have applications in several fields. Applications like surveillance, forensic and satellite need zooming of particular region of concern that is the reason for high resolution images becomes vital in such fields. A high resolution image is created in two methods. One way is to create a high resolution image from many lower resolution images or make a high resolution image from a single low resolution image with the help of a database that learns relationship between low and high resolution images. High resolution images are able to provide images with high pixel density and more details about the original. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [26] High resolution images have use in many areas. Medical imaging is one of the significant area which are using high resolution images for treatment and analysis. Doctors need very clear images for the identification of infirmity. It has applications in satellite, observation and forensic which need some specific part for processing. To generate a high resolution image, there exist two methods. One way is to create a high resolution image from a single low resolution image and another way is to create a high resolution image from many low resolution images. Characteristics existing with high resolution images are images with high pixel density and more information\u2019s. High resolution images are not always accessible. Because of the shortcomings of the sensor and optics building methods, images with high resolution is not always feasible and it is somewhat expensive. Super resolution is an answer for this difficulty, it uses some image processing algorithms. The merits of using super resolution is that the existing low resolution method can still be utilized and it is inexpensive. Early fast and precise detection of imaging biomarkers of the onset and progression of diseases is of great significance to the medical community since early detection and intervention often results in optimal treatment and recovery [2]. The advent of novel imaging systems has for the first time enabled clinicians and medical researchers to visualize the anatomical substructures, pathology, and functional features in vivo. However, earlier biomarkers of disease onset are often critically smaller or weaker in contrast compared to their corresponding features in the advanced stages of disease. Therefore, medical imaging community strives for inventing higherresolution\/contrast imaging systems. Super-resolution can be beneficial in improving the image quality of many medical imaging systems without the need for significant hardware alternation [2]. For optimal treatment and recovery, early detection of diseases is of great importance in the medical field [2]. It is possible to visualize the anatomical structures and other features by the advent of novel imaging systems. In the advanced stages of the disease if we consider the earlier biomarkers of the disease, it will be smaller or weaker. So high resolution or high contrast images are needed for medical imaging community. With no need for significant hardware alternation, there is a technique called super-resolution to improve the quality of image [2]. Super resolution methods are categorized into multi-image super-resolution and single image super-resolution methods. Single image super resolution methods are also called example learning based methods. Multi-image super-resolution techniques take a number of low resolution images to make a high resolution image as result. Registration is one of the complicated task in a multi-image super-resolution technique, the other two steps are deblurring and fusion. Motion estimation in case of multiple blurred and noisy low resolution images is a very difficult step in the image resolution. This is the reason for preferring single image super-resolution. From a single low resolution image example based super-resolution or single-image super-resolution methods generate a high resolution image [1]. This technique does not need many low resolution images of the same view and the registration procedure. In this approach, the correlation between low resolution images and corresponding high resolution images is learnt from a database of known low and high resolution image pairs. LITERATURE REVIEW In [3], H Chang projected a novel method for solving single-image super-resolution issues. By means of a set of training examples create a high resolution image from an image with low resolution image. For a given input low resolution patch, find out its nearest low resolution patches and replace the low resolution patches with its corresponding high resolution patch by using a method called locally linear embedding. Single image super resolution method have a requirement for low and high resolution images that the number of patches needs to be same and there found an inter patch relationship. Patch wise image processing is happening here. Euclidean distance matric is used for getting the nearest neighbors which are available in the database. Take the equivalent high resolution patches and calculate the weighted combination by considering all the k high resolution patches. It need patch overlapping to make use of the entire information. Performance of the neighbor embedding method is affected by the k-candidates selection procedure and its quality. When the image is degraded by noise, nearest embedding method become inefficient scheme [4]. In [4] J Yang et al. projected a single-image super-resolution, based on sparse signal representation. It use the over-complete dictionary concept. In order to represent image patches, it use sparse linear representation of [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [27] elements from the dictionary. For low and high resolution patches there exist two dictionaries. Sparse representation in corresponding patches is used for creating the high resolution image patch. Representation of an input vector as a weighted linear com\n\n",
                "DataExportTag": "AI836626",
                "QuestionID": "QID148",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
                "Choices": {
                    "1": {
                        "Display": "\"FPGA and Chip Design\""
                    },
                    "2": {
                        "Display": "fpga, chip, processor, accelerator, field_programmable, reconfigurable, dsp, gate_array, power_consumption, circuit, clock, instruction, parallel, soc, energy"
                    },
                    "3": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "4": {
                        "Display": "cloud, edge, server, scheduling, job, cloud_computing, workload, mobile, distribute, deep_learning, scheduler, iot, edge_computing, user, center"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID211",
            "SecondaryAttribute": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images are needed for many applications like medical imaging for diagnosis and treatment. Denoising and super-resolution of medical images is proposed in this paper. Construct a database of high and low resolution image patch pairs and with the help of this database estimate a high resolution image from a single noisy low resolution image. In order to find out a high resolution version from the given input low resolution version, get the non-negative sparse linear representation of the input patch over the low resolution patches from the database. Non-negative quadratic programming approach is used for the sparse process. For the low resolution and noisy images, it is a widely adequate method. Edges of the super resolved image can be enhanced by using the blind deconvolution algorithm. INTRODUCTION Number of pixels in an image represents the resolution of an image. The total number of pixels in the image and the width and height of the image represents the image resolution. It is the detail an image occupies. Resolution of image applies to a number of images like raster digital images, film images etc. An image with high resolution is the one which contain more image detail. A number of ways exist to measure the image resolution. Resolution measures how close lines can be to each other. It is the ability of the sensor to observe or measure the smallest object clearly with distinct boundaries. There is a distinction between the resolution and a pixel. A pixel is in fact a unit of the digital image. Resolution depends upon the pixel size. If we consider a lens with smaller the size of the pixel, higher the resolution will be and the clearer the object will be in the image. Images with smaller pixels might consist of more pixels. The amount of information within the image and the number of pixels is correlated. The aim of this work is to estimate a high resolution image from a single noisy low resolution image. This technique is known as super-resolution. Smoothing and interpolation techniques for noise reduction have been commonly used in image processing as simple resolution enhancement techniques. Gaussian, Wiener, and median filters are the spatial filters which are generally used for smoothing. Bicubic interpolation and cubic spline interpolation are the commonly used interpolation techniques. Compared to simple smoothing method, interpolation methods give improved performance. These methods smooth edges as well as regions with slight variations creating blurring problems. Optical images could be captured efficiently by an array of solid state detectors by the invention of charge coupled piece of equipment. Image resolution is determined by the detector size and its number. Most imaging areas require high resolution images. Improving detector array resolution is not always a feasible approach to increase resolution .There is a tendency to decrease signal-to-noise ratio (SNR) and light sensitivity with the use of higher resolution image sensor devices. For most legacy imaging systems it is very difficult to change detectors due to practical cost and physical restrictions. Image processing area is developing a set of algorithms known as superresolution as a solution to this problem generating high resolution image from systems having low-resolution imaging detectors [2]. Medical imaging is one of the applications which are using high resolution images. Medical imaging is one area which uses high resolution images for treatment. Images with high resolution have applications in several fields. Applications like surveillance, forensic and satellite need zooming of particular region of concern that is the reason for high resolution images becomes vital in such fields. A high resolution image is created in two methods. One way is to create a high resolution image from many lower resolution images or make a high resolution image from a single low resolution image with the help of a database that learns relationship between low and high resolution images. High resolution images are able to provide images with high pixel density and more details about the original. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [26] High resolution images have use in many areas. Medical imaging is one of the significant area which are using high resolution images for treatment and analysis. Doctors need very clear images for the identification of infirmity. It has applications in satellite, observation and forensic which need some specific part for processing. To generate a high resolution image, there exist two methods. One way is to create a high resolution image from a single low resolution image and another way is to create a high resolution image from many low resolution images. Characteristics existing with high resolution images are images with high pixel density and more information\u2019s. High resolution images are not always accessible. Because of the shortcomings of the sensor and optics building methods, images with high resolution is not always feasible and it is somewhat expensive. Super resolution is an answer for this difficulty, it uses some image processing algorithms. The merits of using super resolution is that the existing low resolution method can still be utilized and it is inexpensive. Early fast and precise detection of imaging biomarkers of the onset and progression of diseases is of great significance to the medical community since early detection and intervention often results in optimal treatment and recovery [2]. The advent of novel imaging systems has for the first time enabled clinicians and medical researchers to visualize the anatomical substructures, pathology, and functional features in vivo. However, earlier biomarkers of disease onset are often critically smaller or weaker in contrast compared to their corresponding features in the advanced stages of disease. Therefore, medical imaging community strives for inventing higherresolution\/contrast imaging systems. Super-resolution can be beneficial in improving the image quality of many medical imaging systems without the need for significant hardware alternation [2]. For optimal treatment and recovery, early detection of diseases is of great importance in the medical field [2]. It is possible to visualize the anatomical structures and other features by the advent of novel imaging systems. In the advanced stages of the disease if we consider the earlier biomarkers of the disease, it will be smaller or weaker. So high resolution or high contrast images are needed for medical imaging community. With no need for significant hardware alternation, there is a technique called super-resolution to improve the quality of image [2]. Super resolution methods are categorized into multi-image super-resolution and single image super-resolution methods. Single image super resolution methods are also called example learning based methods. Multi-image super-resolution techniques take a number of low resolution images to make a high resolution image as result. Registration is one of the complicated task in a multi-image super-resolution technique, the other two steps are deblurring and fusion. Motion estimation in case of multiple blurred and noisy low resolution images is a very difficult step in the image resolution. This is the reason for preferring single image super-resolution. From a single low resolution image example based super-resolution or single-image super-resolution methods generate a high resolution image [1]. This technique does not need many low resolution images of the same view and the registration procedure. In this approach, the correlation between low resolution images and corresponding high resolution images is learnt from a database of known low and high resolution image pairs. LITERATURE REVIEW In [3], H Chang projected a novel method for solving single-image super-resolution issues. By means of a set of training examples create a high resolution image from an image with low resolution image. For a given input low resolution patch, find out its nearest low resolution patches and replace the low resolution patches with its corresponding high resolution patch by using a method called locally linear embedding. Single image super resolution method have a requirement for low and high resolution images that the number of patches needs to be same and there found an inter patch relationship. Patch wise image processing is happening here. Euclidean distance matric is used for getting the nearest neighbors which are available in the database. Take the equivalent high resolution patches and calculate the weighted combination by considering all the k high resolution patches. It need patch overlapping to make use of the entire information. Performance of the neighbor embedding method is affected by the k-candidates selection procedure and its quality. When the image is degraded by noise, nearest embedding method become inefficient scheme [4]. In [4] J Yang et al. projected a single-image super-resolution, based on sparse signal representation. It use the over-complete dictionary concept. In order to represent image patches, it use sparse linear representation of [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [27] elements from the dictionary. For low and high resolution patches there exist two dictionaries. Sparse representation in corresponding patches is used for creating the high resolution image patch. Representation of an input vector as a weighted linear com\n\n",
                "DataExportTag": "AI836626",
                "QuestionID": "QID211",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
                "Choices": {
                    "1": {
                        "Display": "\"FPGA and Chip Design\""
                    },
                    "2": {
                        "Display": "fpga, chip, processor, accelerator, field_programmable, reconfigurable, dsp, gate_array, power_consumption, circuit, clock, instruction, parallel, soc, energy"
                    },
                    "3": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "4": {
                        "Display": "cloud, edge, server, scheduling, job, cloud_computing, workload, mobile, distribute, deep_learning, scheduler, iot, edge_computing, user, center"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID272",
            "SecondaryAttribute": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images are needed for many applications like medical imaging for diagnosis and treatment. Denoising and super-resolution of medical images is proposed in this paper. Construct a database of high and low resolution image patch pairs and with the help of this database estimate a high resolution image from a single noisy low resolution image. In order to find out a high resolution version from the given input low resolution version, get the non-negative sparse linear representation of the input patch over the low resolution patches from the database. Non-negative quadratic programming approach is used for the sparse process. For the low resolution and noisy images, it is a widely adequate method. Edges of the super resolved image can be enhanced by using the blind deconvolution algorithm. INTRODUCTION Number of pixels in an image represents the resolution of an image. The total number of pixels in the image and the width and height of the image represents the image resolution. It is the detail an image occupies. Resolution of image applies to a number of images like raster digital images, film images etc. An image with high resolution is the one which contain more image detail. A number of ways exist to measure the image resolution. Resolution measures how close lines can be to each other. It is the ability of the sensor to observe or measure the smallest object clearly with distinct boundaries. There is a distinction between the resolution and a pixel. A pixel is in fact a unit of the digital image. Resolution depends upon the pixel size. If we consider a lens with smaller the size of the pixel, higher the resolution will be and the clearer the object will be in the image. Images with smaller pixels might consist of more pixels. The amount of information within the image and the number of pixels is correlated. The aim of this work is to estimate a high resolution image from a single noisy low resolution image. This technique is known as super-resolution. Smoothing and interpolation techniques for noise reduction have been commonly used in image processing as simple resolution enhancement techniques. Gaussian, Wiener, and median filters are the spatial filters which are generally used for smoothing. Bicubic interpolation and cubic spline interpolation are the commonly used interpolation techniques. Compared to simple smoothing method, interpolation methods give improved performance. These methods smooth edges as well as regions with slight variations creating blurring problems. Optical images could be captured efficiently by an array of solid state detectors by the invention of charge coupled piece of equipment. Image resolution is determined by the detector size and its number. Most imaging areas require high resolution images. Improving detector array resolution is not always a feasible approach to increase resolution .There is a tendency to decrease signal-to-noise ratio (SNR) and light sensitivity with the use of higher resolution image sensor devices. For most legacy imaging systems it is very difficult to change detectors due to practical cost and physical restrictions. Image processing area is developing a set of algorithms known as superresolution as a solution to this problem generating high resolution image from systems having low-resolution imaging detectors [2]. Medical imaging is one of the applications which are using high resolution images. Medical imaging is one area which uses high resolution images for treatment. Images with high resolution have applications in several fields. Applications like surveillance, forensic and satellite need zooming of particular region of concern that is the reason for high resolution images becomes vital in such fields. A high resolution image is created in two methods. One way is to create a high resolution image from many lower resolution images or make a high resolution image from a single low resolution image with the help of a database that learns relationship between low and high resolution images. High resolution images are able to provide images with high pixel density and more details about the original. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [26] High resolution images have use in many areas. Medical imaging is one of the significant area which are using high resolution images for treatment and analysis. Doctors need very clear images for the identification of infirmity. It has applications in satellite, observation and forensic which need some specific part for processing. To generate a high resolution image, there exist two methods. One way is to create a high resolution image from a single low resolution image and another way is to create a high resolution image from many low resolution images. Characteristics existing with high resolution images are images with high pixel density and more information\u2019s. High resolution images are not always accessible. Because of the shortcomings of the sensor and optics building methods, images with high resolution is not always feasible and it is somewhat expensive. Super resolution is an answer for this difficulty, it uses some image processing algorithms. The merits of using super resolution is that the existing low resolution method can still be utilized and it is inexpensive. Early fast and precise detection of imaging biomarkers of the onset and progression of diseases is of great significance to the medical community since early detection and intervention often results in optimal treatment and recovery [2]. The advent of novel imaging systems has for the first time enabled clinicians and medical researchers to visualize the anatomical substructures, pathology, and functional features in vivo. However, earlier biomarkers of disease onset are often critically smaller or weaker in contrast compared to their corresponding features in the advanced stages of disease. Therefore, medical imaging community strives for inventing higherresolution\/contrast imaging systems. Super-resolution can be beneficial in improving the image quality of many medical imaging systems without the need for significant hardware alternation [2]. For optimal treatment and recovery, early detection of diseases is of great importance in the medical field [2]. It is possible to visualize the anatomical structures and other features by the advent of novel imaging systems. In the advanced stages of the disease if we consider the earlier biomarkers of the disease, it will be smaller or weaker. So high resolution or high contrast images are needed for medical imaging community. With no need for significant hardware alternation, there is a technique called super-resolution to improve the quality of image [2]. Super resolution methods are categorized into multi-image super-resolution and single image super-resolution methods. Single image super resolution methods are also called example learning based methods. Multi-image super-resolution techniques take a number of low resolution images to make a high resolution image as result. Registration is one of the complicated task in a multi-image super-resolution technique, the other two steps are deblurring and fusion. Motion estimation in case of multiple blurred and noisy low resolution images is a very difficult step in the image resolution. This is the reason for preferring single image super-resolution. From a single low resolution image example based super-resolution or single-image super-resolution methods generate a high resolution image [1]. This technique does not need many low resolution images of the same view and the registration procedure. In this approach, the correlation between low resolution images and corresponding high resolution images is learnt from a database of known low and high resolution image pairs. LITERATURE REVIEW In [3], H Chang projected a novel method for solving single-image super-resolution issues. By means of a set of training examples create a high resolution image from an image with low resolution image. For a given input low resolution patch, find out its nearest low resolution patches and replace the low resolution patches with its corresponding high resolution patch by using a method called locally linear embedding. Single image super resolution method have a requirement for low and high resolution images that the number of patches needs to be same and there found an inter patch relationship. Patch wise image processing is happening here. Euclidean distance matric is used for getting the nearest neighbors which are available in the database. Take the equivalent high resolution patches and calculate the weighted combination by considering all the k high resolution patches. It need patch overlapping to make use of the entire information. Performance of the neighbor embedding method is affected by the k-candidates selection procedure and its quality. When the image is degraded by noise, nearest embedding method become inefficient scheme [4]. In [4] J Yang et al. projected a single-image super-resolution, based on sparse signal representation. It use the over-complete dictionary concept. In order to represent image patches, it use sparse linear representation of [Dalia M.S., 2(9): September, 2015] ISSN 2349-4506 Impact Factor: 2.265 Global Journal of Engineering Science and Research Management http: \/\/ www.gjesrm.com \u00a9 Global Journal of Engineering Science and Research Management [27] elements from the dictionary. For low and high resolution patches there exist two dictionaries. Sparse representation in corresponding patches is used for creating the high resolution image patch. Representation of an input vector as a weighted linear com\n\n",
                "DataExportTag": "AI836626",
                "QuestionID": "QID272",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DENOISING AND SUPER-RESOLUTION OF MEDICAL IMAGES BY SPARSE WEIGHT METHOD High resolution images a...",
                "Choices": {
                    "1": {
                        "Display": "\"FPGA and Chip Design\""
                    },
                    "2": {
                        "Display": "fpga, chip, processor, accelerator, field_programmable, reconfigurable, dsp, gate_array, power_consumption, circuit, clock, instruction, parallel, soc, energy"
                    },
                    "3": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "4": {
                        "Display": "cloud, edge, server, scheduling, job, cloud_computing, workload, mobile, distribute, deep_learning, scheduler, iot, edge_computing, user, center"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID102",
            "SecondaryAttribute": "DESIGN & SAMPLE A descriptive qualitative design was used. Interview participants were instrument...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DESIGN & SAMPLE\nA descriptive qualitative design was used. Interview participants were instrumental in the development and implementation of SCPs within their respective health system.\n\n",
                "DataExportTag": "30",
                "QuestionID": "QID102",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DESIGN & SAMPLE A descriptive qualitative design was used. Interview participants were instrument...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID124",
            "SecondaryAttribute": "DESIGN, SETTING, AND PARTICIPANTS Multicenter, randomized clinical superiority trial recruiting p...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DESIGN, SETTING, AND PARTICIPANTS\nMulticenter, randomized clinical superiority trial recruiting participants from 21 centers in Sweden and Norway from February 2010 to June 2014. The last patient follow-up was in December 2014 and final review and verification of the medical records was assessed in March 2015. Patients with suspected perforated diverticulitis, a clinical indication for emergency surgery, and free air on an abdominal computed tomography scan were eligible. Of 509 patients screened, 415 were eligible and 199 were enrolled.\n\n",
                "DataExportTag": "52",
                "QuestionID": "QID124",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DESIGN, SETTING, AND PARTICIPANTS Multicenter, randomized clinical superiority trial recruiting p...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID338",
            "SecondaryAttribute": "Determining the Transmission Strategy of Cognitive User in IEEE 802 . 11 based Networks Cognitive...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Determining the Transmission Strategy of Cognitive User in IEEE 802 . 11 based Networks Cognitive radio methodologies have the potential to dramatically increase the throughput of wireless systems. we consider the opportunistic channel access scheme in IEEE 802.11 based networks subject to the interference mitigation strategy where primary and secondary users can superimpose their transmissions on the same time or frequency slots. According to the protocol rule, primary user follows backoff counter based DCF channel access scheme in contrast to traditional retransmission based channel access scheme where secondary user can easily get to know which slot is idle and in which slot a transmission is being scheduled. In this work, we propose an opportunistic channel access procedure where secondary user intelligently pick s a backoff counter from a given contention window or stays idle on completion of a transmission. First problem has been derived as a linear program formulation from the markov model of both primary and secondary users. From the insights of problem formulation and structure of the problem, an algorithm has been derived for the strategy of the secondary user. Through nume rical calculation, validity of the algorithm has been proven. Later, an online algorithm has been proposed based on reinforcement learning technique. Validity of this algorithm has also been verified through simulation and finally these two algorthm have been compared in terms of throughput given the constraint of primary user\u2019s throughput loss and failure probability. I. I NTRODUCTION Cognitive radio has been the subject of intense research because of its potential to increase the efficiency of wireless networks. Traditional concept of cognitive radio is th at the unlicensed secondary users opportunistically access t he licensed band while primary user keeps silent. This strateg y is called white space approach. There are several works have been conducted based on zero interference rationale [1]\u2013[3 ]. With this, secondary users sense the channel in order to dete ct time\/frequency slots left unused by the primary users and exploit them for transmission. Main goal of this approach is to not interfere with the primary user at all. But, due to the sensing errors, collision with the primary users is inevita ble and thus degrades the throughput of the primary user. The primary user being dumb, after collision, it tries to send pa cket in the next slot as long as the packet transmission index does not exceed the retry limit. We consider IEEE 802.11 based networks and primary user follows DCF protocol to access the channel. In this work, we have proposed an intelligent strategy for the secondary user who picks a backoff counter for packet transmission or remains idle. in addition, transmission technique we have considered interference mitigation scheme. This idea has been evolved because of the concept of multiple input\/output, where simultaneous transmission of a number of users may enhance the throughput instead of each individual transmission by c ancelling the interference somehow. There are some prior lite rature investigating the coexistence in the same time\/freque ncy band with a focus on physical layer methods for static scenar ios [4]\u2013[7]. There is one work similar to our work [8]. They also followed the retransmission based error control schem e as the specification of DCF protocol. Unlike our work, it assume s time is divided into slots and each slot corresponds to one single packet transmission time. Therefore, at the beginni ng of slot, the secondary user knows whether it is idle or occupied ; if occupied, packet\u2019s transmission index can also be determin d. Their optimal strategy has been based on the state of the slot and thus pretty much straightforward. However, according to the DCF protocol specification, primary user has to go through DIFS period and then backoff period before flushing the packet into the air. While staying i n the DIFS or backoff period, there is no way for the secondary user to know whether the primary user has a packet in a queue or not. Given this situation, secondary user has to determin e intelligently a backoff counter or the decision of being idl e. In this work, we determine the optimal access control policy for the secondary users in IEEE 802.11 based networks where nodes follow the DCF protocol in order to access the channel. W focus on a network with two mutually interfering links, one primary and one secondary. We study the interference that the secondary user causes to the primary user and how this interference impacts on the latter. Activity of the primary user is also affected by the secondary user\u2019s channel access scheme. Our analysis is bas ed on detailed markov model of both primary and secondary users i.e. secondary user\u2019s transmission affects both back off and retransmission mechanism. Backoff procedure is halted if the secondary user transmits in a particular slot, transmit ted packet might be corrupted due to link error or for the collisi on with the secondary user. Secondary user\u2019s backoff procedur e and probability of successful transmission are also affect ed by he primary user\u2019s transmission. An accurate stochastic mo del should capture these behaviors of primary and secondary use s nicely. We have developed two markov models in order to detail the activity of both. In this framework, due to activity of the secondary user affects the steady state distribution of the primary user an d thus it affects the achievable throughput. In the similar ma nner, secondary user is also affected by the primary user. Therefore, constraint on the maximum throughput loss or failure probability controls the achievable throughput of the prim ary user. The optimization problem can be formalized through a linear program. According to DCF protocol, channel access scheme of the primary user is randomized, optimal strategy of the secondary user is randomized i.e. in the beginning of a slot, with some probability, either it stays idle or picks a backoff counter from the given contentin window. As we do not explicitly know the state of the primary user, the optima l policy is random given the maximum throughput loss and failure probability of the primary user. The problem though conceptually simple, unveils important issue and general behaviors. As the primary user implemnets a retransmission-based error control mechanism, the activ ity of the secondary user biases the transmission process via interference. Interference at the primary receiver increa ses the failure probability of primary user\u2019s transmission. There for , due to activity of the secondary user the average number of transmissions of primary user\u2019s packet gets larger, togeth er with the average time required to return the the primary user \u2019s idle state. Interestingly, the increase of the avg number of transmissions of primary user\u2019s packets depends on the inde x of the interfered transmission. For instance, while interf erence from the secondary user in the first transmission of the of primary user\u2019s packets depends on the index of the interfere d transmission. For instance, interference from the in the fir st transmission of the primary user\u2019s packets potentially lea ds to a significant increase of the number of transmission. thus as observed before, the impact of the secondary user\u2019s transmi ssions in the various states critically depend on the state of he primary network. As this approach assumes that the secondary transmitter has some knowledge of the current state and probabilistic model of the primary transmitter\/receiver pair, limiting i ts applicability. For example, while it is likely that the seco ndary might read ACKs for the primary system, it is unlikely that the secondary will have knowledge of the pending workload of packets at the primary transmitter or will know the distribu t on of packet arrivals at the primary transmitter. Therefore, w e address this limitation by developing an on line learning approach that uses one feedback bit sent by the primary user and that approximately converges to the optimal secondary control policy. We will show that when the secondary user has access to such tiny knowledge, an online algorithm can obtain performance similar to an offline algorithm with some state information. Rest of the paper is organized as follows, section II illustrates system model of the network, section III explains the detailed optimization problem and the corresponding st rategy picking algorithm. Formulation of the online solution i s Sp Dp\n\n",
                "DataExportTag": "AI1002968",
                "QuestionID": "QID338",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Determining the Transmission Strategy of Cognitive User in IEEE 802 . 11 based Networks Cognitive...",
                "Choices": {
                    "1": {
                        "Display": "\"Wireless Communication and Spectrum Allocation\""
                    },
                    "2": {
                        "Display": "channel, wireless, radio, spectral, user, cognitive_radio, transmission, interference, cellular, csi, allocation, wireless_communication, deep_learning, mobile, antenna"
                    },
                    "3": {
                        "Display": "\"Edge Computing and Resource Allocation in UAV Networks\""
                    },
                    "4": {
                        "Display": "unmanned_aerial_vehicles, deep_learning, offload, edge_computing, vehicle, mobile, resource_allocation, deep_reinforcement_learning, mec, user, traffic, vehicular, caching, wireless, edge"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID145",
            "SecondaryAttribute": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLAB...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLABELLED\nThis study was undertaken to improve the performance of a Chemotherapy Treatment Unit by increasing the throughput and reducing the average patient's waiting time. In order to achieve this objective, a scheduling template has been built. The scheduling template is a simple tool that can be used to schedule patients' arrival to the clinic. A simulation model of this system was built and several scenarios, that target match the arrival pattern of the patients and resources availability, were designed and evaluated. After performing detailed analysis, one scenario provide the best system's performance. A scheduling template has been developed based on this scenario. After implementing the new scheduling template, 22.5% more patients can be served. 1.\n\n",
                "DataExportTag": "AI810196",
                "QuestionID": "QID145",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLAB...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID208",
            "SecondaryAttribute": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLAB...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLABELLED\nThis study was undertaken to improve the performance of a Chemotherapy Treatment Unit by increasing the throughput and reducing the average patient's waiting time. In order to achieve this objective, a scheduling template has been built. The scheduling template is a simple tool that can be used to schedule patients' arrival to the clinic. A simulation model of this system was built and several scenarios, that target match the arrival pattern of the patients and resources availability, were designed and evaluated. After performing detailed analysis, one scenario provide the best system's performance. A scheduling template has been developed based on this scenario. After implementing the new scheduling template, 22.5% more patients can be served. 1.\n\n",
                "DataExportTag": "AI810196",
                "QuestionID": "QID208",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLAB...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID269",
            "SecondaryAttribute": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLAB...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLABELLED\nThis study was undertaken to improve the performance of a Chemotherapy Treatment Unit by increasing the throughput and reducing the average patient's waiting time. In order to achieve this objective, a scheduling template has been built. The scheduling template is a simple tool that can be used to schedule patients' arrival to the clinic. A simulation model of this system was built and several scenarios, that target match the arrival pattern of the patients and resources availability, were designed and evaluated. After performing detailed analysis, one scenario provide the best system's performance. A scheduling template has been developed based on this scenario. After implementing the new scheduling template, 22.5% more patients can be served. 1.\n\n",
                "DataExportTag": "AI810196",
                "QuestionID": "QID269",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Developing an efficient scheduling template of a chemotherapy treatment unit: A case study. UNLAB...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID73",
            "SecondaryAttribute": "Development of Smart Nano and Microcapsulated Sensing Coatings for improving of Material Durabili...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Development of Smart Nano and Microcapsulated Sensing Coatings for improving of Material Durability\/Performance. The proposal aims to develop an innovative approach to impart sensing functionality and detect substrate degradation. The degradation processes targeted will be corrosion of metallic substrates and mechanical damage by impact on fibre reinforced plastics and composites (FRP), used as structural components in the vehicle industry worldwide.The innovative sensing materials are based on controlled release of active species, encapsulated in polymeric and inorganic capsules with sizes ranging from several micrometres down to the nanometre range. These will be designed and prepared in a way that responds to specific triggers associated with the nature of the degradation process. The functional materials will be subsequently incorporated as additives in organic and hybrid organic-inorganic coating matrices, or directly impregnated in the substrate (FRP). The goal is to get coatings capable of sensing substrate degradation at early stages, making maintenance operations cost-effective without jeopardizing safety.The range of selected materials encloses systems conceptually designed to be prepared and tested for the first time at lab scale (high breakthrough at research level) and others already studied at lab scale with promising results and which can already be tested at pilot scale (high innovation level). Furthermore, the characterization encompasses lab-scale, cutting-edge technologies and modelling, as well as upscaling and industrial validation.The consortium upon which the present proposal is set has strong knowledge and previous experience in the topics above presented, reflected upon previous participation in large FP7 EU-projects as well as on Marie Curie actions (IRSES). Therefore, part of the interdisciplinary exchanging network necessary to successfully achieve the objectives and allow a flow and sharing environment of people, knowledge and methods has already been tested in previous projects with positive results.",
                "DataExportTag": "COR15824",
                "QuestionID": "QID73",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Development of Smart Nano and Microcapsulated Sensing Coatings for improving of Material Durabili...",
                "Choices": {
                    "1": {
                        "Display": "\"Metallurgy\": metal, ceramic, coating, alloy, mineral, glass, aluminium, surface, steel, rare_earth, powder, mining, cement, corrosion, metallic"
                    },
                    "2": {
                        "Display": "\"Material Science and Engineering\": coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID195",
            "SecondaryAttribute": "Discovery and Preclinical Validation of Drug Indications Using Compendia of Public Gene Expressio...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Discovery and Preclinical Validation of Drug Indications Using Compendia of Public Gene Expression Data A systematic computational method predicts new uses for existing drugs by integrating public gene expression signatures of drugs and diseases. Greening Drug Discovery Recycling is good for the environment\u2014and for drug development too. Repurposing existing, approved drugs can speed their adoption in the clinic because they can often take advantage of the existing rigorous safety testing required by the Food and Drug Administration and other regulatory agencies. In a pair of papers, Sirota et al. and Dudley et al. examined publicly available gene expression data and determined the genes affected in 100 diseases and 164 drugs. By pairing drugs that correct abnormal gene expression in diseases, they confirm known effective drug-disease pairs and predict new indications for already approved agents. Experimental validation that an antiulcer drug and an antiepileptic can be reused for lung cancer and inflammatory bowel disease reinforces the promise of this approach. The authors scrutinized the data in Gene Expression Omnibus and identified a disease signature for 100 diseases, which they defined as the set of mRNAs that reliably increase or decrease in patients with that disease compared to normal individuals. They compared each of these disease signatures to each of the gene expression signatures for 164 drugs from the Connectivity Map, a collection of mRNA expression data from cultured human cells treated with bioactive small molecules that is maintained at the Broad Institute at Massachusetts Institute of Technology. A similarity score calculated by the authors for every possible pair of drug and disease ranged from +1 (a perfect correlation of signatures) to \u22121 (exactly opposite signatures). The investigators suggested that a similarity score of \u22121 would predict that the drug would ameliorate the abnormalities in the disease and thus be an effective therapy. This proved to be true for a number of drugs already on the market. The corticosteroid prednisolone, a common treatment for Crohn\u2019s disease and ulcerative colitis, showed a strong similarity score for these two diseases. The histone deacetylase inhibitors trichostatin A, valproic acid, and vorinostat were predicted to work against brain tumors and other cancers (esophagus, lung, and colon), and there is experimental evidence that this is indeed the case. But in the ultimate test of method, the authors confirmed two new predictions in animal experiments: Cimetidine, an antiulcer drug, predicted by the authors to be effective against lung cancer, inhibited tumor cells in vitro and in vivo in mice. In addition, the antiepileptic topiramate, predicted to improve inflammatory bowel disease by similarity score, improved damage in colon tissue of rats treated with trinitrobenzenesulfonic acid, a model of the disease. These two drugs are therefore good candidates for recycling to treat two diseases in need of better therapies\u2014lung cancer and inflammatory bowel disease\u2014and we now have a way to mine available data for fast routes to new disease therapies. The application of established drug compounds to new therapeutic indications, known as drug repositioning, offers several advantages over traditional drug development, including reduced development costs and shorter paths to approval. Recent approaches to drug repositioning use high-throughput experimental approaches to assess a compound\u2019s potential therapeutic qualities. Here, we present a systematic computational approach to predict novel therapeutic indications on the basis of comprehensive testing of molecular signatures in drug-disease pairs. We integrated gene expression measurements from 100 diseases and gene expression measurements on 164 drug compounds, yielding predicted therapeutic potentials for these drugs. We recovered many known drug and disease relationships using computationally derived therapeutic potentials and also predict many new indications for these 164 drugs. We experimentally validated a prediction for the antiulcer drug cimetidine as a candidate therapeutic in the treatment of lung adenocarcinoma, and demonstrate its efficacy both in vitro and in vivo using mouse xenograft models. This computational method provides a systematic approach for repositioning established drugs to treat a wide range of human diseases.\n\n",
                "DataExportTag": "CAN1000053",
                "QuestionID": "QID195",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Discovery and Preclinical Validation of Drug Indications Using Compendia of Public Gene Expressio...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Development and Prevention Strategies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Cancer Screening and Prevention\""
                    },
                    "4": {
                        "Display": "screening, woman, colorectal_cancer, gene, breast, cervical, behavior, human_papillomavirus, cutaneous, physician, belief, prevention, prostate, sun_protection, perception"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID256",
            "SecondaryAttribute": "Discovery and Preclinical Validation of Drug Indications Using Compendia of Public Gene Expressio...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Discovery and Preclinical Validation of Drug Indications Using Compendia of Public Gene Expression Data A systematic computational method predicts new uses for existing drugs by integrating public gene expression signatures of drugs and diseases. Greening Drug Discovery Recycling is good for the environment\u2014and for drug development too. Repurposing existing, approved drugs can speed their adoption in the clinic because they can often take advantage of the existing rigorous safety testing required by the Food and Drug Administration and other regulatory agencies. In a pair of papers, Sirota et al. and Dudley et al. examined publicly available gene expression data and determined the genes affected in 100 diseases and 164 drugs. By pairing drugs that correct abnormal gene expression in diseases, they confirm known effective drug-disease pairs and predict new indications for already approved agents. Experimental validation that an antiulcer drug and an antiepileptic can be reused for lung cancer and inflammatory bowel disease reinforces the promise of this approach. The authors scrutinized the data in Gene Expression Omnibus and identified a disease signature for 100 diseases, which they defined as the set of mRNAs that reliably increase or decrease in patients with that disease compared to normal individuals. They compared each of these disease signatures to each of the gene expression signatures for 164 drugs from the Connectivity Map, a collection of mRNA expression data from cultured human cells treated with bioactive small molecules that is maintained at the Broad Institute at Massachusetts Institute of Technology. A similarity score calculated by the authors for every possible pair of drug and disease ranged from +1 (a perfect correlation of signatures) to \u22121 (exactly opposite signatures). The investigators suggested that a similarity score of \u22121 would predict that the drug would ameliorate the abnormalities in the disease and thus be an effective therapy. This proved to be true for a number of drugs already on the market. The corticosteroid prednisolone, a common treatment for Crohn\u2019s disease and ulcerative colitis, showed a strong similarity score for these two diseases. The histone deacetylase inhibitors trichostatin A, valproic acid, and vorinostat were predicted to work against brain tumors and other cancers (esophagus, lung, and colon), and there is experimental evidence that this is indeed the case. But in the ultimate test of method, the authors confirmed two new predictions in animal experiments: Cimetidine, an antiulcer drug, predicted by the authors to be effective against lung cancer, inhibited tumor cells in vitro and in vivo in mice. In addition, the antiepileptic topiramate, predicted to improve inflammatory bowel disease by similarity score, improved damage in colon tissue of rats treated with trinitrobenzenesulfonic acid, a model of the disease. These two drugs are therefore good candidates for recycling to treat two diseases in need of better therapies\u2014lung cancer and inflammatory bowel disease\u2014and we now have a way to mine available data for fast routes to new disease therapies. The application of established drug compounds to new therapeutic indications, known as drug repositioning, offers several advantages over traditional drug development, including reduced development costs and shorter paths to approval. Recent approaches to drug repositioning use high-throughput experimental approaches to assess a compound\u2019s potential therapeutic qualities. Here, we present a systematic computational approach to predict novel therapeutic indications on the basis of comprehensive testing of molecular signatures in drug-disease pairs. We integrated gene expression measurements from 100 diseases and gene expression measurements on 164 drug compounds, yielding predicted therapeutic potentials for these drugs. We recovered many known drug and disease relationships using computationally derived therapeutic potentials and also predict many new indications for these 164 drugs. We experimentally validated a prediction for the antiulcer drug cimetidine as a candidate therapeutic in the treatment of lung adenocarcinoma, and demonstrate its efficacy both in vitro and in vivo using mouse xenograft models. This computational method provides a systematic approach for repositioning established drugs to treat a wide range of human diseases.\n\n",
                "DataExportTag": "CAN1000053",
                "QuestionID": "QID256",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Discovery and Preclinical Validation of Drug Indications Using Compendia of Public Gene Expressio...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Development and Prevention Strategies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Cancer Screening and Prevention\""
                    },
                    "4": {
                        "Display": "screening, woman, colorectal_cancer, gene, breast, cervical, behavior, human_papillomavirus, cutaneous, physician, belief, prevention, prostate, sun_protection, perception"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID112",
            "SecondaryAttribute": "Discussion Desai and associates1 and other groups have described laparoscopic radical nephrectomy...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Discussion\nDesai and associates1 and other groups have described laparoscopic radical nephrectomy in the setting of a renal vein tumor thrombus using various stapling or clamping techniques. Bhayani and colleagues3 described laparoscopic heminephrectomy in a horseshoe kidney for renal-cell carcinoma. We report the first case of robot-assisted heminephrectomy for kidney cancer in a horseshoe kidney with a renal vein thrombus. The decision was made to use robot assistance in the event that a cavotomy with intracorporeal suture repair of the IVC was needed. Although IVC reconstruction did not prove to be necessary, we did identify a novel technique for retraction and exclusion of a renal vein thrombus using extra-large Hem-o-Lok clips.\n\n",
                "DataExportTag": "40",
                "QuestionID": "QID112",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Discussion Desai and associates1 and other groups have described laparoscopic radical nephrectomy...",
                "Choices": {
                    "1": {
                        "Display": "Early ligation of the renal artery may decrease venous pressure and retract the thrombus slightly to enable placement of an extra-large Hem-o-Lok clip across the renal vein, which can be used to milk the thrombus away from the IVC. A potential advantage of using Hem-o-Lok clips is that they may be easier to place between the IVC and thrombus than a wider endovascular stapling device, and it avoids the possibility of thrombus being incorporated into the staple line. The Hem-o-Lok clip can be partially closed around the renal vein with a controlled amount of pressure to facilitate milking of the thrombus without undue force that could cause trauma to the renal vein. Our technique also allows for confirmation of absence of tumor thrombus at the surgical margin, both by intraoperative ultrasonography and by direct visualization as the renal vein is incised."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "We recognize that some renal veins could potentially be too wide for safe use of the Hem-o-Lok clip, although the extra-large clip should accommodate most situations. Our technique of using Hemo-o-Lok clips to control the tumor thrombus avoided the need for a cavotomy and IVC reconstruction in this patient. This technique can be used with either a robotic or laparoscopic approach."
                    },
                    "2": {
                        "Display": "Robot-assisted radical nephrectomy is feasible in the setting of a horseshoe kidney and\/or a renal vein tumor thrombus. Hemo-o-Lok clips can be used to retract and entrap the renal vein thrombus, facilitating successful tumor resection. Experience with minimally invasive radical nephrectomy is recommended before attempting management of a renal vein thrombus using this technique."
                    },
                    "3": {
                        "Display": "\"Scientific Drug Development and Prevention Strategies\""
                    },
                    "4": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "5": {
                        "Display": "\"Oncology Nursing Education and Communication\""
                    },
                    "6": {
                        "Display": "nurse, training, oncology, community, staff, communication, student, provider, professional, app, team, nursing, barrier, oncology_nurse, resident"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID217",
            "SecondaryAttribute": "DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS FOR INDUSTRIAL INTERNET OF THING...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS FOR INDUSTRIAL INTERNET OF THINGS (IoT) APPLICATIONS Certain conventional compute solutions are restricted to resources present in the networking hardware (e.g., routers\/switches). However, increasing numbers of edge compute workloads are demanding special requirements with Graphical processing unit (GPU)\/tensor processing unit (TPU) support, various in-built Internet of Things (IoT) protocol requirements, high system resources, etc. The techniques presented herein address the market of those specialized edge compute workloads while also extending IoT container orchestration to off-the-shelf devices. DETAILED DESCRIPTION Industrial IoT edge compute workloads are continuously demanding sophisticated applications with specific architectures, artificial intelligence (AI)\/ machine learning (ML) services, compute resources (cpu\/memory\/disk), protocols, etc. For example, a predictive edge analytics use-case can be efficiently solved with a machine learning model based inference engine which needs AI\/ML services and tighter integration with GPU\/TPUs for accelerated computation. Existing Industrial IoT hardware does not meet these needs and there may be low incentives for vendors to provide those special resource requirements in generic network elements, such as like routers\/switches. There is off-the-shelf specialized hardware that satisfies the requirements of special edge compute workloads. Proposed herein is to extend a software framework, referred to as an IoT management framework, to onboard, deploy and manage applications on such off-the-shelf specialized hardware (white box devices). With these techniques, a customer\/developer can have the flexibility to choose any off-the-shelf hardware that 2 Sankaran and Tandon: DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS Published by Technical Disclosure Commons, 2019 2 5877X satisfies his\/her resource consumption, potentially with zero touch onboarding of the white box devices, while maintaining a single pane of glass view for managing applications. That is, the IoT management framework enables a customer to just plug in the white-box device to a router\/switch enabled with the software framework, then the IoT management framework will create a local edge cluster where applications can deployed and managed. In an example workflow: 1. The customer first selects the off-the-shelf compute platform (white box device) to host their container application. This compute platform is expected to run link layer discovery protocol (LLDP) and a container engine (e.g., listening on Transmission Control Protocol socket). 2. The customer plugs the off-the-shelf compute platform into an IoT management framework enabled router\/switch. The IoT management framework may be an in-built feature of the router\/switch. 3. The IoT management framework will also discover the off-the-shelf compute platform using the LLDP protocol. As an outcome of this LLDP discovery, the IoT management framework will know the off-the-shelf device details such as IP address, system unique identifier, MAC address, Interface name, and hostname. 4. The IoT management framework will establish a keep-alive session with the container engine on the off-the-shelf compute platform and prepare to orchestrate container applications on the off-the-shelf platform. 5. The IoT management framework can be managed from a Single-Device controller (Local Manager) or from Scale controllers. With any of these controllers, the customer can select the off-the-shelf device discovered by the IoT management framework and deploy container applications and manage the state of the application. Further details of this workflow implementation are provided below. 3 Defensive Publications Series, Art. 2529 [2019] https:\/\/www.tdcommons.org\/dpubs_series\/2529 3 5877X Step 1: Select off-the-shelf (whitebox) compute platform The customer can select any white box compute platforms assuming several software requirements are satisfied by the compute platform. For example, the compute platform needs to run the LLDP service, which sends out the device's metadata to directly connected neighbor network elements. By default, the LLDP service sends out metadata of the device, such as the IP address, system unique identifier, MAC address, Interface name and hostname. If there are multiple interfaces on the whitebox compute platform, then the LLDP service needs to be configured to send out the metadata via the ethernet network interface that will be used to connect to the IoT management framework enabled router\/switch. In addition, the compute platform needs to run a container engine service, by default, and be able to run containers. Container applications be able to communicate via a supported serial device, USB device or any desired peripherals or desired protocols for a hosted customer application. Moreover, the container engine listens on the TCP web socket at port 2376 (default is unix socket) so that the remote container client (the IoT management framework in this case) will be able to communicate to this container engine and spawn container applications. Step 2: Connect off-the-shelf platform to the IoT management framework enabled router\/switch The customer will connect the whitebox compute platform to the IoT management framework enabled router\/switch's front panel data interface. The LLDP service needs to be enabled on this router\/switch, so that IOS can discover nearby LLDP nodes. Effectively when the system runs \"show lldp neighbor details\", the system should be able to show directly connected off-the-shelf device and its metadata. Today IoT routers and switches prepackages the IoT management framework as part of the router\/switch image, mainly used to setup Day0 configurations of the router\/switch. There is an application container with a special python module to interact with the IOS CLI interface and execute any of IOS CLI SHOW or CONFIG commands. The techniques presented herein propose to run a long-running python script inside this 4 Sankaran and Tandon: DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS Published by Technical Disclosure Commons, 2019 4 5877X shell to execute the IOS CLI command \"show lldp neighbor details\" every few seconds and write the output to \/bootflash\/lldp_nodes file of the router\/switch. The application has access to bootflash dir from inside the container. Step 3: The IoT management framework onboards off-the-shelf compute platform The IoT management framework runs in binos linux userspace of an IoT management framework enabled router\/switch, also has access to \/bootflash dir. The techniques presented herein extend the IoT management framework capability to onboard LLDP nodes. This is done by the IoT management framework reading the LLDP metadata file \/bootflash\/lldp_nodes and updating internal datastructure objects. As such, the IoT management framework now knows metadata, such as the IP address, system unique identifier, MAC address, Interface name and hostname of each directly connected LLDP nodes. At this point, a customer using a controller now will be able to see off-the-shelf LLDP nodes listed along with an IoT management framework enabled router\/switch to orchestrate the IoT management framework container application. An example interface is shown below in FIG. 1.\n\n",
                "DataExportTag": "AI195080",
                "QuestionID": "QID217",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS FOR INDUSTRIAL INTERNET OF THING...",
                "Choices": {
                    "1": {
                        "Display": "\"Resource Allocation and Scheduling in Distributed Systems\""
                    },
                    "2": {
                        "Display": "distribute, cooperative, coordination, allocation, decentralized, resource_allocation, centralized, scheduling, sharing, heterogeneous, collaborative, decentralize, distributed, slice, slicing"
                    },
                    "3": {
                        "Display": "\"Edge Computing and Resource Allocation in UAV Networks\""
                    },
                    "4": {
                        "Display": "unmanned_aerial_vehicles, deep_learning, offload, edge_computing, vehicle, mobile, resource_allocation, deep_reinforcement_learning, mec, user, traffic, vehicular, caching, wireless, edge"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID278",
            "SecondaryAttribute": "DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS FOR INDUSTRIAL INTERNET OF THING...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS FOR INDUSTRIAL INTERNET OF THINGS (IoT) APPLICATIONS Certain conventional compute solutions are restricted to resources present in the networking hardware (e.g., routers\/switches). However, increasing numbers of edge compute workloads are demanding special requirements with Graphical processing unit (GPU)\/tensor processing unit (TPU) support, various in-built Internet of Things (IoT) protocol requirements, high system resources, etc. The techniques presented herein address the market of those specialized edge compute workloads while also extending IoT container orchestration to off-the-shelf devices. DETAILED DESCRIPTION Industrial IoT edge compute workloads are continuously demanding sophisticated applications with specific architectures, artificial intelligence (AI)\/ machine learning (ML) services, compute resources (cpu\/memory\/disk), protocols, etc. For example, a predictive edge analytics use-case can be efficiently solved with a machine learning model based inference engine which needs AI\/ML services and tighter integration with GPU\/TPUs for accelerated computation. Existing Industrial IoT hardware does not meet these needs and there may be low incentives for vendors to provide those special resource requirements in generic network elements, such as like routers\/switches. There is off-the-shelf specialized hardware that satisfies the requirements of special edge compute workloads. Proposed herein is to extend a software framework, referred to as an IoT management framework, to onboard, deploy and manage applications on such off-the-shelf specialized hardware (white box devices). With these techniques, a customer\/developer can have the flexibility to choose any off-the-shelf hardware that 2 Sankaran and Tandon: DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS Published by Technical Disclosure Commons, 2019 2 5877X satisfies his\/her resource consumption, potentially with zero touch onboarding of the white box devices, while maintaining a single pane of glass view for managing applications. That is, the IoT management framework enables a customer to just plug in the white-box device to a router\/switch enabled with the software framework, then the IoT management framework will create a local edge cluster where applications can deployed and managed. In an example workflow: 1. The customer first selects the off-the-shelf compute platform (white box device) to host their container application. This compute platform is expected to run link layer discovery protocol (LLDP) and a container engine (e.g., listening on Transmission Control Protocol socket). 2. The customer plugs the off-the-shelf compute platform into an IoT management framework enabled router\/switch. The IoT management framework may be an in-built feature of the router\/switch. 3. The IoT management framework will also discover the off-the-shelf compute platform using the LLDP protocol. As an outcome of this LLDP discovery, the IoT management framework will know the off-the-shelf device details such as IP address, system unique identifier, MAC address, Interface name, and hostname. 4. The IoT management framework will establish a keep-alive session with the container engine on the off-the-shelf compute platform and prepare to orchestrate container applications on the off-the-shelf platform. 5. The IoT management framework can be managed from a Single-Device controller (Local Manager) or from Scale controllers. With any of these controllers, the customer can select the off-the-shelf device discovered by the IoT management framework and deploy container applications and manage the state of the application. Further details of this workflow implementation are provided below. 3 Defensive Publications Series, Art. 2529 [2019] https:\/\/www.tdcommons.org\/dpubs_series\/2529 3 5877X Step 1: Select off-the-shelf (whitebox) compute platform The customer can select any white box compute platforms assuming several software requirements are satisfied by the compute platform. For example, the compute platform needs to run the LLDP service, which sends out the device's metadata to directly connected neighbor network elements. By default, the LLDP service sends out metadata of the device, such as the IP address, system unique identifier, MAC address, Interface name and hostname. If there are multiple interfaces on the whitebox compute platform, then the LLDP service needs to be configured to send out the metadata via the ethernet network interface that will be used to connect to the IoT management framework enabled router\/switch. In addition, the compute platform needs to run a container engine service, by default, and be able to run containers. Container applications be able to communicate via a supported serial device, USB device or any desired peripherals or desired protocols for a hosted customer application. Moreover, the container engine listens on the TCP web socket at port 2376 (default is unix socket) so that the remote container client (the IoT management framework in this case) will be able to communicate to this container engine and spawn container applications. Step 2: Connect off-the-shelf platform to the IoT management framework enabled router\/switch The customer will connect the whitebox compute platform to the IoT management framework enabled router\/switch's front panel data interface. The LLDP service needs to be enabled on this router\/switch, so that IOS can discover nearby LLDP nodes. Effectively when the system runs \"show lldp neighbor details\", the system should be able to show directly connected off-the-shelf device and its metadata. Today IoT routers and switches prepackages the IoT management framework as part of the router\/switch image, mainly used to setup Day0 configurations of the router\/switch. There is an application container with a special python module to interact with the IOS CLI interface and execute any of IOS CLI SHOW or CONFIG commands. The techniques presented herein propose to run a long-running python script inside this 4 Sankaran and Tandon: DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS Published by Technical Disclosure Commons, 2019 4 5877X shell to execute the IOS CLI command \"show lldp neighbor details\" every few seconds and write the output to \/bootflash\/lldp_nodes file of the router\/switch. The application has access to bootflash dir from inside the container. Step 3: The IoT management framework onboards off-the-shelf compute platform The IoT management framework runs in binos linux userspace of an IoT management framework enabled router\/switch, also has access to \/bootflash dir. The techniques presented herein extend the IoT management framework capability to onboard LLDP nodes. This is done by the IoT management framework reading the LLDP metadata file \/bootflash\/lldp_nodes and updating internal datastructure objects. As such, the IoT management framework now knows metadata, such as the IP address, system unique identifier, MAC address, Interface name and hostname of each directly connected LLDP nodes. At this point, a customer using a controller now will be able to see off-the-shelf LLDP nodes listed along with an IoT management framework enabled router\/switch to orchestrate the IoT management framework container application. An example interface is shown below in FIG. 1.\n\n",
                "DataExportTag": "AI195080",
                "QuestionID": "QID278",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "DYNAMIC ONBOARDING AND MANAGEMENT OF SOPHISTICATED EDGE CLUSTERS FOR INDUSTRIAL INTERNET OF THING...",
                "Choices": {
                    "1": {
                        "Display": "\"Resource Allocation and Scheduling in Distributed Systems\""
                    },
                    "2": {
                        "Display": "distribute, cooperative, coordination, allocation, decentralized, resource_allocation, centralized, scheduling, sharing, heterogeneous, collaborative, decentralize, distributed, slice, slicing"
                    },
                    "3": {
                        "Display": "\"Edge Computing and Resource Allocation in UAV Networks\""
                    },
                    "4": {
                        "Display": "unmanned_aerial_vehicles, deep_learning, offload, edge_computing, vehicle, mobile, resource_allocation, deep_reinforcement_learning, mec, user, traffic, vehicular, caching, wireless, edge"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID187",
            "SecondaryAttribute": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nu...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nutrition and Immune Function Objective Early nutritional support in patients with gastric cancer can improve their nutritional status, but the impact on immune function has not been confirmed. This study aimed to analyze the effects of Qihuang decoction combined with enteral nutrition on nutrition and the immune function of postoperative gastric cancer. Methods 120 patients with postoperative gastric cancer in the study group and 117 in the control group were selected as the study subjects from our hospital at random. Indications of nutrition and immune and the rates of complications were compared the day before surgery and 1, 3, 7, and 14 days after surgery. Results Indications of nutrition except hemoglobin (HB) in the study group were significantly higher than those before operation and the albumin (ALB) and prealbumin (TP) were significantly increased 7 and 14\u2009days after surgery (P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P\u03c72=0.036; P\u03c72=0.041; P\u03c72=0.004). Conclusions Qihuang decoction combined with enteral nutrition can promote the absorption of enteral nutrition with improving the immune and reducing complications of infection.\n\n",
                "DataExportTag": "CAN1178280",
                "QuestionID": "QID187",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nu...",
                "Choices": {
                    "1": {
                        "Display": "\"Reproductive Health and Ovarian Disorders\""
                    },
                    "2": {
                        "Display": "ovarian_cancer, follicle, fertility, reproductive, fibroid, pregnancy, oocyte, uterine_fibroid, gnrh, leiomyoma, uterine_leiomyoma, infertility, follicular, amh, granulosa"
                    },
                    "3": {
                        "Display": "\"Liver Cancer Treatment and Management\""
                    },
                    "4": {
                        "Display": "hepatocellular_carcinoma, liver, transplant, tace, lt, recurrence, portal_vein, sorafenib, hepatectomy, resection, transarterial_chemoembolization, transcatheter_arterial, milan_criterion, chemoembolization, rfa"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID248",
            "SecondaryAttribute": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nu...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nutrition and Immune Function Objective Early nutritional support in patients with gastric cancer can improve their nutritional status, but the impact on immune function has not been confirmed. This study aimed to analyze the effects of Qihuang decoction combined with enteral nutrition on nutrition and the immune function of postoperative gastric cancer. Methods 120 patients with postoperative gastric cancer in the study group and 117 in the control group were selected as the study subjects from our hospital at random. Indications of nutrition and immune and the rates of complications were compared the day before surgery and 1, 3, 7, and 14 days after surgery. Results Indications of nutrition except hemoglobin (HB) in the study group were significantly higher than those before operation and the albumin (ALB) and prealbumin (TP) were significantly increased 7 and 14\u2009days after surgery (P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P\u03c72=0.036; P\u03c72=0.041; P\u03c72=0.004). Conclusions Qihuang decoction combined with enteral nutrition can promote the absorption of enteral nutrition with improving the immune and reducing complications of infection.\n\n",
                "DataExportTag": "CAN1178280",
                "QuestionID": "QID248",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nu...",
                "Choices": {
                    "1": {
                        "Display": "\"Reproductive Health and Ovarian Disorders\""
                    },
                    "2": {
                        "Display": "ovarian_cancer, follicle, fertility, reproductive, fibroid, pregnancy, oocyte, uterine_fibroid, gnrh, leiomyoma, uterine_leiomyoma, infertility, follicular, amh, granulosa"
                    },
                    "3": {
                        "Display": "\"Liver Cancer Treatment and Management\""
                    },
                    "4": {
                        "Display": "hepatocellular_carcinoma, liver, transplant, tace, lt, recurrence, portal_vein, sorafenib, hepatectomy, resection, transarterial_chemoembolization, transcatheter_arterial, milan_criterion, chemoembolization, rfa"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID324",
            "SecondaryAttribute": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nu...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nutrition and Immune Function Objective Early nutritional support in patients with gastric cancer can improve their nutritional status, but the impact on immune function has not been confirmed. This study aimed to analyze the effects of Qihuang decoction combined with enteral nutrition on nutrition and the immune function of postoperative gastric cancer. Methods 120 patients with postoperative gastric cancer in the study group and 117 in the control group were selected as the study subjects from our hospital at random. Indications of nutrition and immune and the rates of complications were compared the day before surgery and 1, 3, 7, and 14 days after surgery. Results Indications of nutrition except hemoglobin (HB) in the study group were significantly higher than those before operation and the albumin (ALB) and prealbumin (TP) were significantly increased 7 and 14\u2009days after surgery (P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P < 0.001 and P < 0.001 versus P < 0.001 and P < 0.001) and the protein (PA) 3, 7, and 14 days after surgery (P=0.011, P=0.002, and P=0.022) in the study group compared to those in the control group. Cellular and humoral immunity indications in the study group are significantly higher than those before operation compared to those in the control group, and the CD3+, CD4+, and CD4+\/CD8+ were significantly increased 7 and 14 days after surgery (P=0.027 and P < 0.001 versus P=0.008 and P < 0.001 versus P=0.010 and P < 0.001) and IgA, IgG, and IgM 3, 7, and 14 days after surgery in the study group (P < 0.001, P < 0.001, and P < 0.001 versus P < 0.001, P < 0.002, and P < 0.001 versus P < 0.001, P < 0.001, and P < 0.001). The complications such as abdominal, lung, wound, and urinary infection were also significantly decreased (P\u03c72=0.017; P\u03c72=0.036; P\u03c72=0.041; P\u03c72=0.004). Conclusions Qihuang decoction combined with enteral nutrition can promote the absorption of enteral nutrition with improving the immune and reducing complications of infection.\n\n",
                "DataExportTag": "CAN452071",
                "QuestionID": "QID324",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Effect of Qihuang Decoction Combined with Enteral Nutrition on Postoperative Gastric Cancer of Nu...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunology and Autoimmune Disease Research\""
                    },
                    "2": {
                        "Display": "lymphocyte, treg_cells, mouse, regulatory, memory, subset, tregs, autoimmune, thymus, tolerance, thymic, peripheral_blood, pbmc, thymocyte, subpopulation"
                    },
                    "3": {
                        "Display": "\"Viral Vectors and Disease Research\""
                    },
                    "4": {
                        "Display": "virus, hiv, vector, gene, epstein_barr_virus, strain, alzheimer_disease, kshv, transduction, immunodeficiency_virus, antiviral, adenovirus, kaposi_sarcoma, hcmv, latency"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID341",
            "SecondaryAttribute": "Enabling Precis Integrative Netw A key challenge in precision medicine lies in understanding mole...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Enabling Precis Integrative Netw A key challenge in precision medicine lies in understanding molecular-level underpinnings of complex human disease. Biological networks in multicellular organisms can generate hypotheses about disease genes, pathways, and their behavior in disease-related tissues. Diverse functional genomic data, including expression, protein\u2013protein interaction, and relevant sequence and literature information, can be utilized to build integrative networks that provide both genome-wide coverage as well as contextual specificity and accuracy. By carefully extracting the relevant signal in thousands of heterogeneous functional genomics experiments through integrative analysis, these networks model how genes work together in specific contexts to carry out cellular processes, thereby contributing to a molecular-level understanding of complex human disease and paving the way toward better therapy and drug treatment. Here, we discuss current methods to build context-specific integrative networks, focusing on tissue-specific networks. We highlight applications of these networks in predicting tissue-specific molecular response, identifying candidate disease genes, and increasing power by amplifying the disease signal in quantitative genetics data. Altogether, these exciting developments enable biomedical scientists to characterize disease from pathophysiology to cellular system and, finally, to specific gene alterations\u2014making significant strides toward the goal of precision medicine. \u00a9 2018 Published by Elsevier Ltd. Networks as models of human biology To realize the promise of precision medicine, we must elucidate the immense molecular complexity that forms the foundation of disease. Most human diseases are polygenic, perhaps even \u201comnigenic\u201d [1]. While decades of targeted disease research and the rise of large-scale quantitative genetics studies such as genome-wide association studies (GWAS) have been valuable in identifying genes and genetic variants that may be linked to a wide range of diseases and phenotypes, it is increasingly clear that there is a \u201cmissing heritability\u201d problem (i.e., even as the sample sizes in these studies continue to grow, only a small proportion of estimated heritability appears to be explained by the identified variants) [2, 3]. Understanding complex disease at the molecular level requires us to model specific molecular-level ed by Elsevier Ltd. changes that lead to disease. These changes can happen through a variety of mechanisms, for example, regulatory abnormalities, modifications to protein interactions, or effects on signaling cascades. Modern genome-scale experimental techniques enable us to monitor and probe thesemolecular events by providing a wealth of data along multiple axes of cellular activity. Diverse high-throughput data vary in relevance depending on the biological process under study (e.g., a specific tissue, disease, or pathway), as both experimental technologies and perturbations capture different biological signal with varying degrees of success. Thus, integrative analysis of these data is paramount because (1) many diseases\/tissues of interest are interrogated by multiple data sets; (2) each data set holds a complex mixture of signals relevant not only to the biological question or disease under study, but also to many other biological events (e.g., cancer data sets J Mol Biol (2018) 430, 2913\u20132923 2914 Review: Enabling precision medicine: integrative networks have very strong immune signals, kidney disease data have strong inflammation signals); and (3) individual data sets are noisy, necessitating the identification of strong, recurring signals in relevant data sets. A powerful set of approaches (Box 1) has emerged for integrating diverse data into functional maps of human cellular biology. These network approaches use a variety of machine learning and statistical algorithms to integrate very large collections of noisy and heterogeneous human \u201comics\u201d data into functional maps, or networks [4\u20136]. Intuitively, an edge between two genes in these functionalmaps typically represents the probability that the genes are, directly or indirectly, participating in the same biological process or pathway (e.g., innate immune response, microtubule polymerization, axonogenesis). The genome-wide gene networks that result from integration of these data allow biologists to generate specific, experimentally testable hypotheses and provide a systems-level view of biological processes. Biological networkmodelshave typically represented general views of organismal biology, not resolved to specific tissues or cell types. However, tissue and cellular context is critical for interpreting the behavior of genes and pathways, as gene function and interactions can vary greatly between tissues and cell types, and dysregulation of tissueand cell-lineage-specific processes underlies many diseases. For example, selective neuronal vulnerability is a key characteristic of neurodegenerative diseases such as Parkinson's disease, and the neuronal subtypes as well as affected brain regions tend to be strong determinants of their corresponding clinical phenotypes [7]. In Parkinson's disease, the dopaminergic neurons in the substantia nigra pars compacta area of the brain are particularly susceptible to cell death, while highly similar dopaminergic neurons in the nearby ventral tegmental area are much less vulnerable. Thus, to fully capture the underlying biological processes relevant for a disease like Parkinson's disease, brain-region-specific networks are necessary. Below, we discuss approaches to construct tissueand cell-type-specific networks from integrations of large collections of public functional genomic data, and how such networks can be applied to study the molecular basis of human disease (Fig. 1). We begin Fig. 1. Tissue-specific functional interaction networks. Tissue-s aware tissue-specific knowledge and a large human data compen method that (b) identifies and weights data sets based on their tis downstream (c) tissue-specific diseaseanalyses.More specifically (a), gene pairs are considered positive examples when they both p and are expressed in the tissue of interest. Negative examples process or are expressed in other tissues (see Methods in Ref. [ integration method can then use this gold standard to mine the sig networks (b). As effective summaries of tissue-specific biology, t specific biology to help generate hypotheses relevant for human d downstream machine learning methods to predict disease gene network itself can also provide functional interpretations for any g with a discussion of methods that integrate heterogeneous data into networks and further technical innovations that effectively \u201csummarize\u201d these data into context-specific maps of the biological landscape of specific tissues and cell types. We then examine applications of these networks to the study of disease. Finally, we argue for the importance of making these networks andaccompanyingmethodsaccessible to the wider community throughuser-friendly interactivepublic systems that are maintained over time. Building tissueand cell-type-specific functional networks Methods that construct tissue-specific networks have historically been limited by the availability of experimental data for specific tissues and cell types, especially in humans. These direct approaches typically assemble available tissue-specific expression data into gene correlation networks [8\u201311] or overlay those expression data on global (non-tissue-specific) protein\u2013protein interaction networks [12\u201314]. More sophisticated methods to construct context-specific regulatory networks by integrating (as opposed to simple overlaying) context-specific (e.g., tissueor celltype specific) expression data with a non-contextspecific network have also been recently developed [15]. These approaches, while valuable, are applicable only to tissues and cell types which can be readily assayed [16] and depend almost entirely on the quality of the available tissueor cell-type-specific data. For example, in cancer, where The Cancer Genome Atlas and other initiatives have amassed large, high-quality collections of diverse, genome-scale data to characterize specific cancer types, there has been significant progress in the development of network models, and they have yielded invaluable insights into the cancer landscape [17\u201321]. However, for the vast majority of normal human tissues, direct experimental assay remains infeasible (especially of living, and not postmortem, tissue), requiring computational methods that can infer tissue-specific interactions from large heterogeneous data compendia. To address this challenge, recent work by Greene et al. introduced a method that can simultaneously pecific networks are constructed by integrating (a) hierarchydium using a tissue-specific regularized Bayesian integration sue-relevant signal. These integrative networks are used for , to construct the tissue-specific functional interaction standard articipate in the same process (i.e., process co-membership) include gene pairs that either do not participate in the same 4] for more details). The tissue-specific regularized Bayesian nal from a large data compendium to construct tissue-specific he network can then be used as a representation of tissueisease. For example, the network itself can be used as input to s or reprioritize quantitative genetics data (see Fig. 2). The ene sets of interest (see Fig. 3). (b) Tissue-specific dataset weighting and functional network (c) Tissue-specific disease analysis (a) Hierarchy-aware tissuespecific knowledge Human Data Compendium Tissuespecific Regularized Bayesian Integration 2 4\n\n",
                "DataExportTag": "AI265629",
                "QuestionID": "QID341",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Enabling Precis Integrative Netw A key challenge in precision medicine lies in understanding mole...",
                "Choices": {
                    "1": {
                        "Display": "\"Immunology and Infection Response\""
                    },
                    "2": {
                        "Display": "receptor, cell, activation, innate_immune, macrophage, cytokine, expression, toll_receptor, immune, mouse, infection, inflammation, immune_response, tlrs, signal"
                    },
                    "3": {
                        "Display": "\"Genomic Analysis and Disease Prediction\""
                    },
                    "4": {
                        "Display": "cell, gene, image, variant, prediction, phenotype, snp, tissue, deep_learning, gene_expression, omic, genomic, disease, analysis, mutation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID190",
            "SecondaryAttribute": "Endoscopic incision for the treatment of refractory esophageal anastomotic strictures: outcomes o...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Endoscopic incision for the treatment of refractory esophageal anastomotic strictures: outcomes of 13 cases with a minimum follow-up of 12 months. BACKGROUND AND AIM\nEndoscopic incision is an alternative method for refractory esophageal strictures; however, little is known about its long-term efficacy. The aim of the study is to assess the long-term outcomes of endoscopic incision for treating refractory esophageal anastomotic strictures.\n\n",
                "DataExportTag": "CAN1454395",
                "QuestionID": "QID190",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Endoscopic incision for the treatment of refractory esophageal anastomotic strictures: outcomes o...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID251",
            "SecondaryAttribute": "Endoscopic incision for the treatment of refractory esophageal anastomotic strictures: outcomes o...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Endoscopic incision for the treatment of refractory esophageal anastomotic strictures: outcomes of 13 cases with a minimum follow-up of 12 months. BACKGROUND AND AIM\nEndoscopic incision is an alternative method for refractory esophageal strictures; however, little is known about its long-term efficacy. The aim of the study is to assess the long-term outcomes of endoscopic incision for treating refractory esophageal anastomotic strictures.\n\n",
                "DataExportTag": "CAN1454395",
                "QuestionID": "QID251",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Endoscopic incision for the treatment of refractory esophageal anastomotic strictures: outcomes o...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID335",
            "SecondaryAttribute": "Engineering-Based New Reservoir Design and Environmental Suitability Analysis with Geospatial Tec...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Engineering-Based New Reservoir Design and Environmental Suitability Analysis with Geospatial Technology As a follow up to the 2007 drought and water scarcity in Georgia and especially in North Georgia, there is a greater need of creating new water reservoirs. The main goal of this study is to use geospatial technology, engineering and environmental knowhow to find suitable locations in North GA for building a reservoir to serve primarily for drinking water supply and irrigation. Another objective is to design the reservoir with proper engineering applications and conduct an environmental impact analysis due to its construction. In order to determine reservoir suitable sites in North Georgia, a geospatial model was created with ArcGIS 9.3 Model Builder based on land use, DEM (Digital Elevation Model), Census Data, and orthoimagery. Buffers of different distances were created based on airport sites, population density, landfill and industrial waste sites, U.S forestry and national parks, railroads, and major roadways of North Georgia, which were designated as unsuitable areas for probable reservoir locations for environmental concerns. Peck\u2019s Mill Watershed, located in Lumpkin County was chosen as the most suitable location for building the reservoir. Then the suitable areas were surveyed using a DEM to find the best location to build a dam for the reservoir. The dam height was determined based on the amount of direct runoff coming from the above catchment area to the dam location and keeping in prospective to have the reservoir filled in four years maximum. A geospatial model was developed to calculate the runoff using the Soil Conservation Service (SCS) method (average intensity of rainfall and Curve Number). Based on the dam height, the reservoir impounding volume was calculated. The total runoff was divided with the proposed impounding volume to determine the years it will take to fill in the reservoir. Annual stream discharge of the Chestatee River (One mile downstream of the proposed dam) was also calculated to pump water from the river to fill the reservoir. The reservoir pool line of 405 meter was chosen with a probable filling time of 2.14 years by watershed runoff and water pumped from Chestatee River. After the reservoir design, flood pool line was calculated based on 100-year flood to find the environmental impact due to the reservoir. INTRODUCTION Due to the El-Ni\u00f1o and La-Nina effect from recent phenomenon of global warming and climate change, the global rainfall pattern is changing year to year (Panda, 2008). Drought conditions across the United States during 2007 dominated the Southeast, West, and Upper Great Lakes regions demonstrating unseasonal and erratic weather during March, May, August, and November. In that year, the northern Georgia went through a severe drought. The US National percent area for moderate to extreme dry conditions resulted in the United States increasing from 16% in January to 42% in August (NOAA) with widespread drought conditions throughout Georgia. According to Panda (2008), due to change in thermohaline circulation, the Northern Hemisphere\u2019s tropics and subtropics (a region between equator and 30 N) including Georgia become drier while Southern Hemisphere\u2019s similar region becomes wetter. With the expansion of urban sprawl in the southeast United States centering on Atlanta, GA, by 2030, almost all of north Alabama, north Georgia, most of South Carolina and Florida, a vast area of the gulf coast, and the entire southern Atlantic coast will be of urban land-use (Hammer et al., 2008). Therefore, demand for water will be immense in the area that includes northern Georgia. The sources for drinking water supply and water for other agricultural use would remain same or diminish if precautionary measures are not taken before hand. Therefore, as a solution to Georgia\u2019s water shortages, it is essential to devise plan to build new reservoirs in drought prone area like north Georgia to arrest wasted runoff and make it available for drinking water supply and irrigation. Reservoirs, in general, are multipurpose. They are important for economic development and serve for flood control, water supply, irrigation, hydropower generation, navigation, recreation, and above all environmental management (TVA, 2010). Economic development through job creation is another important aspect of new reservoir construction. For example, Lake Lanier and Tennessee Valley Authority (TVA) generates thousands of jobs. The TVA last year provided approximately 26,000 jobs across the Tennessee valley and earned $4.2 billion dollars in capital investments across the Tennessee region (TVA, 2010). Not only does a reservoir provide economic development and recreational benefits, but construction of a reservoir without proper decision support system would create environmental, economic, and social hazards. A reservoir can submerge huge amount of quality land, habitable locations, and ecologically important areas and displace people from their property. Engineering and surveying process is mostly used in fixing reservoir locations. Larger reservoirs create many environmental hazards than properly designed small reservoirs. Therefore, it is prudent to construct several smaller reservoirs instead of a single large reservoir so that the impact will be minimum but the main objective of meeting the increased water demand could be met. As we have very recently faced a precarious water scarcity in Atlanta, this study on designing smaller reservoirs in and around Atlanta could ease the water scarcity problem a great deal. Moreover, the emphasis on the use of geospatial technology on decision making would create less possibility of the destruction of ecosystem. Another major drawback on such new reservoir design comes from the land availability. According to US Fish and Wildlife Service GA Ecological Services Branch website, \"more than 90 percent of the land in Georgia is privately owned. Therefore, the future health of Georgia\u2019s land, water, and wildlife depends upon private landowners.\" As discussed above, the reservoirs would create humongous job opportunities for landowners whose land will be submerged with the reservoirs. They will reap the benefit from the reservoir if they own land in the ayacut area of the reservoir through irrigation or water seepage. The land cost surrounding the reservoir would certainly increase. Thus, private landowners would positively be motivated to participate in such new small reservoir designs in and around Atlanta. Not many studies have been conducted to determine the location and design of a multipurpose small reservoir using geospatial technology so that maximum environmental and economic benefit can be obtained from the new reservoir. The objective of this study is to develop a geospatial model to locate suitable reservoir sites in North Georgia and design the reservoir using engineering algorithms along with geospatial technology like geographic information system, remote sensing, and information technology. By using a DEM and a reservoir suitability map, a specific watershed called Peck\u2019s Mill watershed was chosen for determining direct runoff and the duration of time it would take to reach full pool at the 405 meter and 410 meter elevation contours. Once the full pool lines were determined, based on storm runoff, the flood pool line was calculated for the reservoir. MATERIALS AND METHODS Study area. The study area of this study involves the entire North Georgia (Figure 1). The reservoir suitability analysis was conducted over these Appalachian Counties of North Georgia. The entire area is ecologically very rich and environmentally sensitive. Therefore, highest precaution was taken to develop the model so that possible reservoir site would create the least environmental and social risk. The procedure of this geospatial model development is described later. With this suitability analysis, the Pecks Mill watershed, located at coordinates 34.53344 N and 83.91587 W in Lumpkin County was selected as the study site of new reservoir design. Peck\u2019s Mill watershed is a sub watershed totaling 2086.94 acres and is a part of 10-digit HUC Chestatee River basin (0313000105). Figure 1: Appalachian counties of North Georgia. Reservoir suitability analysis. Suitability analysis is one of the most crucial processes in environmental management. A reservoir set up would always jeopardize the ecology and landscape of any region if the site selection is not done with proper scientific procedure. Spatial heterogeneity of regions has important influence on ecological patterns and processes (Shugart, 1998) and GIS has a special role to play in decision making in such scenarios of new developments. Many landscape metrics in GIS environment are used to facilitate the investigation of the relation between new landscape structure and biodiversity (Wikramanayake et al. 2004; Bhagwat et al. 2005; Burel and Baudry 2005; Oja et al. 2005; Riitters 2005; Schindler et al. 2008). The suitability analysis model was created for North Georgia Counties to choose potential locations for reservoir construction with very low environmental, ecological, economical, and social disruption. In order to develop a reservoir suitability analysis, the following parameters were considered for the Environmental Protection Division\u2019s (EPD) regulations. Under Georgia\u2019s ordinance, any new facility handling hazardous waste has to abide by the Department of Natural Resource\u2019s guidelines (Hall County, GA, 2010). If the hazardous waste facility is to be built within seven miles of a water storage facility (water reservoir), then the facility has to install spill and leak collection facilities to ensure that the impermeable surfaces do not harm the water supply (Hall County, GA, 2010). Also, limitations on hazardous and toxic materials based on regulations are applied. The regulation states that no landfills, waste disposal, hazardous and toxic waste facilities is located within the water supply watershed, and no industries or businesses classi\n\n",
                "DataExportTag": "AI40054",
                "QuestionID": "QID335",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Engineering-Based New Reservoir Design and Environmental Suitability Analysis with Geospatial Tec...",
                "Choices": {
                    "1": {
                        "Display": "\"Hydrological Forecasting and Water Management\""
                    },
                    "2": {
                        "Display": "flood, rainfall, forecast, river, drought, runoff, hydrological, rmse, reservoir, precipitation, water, root_mean, temperature, dam, streamflow"
                    },
                    "3": {
                        "Display": "\"Water Management and Flood Risk Assessment\""
                    },
                    "4": {
                        "Display": "water, flood, river, groundwater, flow, runoff, reservoir, catchment, watershed, lake, basin, decision_making, discharge, hydrological, risk"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID404",
            "SecondaryAttribute": "Engineering-Based New Reservoir Design and Environmental Suitability Analysis with Geospatial Tec...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Engineering-Based New Reservoir Design and Environmental Suitability Analysis with Geospatial Technology As a follow up to the 2007 drought and water scarcity in Georgia and especially in North Georgia, there is a greater need of creating new water reservoirs. The main goal of this study is to use geospatial technology, engineering and environmental knowhow to find suitable locations in North GA for building a reservoir to serve primarily for drinking water supply and irrigation. Another objective is to design the reservoir with proper engineering applications and conduct an environmental impact analysis due to its construction. In order to determine reservoir suitable sites in North Georgia, a geospatial model was created with ArcGIS 9.3 Model Builder based on land use, DEM (Digital Elevation Model), Census Data, and orthoimagery. Buffers of different distances were created based on airport sites, population density, landfill and industrial waste sites, U.S forestry and national parks, railroads, and major roadways of North Georgia, which were designated as unsuitable areas for probable reservoir locations for environmental concerns. Peck\u2019s Mill Watershed, located in Lumpkin County was chosen as the most suitable location for building the reservoir. Then the suitable areas were surveyed using a DEM to find the best location to build a dam for the reservoir. The dam height was determined based on the amount of direct runoff coming from the above catchment area to the dam location and keeping in prospective to have the reservoir filled in four years maximum. A geospatial model was developed to calculate the runoff using the Soil Conservation Service (SCS) method (average intensity of rainfall and Curve Number). Based on the dam height, the reservoir impounding volume was calculated. The total runoff was divided with the proposed impounding volume to determine the years it will take to fill in the reservoir. Annual stream discharge of the Chestatee River (One mile downstream of the proposed dam) was also calculated to pump water from the river to fill the reservoir. The reservoir pool line of 405 meter was chosen with a probable filling time of 2.14 years by watershed runoff and water pumped from Chestatee River. After the reservoir design, flood pool line was calculated based on 100-year flood to find the environmental impact due to the reservoir. INTRODUCTION Due to the El-Ni\u00f1o and La-Nina effect from recent phenomenon of global warming and climate change, the global rainfall pattern is changing year to year (Panda, 2008). Drought conditions across the United States during 2007 dominated the Southeast, West, and Upper Great Lakes regions demonstrating unseasonal and erratic weather during March, May, August, and November. In that year, the northern Georgia went through a severe drought. The US National percent area for moderate to extreme dry conditions resulted in the United States increasing from 16% in January to 42% in August (NOAA) with widespread drought conditions throughout Georgia. According to Panda (2008), due to change in thermohaline circulation, the Northern Hemisphere\u2019s tropics and subtropics (a region between equator and 30 N) including Georgia become drier while Southern Hemisphere\u2019s similar region becomes wetter. With the expansion of urban sprawl in the southeast United States centering on Atlanta, GA, by 2030, almost all of north Alabama, north Georgia, most of South Carolina and Florida, a vast area of the gulf coast, and the entire southern Atlantic coast will be of urban land-use (Hammer et al., 2008). Therefore, demand for water will be immense in the area that includes northern Georgia. The sources for drinking water supply and water for other agricultural use would remain same or diminish if precautionary measures are not taken before hand. Therefore, as a solution to Georgia\u2019s water shortages, it is essential to devise plan to build new reservoirs in drought prone area like north Georgia to arrest wasted runoff and make it available for drinking water supply and irrigation. Reservoirs, in general, are multipurpose. They are important for economic development and serve for flood control, water supply, irrigation, hydropower generation, navigation, recreation, and above all environmental management (TVA, 2010). Economic development through job creation is another important aspect of new reservoir construction. For example, Lake Lanier and Tennessee Valley Authority (TVA) generates thousands of jobs. The TVA last year provided approximately 26,000 jobs across the Tennessee valley and earned $4.2 billion dollars in capital investments across the Tennessee region (TVA, 2010). Not only does a reservoir provide economic development and recreational benefits, but construction of a reservoir without proper decision support system would create environmental, economic, and social hazards. A reservoir can submerge huge amount of quality land, habitable locations, and ecologically important areas and displace people from their property. Engineering and surveying process is mostly used in fixing reservoir locations. Larger reservoirs create many environmental hazards than properly designed small reservoirs. Therefore, it is prudent to construct several smaller reservoirs instead of a single large reservoir so that the impact will be minimum but the main objective of meeting the increased water demand could be met. As we have very recently faced a precarious water scarcity in Atlanta, this study on designing smaller reservoirs in and around Atlanta could ease the water scarcity problem a great deal. Moreover, the emphasis on the use of geospatial technology on decision making would create less possibility of the destruction of ecosystem. Another major drawback on such new reservoir design comes from the land availability. According to US Fish and Wildlife Service GA Ecological Services Branch website, \"more than 90 percent of the land in Georgia is privately owned. Therefore, the future health of Georgia\u2019s land, water, and wildlife depends upon private landowners.\" As discussed above, the reservoirs would create humongous job opportunities for landowners whose land will be submerged with the reservoirs. They will reap the benefit from the reservoir if they own land in the ayacut area of the reservoir through irrigation or water seepage. The land cost surrounding the reservoir would certainly increase. Thus, private landowners would positively be motivated to participate in such new small reservoir designs in and around Atlanta. Not many studies have been conducted to determine the location and design of a multipurpose small reservoir using geospatial technology so that maximum environmental and economic benefit can be obtained from the new reservoir. The objective of this study is to develop a geospatial model to locate suitable reservoir sites in North Georgia and design the reservoir using engineering algorithms along with geospatial technology like geographic information system, remote sensing, and information technology. By using a DEM and a reservoir suitability map, a specific watershed called Peck\u2019s Mill watershed was chosen for determining direct runoff and the duration of time it would take to reach full pool at the 405 meter and 410 meter elevation contours. Once the full pool lines were determined, based on storm runoff, the flood pool line was calculated for the reservoir. MATERIALS AND METHODS Study area. The study area of this study involves the entire North Georgia (Figure 1). The reservoir suitability analysis was conducted over these Appalachian Counties of North Georgia. The entire area is ecologically very rich and environmentally sensitive. Therefore, highest precaution was taken to develop the model so that possible reservoir site would create the least environmental and social risk. The procedure of this geospatial model development is described later. With this suitability analysis, the Pecks Mill watershed, located at coordinates 34.53344 N and 83.91587 W in Lumpkin County was selected as the study site of new reservoir design. Peck\u2019s Mill watershed is a sub watershed totaling 2086.94 acres and is a part of 10-digit HUC Chestatee River basin (0313000105). Figure 1: Appalachian counties of North Georgia. Reservoir suitability analysis. Suitability analysis is one of the most crucial processes in environmental management. A reservoir set up would always jeopardize the ecology and landscape of any region if the site selection is not done with proper scientific procedure. Spatial heterogeneity of regions has important influence on ecological patterns and processes (Shugart, 1998) and GIS has a special role to play in decision making in such scenarios of new developments. Many landscape metrics in GIS environment are used to facilitate the investigation of the relation between new landscape structure and biodiversity (Wikramanayake et al. 2004; Bhagwat et al. 2005; Burel and Baudry 2005; Oja et al. 2005; Riitters 2005; Schindler et al. 2008). The suitability analysis model was created for North Georgia Counties to choose potential locations for reservoir construction with very low environmental, ecological, economical, and social disruption. In order to develop a reservoir suitability analysis, the following parameters were considered for the Environmental Protection Division\u2019s (EPD) regulations. Under Georgia\u2019s ordinance, any new facility handling hazardous waste has to abide by the Department of Natural Resource\u2019s guidelines (Hall County, GA, 2010). If the hazardous waste facility is to be built within seven miles of a water storage facility (water reservoir), then the facility has to install spill and leak collection facilities to ensure that the impermeable surfaces do not harm the water supply (Hall County, GA, 2010). Also, limitations on hazardous and toxic materials based on regulations are applied. The regulation states that no landfills, waste disposal, hazardous and toxic waste facilities is located within the water supply watershed, and no industries or businesses classi\n\n",
                "DataExportTag": "AI40054",
                "QuestionID": "QID404",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Engineering-Based New Reservoir Design and Environmental Suitability Analysis with Geospatial Tec...",
                "Choices": {
                    "1": {
                        "Display": "\"Hydrological Forecasting and Water Management\""
                    },
                    "2": {
                        "Display": "flood, rainfall, forecast, river, drought, runoff, hydrological, rmse, reservoir, precipitation, water, root_mean, temperature, dam, streamflow"
                    },
                    "3": {
                        "Display": "\"Water Management and Flood Risk Assessment\""
                    },
                    "4": {
                        "Display": "water, flood, river, groundwater, flow, runoff, reservoir, catchment, watershed, lake, basin, decision_making, discharge, hydrological, risk"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID169",
            "SecondaryAttribute": "Enhancing the bilateral S&T Partnership with the Russian Federation BILAT-RUS focuses on enhancin...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Enhancing the bilateral S&T Partnership with the Russian Federation BILAT-RUS focuses on enhancing the bilateral S&T Partnership between Russia and the EU. The project aims at contributing to the implementation of the Common Space on Research between the EU and Russia. It will ensure coherence and coordination of various activities under the umbrella of the EU-Russian S&T agreement and will contribute to a stronger coordination of bilateral activities with Russia at EU and Member State level. The major objectives of BILAT-RUS are: \u00e2\u20ac\u00a2  to facilitate coherent information dissemination and awareness raising on EU-Russian cooperation (database on S&T research institutes, case studies on good cooperation practice, web-portal, partner search tool, information events); \u00e2\u20ac\u00a2 to contribute to the optimisation of the framework and the instruments for enhanced future EU-Russian cooperation (inventory of existing instruments and regulations, examples of good cooperation practice, options for optimising the common legal and organisational frame, suggestions for practical and efficient joint funding mechanisms); \u00e2\u20ac\u00a2 to create a knowledge base for emerging horizontal issues of cooperation (mobility, S&T infrastructure and innovation); \u00e2\u20ac\u00a2 to meet the needs of joint thematic EU-Russian Working Groups on S&T cooperation (Health, Food\/Agriculture\/Biotechnology, Nanotechnology, Energy); \u00e2\u20ac\u00a2 to exploit the potential of Russian Mirror Technology Platforms through linking them with their European counterparts. The project proposal for BILAT-RUS is built on joint EU-Russian interest and aims at mutual benefit. Synergies will be established with other bilateral or bi-regional EU-Russian projects from FP6 and FP7 (such as SCOPE-EAST and the upcoming IncoNet EECA targeting Eastern Europe and Central Asia), bilateral S&T agreements and other initiatives between the EU Member States and Russia. The consortium consists of ten partners (five of them from Russia) with in-depth experience of EU-Russian cooperation in S&T. The Russian NCPs are associated partners.\n\n",
                "DataExportTag": "COR41692",
                "QuestionID": "QID169",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Enhancing the bilateral S&T Partnership with the Russian Federation BILAT-RUS focuses on enhancin...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\""
                    },
                    "2": {
                        "Display": "child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "3": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\""
                    },
                    "4": {
                        "Display": "farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID230",
            "SecondaryAttribute": "Enhancing the bilateral S&T Partnership with the Russian Federation BILAT-RUS focuses on enhancin...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Enhancing the bilateral S&T Partnership with the Russian Federation BILAT-RUS focuses on enhancing the bilateral S&T Partnership between Russia and the EU. The project aims at contributing to the implementation of the Common Space on Research between the EU and Russia. It will ensure coherence and coordination of various activities under the umbrella of the EU-Russian S&T agreement and will contribute to a stronger coordination of bilateral activities with Russia at EU and Member State level. The major objectives of BILAT-RUS are: \u00e2\u20ac\u00a2  to facilitate coherent information dissemination and awareness raising on EU-Russian cooperation (database on S&T research institutes, case studies on good cooperation practice, web-portal, partner search tool, information events); \u00e2\u20ac\u00a2 to contribute to the optimisation of the framework and the instruments for enhanced future EU-Russian cooperation (inventory of existing instruments and regulations, examples of good cooperation practice, options for optimising the common legal and organisational frame, suggestions for practical and efficient joint funding mechanisms); \u00e2\u20ac\u00a2 to create a knowledge base for emerging horizontal issues of cooperation (mobility, S&T infrastructure and innovation); \u00e2\u20ac\u00a2 to meet the needs of joint thematic EU-Russian Working Groups on S&T cooperation (Health, Food\/Agriculture\/Biotechnology, Nanotechnology, Energy); \u00e2\u20ac\u00a2 to exploit the potential of Russian Mirror Technology Platforms through linking them with their European counterparts. The project proposal for BILAT-RUS is built on joint EU-Russian interest and aims at mutual benefit. Synergies will be established with other bilateral or bi-regional EU-Russian projects from FP6 and FP7 (such as SCOPE-EAST and the upcoming IncoNet EECA targeting Eastern Europe and Central Asia), bilateral S&T agreements and other initiatives between the EU Member States and Russia. The consortium consists of ten partners (five of them from Russia) with in-depth experience of EU-Russian cooperation in S&T. The Russian NCPs are associated partners.\n\n",
                "DataExportTag": "COR41692",
                "QuestionID": "QID230",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Enhancing the bilateral S&T Partnership with the Russian Federation BILAT-RUS focuses on enhancin...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\""
                    },
                    "2": {
                        "Display": "child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "3": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\""
                    },
                    "4": {
                        "Display": "farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID292",
            "SecondaryAttribute": "European Initiative for a better use of the results of agri-food research AgriFoodResults intends...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "European Initiative for a better use of  the results of agri-food research AgriFoodResults intends to answer the need for a better dissemination of food results. The vision is to improve the cost effectiveness of agri-food research activities by enhancing the transfer of the results to the end-users. The project combines capacity building with the creation of sustainable services for dissemination managers. Theses services include web-sites, innovative approach to communicate scientific results and guidelines for project and dissemination managers. The scope of AgriFoodResults is food science with an emphasis on food safety, food processing technology and nutrition & health. The focus is primary on SMEs and small research projects.   AgriFoodResults has three main objectives: 1) To offer innovative and sustainable services for dissemination 2) To raise skills of European food scientists on dissemination practices 3) To successfully disseminate recent results from agri-food research projects.  The consortium includes seven leading research organisations in food science, five organisations involved in knowledge transfer, management and communication of research projects, one company specialised in web development, an association of food enterprises and a European organisation specialised in food science communication.\n\n",
                "DataExportTag": "COR55142",
                "QuestionID": "QID292",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "European Initiative for a better use of the results of agri-food research AgriFoodResults intends...",
                "Choices": {
                    "1": {
                        "Display": "\"Agriculture and Food Production\""
                    },
                    "2": {
                        "Display": "food, farming, crop, agriculture, aquaculture, plant, fish, fruit, pest, wine, food_safety, meat, milk, pesticide, seed"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID354",
            "SecondaryAttribute": "European Initiative for a better use of the results of agri-food research AgriFoodResults intends...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "European Initiative for a better use of  the results of agri-food research AgriFoodResults intends to answer the need for a better dissemination of food results. The vision is to improve the cost effectiveness of agri-food research activities by enhancing the transfer of the results to the end-users. The project combines capacity building with the creation of sustainable services for dissemination managers. Theses services include web-sites, innovative approach to communicate scientific results and guidelines for project and dissemination managers. The scope of AgriFoodResults is food science with an emphasis on food safety, food processing technology and nutrition & health. The focus is primary on SMEs and small research projects.   AgriFoodResults has three main objectives: 1) To offer innovative and sustainable services for dissemination 2) To raise skills of European food scientists on dissemination practices 3) To successfully disseminate recent results from agri-food research projects.  The consortium includes seven leading research organisations in food science, five organisations involved in knowledge transfer, management and communication of research projects, one company specialised in web development, an association of food enterprises and a European organisation specialised in food science communication.\n\n",
                "DataExportTag": "COR55142",
                "QuestionID": "QID354",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "European Initiative for a better use of the results of agri-food research AgriFoodResults intends...",
                "Choices": {
                    "1": {
                        "Display": "\"Agriculture and Food Production\""
                    },
                    "2": {
                        "Display": "food, farming, crop, agriculture, aquaculture, plant, fish, fruit, pest, wine, food_safety, meat, milk, pesticide, seed"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID99",
            "SecondaryAttribute": "Fig 1",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Fig 1\n\n",
                "DataExportTag": "27",
                "QuestionID": "QID99",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Fig 1",
                "Choices": {
                    "1": {
                        "Display": "Rapamycin induces autophagy in human Th1 cell"
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID303",
            "SecondaryAttribute": "Fig 1",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Fig 1\n\n",
                "DataExportTag": "21",
                "QuestionID": "QID303",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Fig 1",
                "Choices": {
                    "1": {
                        "Display": "Rapamycin induces autophagy in human Th1 cell"
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID373",
            "SecondaryAttribute": "Fig 1",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Fig 1\n\n",
                "DataExportTag": "27",
                "QuestionID": "QID373",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Fig 1",
                "Choices": {
                    "1": {
                        "Display": "Rapamycin induces autophagy in human Th1 cell"
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID184",
            "SecondaryAttribute": "FIGURE 1",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "FIGURE 1\n\n",
                "DataExportTag": "24",
                "QuestionID": "QID184",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "FIGURE 1",
                "Choices": {
                    "1": {
                        "Display": "Activation\u2013inhibition and molecules of major histocompatibility complex, class i (mhc-i). Tumours are killed either when they are able to express mhc-i molecules containing tumour peptide antigens recognized by the rearranging T-cell receptor ..."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID245",
            "SecondaryAttribute": "FIGURE 1",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "FIGURE 1\n\n",
                "DataExportTag": "24",
                "QuestionID": "QID245",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "FIGURE 1",
                "Choices": {
                    "1": {
                        "Display": "Activation\u2013inhibition and molecules of major histocompatibility complex, class i (mhc-i). Tumours are killed either when they are able to express mhc-i molecules containing tumour peptide antigens recognized by the rearranging T-cell receptor ..."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID322",
            "SecondaryAttribute": "Figure 1. Signaling crosstalk between pericyte and endothelial cell. Endothelial cells secrete PD...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Figure\u00a01. Signaling crosstalk between pericyte and endothelial cell. Endothelial cells secrete PDGF-B, which binds to its receptor on the pericytes to recruit them to the newly formed vasculature. The pericytes in turn secrete vitronectin that ...\n\n",
                "DataExportTag": "40",
                "QuestionID": "QID322",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Figure 1. Signaling crosstalk between pericyte and endothelial cell. Endothelial cells secrete PD...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID309",
            "SecondaryAttribute": "Findings from the National Navigation Roundtable: A call for competency\u2010based patient navigation..",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Findings from the National Navigation Roundtable: A call for competency\u2010based patient navigation training INTRODUCTION The purpose of this article is to describe the availability of patient navigation training programs in the United States, assess the content of these programs, and report which address the core competencies of patient navigation. According to Harold P. Freeman, the founder of the patient navigation model, \u201cpatient navigation is a patient-centric healthcare service delivery model. It is a patient-centric concept that concentrates on the movement of patients along the continuum of medical care ... beginning in the community and continuing on through testing, diagnosis, and survivorship to the end of life.\u201d The goal of patient navigation is to improve outcomes in underserved populations by eliminating barriers to a timely cancer diagnosis and treatment in a culturally sensitive manner. Patient navigators (PNs) may be employed as community-based navigators addressing screening barriers and helping prepatients access portals to health care, as health system navigators helping patients to overcome structural and psychosocial barriers to quality care, and as survivorship navigators helping patients who are post\u2013active treatment to overcome barriers to ongoing surveillance and supportive care while transitioning from oncology care back to a primary care provider or in the transition to other end-of-life care. Throughout the research literature, there have been challenges to clearly defining the role of PNs, including overlapping convergence of the PN role with other roles such as care coordinators and community health workers (CHWs). These challenges in large part have been driven and exacerbated by the types of individuals providing patient navigation, who range from PNs without a clinical practitioner license (called lay or nonclinical) to social workers and nurses who have professional licenses and are cross-trained in patient navigation. This inexact scope of work for someone identified as a PN proves problematic when one is outlining the training needed to meet health system navigation needs. Some health systems, particularly those with high patient volumes, use a navigation matrix with navigational tasks assigned across a team. Evidence to date supports the use of individual and team-based navigation (eg, lay and licensed PNs) to improve health outcomes. However, evidence-based research has been limited in assessing the types of training required for PNs. A systematic review of patient navigation programs indicates that training tends to be specific to research protocols rather than public patient navigation programs. Some national associations and state organizations are attempting to standardize competencies for PN knowledge, skills, and performance, but they vary in their strategies for standardizing competencies, certification, and training curricula. Several organizations have identified core PN competencies (Oncology Nursing Society, Academy of Oncology Nurse & Patient Navigators [AONN+], National Accreditation Program for Breast Centers, Patient Navigator Training Collaborative, and George Washington Cancer Center\u2019s Online Academy). Because of the blurred scope of the work, the adoption of consistent PN competencies provides an impetus for defining the functions of the role. Training based on a set of competencies ensures that knowledge and skills acquired are relevant and specific to the role of the PN. It also guides performance monitoring and ensures that the expectations of the role are met. The need for training standardization revolves around the basic premise of why patient navigation was created: to save lives from cancer by eliminating barriers to care and ensuring timely delivery of services.\n\n",
                "DataExportTag": "CAN1480974",
                "QuestionID": "QID309",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Findings from the National Navigation Roundtable: A call for competency\u2010based patient navigation...",
                "Choices": {
                    "1": {
                        "Display": "\"Oncology Healthcare and Patient Management\""
                    },
                    "2": {
                        "Display": "physician, cam, hospital, oncologist, consultation, gps, laboratory, consensus, specialist, oncology, audit, screening, clinic, multidisciplinary, centre"
                    },
                    "3": {
                        "Display": "\"Oncology Nursing Training and Communication\""
                    },
                    "4": {
                        "Display": "nurse, training, oncology, community, staff, communication, student, provider, professional, app, team, nursing, barrier, oncology_nurse, resident"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID370",
            "SecondaryAttribute": "Five-Year Follow-Up of a 13-Year-Old Boy with a Pituitary Adenoma Causing Gigantism \u2013 Effect of O.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Five-Year Follow-Up of a 13-Year-Old Boy with a Pituitary Adenoma Causing Gigantism \u2013 Effect of Octreotide Therapy Background\/Aim: In children, there is little experience with octreotide therapy for pituitary tumors, especially growth hormone (GH) producing adenomas. We report on a 13-year-old boy with gigantism due to a GH-producing pituitary adenoma caused by a Gs\u03b1 mutation on the basis of McCune-Albright syndrome. Methods: At the age of 6.5 years a GH- and prolactin-producing pituitary adenoma was diagnosed. The adenoma was surgically removed. Immediately thereafter, the small adenoma residuum was treated with octreotide (2 \u00d7 100 \u00b5g\/day s.c.). Results: During therapy with octreotide, the growth rate dropped to normal values; however, rose again after 2 years of treatment. The insulin-like growth factor I (IGF-I) levels remained above the 95th percentile, the GH level mostly >2 \u00b5g\/l. After 5 years of octreotide therapy, GH (6.9 \u00b5g\/l), IGF-I (620 \u00b5g\/l), IGF-binding protein 3 (5.4 mg\/l), and prolactin (17.0 ng\/ml) levels were still elevated. The growth velocity was +2.4 SDS (standard deviation score), the pubertal status was mature, and the bone age was 14.3 years (prospective final height 208 cm). A magnetic resonance imaging scan showed an unchanged residual 4-mm rim of adenoma at the pituitary site. Side effects from octreotide therapy were not reported by the patient or his family. The therapy was changed to the long-acting release octreotide analog octreotide-LAR. After 1 year of treatment with octreotide-LAR, the GH level was 1.0 \u00b5g\/l, and the prospective final height dropped by 10 cm. Conclusions: This case demonstrates that combined surgical and medical treatment can influence the prognosis of childhood gigantism; however, the prognosis of this rare condition remains uncertain.\n\n",
                "DataExportTag": "CAN63142",
                "QuestionID": "QID370",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Five-Year Follow-Up of a 13-Year-Old Boy with a Pituitary Adenoma Causing Gigantism \u2013 Effect of O...",
                "Choices": {
                    "1": {
                        "Display": "\"Endocrine Disorders and Hormone Diseases\""
                    },
                    "2": {
                        "Display": "pituitary, adrenal, growth_hormone, cortisol, acth, acromegaly, adenoma, adrenocortical, prl, hormone, prolactin, pheochromocytoma, cushing_syndrome, endocrine, cushing"
                    },
                    "3": {
                        "Display": "\"Kidney Disease and Dialysis Management\""
                    },
                    "4": {
                        "Display": "renal, vitamin_d, kidney, ckd, pth, calcium, thyroid, urinary, hemodialysis, hypercalcemia, urine, dialysis, creatinine, phosphate, parathyroid_hormone"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID176",
            "SecondaryAttribute": "Flow Chemistry for C-H Activation The metal-catalysed group-directed functionalisation of C-H bon...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Flow Chemistry for C-H Activation The metal-catalysed group-directed functionalisation of C-H bonds is emerging as an important synthetic methodology in organic chemistry. Despite the advances in the field, however, C-H functionalisation processes are still plagued by several disadvantages, such as selectivity and reproducibility issues, the necessity of directing-group (DG) introduction, and the often difficult DG cleavage. This proposal combines the development of novel chemical transformations with the use of flow microreactors to: 1)make the DG introduction and cleavage more efficient and synthetically useful, and 2) make the C-H functionalisation processes more selective and atom economic, thus addressing several important weaknesses of this type of chemistry. After the development of the individual transformations in continuous-flow, the three crucial steps (DG introduction, group-directed C-H functionalisation and DG cleavage) will be integrated in a combined, multistep protocol, with the aim of minimising purification steps, and ultimately provide a cleaner and faster synthesis of functionalised molecules in a single continuous process.\n\n",
                "DataExportTag": "COR26939",
                "QuestionID": "QID176",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Flow Chemistry for C-H Activation The metal-catalysed group-directed functionalisation of C-H bon...",
                "Choices": {
                    "1": {
                        "Display": "\"Chemical Synthesis and Catalysis\""
                    },
                    "2": {
                        "Display": "catalysis, reaction, synthesis, enzyme, metal, bond, synthetic, chemical, ligand, molecule, mof, c_h, selectivity, reactivity, activation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID237",
            "SecondaryAttribute": "Flow Chemistry for C-H Activation The metal-catalysed group-directed functionalisation of C-H bon...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Flow Chemistry for C-H Activation The metal-catalysed group-directed functionalisation of C-H bonds is emerging as an important synthetic methodology in organic chemistry. Despite the advances in the field, however, C-H functionalisation processes are still plagued by several disadvantages, such as selectivity and reproducibility issues, the necessity of directing-group (DG) introduction, and the often difficult DG cleavage. This proposal combines the development of novel chemical transformations with the use of flow microreactors to: 1)make the DG introduction and cleavage more efficient and synthetically useful, and 2) make the C-H functionalisation processes more selective and atom economic, thus addressing several important weaknesses of this type of chemistry. After the development of the individual transformations in continuous-flow, the three crucial steps (DG introduction, group-directed C-H functionalisation and DG cleavage) will be integrated in a combined, multistep protocol, with the aim of minimising purification steps, and ultimately provide a cleaner and faster synthesis of functionalised molecules in a single continuous process.\n\n",
                "DataExportTag": "COR26939",
                "QuestionID": "QID237",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Flow Chemistry for C-H Activation The metal-catalysed group-directed functionalisation of C-H bon...",
                "Choices": {
                    "1": {
                        "Display": "\"Chemical Synthesis and Catalysis\""
                    },
                    "2": {
                        "Display": "catalysis, reaction, synthesis, enzyme, metal, bond, synthetic, chemical, ligand, molecule, mof, c_h, selectivity, reactivity, activation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID344",
            "SecondaryAttribute": "FRAUD ANALYTICS: A SURVEY ON BANK FRAUD AND FRAUD PREDICTION USING UNSUPERVISED LEARNING BASED AP...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "FRAUD ANALYTICS: A SURVEY ON BANK FRAUD AND FRAUD PREDICTION USING UNSUPERVISED LEARNING BASED APPROACH Fraud in banks has been steadily growing over the past years and is a challenge to banks worldwide. The complexity involved in detection of such fraudulent activities further adds to the problem. A thorough examination of fraud and its possibilities is necessary to pinpoint and distinguish the few fraudulent cases within the vast volumes of banking data. In this paper we have discussed various scenarios in which fraud could take place and applied unsupervised learning approaches to detect fraudulent acts in areas such as credit cards, money laundering and financial statements. We have keenly analyzed various attributes which would be necessary in detection of culprits who may cause a loss to the banks\/organizations. Our analysis assists in discovering anomalous behavior among peer groups to more consistently uncover frauds with lesser amount of false positives. INTRODUCTION With the evolution of internet in the banking sectors, people have changed the way they used to bank. But this digital evolution is also creating new opportunities for fraudsters to hack into personal accounts. Banking sector frauds have been in existence for centuries, with the earliest known frauds pertaining to insider trading, stock manipulation, accounting irregularity\/ inflated assists etc. Fraud is a dominant form of white collar crime that continues to extract a significant toll not only on the organizations, but also on investors, financial institutions, and the economy in general. There are many issues that make effective fraud management a challenging task. These include: enormous and ever-expanding volumes of data, the growing complexity of systems, changes in business processes and activities and continuous evolution of newer fraud schemes to bypass existing detection techniques. Detecting fraudulent financial statements is a difficult task when using normal audit procedures due to limitation in understanding the characteristics of financial statements, lack of experience and dynamically changing strategies of fraudsters. According to the Basel II definition, Fraud is a part of operational risk and has been classified as Internal and External fraud. Internal Fraud is the risk of unexpected financial, material or reputational loss as the result of fraudulent action of persons internal to the firm. Losses are due to acts of a type intended to defraud, misappropriate property or circumvent regulations, the law or company policy, excluding diversity\/discrimination events, which involves at least one internal party. It includes misappropriation of assets, tax evasion, intentional mismarking of positions, bribery. The Basel Committee is the primary global standard-setter for the prudential regulation of banks and provides a forum for cooperation on banking supervisory matters. Reserve Bank of India has defined fraud as \u201cAll instances wherein Banks have been put to loss through misrepresentation of books of accounts, fraudulent encashment of instruments like cheques, drafts and bills of exchange, unauthorized handling of securities charged to NOVATEUR PUBLICATIONS INTERNATIONAL JOURNAL OF INNOVATIONS IN ENGINEERING RESEARCH AND TECHNOLOGY [IJIERT] ISSN: 2394-3696 VOLUME 3, ISSUE3, MAR.-2016 2 | P a g e banks, misfeasance, embezzlement, theft, misappropriation of funds, conversion of property, cheating, shortages, irregularities etc.\u201d According to a survey done by EY which included more than 2,700 executives across 59 countries, the risks businesses are facing are not receding. More than 1 in 10 executives surveyed reported their company as having experienced a significant fraud in the past two years. This implies that there is a 5-7% chance of occurrence of fraud cases within a year. Also, according to a Delloite survey, 93% respondents indicated that there has been an increase in fraud incidents in the banking industry in the last two years. There are dozens of ways in which an individual can commit bank fraud. Some of these schemes are more complex, and affect more people or institutions, garnering harsher penalties than others do. Typically, fraud in banks can be categorized into 3 main categories: Corruption, Asset Misappropriation and Financial Statement Fraud (Fig 1). Corruption includes cases of conflict of interest, bribery, illegal gratuities and extortions. Asset misappropriation may be embezzlement or inventory related fraud. Financial statement fraud involves overstating or understating the assets or revenue generated. Below listed are some typical fraud scenarios in the financial domain: Fraud cases involving theft of identity are a serious and growing problem in the era of internet banking. With so many transactions done online, hackers have the ability to frequently access bank account and credit card information from unwitting consumers. Fraudsters can also use obtained names and addresses to apply for fraudulent accounts, credit cards and loans. \uf0b7 Embezzlement could occur when a bank employee misappropriates funds from customers or from the bank itself. Banks usually guard rigorously against embezzlement in a variety of ways, since this type of bank fraud can be extremely harmful to the institution's reputation. Bank fraud cases involving internal theft usually are managed by people with considerable power within a bank branch, since they have the most access and opportunity and are generally perceived as trustworthy. Fig. 1 Types of Common Fraud Schemes \uf0b7 Bank Impersonation is where one or more individuals act as a financial institution, often by setting up fake companies, or creating websites, in order to lure people into depositing funds. NOVATEUR PUBLICATIONS INTERNATIONAL JOURNAL OF INNOVATIONS IN ENGINEERING RESEARCH AND TECHNOLOGY [IJIERT] ISSN: 2394-3696 VOLUME 3, ISSUE3, MAR.-2016 3 | P a g e \uf0b7 In the context of bank fraud, internet fraud occurs when someone creates a website for the purpose of presenting themselves as a bank or other financial institution, to fraudulently obtain money deposited by other people or get the login credentials of the customers of the bank. \uf0b7 A fraudulent loan is one in which the borrower is an individual or a business entity controlled by a dishonest bank officer or an accomplice; the \"borrower\" then declares bankruptcy or vanishes and the money is gone. The borrower may even be a nonexistent entity and the loan simply a deception to conceal a theft of a large amount of money from the bank. An individual who takes out a loan, knowing that he will immediately file bankruptcy, has committed bank fraud. \uf0b7 When money is obtained from criminal acts such as illegal gambling or drug trafficking, the money is considered \u201cdirty\u201d in that it may seem dubious if deposited directly into a bank or other financial institution. Since the criminal needs to create financial records ostensibly showing where the money came from, the money must be \u201ccleaned,\u201d by running it through a number of legitimate businesses before depositing it, hence the term \u201cmoney laundering.\u201d Because the act is specifically used to hide illegally obtained money, it too is unlawful. A lot of work has already been done in the field of fraud analytics where most researchers have used supervised classification methods like logistic regression, decision tree, neural networks and SVM. In these kinds of methods there is the need for the model training process where the class labels of the cases in the sample to be used to train should be known first. But in practical scenario, where the probability of occurrence of a fraud is less, it is very difficult to find the train dataset from an organization\u2019s database. Also, being dependent on the train set to predict the future activities as fraudulent is a na\u00efve approach and the probability of detecting a fraudulent attack is low. As the probability of occurrence of the next consequent attack to follow the same pattern as of the previous attackers is also very less, the supervised trained model won\u2019t be as effective as unsupervised technique. In this paper we have discussed various scenarios in which an individual could commit a fraud. We have adopted an unsupervised clustering based approach (Self organizing map) in combination with association rule mining to predict fraudulent activities as this approach does not need the class labels of the cases in the sample.\n\n",
                "DataExportTag": "AI47245",
                "QuestionID": "QID344",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "FRAUD ANALYTICS: A SURVEY ON BANK FRAUD AND FRAUD PREDICTION USING UNSUPERVISED LEARNING BASED AP...",
                "Choices": {
                    "1": {
                        "Display": "\"Cybersecurity and Forensic Investigation\""
                    },
                    "2": {
                        "Display": "crime, authentication, forensic, criminal, biometric, password, police, deception, signature, fingerprint, transaction, fraud, security, fake, law_enforcement"
                    },
                    "3": {
                        "Display": "\"Child Violence and Artificial Intelligence in Spanish Language\""
                    },
                    "4": {
                        "Display": "la, el, para, child, en_el, los, violence, como, del, en, se, student, una, esta, inteligencia_artificial"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID284",
            "SecondaryAttribute": "Functional Biomarkers as in vitro diagnostic tools for managing patients with chronic disease The...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Functional Biomarkers as in vitro diagnostic tools for managing patients with chronic disease The aim of this proposal is to establish a business plan for the development of a new class of Functional Biomarkers as in vitro diagnostic tools for managing patients with chronic disease. The proof of concept will be established by focusing on substrates of the X-prolyl dipeptidylpeptidases (DPP), a family of pleotropic enzymes that cleave the N-terminal two amino acids of proteins. There are >30 known substrates for DPPIV, the best characterized member of this family. Importantly the catalytic activity of DPPIV results in the conversion of agonist ligands into antagonist or non-functional proteins.In the context of our funded ERC project, we have provided the first in vivo evidence of DPPIV-mediated cleavage of the chemokine interferon-induced protein 10 (IP-10 or CXCL10) [Casrouge, JCI 2011]. These initial observations have recently been validated, conclusively demonstrating the ability to predict treatment failure in chronic HCV patients (p-value <0.001 for antagonist CXCL10 and negative predictive value = 93.5%).Based on the important role of DDPIV in disease pathogenesis and following from our findings, there are two ideas that we wish to develop further as proof of concept for \u00e2\u20ac\u02dcFunctional Biomarkers\u00e2\u20ac\u2122 in the management of patients. First, we wish to evaluate the market needs for developing a privately run SME, aimed at providing theragnostic testing useful for stratification of chronic HCV patients. We will also examine the potential value of applying these in vitro diagnostic (IVD) tools in other disease settings in which DPPIV and\/or CXCL10 have been identified as a predictive biomarker. Second, we aim to extend our repertoire of assays to include other disease relevant DPPIV substrates.\n\n",
                "DataExportTag": "COR11405",
                "QuestionID": "QID284",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Functional Biomarkers as in vitro diagnostic tools for managing patients with chronic disease The...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Drug Development for Rare Diseases\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Public Health and Preventive Medicine\""
                    },
                    "4": {
                        "Display": "health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID410",
            "SecondaryAttribute": "Gauging the performance of SNPs, biomarkers, and clinical factors for predicting risk of breast c...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Gauging the performance of SNPs, biomarkers, and clinical factors for predicting risk of breast cancer. Predicting risk of cancer for individuals has long been a goal of medical research. If an individual\u2019s risk could be predicted, then prevention and screening modalities could be targeted toward those at meaningfully high risk. This approach is not only more cost efficient than targeting the whole population but also more ethical, at least when interventions are burdensome to the individual. The quest for risk predictors has been revitalized with the emergence of technologies that measure genetic information and other molecular and physiological attributes of the individual. In this issue of the Journal, Gail (1) asks to what extent newly discovered associations between seven single-nucleotide polymorphisms (SNPs) and incidence of breast cancer can improve assessment of breast cancer risk. Comparisons are made with models that employ standard clinical factors to evaluate the incremental value of the SNPs for prediction over the standard clinical information. Using estimated relative risks and allele frequencies, Gail finds that the SNPs are expected to have a small effect on the capacity of prediction models to distinguish women who will and will not develop breast cancer. Because he assumes best-case scenarios, his results probably provide upper limits on expected increments in risk prediction with SNPs. He postulates that many more SNPs with these levels of association with breast cancer will need to be discovered to substantially improve risk prediction. Gail\u2019s arguments demonstrate that the sample sizes needed to discover an adequate number of SNPs will need to be very large indeed. Although his calculations are based on many assumptions, they provide a good place from which to start the discussion about what types of markers and studies will be needed to make progress in this field.\n\n",
                "DataExportTag": "AI550411",
                "QuestionID": "QID410",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Gauging the performance of SNPs, biomarkers, and clinical factors for predicting risk of breast c...",
                "Choices": {
                    "1": {
                        "Display": "Gail uses the area under the receiver operating characteristic (ROC) curve (AUC) to summarize and compare prediction models. Although this is a popular statistical approach with a long history (2), there has been considerable criticism of it, with recent criticisms coming from the cardiovascular research community (3, 4). Reexamining the role of AUC has been motivated in part by frustration at not being able to identify valuable biomarkers on the basis of AUC. The AUC is often interpreted as the probability of correct ordering\u2014correct in the sense that when comparing the risk predictions for two subjects, only one of whom develops breast cancer, the risk calculated for the breast cancer subject is the larger of the two values. The first criticism of the AUC is that this probability is not a clinically relevant way of summarizing predictive performance. Subjects do not present to the clinic in pairs, and the problem is not to determine which of the pair will develop cancer. A second criticism is that the scale is somewhat deceptive. A substantial gain in performance may not result in a substantial increase in AUC. For example, suppose that the sensitivity changes from 40% to 90% but only over the range of specificities corresponding to 90% \u2013 100%. This is an enormous improvement in performance: while maintaining specificity at 100%, now 90% rather than 40% of cancers can be predicted. However, the change in AUC is only 0.05. Although an extreme example, it illustrates the point. These two criticisms of AUC apply generally, not solely to risk prediction. The AUC really is a poor metric for evaluating markers for disease diagnosis, screening, or prognosis. The third criticism, which is specific to risk prediction, is that the AUC, and indeed the ROC curve itself, hides the values of risk calculated for subjects in the population. Indeed, the risk values are not visible from the ROC curve or the related curves in figure 2 of Gail (1). Moreover, the same ROC curve results if risk values are transformed monotonically, say, multiplied by a factor of 10, yet the clinical implications of these risk values would be very different."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "The key question in evaluating risk prediction models concerns the number of subjects who are identified as being at high risk. Does a model that includes SNPs identify a substantially larger number of women at high risk for breast cancer who might therefore benefit from an intervention? In other words, does it do a better job than a model without SNPs at risk stratifying the population? The population distributions of absolute risk derived from the two models can be displayed to allow this sort of assessment. At any chosen high-risk threshold, the proportions of subjects with risks above that threshold can be compared. We refer to these plots as \u201cpredictiveness curves\u201d (5, 6). Cook (3) tabulates these proportions for specific thresholds considered therapeutically relevant in preventing cardiovascular events. Unfortunately, it is not possible to determine the population distributions of absolute risk for the models described in Gail (1). The distributions of relative risk are shown instead. Gail notes that to derive the absolute risk distribution from the relative risk distribution, one needs to know the absolute risk in the baseline group, those with the lowest level of risk for all factors in the model, denoted by Gail with the letter k. The effect of k would be to shift the relative risk distribution shown in his figure 1 by log(k) to arrive at the distribution curve for absolute risk. Because the models differ in risk factors included, the baseline group varies across models and so too does the corresponding risk, k. This means that the curves in figure 1 would need to be shifted by different degrees to assemble the absolute risk distributions from them. In conclusion, the comparison of relative risk distributions does not give direct information about the comparison of absolute risk distributions, which is of key interest for comparing risk prediction models."
                    },
                    "2": {
                        "Display": "Interestingly, the absolute risk distributions could have been calculated by Gail if population incidence rates were specified. In particular, the absolute risk in the baseline group, k, for each model is simply the population incidence divided by the average relative risk. Because k is the factor that links the relative risk distribution to the absolute risk distribution, values for age-specific incidence of breast cancer could therefore be used in conjunction with Gail\u2019s calculations to determine the age-specific risk distributions for women using models with and without SNPs. The age-specific proportions of women identified at high risk could then be compared across models. This would be an interesting exercise to complement Gail\u2019s calculations."
                    },
                    "3": {
                        "Display": "Appropriate evaluation of risk prediction models requires specification of a risk threshold for defining individuals as high risk. What high-risk threshold should be used in the breast cancer setting? A consensus on this fundamental question does not exist at present. The choice depends on costs and benefits associated with interventions that will be employed for women designated as high risk. Tamoxifen therapy and screening with magnetic resonance imaging are among the set of options for breast cancer. Medical decision theory provides an explicit solution for high-risk designation in terms of 1) the net benefit, B, of being classified as high risk if, in the absence of intervention, one is destined to develop breast cancer, and 2) the net cost, C, of being classified as high risk if, in the absence of intervention, one is destined not to develop breast cancer. The risk threshold at which expected benefit exceeds expected cost is C\/(C + B) (7). The higher the cost:benefit ratio, the higher the optimal threshold. Cardiovascular consensus groups (8) have determined risk thresholds based on costs and benefits of different therapy options. Corresponding guidelines for defining high risk in the context of breast cancer prevention must be developed. We need them to gauge the value of risk prediction models. Risk thresholds might be chosen to vary with factors such as age, recognizing that costs and benefits of high-risk interventions are not uniform across the population. Moreover, in practice each woman may have her own tolerance for risk that could vary from guidelines developed by consensus groups."
                    },
                    "4": {
                        "Display": "Gail\u2019s ROC analysis indicates that even under optimistic assumptions, SNPs\u2014or, for that matter, other risk factors with moderate relative risks\u2014are unlikely to substantially improve current algorithms for breast cancer risk prediction. The same conclusion may well hold with analyses that focus on proportions of high-risk (or low-risk) women identified. Indeed, this has been the experience in cardiovascular research. Biomarkers such as C-reactive protein (CRP) and high-density lipoproteins that do not increase AUC statistics do not appear to improve risk stratification either, at least when considering the population as a whole (9). Subsets of the population may, however, benefit from information in these markers. Ridker and Cook (10) report that for subjects at intermediate risk according to standard risk factors, CRP can further stratify a large fraction of subjects into high- and low-risk categories. Similarly, for breast cancer risk prediction SNPs and biomarkers may have their greatest impact on subpopulations."
                    },
                    "5": {
                        "Display": "Risk stratification is not the only component of prediction model evaluation. As Gail notes, calibration is of paramount importance. A well-calibrated model ensures that the calculated risks reflect the actual proportions of subjects who develop disease. Also of interest is the accuracy of risk classifications, defined as the proportion of women who develop breast cancer who are classified as high risk and the proportion of women who do not develop breast cancer who are classified as low risk (5). These can also be calculated using the absolute risk distribution, a fact previously noted by Gail and Pfeiffer (11)."
                    },
                    "6": {
                        "Display": "\"Maternal Health and Pregnancy Risk Factors\""
                    },
                    "7": {
                        "Display": "age, woman, pregnancy, ci, child, risk, sex, infant, risk_factor, exposure, hiv, birth, maternal, body_mass, embryo"
                    },
                    "8": {
                        "Display": "\"Mental Health Assessment and Intervention\""
                    },
                    "9": {
                        "Display": "depression, pain, mental_health, dementia, symptom, suicide, intervention, cognitive, participant, physical, questionnaire, rehabilitation, behavior, digital, sleep"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6",
                    "7",
                    "8",
                    "9"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID285",
            "SecondaryAttribute": "Generation of mouse models for the study of cellular senescence in aging and cancer Cellular sene...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Generation of mouse models for the study of cellular senescence in aging and cancer Cellular senescence is a central biological mechanism for cancer prevention in mammals. Cells enter senescence when tumor-inducing genes are aberrantly activated, and in response undergo an arrest in proliferation accompanied by dramatic changes in metabolism and morphology, thus preventing their further development into tumors. Various types of physiologic stresses can also induce senescence, and this process is thought to play a central role in promoting aging, possibly by limiting the proliferative potential of stem-cell pools. While much is known about senescence in cultured cells, basic questions regarding the roles of this process in the living organism are still unanswered. In this proposal we describe the development of systems for the study of senescence in the living organism. We utilize the two central molecular activators of senescence: the p16INK4a and p19ARF genes. These genes are specifically induced during cellular senescence, and their expression is sufficient to induce this state. We will develop and characterize mice in which the expression of p16INK4a or p19ARF can be induced in multiple tissues through tetracycline administration, in order to induce cellular senescence in these tissues. This will allow us to experimentally generate senescent cells in the adult mouse and characterize their molecular and functional traits. We will study the effects of cellular senescence on tissue and organism physiology, and assess whether induction of senescence induces aging-like phenotypes. This system will provide the first genetic system to study this central biological program in its proper physiologic context, shedding light on aging processes and cancer prevention mechanisms.\n\n",
                "DataExportTag": "COR50164",
                "QuestionID": "QID285",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Generation of mouse models for the study of cellular senescence in aging and cancer Cellular sene...",
                "Choices": {
                    "1": {
                        "Display": "\"Evolutionary Genetics and Species Adaptation\""
                    },
                    "2": {
                        "Display": "evolution, genomic, genetic, specie, trait, population, adaptation, insect, variation, fitness, phenotype, sequence, speciation, selection, lineage"
                    },
                    "3": {
                        "Display": "\"Cancer Research and Aging Studies\""
                    },
                    "4": {
                        "Display": "cancer, tumor, age, disease, mutation, patient, therapeutic, genetic, cellular, mouse, mitochondrial, ageing, aging, disorder, protein"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID130",
            "SecondaryAttribute": "gov Identifier: NCT01047462.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "gov Identifier: NCT01047462.\n\n",
                "DataExportTag": "clinicaltrials",
                "QuestionID": "QID130",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "gov Identifier: NCT01047462.",
                "Choices": {
                    "1": {
                        "Display": "\"Robotic Surgery and Urological Procedures\""
                    },
                    "2": {
                        "Display": "laparoscopic, rectal, robotic, renal, partial_nephrectomy, bladder, laparoscopy, robot_assist, pelvic, post_radical_prostatectomy, laparoscopic_radical, nephrectomy, prostatectomy, rectum, port"
                    },
                    "3": {
                        "Display": "\"Surgical Procedures and Complications in Thoracic and Gastrointestinal Medicine\""
                    },
                    "4": {
                        "Display": "rectal, resection, pancreatic, lung, esophagectomy, esophageal, vat, anastomosis, video_assist, lobectomy, anastomotic_leakage, total_mesorectal, mortality, gastric, thoracic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID293",
            "SecondaryAttribute": "GraphRules: Rule Discovery, Exploration and Visualization of Collaborative Graph Structures In th...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "GraphRules: Rule Discovery, Exploration and Visualization of Collaborative Graph Structures In this proposal we will offer a unified view of graph mining systems and recommender systems. We propose a system that monitors the time evolution of graph entities (such as customer-product graphs) and allows for a broad set of knowledge extraction operators. The proposed system will feature advanced collaborative tagging techniques, which we call 'auto-tagging' that aim at automating information extraction about the monitored graph entities from various web sources, such as RSS feeds, blog posts and news events. The system will offer several exhiting innovations such as:- Incorporation of Ontologies and hierarchies on top of the graph structure- Automatic expansion of entity information from diverse web sources- Interactive Exploration of the graph space- Knowledge extraction algorithms, including clustering and bi-clustering, recommendations, anomaly detection and prediction.\n\n",
                "DataExportTag": "COR41811",
                "QuestionID": "QID293",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "GraphRules: Rule Discovery, Exploration and Visualization of Collaborative Graph Structures In th...",
                "Choices": {
                    "1": {
                        "Display": "\"Electric Vehicle Manufacturing and Assembly\""
                    },
                    "2": {
                        "Display": "electric, battery, electric_vehicle, assembly, energy, automotive, vehicle, sensor, motor, packaging, pilot_line, modular, component, weight, welding"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and Cloud Systems\""
                    },
                    "4": {
                        "Display": "computation, heterogeneous, hpc, code, hardware, cps, processor, programming, cryptography, storage, layer, machine_learning, cloud_computing, compute, exascale"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID295",
            "SecondaryAttribute": "Gulag Echoes in the \u201cmulticultural prison\u201d: historical and geographical influences on the identi",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Gulag Echoes in the \u201cmulticultural prison\u201d: historical and geographical influences on the identity and politics of ethnic minority prisoners in the communist successor states of Russia Europe. \"The project will examine the impact of the system of penality developed in the Soviet gulag on the ethnic identification and political radicalisation of prisoners in the Soviet Union and the communist successor states of Europe today. It is informed by the proposition that prisons are sites of ethnic identity construction but that the processes involved vary within and between states. In the project, the focus is on the extent to which particular \"\"prison-styles\"\" affect the social relationships, self-identification and political association of ethnic minority prisoners. After the collapse of the Soviet Union, the communist successor states all set about reforming their prison systems to bring them into line with international and European norms. However, all to a lesser or greater extent still have legacies of the system gestated in the Soviet Gulag and exported to East-Central-Europe after WWII.  These may include the internal organisation of penal space, a collectivist approach to prisoner management, penal labour and, as in Russian case,  a geographical distribution of the penal estate that results in prisoners being sent excessively long distances to serve their sentences.  It is the how these legacies, interacting with other forces (including official and popular discourses, formal policy and individual life-histories) transform, confirm, and suppress the ethnic identification of prisoners that the project seeks to excavate.  It will use a mixed method approach to answer research questions, including interviews with ex-prisoners and prisoners' families, the use of archival and documentary sources and social media. The research will use case studies to analyze the experiences of ethnic minority prisoners over time and through space. These  provisionally will be  Chechens, Tartars, Ukrainians, Estonians, migrant Uzbek and Tadjik workers and Roma and the country case studies are the Russian Federation, Georgia and Romania.\"\n\n",
                "DataExportTag": "COR1045",
                "QuestionID": "QID295",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Gulag Echoes in the \u201cmulticultural prison\u201d: historical and geographical influences on the identit...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\""
                    },
                    "2": {
                        "Display": "child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID98",
            "SecondaryAttribute": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (HCT), which is a treatment option for leukemia and lymphoma (reviewed in[1]), represents a form of adoptive T cell therapy (the graft-versus-tumor [GVT] effect). The majority of allogeneic transplants are performed using un-manipulated donor T cells, although as we will describe below, ex vivo manipulated donor T cell therapy represents a promising new area of clinical research. And, in the autologous setting, ex vivo expanded T cells, in particular T cells that are gene-modified to enhance tumor targeting or avoid viral infection, have been evaluated in a diversity of diseases including melanoma and sarcoma[2], leukemia [3], and HIV disease[4] [5]. Successful adoptive T cell therapy in part requires that the transferred T cells: (1) differentiate into a favorable T cell subset, which may minimally include Th1, Th2, TREG, or Th17 populations depending on the clinical context [6]; and (2) persist in vivo [7]. In this review, we will focus on the ex vivo use of rapamycin to generate functionally defined T cell subsets that possess a favorable pattern of in vivo persistence after adoptive transfer. Current results indicate that rapamycin induces autophagy during ex vivo T cell differentiation, with resultant T cell acquisition of an anti-apoptotic phenotype that confers in vivo persistence.\n\n",
                "DataExportTag": "CAN856688",
                "QuestionID": "QID98",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (...",
                "Choices": {
                    "1": {
                        "Display": "Autophagy is triggered during cellular stresses such as starvation, growth factor withdrawal, or through rapamycin pharmacologic blockade of mTOR [8], which is a central gatekeeper of cell signaling pathways and cellular energetics. Three types of autophagy exist: (1) macroautophagy, which involves the enclosure of cytoplasm, cytoplasmic protein aggregates, or whole organelles in a double membrane bound structure that is degraded by lysosomes; (2) microautophagy, which is the capture of cytoplasmic contents in small vesicles that bud directly into lysosomes; and (3) chaperone-mediated autophagy, which involves translocation of proteins containing a target sequence into the lysosomal membrane. Rapamycin primarily results in macroautophagy (hereafter referred to as autophagy) that occurs in three stages: (1) initiation, which involves the formation of an isolation membrane around the cytoplasm; (2) maturation, where membrane expansion results in the formation of an autophagosomal double membrane; and (3) degradation in the lysosomes. These three events are successfully orchestrated by various molecules such as the ATG series of proteins and beclin-1, with mTOR serving as a distal autophagy gatekeeper (mTOR on: autophagy inhibition; mTOR off: autophagy promotion) (reviewed in [9])."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "Given the importance of autophagy in determining T cell survival (reviewed in [10]), we evaluated whether rapamycin exposure during ex vivo T cell culture induced autophagy and modulated T cell survival. For these studies, we evaluated human CD4+ T cells that were polarized to a Th1 cytokine phenotype. Indeed, as shown in the electron microscopy images (Fig 1 (A and B), human T cells manufactured ex vivo in rapamycin were replete with autophagosomes; consistent with an autophagy mechanism, T cells knocked-down for Beclin-1 had reduced autophagosome formation during mTOR blockade. Initially, rapamycin was characterized as an agent that induces T cell anergy even in the presence of co-stimulation [11]; in this light, it was somewhat paradoxical that the post-autophagy, rapamycin-resistant T cells that we evaluated had increased in vivo function upon adoptive transfer into a human-into-mouse model of xenogeneic graft-versus-host disease (x-GVHD) [12]. In these experiments, we also found that rapamycin-resistant T cells underwent mitochondrial autophagy (mitophagy), thereby resulting in reduced T cell mitochondrial mass but increased mitochondrial function (more stable mitochondrial membrane potential). These results extended previous findings in human cell lines, where it was observed that autophagy altered mitochondrial biology in T cells, including a reduction in mitochondrial mass [13]."
                    }
                },
                "AnswerOrder": [
                    "1"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID302",
            "SecondaryAttribute": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (HCT), which is a treatment option for leukemia and lymphoma (reviewed in[1]), represents a form of adoptive T cell therapy (the graft-versus-tumor [GVT] effect). The majority of allogeneic transplants are performed using un-manipulated donor T cells, although as we will describe below, ex vivo manipulated donor T cell therapy represents a promising new area of clinical research. And, in the autologous setting, ex vivo expanded T cells, in particular T cells that are gene-modified to enhance tumor targeting or avoid viral infection, have been evaluated in a diversity of diseases including melanoma and sarcoma[2], leukemia [3], and HIV disease[4] [5]. Successful adoptive T cell therapy in part requires that the transferred T cells: (1) differentiate into a favorable T cell subset, which may minimally include Th1, Th2, TREG, or Th17 populations depending on the clinical context [6]; and (2) persist in vivo [7]. In this review, we will focus on the ex vivo use of rapamycin to generate functionally defined T cell subsets that possess a favorable pattern of in vivo persistence after adoptive transfer. Current results indicate that rapamycin induces autophagy during ex vivo T cell differentiation, with resultant T cell acquisition of an anti-apoptotic phenotype that confers in vivo persistence.\n\n",
                "DataExportTag": "CAN856688",
                "QuestionID": "QID302",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (...",
                "Choices": {
                    "1": {
                        "Display": "Autophagy is triggered during cellular stresses such as starvation, growth factor withdrawal, or through rapamycin pharmacologic blockade of mTOR [8], which is a central gatekeeper of cell signaling pathways and cellular energetics. Three types of autophagy exist: (1) macroautophagy, which involves the enclosure of cytoplasm, cytoplasmic protein aggregates, or whole organelles in a double membrane bound structure that is degraded by lysosomes; (2) microautophagy, which is the capture of cytoplasmic contents in small vesicles that bud directly into lysosomes; and (3) chaperone-mediated autophagy, which involves translocation of proteins containing a target sequence into the lysosomal membrane. Rapamycin primarily results in macroautophagy (hereafter referred to as autophagy) that occurs in three stages: (1) initiation, which involves the formation of an isolation membrane around the cytoplasm; (2) maturation, where membrane expansion results in the formation of an autophagosomal double membrane; and (3) degradation in the lysosomes. These three events are successfully orchestrated by various molecules such as the ATG series of proteins and beclin-1, with mTOR serving as a distal autophagy gatekeeper (mTOR on: autophagy inhibition; mTOR off: autophagy promotion) (reviewed in [9])."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "Given the importance of autophagy in determining T cell survival (reviewed in [10]), we evaluated whether rapamycin exposure during ex vivo T cell culture induced autophagy and modulated T cell survival. For these studies, we evaluated human CD4+ T cells that were polarized to a Th1 cytokine phenotype. Indeed, as shown in the electron microscopy images (Fig 1 (A and B), human T cells manufactured ex vivo in rapamycin were replete with autophagosomes; consistent with an autophagy mechanism, T cells knocked-down for Beclin-1 had reduced autophagosome formation during mTOR blockade. Initially, rapamycin was characterized as an agent that induces T cell anergy even in the presence of co-stimulation [11]; in this light, it was somewhat paradoxical that the post-autophagy, rapamycin-resistant T cells that we evaluated had increased in vivo function upon adoptive transfer into a human-into-mouse model of xenogeneic graft-versus-host disease (x-GVHD) [12]. In these experiments, we also found that rapamycin-resistant T cells underwent mitochondrial autophagy (mitophagy), thereby resulting in reduced T cell mitochondrial mass but increased mitochondrial function (more stable mitochondrial membrane potential). These results extended previous findings in human cell lines, where it was observed that autophagy altered mitochondrial biology in T cells, including a reduction in mitochondrial mass [13]."
                    }
                },
                "AnswerOrder": [
                    "1"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID372",
            "SecondaryAttribute": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (HCT), which is a treatment option for leukemia and lymphoma (reviewed in[1]), represents a form of adoptive T cell therapy (the graft-versus-tumor [GVT] effect). The majority of allogeneic transplants are performed using un-manipulated donor T cells, although as we will describe below, ex vivo manipulated donor T cell therapy represents a promising new area of clinical research. And, in the autologous setting, ex vivo expanded T cells, in particular T cells that are gene-modified to enhance tumor targeting or avoid viral infection, have been evaluated in a diversity of diseases including melanoma and sarcoma[2], leukemia [3], and HIV disease[4] [5]. Successful adoptive T cell therapy in part requires that the transferred T cells: (1) differentiate into a favorable T cell subset, which may minimally include Th1, Th2, TREG, or Th17 populations depending on the clinical context [6]; and (2) persist in vivo [7]. In this review, we will focus on the ex vivo use of rapamycin to generate functionally defined T cell subsets that possess a favorable pattern of in vivo persistence after adoptive transfer. Current results indicate that rapamycin induces autophagy during ex vivo T cell differentiation, with resultant T cell acquisition of an anti-apoptotic phenotype that confers in vivo persistence.\n\n",
                "DataExportTag": "CAN615881",
                "QuestionID": "QID372",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Harnessing autophagy for adoptive T-cell therapy. Allogeneic hematopoietic cell transplantation (...",
                "Choices": {
                    "1": {
                        "Display": "Autophagy is triggered during cellular stresses such as starvation, growth factor withdrawal, or through rapamycin pharmacologic blockade of mTOR [8], which is a central gatekeeper of cell signaling pathways and cellular energetics. Three types of autophagy exist: (1) macroautophagy, which involves the enclosure of cytoplasm, cytoplasmic protein aggregates, or whole organelles in a double membrane bound structure that is degraded by lysosomes; (2) microautophagy, which is the capture of cytoplasmic contents in small vesicles that bud directly into lysosomes; and (3) chaperone-mediated autophagy, which involves translocation of proteins containing a target sequence into the lysosomal membrane. Rapamycin primarily results in macroautophagy (hereafter referred to as autophagy) that occurs in three stages: (1) initiation, which involves the formation of an isolation membrane around the cytoplasm; (2) maturation, where membrane expansion results in the formation of an autophagosomal double membrane; and (3) degradation in the lysosomes. These three events are successfully orchestrated by various molecules such as the ATG series of proteins and beclin-1, with mTOR serving as a distal autophagy gatekeeper (mTOR on: autophagy inhibition; mTOR off: autophagy promotion) (reviewed in [9])."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "Given the importance of autophagy in determining T cell survival (reviewed in [10]), we evaluated whether rapamycin exposure during ex vivo T cell culture induced autophagy and modulated T cell survival. For these studies, we evaluated human CD4+ T cells that were polarized to a Th1 cytokine phenotype. Indeed, as shown in the electron microscopy images (Fig 1 (A and B), human T cells manufactured ex vivo in rapamycin were replete with autophagosomes; consistent with an autophagy mechanism, T cells knocked-down for Beclin-1 had reduced autophagosome formation during mTOR blockade. Initially, rapamycin was characterized as an agent that induces T cell anergy even in the presence of co-stimulation [11]; in this light, it was somewhat paradoxical that the post-autophagy, rapamycin-resistant T cells that we evaluated had increased in vivo function upon adoptive transfer into a human-into-mouse model of xenogeneic graft-versus-host disease (x-GVHD) [12]. In these experiments, we also found that rapamycin-resistant T cells underwent mitochondrial autophagy (mitophagy), thereby resulting in reduced T cell mitochondrial mass but increased mitochondrial function (more stable mitochondrial membrane potential). These results extended previous findings in human cell lines, where it was observed that autophagy altered mitochondrial biology in T cells, including a reduction in mitochondrial mass [13]."
                    }
                },
                "AnswerOrder": [
                    "1"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID221",
            "SecondaryAttribute": "Hierarchical Manifold Sensing with Foveation and Adaptive Partitioning of the Dataset The authors...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Hierarchical Manifold Sensing with Foveation and Adaptive Partitioning of the Dataset The authors present a novel method, Hierarchical Manifold Sensing, for adaptive and efficient visual sensing. As opposed to the previously introduced Manifold Sensing algorithm, the new version introduces a way of learning a hierarchical partitioning of the dataset based on k-means clustering. The algorithm can perform on whole images but also on a foveated dataset, where only salient regions are sensed. The authors evaluate the proposed algorithms on the COIL, ALOI, and MNIST datasets. Although they use a very simple nearest-neighbor classifier, on the easier benchmarks, COIL and ALOI, perfect recognition is possible with only six or ten sensing values. Moreover, they show that their sensing scheme yields a better recognition performance than compressive sensing with random projections. On MNIST, state-of-the-art performance cannot be reached, but they show that a large number of test images can be recognized with only very few sensing values. However, for many applications, performance on challenging benchmarks may be less relevant than the simplicity of the solution (processing power, bandwidth) when solving a less challenging problem. c \u00a9 2016 Society for Imaging Science and Technology. [DOI: 10.2352\/J.ImagingSci.Technol.2016.60.2.020402] INTRODUCTION We present a novel method called Hierarchical Manifold Sensing (HMS). The objective is to develop appropriate sensing algorithms such as to increase the efficiency of visual sensing by adopting adaptive sensing strategies. The algorithms can also be used for resampling and compression before transmitting a densely sampled signal over a lowbandwidth channel. In other words, we address the question of how to efficiently sample the visual world under the constraint of a limited bandwidth. For example, the bandwidth is limited in human vision by the capacity of the optic nerve and in technical systems by the performance and cost of hardware. As opposed to classical sensing and compression schemes, HMS is based on unsupervised learning, which involves a hierarchical partitioning of the dataset. Hierarchical Manifold Sensing is inspired by Compressive Sensing (CS).1 Compressive Sensing is based on the fact that natural images can be encoded sparsely,2 and thus the number of samples used for representing an image accurately can be reduced by sensing with a random matrix.3 As opposed to classical sampling, with CS each acquired sensing value is a weighted sum of the original unknown signal. HierarchicalManifold Sensing works in a similar way, i.e., CS Received June 30, 2015; accepted for publication Nov. 8, 2015; published online Dec. 10, 2015. Associate Editor: Chunghui Kuo. 1062-3701\/2016\/60(2)\/020402\/10\/$25.00 and HMS both make use of a sensing matrix. As opposed to CS, where the sensing matrix does not depend on the sensed data, HMS introduces a two-fold adaptivity: (i) the sensing algorithm adapts to a particular dataset, and (ii) every new sensing value depends on the already acquired sensing values. Thus, sensing in HMS is performed adaptively with optimized weights, and not randomly as in CS. Sch\u00fctze et al.4 presented an alternative adaptive hierarchical sensing (AHS) scheme for efficiently obtaining the sparse coefficients of an image. The sensing process is performed by partially traversing a binary tree and making a measurement at each visited node. The method is adaptive in the sense that after each sensing action, depending on how much gain the sensing operation brings, it is decidedwhether the entire subtree of the current node is further traversed or whether it is omitted. Adaptive hierarchical sensing was applied on patches of natural images and it was shown that the performance of themethod can be improved by choosing an appropriate sparse coding basis and by properly arranging the AHS tree. The results of the method strongly depend on the decision step where a threshold is compared with the measurement values corresponding to the binary tree. Baraniuk presented a theoretical analysis of CS for manifolds in 20095 and showed that, similarly to the theory of CS, only a small number of random linear projections is sufficient to preserve the key information on a signal modeled by a manifold. Later, Chen et al.6 proposed a statistical framework for CS on manifolds. Their article presents a nonparametric hierarchical Bayesian algorithm that learns a mixture of factor analyzers for manifolds based on the training data. Afterwards, the signal is reconstructed using a limited number of random projections. The method is validated on synthetic and on real datasets, but it is evaluated only for a subset of the MNIST database. The Manifold Sensing concept was introduced before by Burciu et al. as visual Manifold Sensing (MS),7 and it was extended afterwards to the foveated version of Manifold Sensing (FMS),8 which was inspired by the sampling strategy of biological systems. Like Manifold Sensing, HMS is based on a geometric approach. Both MS and FMS are based on learning manifolds of increasing but low dimensionality by using a nonlinear algorithm, namely Locally Linear Embedding (LLE).9While sensing, the dataset is continuously adapted and the corresponding embedding is learned. As a further and optional feature, HMS can involve foveation as in the FMS approach. J. Imaging Sci. Technol. 020402-1 Mar.-Apr. 2016 Burciu, Martinetz, and Barth: Hierarchical Manifold Sensing with foveation and adaptive partitioning of the dataset Both MS and FMS strongly depend on the choice of the following parameters: (i) the number of neighbors used for LLE, (ii) the decreasing sizes of the adaptive dataset, and (iii) the dimensions of the manifolds at each iteration of the algorithm. Moreover, MS and FMS are operating on the entire dataset while sensing. As highlighted in the FMS article,8 in a real-time sensing scenario one would need to learn all of the manifolds corresponding to all possible subsets of data before performing the actual sensing. In this article we therefore provide an extended version of MS and FMS, which includes an adequate partitioning learned prior to sensing. Hierarchical Manifold Sensing as used in this article is also based on learning manifolds of different and low dimensionality. However, for simplicity we here use a linear method Principal Component Analysis (PCA) to learn the low-dimensional representations of the foveated dataset. The hierarchical partitioning of the dataset is performed by clustering the data in the low-dimensional manifolds using the k-means algorithm. Several approaches aim at developing efficient clustering methods for high-dimensional data; see, for example, Ref. 10. In this work we focus on solving the sensing problem and not on optimizing the approach for hierarchical partitioning of the data. Therefore, we just combine two simple approaches: PCA for dimensionality reduction and k-means for clustering (k-means++ implementation11). LikeMS and FMS,HMS is optimized and evaluatedwith respect to particular recognition tasks and not with respect to the reconstruction error. In the following section we present the Hierarchical Manifold Sensing method: we first explain how the foveated dataset is created, we then present in detail the steps of hierarchical partitioning of the dataset, and we finally show how the unknown scenes are sensed in a hierarchical way. After that we present the results of this work and conclusions. HIERARCHICALMANIFOLD SENSING Hierarchical Manifold Sensing (HMS) is based on a geometric approach to the problem of efficient sensing. A particular type of environment is represented by the images I i in a datasetD= {I1, . . . , Ip}, with p data points of dimension D. In the foveated version of HMS, which is considered here, the dataset D is first transformed into a foveated dataset Dfoveated that contains only regions of interest out of the original dataset. The goal is to learn efficient features for classification. This problem is, however, not approached by just unsupervised learning on the whole dataset Dfoveated. Instead, a tree structure that involves a hierarchical partitioning of the dataset is learned. The resulting partitioning is used to solve the sensing problem more efficiently, i.e., to use as few sensing actions as possible in order to sense and classify an unknown scene or object. In the following subsections we first review the procedure of creating the foveated dataset, which was presented in more detail in Ref. 8. Next, we describe the approach for the hierarchical partitioning of the dataset. These two steps define the offline part of the HMS algorithm. After we have learned the foveated hierarchical representation of the given dataset, we can project on it an unknown scene, i.e., a test point outside Dfoveated that we wish to sense. Hierarchical Manifold Sensing thus includes the following main steps. Creating a Foveated Dataset based on a dataset containing images of known scenes. Hierarchical Partitioning of the Dataset. Hierarchical Sensing of Unknown Scenes (here implemented by resampling of unknown test images). Creating a Foveated Dataset The foveated dataset Dfoveated contains only the pixels that are salient on average over the dataset. Although these pixels do not necessarily form a compact region of interest (ROI), we will denote the collection of salient pixels as the ROI. The ROI is extracted by using a saliency model based on the geometric invariants of the structure tensor of the images in the dataset D. The invariants of the structure tensor are known to be good predictors of human eye movements for static scenes.12 In Ref. 12 the properties of the image regions selected by the saccadic eye movements during experiments were analyzed in terms of higher-order statistics. It was shown that image regions with a statistically less redundant structure, such as the ones given by the signals with intrinsic dimension two, contain all the necessary information of a static scene. Therefore\n\n",
                "DataExportTag": "AI942571",
                "QuestionID": "QID221",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Hierarchical Manifold Sensing with Foveation and Adaptive Partitioning of the Dataset The authors...",
                "Choices": {
                    "1": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "2": {
                        "Display": "prediction, bias, uncertainty, meta, generalization, semi_supervised, active, instance, fairness, loss, calibration, deep_learning, interpretability, surrogate, hyperparameter"
                    },
                    "3": {
                        "Display": "\"Quantum Computing and Machine Learning Theory\""
                    },
                    "4": {
                        "Display": "decision_tree, bound, quantum, probabilistic, query, polynomial, perceptron, automata, temporal, boolean, binary, finite, theorem, bind, pac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID282",
            "SecondaryAttribute": "Hierarchical Manifold Sensing with Foveation and Adaptive Partitioning of the Dataset The authors...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Hierarchical Manifold Sensing with Foveation and Adaptive Partitioning of the Dataset The authors present a novel method, Hierarchical Manifold Sensing, for adaptive and efficient visual sensing. As opposed to the previously introduced Manifold Sensing algorithm, the new version introduces a way of learning a hierarchical partitioning of the dataset based on k-means clustering. The algorithm can perform on whole images but also on a foveated dataset, where only salient regions are sensed. The authors evaluate the proposed algorithms on the COIL, ALOI, and MNIST datasets. Although they use a very simple nearest-neighbor classifier, on the easier benchmarks, COIL and ALOI, perfect recognition is possible with only six or ten sensing values. Moreover, they show that their sensing scheme yields a better recognition performance than compressive sensing with random projections. On MNIST, state-of-the-art performance cannot be reached, but they show that a large number of test images can be recognized with only very few sensing values. However, for many applications, performance on challenging benchmarks may be less relevant than the simplicity of the solution (processing power, bandwidth) when solving a less challenging problem. c \u00a9 2016 Society for Imaging Science and Technology. [DOI: 10.2352\/J.ImagingSci.Technol.2016.60.2.020402] INTRODUCTION We present a novel method called Hierarchical Manifold Sensing (HMS). The objective is to develop appropriate sensing algorithms such as to increase the efficiency of visual sensing by adopting adaptive sensing strategies. The algorithms can also be used for resampling and compression before transmitting a densely sampled signal over a lowbandwidth channel. In other words, we address the question of how to efficiently sample the visual world under the constraint of a limited bandwidth. For example, the bandwidth is limited in human vision by the capacity of the optic nerve and in technical systems by the performance and cost of hardware. As opposed to classical sensing and compression schemes, HMS is based on unsupervised learning, which involves a hierarchical partitioning of the dataset. Hierarchical Manifold Sensing is inspired by Compressive Sensing (CS).1 Compressive Sensing is based on the fact that natural images can be encoded sparsely,2 and thus the number of samples used for representing an image accurately can be reduced by sensing with a random matrix.3 As opposed to classical sampling, with CS each acquired sensing value is a weighted sum of the original unknown signal. HierarchicalManifold Sensing works in a similar way, i.e., CS Received June 30, 2015; accepted for publication Nov. 8, 2015; published online Dec. 10, 2015. Associate Editor: Chunghui Kuo. 1062-3701\/2016\/60(2)\/020402\/10\/$25.00 and HMS both make use of a sensing matrix. As opposed to CS, where the sensing matrix does not depend on the sensed data, HMS introduces a two-fold adaptivity: (i) the sensing algorithm adapts to a particular dataset, and (ii) every new sensing value depends on the already acquired sensing values. Thus, sensing in HMS is performed adaptively with optimized weights, and not randomly as in CS. Sch\u00fctze et al.4 presented an alternative adaptive hierarchical sensing (AHS) scheme for efficiently obtaining the sparse coefficients of an image. The sensing process is performed by partially traversing a binary tree and making a measurement at each visited node. The method is adaptive in the sense that after each sensing action, depending on how much gain the sensing operation brings, it is decidedwhether the entire subtree of the current node is further traversed or whether it is omitted. Adaptive hierarchical sensing was applied on patches of natural images and it was shown that the performance of themethod can be improved by choosing an appropriate sparse coding basis and by properly arranging the AHS tree. The results of the method strongly depend on the decision step where a threshold is compared with the measurement values corresponding to the binary tree. Baraniuk presented a theoretical analysis of CS for manifolds in 20095 and showed that, similarly to the theory of CS, only a small number of random linear projections is sufficient to preserve the key information on a signal modeled by a manifold. Later, Chen et al.6 proposed a statistical framework for CS on manifolds. Their article presents a nonparametric hierarchical Bayesian algorithm that learns a mixture of factor analyzers for manifolds based on the training data. Afterwards, the signal is reconstructed using a limited number of random projections. The method is validated on synthetic and on real datasets, but it is evaluated only for a subset of the MNIST database. The Manifold Sensing concept was introduced before by Burciu et al. as visual Manifold Sensing (MS),7 and it was extended afterwards to the foveated version of Manifold Sensing (FMS),8 which was inspired by the sampling strategy of biological systems. Like Manifold Sensing, HMS is based on a geometric approach. Both MS and FMS are based on learning manifolds of increasing but low dimensionality by using a nonlinear algorithm, namely Locally Linear Embedding (LLE).9While sensing, the dataset is continuously adapted and the corresponding embedding is learned. As a further and optional feature, HMS can involve foveation as in the FMS approach. J. Imaging Sci. Technol. 020402-1 Mar.-Apr. 2016 Burciu, Martinetz, and Barth: Hierarchical Manifold Sensing with foveation and adaptive partitioning of the dataset Both MS and FMS strongly depend on the choice of the following parameters: (i) the number of neighbors used for LLE, (ii) the decreasing sizes of the adaptive dataset, and (iii) the dimensions of the manifolds at each iteration of the algorithm. Moreover, MS and FMS are operating on the entire dataset while sensing. As highlighted in the FMS article,8 in a real-time sensing scenario one would need to learn all of the manifolds corresponding to all possible subsets of data before performing the actual sensing. In this article we therefore provide an extended version of MS and FMS, which includes an adequate partitioning learned prior to sensing. Hierarchical Manifold Sensing as used in this article is also based on learning manifolds of different and low dimensionality. However, for simplicity we here use a linear method Principal Component Analysis (PCA) to learn the low-dimensional representations of the foveated dataset. The hierarchical partitioning of the dataset is performed by clustering the data in the low-dimensional manifolds using the k-means algorithm. Several approaches aim at developing efficient clustering methods for high-dimensional data; see, for example, Ref. 10. In this work we focus on solving the sensing problem and not on optimizing the approach for hierarchical partitioning of the data. Therefore, we just combine two simple approaches: PCA for dimensionality reduction and k-means for clustering (k-means++ implementation11). LikeMS and FMS,HMS is optimized and evaluatedwith respect to particular recognition tasks and not with respect to the reconstruction error. In the following section we present the Hierarchical Manifold Sensing method: we first explain how the foveated dataset is created, we then present in detail the steps of hierarchical partitioning of the dataset, and we finally show how the unknown scenes are sensed in a hierarchical way. After that we present the results of this work and conclusions. HIERARCHICALMANIFOLD SENSING Hierarchical Manifold Sensing (HMS) is based on a geometric approach to the problem of efficient sensing. A particular type of environment is represented by the images I i in a datasetD= {I1, . . . , Ip}, with p data points of dimension D. In the foveated version of HMS, which is considered here, the dataset D is first transformed into a foveated dataset Dfoveated that contains only regions of interest out of the original dataset. The goal is to learn efficient features for classification. This problem is, however, not approached by just unsupervised learning on the whole dataset Dfoveated. Instead, a tree structure that involves a hierarchical partitioning of the dataset is learned. The resulting partitioning is used to solve the sensing problem more efficiently, i.e., to use as few sensing actions as possible in order to sense and classify an unknown scene or object. In the following subsections we first review the procedure of creating the foveated dataset, which was presented in more detail in Ref. 8. Next, we describe the approach for the hierarchical partitioning of the dataset. These two steps define the offline part of the HMS algorithm. After we have learned the foveated hierarchical representation of the given dataset, we can project on it an unknown scene, i.e., a test point outside Dfoveated that we wish to sense. Hierarchical Manifold Sensing thus includes the following main steps. Creating a Foveated Dataset based on a dataset containing images of known scenes. Hierarchical Partitioning of the Dataset. Hierarchical Sensing of Unknown Scenes (here implemented by resampling of unknown test images). Creating a Foveated Dataset The foveated dataset Dfoveated contains only the pixels that are salient on average over the dataset. Although these pixels do not necessarily form a compact region of interest (ROI), we will denote the collection of salient pixels as the ROI. The ROI is extracted by using a saliency model based on the geometric invariants of the structure tensor of the images in the dataset D. The invariants of the structure tensor are known to be good predictors of human eye movements for static scenes.12 In Ref. 12 the properties of the image regions selected by the saccadic eye movements during experiments were analyzed in terms of higher-order statistics. It was shown that image regions with a statistically less redundant structure, such as the ones given by the signals with intrinsic dimension two, contain all the necessary information of a static scene. Therefore\n\n",
                "DataExportTag": "AI942571",
                "QuestionID": "QID282",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Hierarchical Manifold Sensing with Foveation and Adaptive Partitioning of the Dataset The authors...",
                "Choices": {
                    "1": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "2": {
                        "Display": "prediction, bias, uncertainty, meta, generalization, semi_supervised, active, instance, fairness, loss, calibration, deep_learning, interpretability, surrogate, hyperparameter"
                    },
                    "3": {
                        "Display": "\"Quantum Computing and Machine Learning Theory\""
                    },
                    "4": {
                        "Display": "decision_tree, bound, quantum, probabilistic, query, polynomial, perceptron, automata, temporal, boolean, binary, finite, theorem, bind, pac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID175",
            "SecondaryAttribute": "High performance composites for demanding high Temperature applications One of the major limitati...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "High performance composites for demanding high Temperature applications One of the major limitations of polymeric composites for structural applications is the ability of the matrix to withstand and maintain load transfer capability at high temperatures. The current project, HicTac, address this drawback and aims at development of composites capable to withstand temperatures above 360 degrees. The technical strategy is to continue the development of polyimide (PI) based chemistries.  Such development is already ongoing at one of the partners, Nexam AB \u00e2\u20ac\u201c a company that develops and produces high temperature polymers and chemistries. Effort will be spent on adapting and proving that such polymers can be used for manufacturing composite samples and simplified composite structures. This development will be performed at Swerea SICOMP AB - research institute devoted to development of composite material technologies. The properties of the composites material will be thoroughly characterized in terms of mechanical properties under environmental loadings. The project also includes development, manufacturing and testing of simplified but relevant sub-elements as well as manufacturing of a certain number of aerodynamically shaped parts.\n\n",
                "DataExportTag": "COR49220",
                "QuestionID": "QID175",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "High performance composites for demanding high Temperature applications One of the major limitati...",
                "Choices": {
                    "1": {
                        "Display": "\"Chemical Synthesis and Catalysis\""
                    },
                    "2": {
                        "Display": "catalysis, reaction, synthesis, enzyme, metal, bond, synthetic, chemical, ligand, molecule, mof, c_h, selectivity, reactivity, activation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID236",
            "SecondaryAttribute": "High performance composites for demanding high Temperature applications One of the major limitati...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "High performance composites for demanding high Temperature applications One of the major limitations of polymeric composites for structural applications is the ability of the matrix to withstand and maintain load transfer capability at high temperatures. The current project, HicTac, address this drawback and aims at development of composites capable to withstand temperatures above 360 degrees. The technical strategy is to continue the development of polyimide (PI) based chemistries.  Such development is already ongoing at one of the partners, Nexam AB \u00e2\u20ac\u201c a company that develops and produces high temperature polymers and chemistries. Effort will be spent on adapting and proving that such polymers can be used for manufacturing composite samples and simplified composite structures. This development will be performed at Swerea SICOMP AB - research institute devoted to development of composite material technologies. The properties of the composites material will be thoroughly characterized in terms of mechanical properties under environmental loadings. The project also includes development, manufacturing and testing of simplified but relevant sub-elements as well as manufacturing of a certain number of aerodynamically shaped parts.\n\n",
                "DataExportTag": "COR49220",
                "QuestionID": "QID236",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "High performance composites for demanding high Temperature applications One of the major limitati...",
                "Choices": {
                    "1": {
                        "Display": "\"Chemical Synthesis and Catalysis\""
                    },
                    "2": {
                        "Display": "catalysis, reaction, synthesis, enzyme, metal, bond, synthetic, chemical, ligand, molecule, mof, c_h, selectivity, reactivity, activation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID291",
            "SecondaryAttribute": "High Temperature Sensors High-temperature piezoelectric sensing technology is of major importance...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "High Temperature Sensors High-temperature piezoelectric sensing technology is of major importance for the chemical and material processing, automotive, aerospace, and power generating industries. Aurivillius phase ceramics are of interest for these applications because of their high Curie points (Tc up to ~950C). Modified bismuth titanate compositions can be used for sensor applications up to 500C. If a higher operating temperature is required there are currently no suitable polycrystalline ceramics available. The objective of this project is to systematically investigate the effect of doping on the properties (ferroelectric, piezoelectric, dielectric, Curie point, electrical conductivity, thermal depoling and ageing) of candidate Aurivillius phase ceramics. Having identified promising optimised compositions, we will then determine a processing route using Spark Plasma Sintering (SPS) to produce highly textured microstructures with enhanced piezoelectric coefficients.\n\n",
                "DataExportTag": "COR41684",
                "QuestionID": "QID291",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "High Temperature Sensors High-temperature piezoelectric sensing technology is of major importance...",
                "Choices": {
                    "1": {
                        "Display": "\"Agriculture and Food Production\""
                    },
                    "2": {
                        "Display": "food, farming, crop, agriculture, aquaculture, plant, fish, fruit, pest, wine, food_safety, meat, milk, pesticide, seed"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID353",
            "SecondaryAttribute": "High Temperature Sensors High-temperature piezoelectric sensing technology is of major importance...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "High Temperature Sensors High-temperature piezoelectric sensing technology is of major importance for the chemical and material processing, automotive, aerospace, and power generating industries. Aurivillius phase ceramics are of interest for these applications because of their high Curie points (Tc up to ~950C). Modified bismuth titanate compositions can be used for sensor applications up to 500C. If a higher operating temperature is required there are currently no suitable polycrystalline ceramics available. The objective of this project is to systematically investigate the effect of doping on the properties (ferroelectric, piezoelectric, dielectric, Curie point, electrical conductivity, thermal depoling and ageing) of candidate Aurivillius phase ceramics. Having identified promising optimised compositions, we will then determine a processing route using Spark Plasma Sintering (SPS) to produce highly textured microstructures with enhanced piezoelectric coefficients.\n\n",
                "DataExportTag": "COR41684",
                "QuestionID": "QID353",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "High Temperature Sensors High-temperature piezoelectric sensing technology is of major importance...",
                "Choices": {
                    "1": {
                        "Display": "\"Agriculture and Food Production\""
                    },
                    "2": {
                        "Display": "food, farming, crop, agriculture, aquaculture, plant, fish, fruit, pest, wine, food_safety, meat, milk, pesticide, seed"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID214",
            "SecondaryAttribute": "Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate Extraction Image segmentation is the technique of dividing an electronic digital image into a number of sub-images. The aim is always to simplify and\/or change the representation of a graphic into something more meaningful and easier to analyze. The majorreason behind blindness and vision problems in working people is a serious public health issue. Identifying diabetic retinopathy lesions in retinal images can help in diagnosing a problem early. Lack of accuracy in these techniques can lead to fatal results because of incorrect treatment. So, there is a great need for automation techniques with high accuracy for retinal disease identification. Several automation techniques have been proposed for retinal image analysis. This paper proposes new unsupervised technique which gives improved results but it is suffers from the problem of the noise. So to remove this problem a hybrid Gabor filter bank with trilateral based filtering technique is proposed. Keywords\u2014 Image segmentation, Exudates segmentation, Gabor filter, Fundus image INTRODUCTION Image processing aims at performing some operations on the image to get an improved image with an aim to extract useful information from the image. Image processing is a kind of signal processing in which takes the image as input and produces some characteristics of the image as the output. These days, image processing is a widely growing technology and is one of the main areas of research. Image processing basically includes the following three steps: 1. Using image acquisition tools to import the image 2. Analysis and manipulation of the image. 3. Altering or modifying the Output Basically, two approaches, analogue and digital image processing are majorly used for image processing. Analogue technique is generally used with hard copies of the image like printouts and photographs. Digital image processing deals with manipulating digital images with help of a computer. The data undergoes three steps, namely pre-processing, image enhancement, and information extraction. Fig 1: Image processing[9] A digital image is a way to represent a 2-D image in form of image pixels as a finite set of digital values. The values of pixels depict characteristics such as gray levels, colors, heights, opacities etc. Digital Image Processing is the technique of manipulation of digital binary images with the help of a digital computer. DIP deals with the computer-based processing of digital images. The input to the image processing system is a binary image which is processed using efficient algorithms, and output is in form of a processed image. Adobe Photoshop is a commonly used application for processing digital images. Main tasks of Digital image processing are1. Improving the images for better human understanding. Paper Title:Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate Extraction ISSN:-2349-3585 |www.ijrdt.org 153 2. Processing the digital images for the use of transmission, storage and representation for autonomous machine perception. 1. SEGMENTATION PROCESS Image segmentation is a technique which divides a binary image into numerous sub-images which is a group of pixels. These pixels have similarity depending upon some homogeneity criterion like color, intensity and texture of the image. This is helpful in identifying the object of interest from the images. In good segmentation approach:1. Pixels having similar grayscale value and forming a connected region belong to the same category. 2. Pixels belonging to different categories and having dissimilar grayscale values are neighboring pixels. The choice Image segmentation techniques dependent on following:1. Detecting DiscontinuitiesThis approach divides an image into sub-images depending upon an abrupt change in the intensity value. Example: edge detection approach. 2. Detecting SimilaritiesThis approach divides an image into sub-images depending upon some predefined similarity criterion. Example: thresholding, Region growing, Region splitting and merging approaches. Fig 2: Segmentation Techniques 2. GABOR FILTER Dennis Gabor proposed the Gabor filtering technique which was later on used by Daugman to model the response of cells in the visual cortex of some mammals. Gabor filter is a linear filtering technique which is widely used for edge detection. Gabor filters have an advantage of having frequency and orientation representations identical to the human visual system, and so it is very helpful in segmenting retinal vessel structures. Gabor filters are basically bandpass filters and are mainly used in feature extraction and texture analysis for processing images. This filtering technique makes use of multi-scale Gabor filtercharacterized by one free parameter that determines the scale to identify the pixels called keypoints using the scale invariant feature transform (SIFT). Filterresponse vectors derived from keypoints are used to initialize a clustering algorithm which generates textonswhich are eventually used for the classification of vessel pixels or nonvessel pixels in the input images. Then a process called model selection is used to identify keypoints from a small training set (N = 10) of retinal images. Then using these keypoints, a textondictionary is built. In this second training stage, a k-NN clustering algorithm is initialized with seeds found by matching keypoints identified by the model selection stage to those in the training set Leung and Malik characterize textons by its response to a filter bank (F1 ,F2 ,. . .,Fn). R = F1\u00d7 I (x, y) , F2\u00d7 I (x, y) , . . ., Fn\u00d7(x, y) If the filter bank convolved with an input image I(x,y) the response at each pixel will depend on specific local structures and on the design of the filter bank. 3. RELATED WORK Zhang et al. [2015] [6] presented an algorithm for segmenting retinal vessel structures which makes use of a texton dictionary to categorize vessel pixels or non-vessel pixels. This algorithm also uses Key points which are a small set of image features to derive the parameters for filtering. A Gabor filter bank presented in this paper has main aim of extracting key points from an image which represent significant information regarding vessel features with the help of SIFT algorithm. Pereira et al. [2015] [7] segmented exudates from the retinal images on the basis of the ant colony optimization technique. The efficiency of the method was evaluated with an online database and the experimental results proved that the technique is better in performance as compared to traditional Kirsch filter for exudates detection from the retinal images. Arraign et al. [2014] [8] presented a technique to identify glaucoma by measuring the displacement of blood vessels in the OD (optic disc) in fundus images due to increase in size of the excavation or cup. This technique deals with segmenting the area of the vascular bundle in an optic disc to set a reference point in the temporal side of the cup. Roychowdhury Paper Title:Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate Extraction ISSN:-2349-3585 |www.ijrdt.org 154 et al. [2014] [9] presented a new three-step vessel segmentation technique using fundus images. Initially, high-pass filtering is used to extract a binary image by preprocessing the green plane of fundus images, and the other binary image is extracted from the reconstructed enhanced image.Akram et al. [2013] [10] gave a methodology to accurately detect drusen in the fundus images. This method makes use of a filter which extracts all possible drusen regions from fundus images and also removes false pixels from the fundus image which may appear due to the resemblance of drusen with optic disc. This methodology represents each area using several characteristics and then classifies areas as drusen region and non-drusen region by applying support vector machine.Bhuiyan et al. [2013] [11] have discussed that the Age-related macular degeneration (AMD) is the main reasons for vision defect among the aged people and identification of persons with first stages of AMD is vital while considering the preventative strategies for late AMD. They proposed a technique for drusen detection from standard color retinal images which enables quick, accurate and automated method to determine characteristics of drusen. Kambojet al. [2013] [12] explained that enhancing a corrupted image by removing noise is an important step in digital image processing for better perception. Various filters are used for removing various types of the noise from the images. Numerous noise models and filtering algorithms are explained. Odstrcilik et al. [2013] [13] improved the concept of matched filtering, and proposed a technique for segmenting retinal vessels from the fundus images. This proposed method aims to segment blood vessels which have different vessel from color retinal images.Krishnan et al. [2012] [14] used Attanassov intuitionist fuzzy histon (A-IFSH) based segmentation technique to propose an algorithm to extract the optic disc from digital retinal images. To find and locate the optic disc region, Optic disc pixel intensity and column wise neighborhood techniques are used in this paper. The strategy was tested on 100 images containing of 30 normal images, 39 images with glaucoma and 31 DR images.Rozlan et al. [2012] [15] developed a Graphical User Interface (GUI) which enhances the process of segmentation of blood vessels in retinal images, It can be helpful for the ophthalmologist to improve the morphological processes by analyzing key characteristics in processed images for faster diagnosis. Shanmugavadivu et al. [2012] [16] gave a filtering technique for the purpose of restoration of the retinal images which have been corrupted with a fixed-value impulse noise. This technique is simpler as compared to other techniques, so the rate of restoration rate is faster.Yin et al. [2012] [17] gave a method to segment the optic disc and opt\n\n",
                "DataExportTag": "AI49040",
                "QuestionID": "QID214",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate...",
                "Choices": {
                    "1": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "2": {
                        "Display": "prediction, bias, uncertainty, meta, generalization, semi_supervised, active, instance, fairness, loss, calibration, deep_learning, interpretability, surrogate, hyperparameter"
                    },
                    "3": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "4": {
                        "Display": "prediction, causal, uncertainty, loss, classification, active, fairness, semi_supervised, imputation, bias, calibration, estimation, real_world, ranking, instance"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID275",
            "SecondaryAttribute": "Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate Extraction Image segmentation is the technique of dividing an electronic digital image into a number of sub-images. The aim is always to simplify and\/or change the representation of a graphic into something more meaningful and easier to analyze. The majorreason behind blindness and vision problems in working people is a serious public health issue. Identifying diabetic retinopathy lesions in retinal images can help in diagnosing a problem early. Lack of accuracy in these techniques can lead to fatal results because of incorrect treatment. So, there is a great need for automation techniques with high accuracy for retinal disease identification. Several automation techniques have been proposed for retinal image analysis. This paper proposes new unsupervised technique which gives improved results but it is suffers from the problem of the noise. So to remove this problem a hybrid Gabor filter bank with trilateral based filtering technique is proposed. Keywords\u2014 Image segmentation, Exudates segmentation, Gabor filter, Fundus image INTRODUCTION Image processing aims at performing some operations on the image to get an improved image with an aim to extract useful information from the image. Image processing is a kind of signal processing in which takes the image as input and produces some characteristics of the image as the output. These days, image processing is a widely growing technology and is one of the main areas of research. Image processing basically includes the following three steps: 1. Using image acquisition tools to import the image 2. Analysis and manipulation of the image. 3. Altering or modifying the Output Basically, two approaches, analogue and digital image processing are majorly used for image processing. Analogue technique is generally used with hard copies of the image like printouts and photographs. Digital image processing deals with manipulating digital images with help of a computer. The data undergoes three steps, namely pre-processing, image enhancement, and information extraction. Fig 1: Image processing[9] A digital image is a way to represent a 2-D image in form of image pixels as a finite set of digital values. The values of pixels depict characteristics such as gray levels, colors, heights, opacities etc. Digital Image Processing is the technique of manipulation of digital binary images with the help of a digital computer. DIP deals with the computer-based processing of digital images. The input to the image processing system is a binary image which is processed using efficient algorithms, and output is in form of a processed image. Adobe Photoshop is a commonly used application for processing digital images. Main tasks of Digital image processing are1. Improving the images for better human understanding. Paper Title:Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate Extraction ISSN:-2349-3585 |www.ijrdt.org 153 2. Processing the digital images for the use of transmission, storage and representation for autonomous machine perception. 1. SEGMENTATION PROCESS Image segmentation is a technique which divides a binary image into numerous sub-images which is a group of pixels. These pixels have similarity depending upon some homogeneity criterion like color, intensity and texture of the image. This is helpful in identifying the object of interest from the images. In good segmentation approach:1. Pixels having similar grayscale value and forming a connected region belong to the same category. 2. Pixels belonging to different categories and having dissimilar grayscale values are neighboring pixels. The choice Image segmentation techniques dependent on following:1. Detecting DiscontinuitiesThis approach divides an image into sub-images depending upon an abrupt change in the intensity value. Example: edge detection approach. 2. Detecting SimilaritiesThis approach divides an image into sub-images depending upon some predefined similarity criterion. Example: thresholding, Region growing, Region splitting and merging approaches. Fig 2: Segmentation Techniques 2. GABOR FILTER Dennis Gabor proposed the Gabor filtering technique which was later on used by Daugman to model the response of cells in the visual cortex of some mammals. Gabor filter is a linear filtering technique which is widely used for edge detection. Gabor filters have an advantage of having frequency and orientation representations identical to the human visual system, and so it is very helpful in segmenting retinal vessel structures. Gabor filters are basically bandpass filters and are mainly used in feature extraction and texture analysis for processing images. This filtering technique makes use of multi-scale Gabor filtercharacterized by one free parameter that determines the scale to identify the pixels called keypoints using the scale invariant feature transform (SIFT). Filterresponse vectors derived from keypoints are used to initialize a clustering algorithm which generates textonswhich are eventually used for the classification of vessel pixels or nonvessel pixels in the input images. Then a process called model selection is used to identify keypoints from a small training set (N = 10) of retinal images. Then using these keypoints, a textondictionary is built. In this second training stage, a k-NN clustering algorithm is initialized with seeds found by matching keypoints identified by the model selection stage to those in the training set Leung and Malik characterize textons by its response to a filter bank (F1 ,F2 ,. . .,Fn). R = F1\u00d7 I (x, y) , F2\u00d7 I (x, y) , . . ., Fn\u00d7(x, y) If the filter bank convolved with an input image I(x,y) the response at each pixel will depend on specific local structures and on the design of the filter bank. 3. RELATED WORK Zhang et al. [2015] [6] presented an algorithm for segmenting retinal vessel structures which makes use of a texton dictionary to categorize vessel pixels or non-vessel pixels. This algorithm also uses Key points which are a small set of image features to derive the parameters for filtering. A Gabor filter bank presented in this paper has main aim of extracting key points from an image which represent significant information regarding vessel features with the help of SIFT algorithm. Pereira et al. [2015] [7] segmented exudates from the retinal images on the basis of the ant colony optimization technique. The efficiency of the method was evaluated with an online database and the experimental results proved that the technique is better in performance as compared to traditional Kirsch filter for exudates detection from the retinal images. Arraign et al. [2014] [8] presented a technique to identify glaucoma by measuring the displacement of blood vessels in the OD (optic disc) in fundus images due to increase in size of the excavation or cup. This technique deals with segmenting the area of the vascular bundle in an optic disc to set a reference point in the temporal side of the cup. Roychowdhury Paper Title:Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate Extraction ISSN:-2349-3585 |www.ijrdt.org 154 et al. [2014] [9] presented a new three-step vessel segmentation technique using fundus images. Initially, high-pass filtering is used to extract a binary image by preprocessing the green plane of fundus images, and the other binary image is extracted from the reconstructed enhanced image.Akram et al. [2013] [10] gave a methodology to accurately detect drusen in the fundus images. This method makes use of a filter which extracts all possible drusen regions from fundus images and also removes false pixels from the fundus image which may appear due to the resemblance of drusen with optic disc. This methodology represents each area using several characteristics and then classifies areas as drusen region and non-drusen region by applying support vector machine.Bhuiyan et al. [2013] [11] have discussed that the Age-related macular degeneration (AMD) is the main reasons for vision defect among the aged people and identification of persons with first stages of AMD is vital while considering the preventative strategies for late AMD. They proposed a technique for drusen detection from standard color retinal images which enables quick, accurate and automated method to determine characteristics of drusen. Kambojet al. [2013] [12] explained that enhancing a corrupted image by removing noise is an important step in digital image processing for better perception. Various filters are used for removing various types of the noise from the images. Numerous noise models and filtering algorithms are explained. Odstrcilik et al. [2013] [13] improved the concept of matched filtering, and proposed a technique for segmenting retinal vessels from the fundus images. This proposed method aims to segment blood vessels which have different vessel from color retinal images.Krishnan et al. [2012] [14] used Attanassov intuitionist fuzzy histon (A-IFSH) based segmentation technique to propose an algorithm to extract the optic disc from digital retinal images. To find and locate the optic disc region, Optic disc pixel intensity and column wise neighborhood techniques are used in this paper. The strategy was tested on 100 images containing of 30 normal images, 39 images with glaucoma and 31 DR images.Rozlan et al. [2012] [15] developed a Graphical User Interface (GUI) which enhances the process of segmentation of blood vessels in retinal images, It can be helpful for the ophthalmologist to improve the morphological processes by analyzing key characteristics in processed images for faster diagnosis. Shanmugavadivu et al. [2012] [16] gave a filtering technique for the purpose of restoration of the retinal images which have been corrupted with a fixed-value impulse noise. This technique is simpler as compared to other techniques, so the rate of restoration rate is faster.Yin et al. [2012] [17] gave a method to segment the optic disc and opt\n\n",
                "DataExportTag": "AI49040",
                "QuestionID": "QID275",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Hybrid Gabor Filter Bank with Trilateral Based Filtering Image Segmentation Technique for Exudate...",
                "Choices": {
                    "1": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "2": {
                        "Display": "prediction, bias, uncertainty, meta, generalization, semi_supervised, active, instance, fairness, loss, calibration, deep_learning, interpretability, surrogate, hyperparameter"
                    },
                    "3": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "4": {
                        "Display": "prediction, causal, uncertainty, loss, classification, active, fairness, semi_supervised, imputation, bias, calibration, estimation, real_world, ranking, instance"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID356",
            "SecondaryAttribute": "Hyperrealistic Imaging Experience The aim of realistic digital imaging is the creation of high qu...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Hyperrealistic Imaging Experience The aim of realistic digital imaging is the creation of high quality imagery, which faithfully represents the physicalenvironment. The ultimate goal is to create images, which are perceptually indistinguishable from a real scene. TheRealVision network brings together leading universities, centers focused on industrial development and companies inMultimedia, Optics, Visual Communication, Visual Computing, Computer Graphics and Human Vision research acrossEurope, with the aim of training a new generation of scientists, technologists, and entrepreneurs that will move Europe into aleading role in innovative hyper-realistic imaging technologies. Current imaging technologies capture only a fraction of visualinformation that the human eye can see. The colours and dynamic range are inadequate for most real-world scenes and notall depth cues required for natural 3D vision are captured. This limits the realism of the experience and has hampered theintroduction of 3D technology. Advancement in imaging technologies makes it possible to circumvent these bottlenecks invisual systems. As a result, new visual signal-processing areas have emerged such as light fields, ultra-high definition,highframerate and high dynamic range imaging. The novel combinations of those technologies can facilitate a hyper-realisticvisual experience. This will without doubt be the future frontier for new imaging systems. However there are severaltechnological barriers that need to be overcome as well as challenges in what are the best solutions perceptually. The goalof this network is to combine expertise from several disciplines as engineering, computer science, physics, vision scienceand psychology \u2013 usually disconnected \u2013 and train ESRs to be capable of working with all stages and aspects of visualprocessing to overcome existing interdisciplinary and intersectorial barriers to efficiently develop truly perceptually bettervisual imaging systems.\n\n",
                "DataExportTag": "COR11813",
                "QuestionID": "QID356",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Hyperrealistic Imaging Experience The aim of realistic digital imaging is the creation of high qu...",
                "Choices": {
                    "1": {
                        "Display": "\"Imaging Techniques and Molecular Detection\""
                    },
                    "2": {
                        "Display": "imaging, x_ray, image, microscopy, sensor, nuclear_magnetic_resonance, optic, resolution, measurement, detection, probe, sensitivity, detector, spectroscopy, molecule"
                    },
                    "3": {
                        "Display": "\"Astronomy and Cosmology\""
                    },
                    "4": {
                        "Display": "galaxy, star, universe, observation, black_hole, stellar, gravitational_wave, observational, star_formation, cosmic, telescope, dust, cosmology, cosmological, dark_matter"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID202",
            "SecondaryAttribute": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Brea...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Breast cancer is a heterogeneous disease and markers for therapy response remain poorly defined. Since the effectiveness of treatment differs between individual patients, during the last years much effort has being invested in the identification of new markers, to estimate patients's outcome (prognostic markers) and to indicate which treatment is most effective for an individual patient (predictive markers).\nThe implementation of predictive factors in clinical setting is a big challenge of the cancer research and it will provide the opportunity to guide treatment decisions. Only patients that are likely to benefit from a specific treatment will receive this specific treatment. An individualized therapy will avoid the administration of ineffective chemotherapy that increases mortality and decreases quality of life in cancer patients.\nFor many years research has focused on the identification of single markers predicting tumour response to chemotherapy. However it is unlikely that the chemotherapy resistance\/responsiveness in breast cancer is the result of one or limited number of genes, because of the complexity of pathways involved in tumour response to chemotherapy and the heterogeneity of the individual tumours. The microarray technology made possible to study gene expression profiling of breast cancer on a global scale. It was successfully applied on the identification of breast cancer subgroups and profiles predicting patient's prognosis. More recently microarrays have been also focused on identifying gene expression profiles predicting response to chemotherapy. With the introduction of preoperative chemotherapy (neoadjuvant chemotherapy) it has become possible to directly evaluate the sensitivity of breast cancer to chemotherapy by the clinical\/pathological response of the patient to the treatment. The main goal of this thesis was to identify predictive genes of response to a specific neoadjuvant chemotherapy regimen based on paclitaxel and anthracyclines (doxorubicin and epirubicin) drugs in breast cancer patients.\nFrom 41 pre-treatment breast tumours biopsies good quality RNA was obtained and gene expression profiling was performed. Gene expression patterns of 37 patients were analyzed using Operon v2.0 70mer oligos collection at CRIBI Biotech centre and 4 patients were profiled with Operon v3.0 70mer oligos collection at Netherlands Cancer Institute. Clinical responses of 34 (out of 41) patients were recorded after administration of the neoadjuvant chemotherapy. Complete Responses (CR) to the treatment were observed in 3 patients, Partial Responses in 18 (PR) patients, No Change of the tumour mass (NC) in 11 patients and Progressive Disease (PD) in 2 patients.\nFirst of all, a correlation analysis between the ImmunoHistoChemical data of six prognostic markers (ER, PR, Erb-B2, Bcl-2, Ki-67, p53) and the gene expression data was carried out. The results showed a significant correlation for ER, PR and Bcl-2 markers. Moreover Bcl-2 status measured by ImmunoHistoChemistry (IHC) was significantly associated with the clinical response to neoadjuvant chemotherapy.\nThe molecular subtypes of 37 breast tumours analyzed with Operon v2.0 were identified using the \"intrinsic gene signature\" of Perou and colleagues. Most part of the patients were luminal-like subtype (28 of 37), 7 patients showed an erb-B2+ molecular subtype and 2 patients belonged to the basal-like group. Since it was reported that breast cancer molecular subtypes respond differently to neoadjuvant chemotherapy, I also checked how the clinical response to the treatment were associated to the molecular subtypes. From the analysis emerged that the luminal-like and erb-B2+ molecular subtypes were enriched of PR patients.\nA hierarchical cluster analysis on the pre-treatment tumours (analyzed with Operon v2.0 and with clinical response available) was performed in order to evaluate how the patients would have been separated on the basis of their gene expression profile, using an unsupervised approach. As expected, no clear separation between Responders (PR + CR) and Non Responders (NC + PD) was found. The results did not change if we included in the responder group only the PR patients. We hypothesized that the predictive genes of resistance\/sensitivity to the chemotherapy were a subtle set. The high number of differentially expressed genes would have masked the \"real\" predictive gene set, leading to a clustering of the patients based on biological parameters different from the clinical response. In addition the small size of the dataset was a limiting factor in the analysis.\nIn light of this result we opted for a supervised approach that consisted in dividing the tumours into Responders and Non Responders and searching for the genes (the drug-resistance predictive genes) that could correctly distinguish the two classes of response. I considered two datasets of patients, the dataset I including PR patients against not responders patients (NC + PD) and the dataset II with responders patients (PR and CR) against not responders patients (NC + PD).\nThe first approach, based on the software PAM (Prediction Analysis of Microarray), did not give a good prediction performance on both dataset of patients, misclassifying ca 36% of patients. Therefore, a more effective analysis in terms of classification accuracy was requested. A gene selection process based on the Support Vector Machines (SVMs) was considered a good choice in light of the characteristics of the study: low number of patients (examples) and high number of genes (or features). SVMs are a supervised learning algorithm that work well at high dimensionality, overcoming the risk of overfitting due to a number of features much larger than the numbers of examples. A specific recursively feature selection procedure based on SVMs (R-SVM) was used to select the set of genes with the lowest error of classification on the dataset of patients. Because of the small sample size, it was not possible to have a training set and a test set completely separated, so a Leave-One-Out Cross Validation (LOO-CV) procedure was used to assess the performance of the feature selection process. The analysis identified a set of 54 genes able to classify the 28 patients of the dataset I with an accuracy of 85% (4 patients misclassified on 28) and a set of 14 genes able to classify the 30 patients of the dataset II with an accuracy of 76% (7 patients misclassified on 30). The lower accuracy obtained on the dataset II was attributed to the introduction of the cCR patients in the group of Responders. The cCR patients were probably too much dissimilar in terms of clinical response in respect to the PR patients, thus rendering the group of Responders not enough homogeneous. For this reason I focused the following analysis only on the dataset I.\nThe accuracy of 85% obtained for the dataset I was an encouraging result although the small size of the dataset.\nThe biological function and cellular localization of the 54 genes was examined by using GoMiner, a web tool to find associations of Gene Ontology categories within a specific group of genes. As emerged from the analysis, there were several functional categories related to the tumourigenesis processes (\"cell adhesion\", \"insulin receptor signaling pathway\", \"cell proliferation\", \"regulation of cell proliferation\"). Some categories were more closely related to cellular processes and compartments target of the chemotherapy agents used in this study (\"cell cycle\", \"cell cycle arrest\", \"nucleus\") and to responsiveness to the treatment (\"response to hypoxia\").\nA literature research focused on each gene of the predictive signature showed that some of these genes (MYC, NUF2, SPC25; KFL5, CDKN1b, ITGA6, POSTN) are 'biologically plausible', since they have some connections with the drug resitance phenomenon investigated in this study. Others of the 54 genes are related to breast cancer progression and metastasis (CXCL9, CEBPD, IRS2, TCF8, ADAMTS5, PPARGC1A), but their direct involvement in drug resistance to paclitaxel\/anthracycline neoadjuvant chemotherapy did not emerged.\nAt this point of my analysis, I tried to find out how to use the 54 genes signature as a predictive tool of responsiveness to paclitaxel\/anthracyclines based chemotherapy treatment. On the basis of the 54 genes was trained a SVM model that could be used to classify a new patient, not yet classified, as partial responder or not responder. However, the SVM output is a value not so easily usable in statistics prediction problems. Therefore using a sigmoid function, we translated the SVM outputs into probability values that offered a more direct evaluation of the response class of the patient. In practice we transformed the SVM scores obtained for each patient of the dataset in a measure of probability, from 0 to 1, of belonging to the positive class of response (PR patients). Using the trained SVM model on a new, not-yet classified patient, it will make possible to map his SVM score on the sigmoid function and to have a corresponding probability value to belong to the positive class of response.\nThe results reported in this thesis look promising but have to be considered as preliminary, since they were obtained from a study investigating only a small number of patients and need to be validated in a completely independent test set of patients. Thus a validated gene expression signature may improve our understanding of neoadjuvant chemotherapy response mechanisms and in the future may lead to more individual, patient-tailored therapy decisions.\n\n",
                "DataExportTag": "AI228293",
                "QuestionID": "QID202",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Brea...",
                "Choices": {
                    "1": {
                        "Display": "\"Genomics and Genetic Sequencing\""
                    },
                    "2": {
                        "Display": "sequence, deoxyribonucleic_acid, genome, gene, ribonucleic_acid, genomic, motif, prediction, methylation, variant, promoter, transcription_factor, phylogenetic, nucleotide, enhancer"
                    },
                    "3": {
                        "Display": "\"Cancer Diagnosis and Gene Expression Analysis\""
                    },
                    "4": {
                        "Display": "gene, patient, cancer, tumor, biomarker, prediction, expression, gene_expression, diagnosis, signature, prognostic, analysis, survival, healthcare, cohort"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID263",
            "SecondaryAttribute": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Brea...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Breast cancer is a heterogeneous disease and markers for therapy response remain poorly defined. Since the effectiveness of treatment differs between individual patients, during the last years much effort has being invested in the identification of new markers, to estimate patients's outcome (prognostic markers) and to indicate which treatment is most effective for an individual patient (predictive markers).\nThe implementation of predictive factors in clinical setting is a big challenge of the cancer research and it will provide the opportunity to guide treatment decisions. Only patients that are likely to benefit from a specific treatment will receive this specific treatment. An individualized therapy will avoid the administration of ineffective chemotherapy that increases mortality and decreases quality of life in cancer patients.\nFor many years research has focused on the identification of single markers predicting tumour response to chemotherapy. However it is unlikely that the chemotherapy resistance\/responsiveness in breast cancer is the result of one or limited number of genes, because of the complexity of pathways involved in tumour response to chemotherapy and the heterogeneity of the individual tumours. The microarray technology made possible to study gene expression profiling of breast cancer on a global scale. It was successfully applied on the identification of breast cancer subgroups and profiles predicting patient's prognosis. More recently microarrays have been also focused on identifying gene expression profiles predicting response to chemotherapy. With the introduction of preoperative chemotherapy (neoadjuvant chemotherapy) it has become possible to directly evaluate the sensitivity of breast cancer to chemotherapy by the clinical\/pathological response of the patient to the treatment. The main goal of this thesis was to identify predictive genes of response to a specific neoadjuvant chemotherapy regimen based on paclitaxel and anthracyclines (doxorubicin and epirubicin) drugs in breast cancer patients.\nFrom 41 pre-treatment breast tumours biopsies good quality RNA was obtained and gene expression profiling was performed. Gene expression patterns of 37 patients were analyzed using Operon v2.0 70mer oligos collection at CRIBI Biotech centre and 4 patients were profiled with Operon v3.0 70mer oligos collection at Netherlands Cancer Institute. Clinical responses of 34 (out of 41) patients were recorded after administration of the neoadjuvant chemotherapy. Complete Responses (CR) to the treatment were observed in 3 patients, Partial Responses in 18 (PR) patients, No Change of the tumour mass (NC) in 11 patients and Progressive Disease (PD) in 2 patients.\nFirst of all, a correlation analysis between the ImmunoHistoChemical data of six prognostic markers (ER, PR, Erb-B2, Bcl-2, Ki-67, p53) and the gene expression data was carried out. The results showed a significant correlation for ER, PR and Bcl-2 markers. Moreover Bcl-2 status measured by ImmunoHistoChemistry (IHC) was significantly associated with the clinical response to neoadjuvant chemotherapy.\nThe molecular subtypes of 37 breast tumours analyzed with Operon v2.0 were identified using the \"intrinsic gene signature\" of Perou and colleagues. Most part of the patients were luminal-like subtype (28 of 37), 7 patients showed an erb-B2+ molecular subtype and 2 patients belonged to the basal-like group. Since it was reported that breast cancer molecular subtypes respond differently to neoadjuvant chemotherapy, I also checked how the clinical response to the treatment were associated to the molecular subtypes. From the analysis emerged that the luminal-like and erb-B2+ molecular subtypes were enriched of PR patients.\nA hierarchical cluster analysis on the pre-treatment tumours (analyzed with Operon v2.0 and with clinical response available) was performed in order to evaluate how the patients would have been separated on the basis of their gene expression profile, using an unsupervised approach. As expected, no clear separation between Responders (PR + CR) and Non Responders (NC + PD) was found. The results did not change if we included in the responder group only the PR patients. We hypothesized that the predictive genes of resistance\/sensitivity to the chemotherapy were a subtle set. The high number of differentially expressed genes would have masked the \"real\" predictive gene set, leading to a clustering of the patients based on biological parameters different from the clinical response. In addition the small size of the dataset was a limiting factor in the analysis.\nIn light of this result we opted for a supervised approach that consisted in dividing the tumours into Responders and Non Responders and searching for the genes (the drug-resistance predictive genes) that could correctly distinguish the two classes of response. I considered two datasets of patients, the dataset I including PR patients against not responders patients (NC + PD) and the dataset II with responders patients (PR and CR) against not responders patients (NC + PD).\nThe first approach, based on the software PAM (Prediction Analysis of Microarray), did not give a good prediction performance on both dataset of patients, misclassifying ca 36% of patients. Therefore, a more effective analysis in terms of classification accuracy was requested. A gene selection process based on the Support Vector Machines (SVMs) was considered a good choice in light of the characteristics of the study: low number of patients (examples) and high number of genes (or features). SVMs are a supervised learning algorithm that work well at high dimensionality, overcoming the risk of overfitting due to a number of features much larger than the numbers of examples. A specific recursively feature selection procedure based on SVMs (R-SVM) was used to select the set of genes with the lowest error of classification on the dataset of patients. Because of the small sample size, it was not possible to have a training set and a test set completely separated, so a Leave-One-Out Cross Validation (LOO-CV) procedure was used to assess the performance of the feature selection process. The analysis identified a set of 54 genes able to classify the 28 patients of the dataset I with an accuracy of 85% (4 patients misclassified on 28) and a set of 14 genes able to classify the 30 patients of the dataset II with an accuracy of 76% (7 patients misclassified on 30). The lower accuracy obtained on the dataset II was attributed to the introduction of the cCR patients in the group of Responders. The cCR patients were probably too much dissimilar in terms of clinical response in respect to the PR patients, thus rendering the group of Responders not enough homogeneous. For this reason I focused the following analysis only on the dataset I.\nThe accuracy of 85% obtained for the dataset I was an encouraging result although the small size of the dataset.\nThe biological function and cellular localization of the 54 genes was examined by using GoMiner, a web tool to find associations of Gene Ontology categories within a specific group of genes. As emerged from the analysis, there were several functional categories related to the tumourigenesis processes (\"cell adhesion\", \"insulin receptor signaling pathway\", \"cell proliferation\", \"regulation of cell proliferation\"). Some categories were more closely related to cellular processes and compartments target of the chemotherapy agents used in this study (\"cell cycle\", \"cell cycle arrest\", \"nucleus\") and to responsiveness to the treatment (\"response to hypoxia\").\nA literature research focused on each gene of the predictive signature showed that some of these genes (MYC, NUF2, SPC25; KFL5, CDKN1b, ITGA6, POSTN) are 'biologically plausible', since they have some connections with the drug resitance phenomenon investigated in this study. Others of the 54 genes are related to breast cancer progression and metastasis (CXCL9, CEBPD, IRS2, TCF8, ADAMTS5, PPARGC1A), but their direct involvement in drug resistance to paclitaxel\/anthracycline neoadjuvant chemotherapy did not emerged.\nAt this point of my analysis, I tried to find out how to use the 54 genes signature as a predictive tool of responsiveness to paclitaxel\/anthracyclines based chemotherapy treatment. On the basis of the 54 genes was trained a SVM model that could be used to classify a new patient, not yet classified, as partial responder or not responder. However, the SVM output is a value not so easily usable in statistics prediction problems. Therefore using a sigmoid function, we translated the SVM outputs into probability values that offered a more direct evaluation of the response class of the patient. In practice we transformed the SVM scores obtained for each patient of the dataset in a measure of probability, from 0 to 1, of belonging to the positive class of response (PR patients). Using the trained SVM model on a new, not-yet classified patient, it will make possible to map his SVM score on the sigmoid function and to have a corresponding probability value to belong to the positive class of response.\nThe results reported in this thesis look promising but have to be considered as preliminary, since they were obtained from a study investigating only a small number of patients and need to be validated in a completely independent test set of patients. Thus a validated gene expression signature may improve our understanding of neoadjuvant chemotherapy response mechanisms and in the future may lead to more individual, patient-tailored therapy decisions.\n\n",
                "DataExportTag": "AI228293",
                "QuestionID": "QID263",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Brea...",
                "Choices": {
                    "1": {
                        "Display": "\"Genomics and Genetic Sequencing\""
                    },
                    "2": {
                        "Display": "sequence, deoxyribonucleic_acid, genome, gene, ribonucleic_acid, genomic, motif, prediction, methylation, variant, promoter, transcription_factor, phylogenetic, nucleotide, enhancer"
                    },
                    "3": {
                        "Display": "\"Cancer Diagnosis and Gene Expression Analysis\""
                    },
                    "4": {
                        "Display": "gene, patient, cancer, tumor, biomarker, prediction, expression, gene_expression, diagnosis, signature, prognostic, analysis, survival, healthcare, cohort"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID329",
            "SecondaryAttribute": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Brea...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Breast cancer is a heterogeneous disease and markers for therapy response remain poorly defined. Since the effectiveness of treatment differs between individual patients, during the last years much effort has being invested in the identification of new markers, to estimate patients's outcome (prognostic markers) and to indicate which treatment is most effective for an individual patient (predictive markers).\nThe implementation of predictive factors in clinical setting is a big challenge of the cancer research and it will provide the opportunity to guide treatment decisions. Only patients that are likely to benefit from a specific treatment will receive this specific treatment. An individualized therapy will avoid the administration of ineffective chemotherapy that increases mortality and decreases quality of life in cancer patients.\nFor many years research has focused on the identification of single markers predicting tumour response to chemotherapy. However it is unlikely that the chemotherapy resistance\/responsiveness in breast cancer is the result of one or limited number of genes, because of the complexity of pathways involved in tumour response to chemotherapy and the heterogeneity of the individual tumours. The microarray technology made possible to study gene expression profiling of breast cancer on a global scale. It was successfully applied on the identification of breast cancer subgroups and profiles predicting patient's prognosis. More recently microarrays have been also focused on identifying gene expression profiles predicting response to chemotherapy. With the introduction of preoperative chemotherapy (neoadjuvant chemotherapy) it has become possible to directly evaluate the sensitivity of breast cancer to chemotherapy by the clinical\/pathological response of the patient to the treatment. The main goal of this thesis was to identify predictive genes of response to a specific neoadjuvant chemotherapy regimen based on paclitaxel and anthracyclines (doxorubicin and epirubicin) drugs in breast cancer patients.\nFrom 41 pre-treatment breast tumours biopsies good quality RNA was obtained and gene expression profiling was performed. Gene expression patterns of 37 patients were analyzed using Operon v2.0 70mer oligos collection at CRIBI Biotech centre and 4 patients were profiled with Operon v3.0 70mer oligos collection at Netherlands Cancer Institute. Clinical responses of 34 (out of 41) patients were recorded after administration of the neoadjuvant chemotherapy. Complete Responses (CR) to the treatment were observed in 3 patients, Partial Responses in 18 (PR) patients, No Change of the tumour mass (NC) in 11 patients and Progressive Disease (PD) in 2 patients.\nFirst of all, a correlation analysis between the ImmunoHistoChemical data of six prognostic markers (ER, PR, Erb-B2, Bcl-2, Ki-67, p53) and the gene expression data was carried out. The results showed a significant correlation for ER, PR and Bcl-2 markers. Moreover Bcl-2 status measured by ImmunoHistoChemistry (IHC) was significantly associated with the clinical response to neoadjuvant chemotherapy.\nThe molecular subtypes of 37 breast tumours analyzed with Operon v2.0 were identified using the \"intrinsic gene signature\" of Perou and colleagues. Most part of the patients were luminal-like subtype (28 of 37), 7 patients showed an erb-B2+ molecular subtype and 2 patients belonged to the basal-like group. Since it was reported that breast cancer molecular subtypes respond differently to neoadjuvant chemotherapy, I also checked how the clinical response to the treatment were associated to the molecular subtypes. From the analysis emerged that the luminal-like and erb-B2+ molecular subtypes were enriched of PR patients.\nA hierarchical cluster analysis on the pre-treatment tumours (analyzed with Operon v2.0 and with clinical response available) was performed in order to evaluate how the patients would have been separated on the basis of their gene expression profile, using an unsupervised approach. As expected, no clear separation between Responders (PR + CR) and Non Responders (NC + PD) was found. The results did not change if we included in the responder group only the PR patients. We hypothesized that the predictive genes of resistance\/sensitivity to the chemotherapy were a subtle set. The high number of differentially expressed genes would have masked the \"real\" predictive gene set, leading to a clustering of the patients based on biological parameters different from the clinical response. In addition the small size of the dataset was a limiting factor in the analysis.\nIn light of this result we opted for a supervised approach that consisted in dividing the tumours into Responders and Non Responders and searching for the genes (the drug-resistance predictive genes) that could correctly distinguish the two classes of response. I considered two datasets of patients, the dataset I including PR patients against not responders patients (NC + PD) and the dataset II with responders patients (PR and CR) against not responders patients (NC + PD).\nThe first approach, based on the software PAM (Prediction Analysis of Microarray), did not give a good prediction performance on both dataset of patients, misclassifying ca 36% of patients. Therefore, a more effective analysis in terms of classification accuracy was requested. A gene selection process based on the Support Vector Machines (SVMs) was considered a good choice in light of the characteristics of the study: low number of patients (examples) and high number of genes (or features). SVMs are a supervised learning algorithm that work well at high dimensionality, overcoming the risk of overfitting due to a number of features much larger than the numbers of examples. A specific recursively feature selection procedure based on SVMs (R-SVM) was used to select the set of genes with the lowest error of classification on the dataset of patients. Because of the small sample size, it was not possible to have a training set and a test set completely separated, so a Leave-One-Out Cross Validation (LOO-CV) procedure was used to assess the performance of the feature selection process. The analysis identified a set of 54 genes able to classify the 28 patients of the dataset I with an accuracy of 85% (4 patients misclassified on 28) and a set of 14 genes able to classify the 30 patients of the dataset II with an accuracy of 76% (7 patients misclassified on 30). The lower accuracy obtained on the dataset II was attributed to the introduction of the cCR patients in the group of Responders. The cCR patients were probably too much dissimilar in terms of clinical response in respect to the PR patients, thus rendering the group of Responders not enough homogeneous. For this reason I focused the following analysis only on the dataset I.\nThe accuracy of 85% obtained for the dataset I was an encouraging result although the small size of the dataset.\nThe biological function and cellular localization of the 54 genes was examined by using GoMiner, a web tool to find associations of Gene Ontology categories within a specific group of genes. As emerged from the analysis, there were several functional categories related to the tumourigenesis processes (\"cell adhesion\", \"insulin receptor signaling pathway\", \"cell proliferation\", \"regulation of cell proliferation\"). Some categories were more closely related to cellular processes and compartments target of the chemotherapy agents used in this study (\"cell cycle\", \"cell cycle arrest\", \"nucleus\") and to responsiveness to the treatment (\"response to hypoxia\").\nA literature research focused on each gene of the predictive signature showed that some of these genes (MYC, NUF2, SPC25; KFL5, CDKN1b, ITGA6, POSTN) are 'biologically plausible', since they have some connections with the drug resitance phenomenon investigated in this study. Others of the 54 genes are related to breast cancer progression and metastasis (CXCL9, CEBPD, IRS2, TCF8, ADAMTS5, PPARGC1A), but their direct involvement in drug resistance to paclitaxel\/anthracycline neoadjuvant chemotherapy did not emerged.\nAt this point of my analysis, I tried to find out how to use the 54 genes signature as a predictive tool of responsiveness to paclitaxel\/anthracyclines based chemotherapy treatment. On the basis of the 54 genes was trained a SVM model that could be used to classify a new patient, not yet classified, as partial responder or not responder. However, the SVM output is a value not so easily usable in statistics prediction problems. Therefore using a sigmoid function, we translated the SVM outputs into probability values that offered a more direct evaluation of the response class of the patient. In practice we transformed the SVM scores obtained for each patient of the dataset in a measure of probability, from 0 to 1, of belonging to the positive class of response (PR patients). Using the trained SVM model on a new, not-yet classified patient, it will make possible to map his SVM score on the sigmoid function and to have a corresponding probability value to belong to the positive class of response.\nThe results reported in this thesis look promising but have to be considered as preliminary, since they were obtained from a study investigating only a small number of patients and need to be validated in a completely independent test set of patients. Thus a validated gene expression signature may improve our understanding of neoadjuvant chemotherapy response mechanisms and in the future may lead to more individual, patient-tailored therapy decisions.\n\n",
                "DataExportTag": "AI228293",
                "QuestionID": "QID329",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Identification of drug-resistance predictive genes in breast cancer neoadjuvant chemotherapy Brea...",
                "Choices": {
                    "1": {
                        "Display": "\"Genomics and Genetic Sequencing\""
                    },
                    "2": {
                        "Display": "sequence, deoxyribonucleic_acid, genome, gene, ribonucleic_acid, genomic, motif, prediction, methylation, variant, promoter, transcription_factor, phylogenetic, nucleotide, enhancer"
                    },
                    "3": {
                        "Display": "\"Cancer Diagnosis and Gene Expression Analysis\""
                    },
                    "4": {
                        "Display": "gene, patient, cancer, tumor, biomarker, prediction, expression, gene_expression, diagnosis, signature, prognostic, analysis, survival, healthcare, cohort"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID115",
            "SecondaryAttribute": "Image guided surgery for the resection of brain tumours. BACKGROUND Extent of resection is believ...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Image guided surgery for the resection of brain tumours. BACKGROUND\nExtent of resection is believed to be a key prognostic factor in neuro-oncology. Image guided surgery uses a variety of tools or technologies to help achieve this goal.\u00a0It is not clear whether any of these, sometimes very expensive, tools (or their combination) should be recommended as part of standard care for patient with brain tumours. We set out to determine if image guided surgery offers any advantage in terms of extent of resection over surgery without any image guidance and if any one tool or technology was more effective.\n\n",
                "DataExportTag": "CAN5264",
                "QuestionID": "QID115",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Image guided surgery for the resection of brain tumours. BACKGROUND Extent of resection is believ...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID80",
            "SecondaryAttribute": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to develop an assortment of high added value eco-friendly and cost-effective bio-products The overall aim of this program is to improve technologies and biological tools for conversion of wastes that are abundantly discharged in EU, by eco-friendly and eco-efficient production of high added values bioproducts such as ligno-cellulolytic enzymes, fuel ethanol (EtOH), polylactic acid (PLA) and polyhydroxyalkanoates (PHAs). One of the main objectives is enlarging the patrimony of microbes for waste valorisation by i) screening the assortment of thousand microbes belonging to the collections of Brazilian and Indian partners to select the microbes most efficient in producing enzymes, EtOH, LA, PHAs ii) discovering novel strains with high potential of production of these products exploring the biodiversity of micro flora from India and Brazil and iii) exchanging technologies and tools developed by different EU and ICPC partners for genetic engineering of different microbes. Optimization of processes for production of different products and assessment of their economic and environmental feasibility will be performed with the above microbes taking advantage from the complementary consolidated expertises of the partners. Partners will have open access to complex databases developed during program implementation on the microbial patrimony of the network and the main systems, tools and technologies developed. Besides training through research performed during secondments of researchers to implement the above scientific activities, transfer of knowledge will be also achieved by summer schools, workshops and intensive virtual networking activities, covering not only scientific but also parallel sectors. Staff exchanges and networking activities between European and Indian and Brazilian research organisations will strengthen research partnerships. Dissemination of program results and best practices to external target groups to improve social acceptance and public perception of the new processes and materials and their exploitation in the market will be carried out.\n\n",
                "DataExportTag": "COR49209",
                "QuestionID": "QID80",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to...",
                "Choices": {
                    "1": {
                        "Display": "\"Biomass Energy and Waste Recycling\": waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "2": {
                        "Display": "\"Chemical Reactions and Catalysis\": catalysis, reaction, enzyme, synthesis, bond, molecule, c_h, ligand, activation, chiral, transition_metal, metal, reactivity, synthetic, asymmetric"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID172",
            "SecondaryAttribute": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to develop an assortment of high added value eco-friendly and cost-effective bio-products The overall aim of this program is to improve technologies and biological tools for conversion of wastes that are abundantly discharged in EU, by eco-friendly and eco-efficient production of high added values bioproducts such as ligno-cellulolytic enzymes, fuel ethanol (EtOH), polylactic acid (PLA) and polyhydroxyalkanoates (PHAs). One of the main objectives is enlarging the patrimony of microbes for waste valorisation by i) screening the assortment of thousand microbes belonging to the collections of Brazilian and Indian partners to select the microbes most efficient in producing enzymes, EtOH, LA, PHAs ii) discovering novel strains with high potential of production of these products exploring the biodiversity of micro flora from India and Brazil and iii) exchanging technologies and tools developed by different EU and ICPC partners for genetic engineering of different microbes. Optimization of processes for production of different products and assessment of their economic and environmental feasibility will be performed with the above microbes taking advantage from the complementary consolidated expertises of the partners. Partners will have open access to complex databases developed during program implementation on the microbial patrimony of the network and the main systems, tools and technologies developed. Besides training through research performed during secondments of researchers to implement the above scientific activities, transfer of knowledge will be also achieved by summer schools, workshops and intensive virtual networking activities, covering not only scientific but also parallel sectors. Staff exchanges and networking activities between European and Indian and Brazilian research organisations will strengthen research partnerships. Dissemination of program results and best practices to external target groups to improve social acceptance and public perception of the new processes and materials and their exploitation in the market will be carried out.\n\n",
                "DataExportTag": "COR49209",
                "QuestionID": "QID172",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to...",
                "Choices": {
                    "1": {
                        "Display": "\"Biomass Energy and Waste Recycling\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Chemical Reactions and Catalysis\""
                    },
                    "4": {
                        "Display": "catalysis, reaction, enzyme, synthesis, bond, molecule, c_h, ligand, activation, chiral, transition_metal, metal, reactivity, synthetic, asymmetric"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID233",
            "SecondaryAttribute": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to develop an assortment of high added value eco-friendly and cost-effective bio-products The overall aim of this program is to improve technologies and biological tools for conversion of wastes that are abundantly discharged in EU, by eco-friendly and eco-efficient production of high added values bioproducts such as ligno-cellulolytic enzymes, fuel ethanol (EtOH), polylactic acid (PLA) and polyhydroxyalkanoates (PHAs). One of the main objectives is enlarging the patrimony of microbes for waste valorisation by i) screening the assortment of thousand microbes belonging to the collections of Brazilian and Indian partners to select the microbes most efficient in producing enzymes, EtOH, LA, PHAs ii) discovering novel strains with high potential of production of these products exploring the biodiversity of micro flora from India and Brazil and iii) exchanging technologies and tools developed by different EU and ICPC partners for genetic engineering of different microbes. Optimization of processes for production of different products and assessment of their economic and environmental feasibility will be performed with the above microbes taking advantage from the complementary consolidated expertises of the partners. Partners will have open access to complex databases developed during program implementation on the microbial patrimony of the network and the main systems, tools and technologies developed. Besides training through research performed during secondments of researchers to implement the above scientific activities, transfer of knowledge will be also achieved by summer schools, workshops and intensive virtual networking activities, covering not only scientific but also parallel sectors. Staff exchanges and networking activities between European and Indian and Brazilian research organisations will strengthen research partnerships. Dissemination of program results and best practices to external target groups to improve social acceptance and public perception of the new processes and materials and their exploitation in the market will be carried out.\n\n",
                "DataExportTag": "COR49209",
                "QuestionID": "QID233",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Improvement of technologies and tools, e.g. biosystems and biocatalysts, for waste conversion to...",
                "Choices": {
                    "1": {
                        "Display": "\"Biomass Energy and Waste Recycling\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Chemical Reactions and Catalysis\""
                    },
                    "4": {
                        "Display": "catalysis, reaction, enzyme, synthesis, bond, molecule, c_h, ligand, activation, chiral, transition_metal, metal, reactivity, synthetic, asymmetric"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID100",
            "SecondaryAttribute": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell f...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell free oxygen radical formation, was associated with reduced T cell expression of pro-apoptotic bcl-2 family gene members (BAX and BAK), and yielded an anti-apoptotic phenotype in rapamycin-resistant T cells[12]. We found that the anti-apoptotic phenotype of rapamycin-resistant T cells was relatively optimized: that is, such T cells had similar engraftment and persistence characteristics as transgenic T cells over-expressing anti-apoptotic bcl-2. Importantly, we observed a nearly all-or-nothing phenomenon with respect to human T cell engraftment and persistence in the x-GVHD model: that is, rapamycin-resistant T cells (but not control, co-stimulated T cells) engrafted and persisted for months after adoptive transfer. In addition, such long-term engrafting, rapamycin-resistant human T cells vigorously engrafted upon transfer to secondary immune-deficient murine hosts (S. Amarnath, unpublished observation); this latter observation is consistent with a conclusion that ex vivo rapamycin promoted long-term memory T cell characteristics. These results run parallel to findings in models of malignant glioma[14] and tuberous sclerosis complex[15], where autophagy promotes tumor cell clonogenicity.\n\n",
                "DataExportTag": "28",
                "QuestionID": "QID100",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell f...",
                "Choices": {
                    "1": {
                        "Display": "We have found that post-autophagy, rapamycin-resistant T cells can be manufactured in both Th1- and Th2-polarizing conditions [16], thereby offering diverse opportunities for clinical translation. In this study, we found that ex vivo rapamycin promoted a T central memory (TCM) phenotype; this result is consistent with the known linkage of mTOR and T cell homing through modulation of KLF2 transcription factor levels[17] and with findings of the superior in vivo persistence of TCM cells [18]. An ability to polarize towards both Th1 and Th2 phenotypes in the presence of high-dose rapamycin contrasts somewhat with results obtained using T cells genetically-deficient in TORC1\/TORC2, where mTOR status was shown to influence T cell polarization [19]; and, our findings differ somewhat from studies using T cell culture without polarizing cytokine addition, where rapamycin was shown to promote TREG cell differentiation [20]. Because of the association of Th1-type cells with GVHD[21], rapamycin-resistant T cells may have application primarily in the autologous transplantation setting; along these lines, we have sponsored an Investigational New Drug Application (IND) with the U.S. Food and Drug Administration for evaluation of rapamycin-resistant Th1 cells for immune reconstitution after autologous HCT therapy of multiple myeloma (clinical trial registration, {\"type\":\"clinical-trial\",\"attrs\":{\"text\":\"NCT 01239368\",\"term_id\":\"NCT01239368\"}}NCT 01239368)."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "On the other hand, rapamycin-resistant Th2-type cells may have clinical translational application in the setting of allogeneic transplantation. We found that allogeneic murine Th2 cells manufactured ex vivo in rapamycin prevented GVHD in an IL-4 dependent manner, and when combined with T cells that induced a Th1-type response, could yield a favorable balance between GVHD protection and mediation of GVT effects[22]. Furthermore, we found that anti-apoptotic, rapamycin-resistant Th2-type cells prevented murine hematopoietic cell graft rejection [23]; in other studies, we demonstrated that host-based therapy with rapamycin-generated Th2 cells prevented graft rejection in a rat model of cardiac transplantation [24]. Based on these results, we have initiated a pilot clinical trial of rapamycin-resistant human Th2 cells in the setting of low-intensity allogeneic HCT for therapy of refractory hematologic malignancy ({\"type\":\"clinical-trial\",\"attrs\":{\"text\":\"NCT 00074490\",\"term_id\":\"NCT00074490\"}}NCT 00074490). Initial results, presented in abstract form [25, 26], indicate that this approach can promote alloengraftment and preserve anti-tumor effects with a low rate of GVHD."
                    },
                    "2": {
                        "Display": "In conclusion, through use of ex vivo methods of T cell expansion that incorporate rapamycin and polarizing cytokines, autophagy can be harnessed for the generation of Th1- or Th2-type T cells that mediate increased in vivo effects upon adoptive transfer. This flexibility in generation of cross-regulatory Th1\/Th2 subsets indicates that this approach may have application for both autologous and allogeneic transplantation. The beneficial effects of autophagy on T cell function are certainly complex, and are likely mediated at least in part through alterations in mitochondrial biology, apoptotic tendency, and effector maturation. Further studies will be required to more fully characterize the role of autophagy for enhancement of T cell function and to evaluate whether alternative pro-autophagy agents may work in a different or synergistic manner with rapamycin."
                    },
                    "3": {
                        "Display": "\"Gene Therapy and Cancer Treatment Research\""
                    },
                    "4": {
                        "Display": "mouse, cart_t_cell_therapy, vector, adenovirus, transfer, transduction, transgene, transduce, antitumor, animal, alzheimer_disease, chimeric_antigen, gfp, injection, oncolytic"
                    },
                    "5": {
                        "Display": "\"Virology and Gene Therapy Research\""
                    },
                    "6": {
                        "Display": "virus, hiv, vector, gene, epstein_barr_virus, strain, alzheimer_disease, kshv, transduction, immunodeficiency_virus, antiviral, adenovirus, kaposi_sarcoma, hcmv, latency"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID304",
            "SecondaryAttribute": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell f...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell free oxygen radical formation, was associated with reduced T cell expression of pro-apoptotic bcl-2 family gene members (BAX and BAK), and yielded an anti-apoptotic phenotype in rapamycin-resistant T cells[12]. We found that the anti-apoptotic phenotype of rapamycin-resistant T cells was relatively optimized: that is, such T cells had similar engraftment and persistence characteristics as transgenic T cells over-expressing anti-apoptotic bcl-2. Importantly, we observed a nearly all-or-nothing phenomenon with respect to human T cell engraftment and persistence in the x-GVHD model: that is, rapamycin-resistant T cells (but not control, co-stimulated T cells) engrafted and persisted for months after adoptive transfer. In addition, such long-term engrafting, rapamycin-resistant human T cells vigorously engrafted upon transfer to secondary immune-deficient murine hosts (S. Amarnath, unpublished observation); this latter observation is consistent with a conclusion that ex vivo rapamycin promoted long-term memory T cell characteristics. These results run parallel to findings in models of malignant glioma[14] and tuberous sclerosis complex[15], where autophagy promotes tumor cell clonogenicity.\n\n",
                "DataExportTag": "22",
                "QuestionID": "QID304",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell f...",
                "Choices": {
                    "1": {
                        "Display": "We have found that post-autophagy, rapamycin-resistant T cells can be manufactured in both Th1- and Th2-polarizing conditions [16], thereby offering diverse opportunities for clinical translation. In this study, we found that ex vivo rapamycin promoted a T central memory (TCM) phenotype; this result is consistent with the known linkage of mTOR and T cell homing through modulation of KLF2 transcription factor levels[17] and with findings of the superior in vivo persistence of TCM cells [18]. An ability to polarize towards both Th1 and Th2 phenotypes in the presence of high-dose rapamycin contrasts somewhat with results obtained using T cells genetically-deficient in TORC1\/TORC2, where mTOR status was shown to influence T cell polarization [19]; and, our findings differ somewhat from studies using T cell culture without polarizing cytokine addition, where rapamycin was shown to promote TREG cell differentiation [20]. Because of the association of Th1-type cells with GVHD[21], rapamycin-resistant T cells may have application primarily in the autologous transplantation setting; along these lines, we have sponsored an Investigational New Drug Application (IND) with the U.S. Food and Drug Administration for evaluation of rapamycin-resistant Th1 cells for immune reconstitution after autologous HCT therapy of multiple myeloma (clinical trial registration, {\"type\":\"clinical-trial\",\"attrs\":{\"text\":\"NCT 01239368\",\"term_id\":\"NCT01239368\"}}NCT 01239368)."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "On the other hand, rapamycin-resistant Th2-type cells may have clinical translational application in the setting of allogeneic transplantation. We found that allogeneic murine Th2 cells manufactured ex vivo in rapamycin prevented GVHD in an IL-4 dependent manner, and when combined with T cells that induced a Th1-type response, could yield a favorable balance between GVHD protection and mediation of GVT effects[22]. Furthermore, we found that anti-apoptotic, rapamycin-resistant Th2-type cells prevented murine hematopoietic cell graft rejection [23]; in other studies, we demonstrated that host-based therapy with rapamycin-generated Th2 cells prevented graft rejection in a rat model of cardiac transplantation [24]. Based on these results, we have initiated a pilot clinical trial of rapamycin-resistant human Th2 cells in the setting of low-intensity allogeneic HCT for therapy of refractory hematologic malignancy ({\"type\":\"clinical-trial\",\"attrs\":{\"text\":\"NCT 00074490\",\"term_id\":\"NCT00074490\"}}NCT 00074490). Initial results, presented in abstract form [25, 26], indicate that this approach can promote alloengraftment and preserve anti-tumor effects with a low rate of GVHD."
                    },
                    "2": {
                        "Display": "In conclusion, through use of ex vivo methods of T cell expansion that incorporate rapamycin and polarizing cytokines, autophagy can be harnessed for the generation of Th1- or Th2-type T cells that mediate increased in vivo effects upon adoptive transfer. This flexibility in generation of cross-regulatory Th1\/Th2 subsets indicates that this approach may have application for both autologous and allogeneic transplantation. The beneficial effects of autophagy on T cell function are certainly complex, and are likely mediated at least in part through alterations in mitochondrial biology, apoptotic tendency, and effector maturation. Further studies will be required to more fully characterize the role of autophagy for enhancement of T cell function and to evaluate whether alternative pro-autophagy agents may work in a different or synergistic manner with rapamycin."
                    },
                    "3": {
                        "Display": "\"Gene Therapy and Cancer Treatment Research\""
                    },
                    "4": {
                        "Display": "mouse, cart_t_cell_therapy, vector, adenovirus, transfer, transduction, transgene, transduce, antitumor, animal, alzheimer_disease, chimeric_antigen, gfp, injection, oncolytic"
                    },
                    "5": {
                        "Display": "\"Virology and Gene Therapy Research\""
                    },
                    "6": {
                        "Display": "virus, hiv, vector, gene, epstein_barr_virus, strain, alzheimer_disease, kshv, transduction, immunodeficiency_virus, antiviral, adenovirus, kaposi_sarcoma, hcmv, latency"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID374",
            "SecondaryAttribute": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell f...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell free oxygen radical formation, was associated with reduced T cell expression of pro-apoptotic bcl-2 family gene members (BAX and BAK), and yielded an anti-apoptotic phenotype in rapamycin-resistant T cells[12]. We found that the anti-apoptotic phenotype of rapamycin-resistant T cells was relatively optimized: that is, such T cells had similar engraftment and persistence characteristics as transgenic T cells over-expressing anti-apoptotic bcl-2. Importantly, we observed a nearly all-or-nothing phenomenon with respect to human T cell engraftment and persistence in the x-GVHD model: that is, rapamycin-resistant T cells (but not control, co-stimulated T cells) engrafted and persisted for months after adoptive transfer. In addition, such long-term engrafting, rapamycin-resistant human T cells vigorously engrafted upon transfer to secondary immune-deficient murine hosts (S. Amarnath, unpublished observation); this latter observation is consistent with a conclusion that ex vivo rapamycin promoted long-term memory T cell characteristics. These results run parallel to findings in models of malignant glioma[14] and tuberous sclerosis complex[15], where autophagy promotes tumor cell clonogenicity.\n\n",
                "DataExportTag": "28",
                "QuestionID": "QID374",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "In these experiments, we also observed that ex vivo manufacturing in rapamycin decreased T cell f...",
                "Choices": {
                    "1": {
                        "Display": "We have found that post-autophagy, rapamycin-resistant T cells can be manufactured in both Th1- and Th2-polarizing conditions [16], thereby offering diverse opportunities for clinical translation. In this study, we found that ex vivo rapamycin promoted a T central memory (TCM) phenotype; this result is consistent with the known linkage of mTOR and T cell homing through modulation of KLF2 transcription factor levels[17] and with findings of the superior in vivo persistence of TCM cells [18]. An ability to polarize towards both Th1 and Th2 phenotypes in the presence of high-dose rapamycin contrasts somewhat with results obtained using T cells genetically-deficient in TORC1\/TORC2, where mTOR status was shown to influence T cell polarization [19]; and, our findings differ somewhat from studies using T cell culture without polarizing cytokine addition, where rapamycin was shown to promote TREG cell differentiation [20]. Because of the association of Th1-type cells with GVHD[21], rapamycin-resistant T cells may have application primarily in the autologous transplantation setting; along these lines, we have sponsored an Investigational New Drug Application (IND) with the U.S. Food and Drug Administration for evaluation of rapamycin-resistant Th1 cells for immune reconstitution after autologous HCT therapy of multiple myeloma (clinical trial registration, {\"type\":\"clinical-trial\",\"attrs\":{\"text\":\"NCT 01239368\",\"term_id\":\"NCT01239368\"}}NCT 01239368)."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "On the other hand, rapamycin-resistant Th2-type cells may have clinical translational application in the setting of allogeneic transplantation. We found that allogeneic murine Th2 cells manufactured ex vivo in rapamycin prevented GVHD in an IL-4 dependent manner, and when combined with T cells that induced a Th1-type response, could yield a favorable balance between GVHD protection and mediation of GVT effects[22]. Furthermore, we found that anti-apoptotic, rapamycin-resistant Th2-type cells prevented murine hematopoietic cell graft rejection [23]; in other studies, we demonstrated that host-based therapy with rapamycin-generated Th2 cells prevented graft rejection in a rat model of cardiac transplantation [24]. Based on these results, we have initiated a pilot clinical trial of rapamycin-resistant human Th2 cells in the setting of low-intensity allogeneic HCT for therapy of refractory hematologic malignancy ({\"type\":\"clinical-trial\",\"attrs\":{\"text\":\"NCT 00074490\",\"term_id\":\"NCT00074490\"}}NCT 00074490). Initial results, presented in abstract form [25, 26], indicate that this approach can promote alloengraftment and preserve anti-tumor effects with a low rate of GVHD."
                    },
                    "2": {
                        "Display": "In conclusion, through use of ex vivo methods of T cell expansion that incorporate rapamycin and polarizing cytokines, autophagy can be harnessed for the generation of Th1- or Th2-type T cells that mediate increased in vivo effects upon adoptive transfer. This flexibility in generation of cross-regulatory Th1\/Th2 subsets indicates that this approach may have application for both autologous and allogeneic transplantation. The beneficial effects of autophagy on T cell function are certainly complex, and are likely mediated at least in part through alterations in mitochondrial biology, apoptotic tendency, and effector maturation. Further studies will be required to more fully characterize the role of autophagy for enhancement of T cell function and to evaluate whether alternative pro-autophagy agents may work in a different or synergistic manner with rapamycin."
                    },
                    "3": {
                        "Display": "\"Inflammatory Response and Immune System Regulation\""
                    },
                    "4": {
                        "Display": "macrophage, neutrophil, monocyte, inflammation, chemokine, inflammatory, mouse, leukocyte, endothelial, cytokine, inflammasome, pmn, tlr, toll_receptor, e_selectin"
                    },
                    "5": {
                        "Display": "\"Neurological Injury and Treatment Research\""
                    },
                    "6": {
                        "Display": "cerebral, injury, rat, central_nervous_system, microglia, spinal_cord, astrocyte, neuronal, eae, stroke, retinal, tbi, cerebral_ischemia, transplant, mass_spectrometry"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID203",
            "SecondaryAttribute": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X 214. ACKERMANN, WILHELM : See Hermes, Hans. ADAN, S. I . : Unsolvability of algorithmic problems in theory of groups (Russian) X X X 391. See Novikov, P. S., and S. I. Adan. ADDISON, J . W.: Separation principles in .hierarchies of classical and effective descriptive set theory X X I X 60. Theory of hierarchies X X I X 60. Problems of hierarchy theory X X I X 61. AJDUKIEWICZ, Kazimierz: See Kotarbinski, Tadeusz. [Ajdukiewicz ( X X X 110).] ALDRICH, Virgil C : Review of Ziff ( X X X 259). ALLEN, Layman E. : Symbolic logic X X I X 43. Propositional calculi X X I X 44. Alethic logic X X I X 44. Deontic logic X X I X 44. Simplifying the reiteration rule X X I X 44. Use of definitions to justify steps in proofs X X I X 44. Wff 'n proof X X X 105. Wff X X X 105. See Allen, Layman E., and Gabriel Orechkoff. ALLEN, Layman E., and Gabriel Orechkoff: More systematic drafting and interpreting of internal revenue code X X I X 44. ALSTON, William P . : Review of Ziff ( X X I X 216). AMEMIYA, Ichiro, and Israel Halperin: Complemented modular lattices ( X X X 402). ANDERSON, Alan Ross, and Nuel D. Belnap, J r . : Pure calculus of entailment X X X 240. Review by Orlovskij (Russian) ( X X X 259). ANDERSON, John M., and Henry W. Johnstone, J r . : Natural deduction X X I X 93. Review by Cameron ( X X X 401). ANDREE, Richard V.: Selections from modern abstract algebra ( X X I X 148). ANDREWS, P . : Reduction of axioms for theory of propositional types X X X 385. A P E L , Karl Otto: Sprache und Wahrheit ( X X X 111). APOSTEL, Leo: Game theory and interpretation of deontic logic X X X 242. Structure et gdnese ( X X X 402). ARNOLD, B. H. : Logic and Boolean algebra X X I X 95. ASMUS, V. F . : Intuition in philosophy and mathematics (Russian) ( X X X 258). ASSER, Giinter: Einfuhrung in mathematische Logik I, XXV 276. Review by Czajsner (Polish) ( X X I X 217). Review of Kobrinskij and Trahtenbrot ( X X I X 113). Review of Carnap ( X X I X 149). Rekursive Wortfunktionen X X I X 199. Review of Trahtenbrot ( X X X 259). AUSTIN, J . L.: Ifs and cans X X I I I 74. Reprinted ( X X X 110). AVEY, Albert E. : Recent schools of logic X X I V 184. Reprinted ( X X I X 67). AXT, Paul: 3-recursive functions X X I X 199. Enumeration and Grzegorczyk hierarchy X X X 90. AYER, A. J . : Language t ru th and logic (X 134). Excerpt therefrom (Elimination of metaphysics) ( X X X 110). Names and descriptions X X I X 197. Discussion by Russell, Dopp, Findlay, and Ayer X X I X 197. BAGEMIHL, F . : Review of Borel ( X X I X 217). BANKS, P. (pseudonym): Philosophical interpretation of logic XXVII 116. Reprinted X X X 363. Reviews by Lejewski ( X X X 400), Kamihski (Polish) ( X X X 400). B A R H I L L E L , Yehoshua: On syntactical categories XV 220. Reprinted X X X 382. Linguistic problems connected with machine translation X X 192. Partially reprinted (Intertranslatability of natural languages) X X X 383. Notation for syntactic description X X 193. Reprinted X X X 383. Logical syntax and semantics XX 290.\n\n",
                "DataExportTag": "AI546540",
                "QuestionID": "QID203",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X...",
                "Choices": {
                    "1": {
                        "Display": "\"Genomics and Genetic Sequencing\""
                    },
                    "2": {
                        "Display": "sequence, deoxyribonucleic_acid, genome, gene, ribonucleic_acid, genomic, motif, prediction, methylation, variant, promoter, transcription_factor, phylogenetic, nucleotide, enhancer"
                    },
                    "3": {
                        "Display": "\"Cancer Diagnosis and Gene Expression Analysis\""
                    },
                    "4": {
                        "Display": "gene, patient, cancer, tumor, biomarker, prediction, expression, gene_expression, diagnosis, signature, prognostic, analysis, survival, healthcare, cohort"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID264",
            "SecondaryAttribute": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X 214. ACKERMANN, WILHELM : See Hermes, Hans. ADAN, S. I . : Unsolvability of algorithmic problems in theory of groups (Russian) X X X 391. See Novikov, P. S., and S. I. Adan. ADDISON, J . W.: Separation principles in .hierarchies of classical and effective descriptive set theory X X I X 60. Theory of hierarchies X X I X 60. Problems of hierarchy theory X X I X 61. AJDUKIEWICZ, Kazimierz: See Kotarbinski, Tadeusz. [Ajdukiewicz ( X X X 110).] ALDRICH, Virgil C : Review of Ziff ( X X X 259). ALLEN, Layman E. : Symbolic logic X X I X 43. Propositional calculi X X I X 44. Alethic logic X X I X 44. Deontic logic X X I X 44. Simplifying the reiteration rule X X I X 44. Use of definitions to justify steps in proofs X X I X 44. Wff 'n proof X X X 105. Wff X X X 105. See Allen, Layman E., and Gabriel Orechkoff. ALLEN, Layman E., and Gabriel Orechkoff: More systematic drafting and interpreting of internal revenue code X X I X 44. ALSTON, William P . : Review of Ziff ( X X I X 216). AMEMIYA, Ichiro, and Israel Halperin: Complemented modular lattices ( X X X 402). ANDERSON, Alan Ross, and Nuel D. Belnap, J r . : Pure calculus of entailment X X X 240. Review by Orlovskij (Russian) ( X X X 259). ANDERSON, John M., and Henry W. Johnstone, J r . : Natural deduction X X I X 93. Review by Cameron ( X X X 401). ANDREE, Richard V.: Selections from modern abstract algebra ( X X I X 148). ANDREWS, P . : Reduction of axioms for theory of propositional types X X X 385. A P E L , Karl Otto: Sprache und Wahrheit ( X X X 111). APOSTEL, Leo: Game theory and interpretation of deontic logic X X X 242. Structure et gdnese ( X X X 402). ARNOLD, B. H. : Logic and Boolean algebra X X I X 95. ASMUS, V. F . : Intuition in philosophy and mathematics (Russian) ( X X X 258). ASSER, Giinter: Einfuhrung in mathematische Logik I, XXV 276. Review by Czajsner (Polish) ( X X I X 217). Review of Kobrinskij and Trahtenbrot ( X X I X 113). Review of Carnap ( X X I X 149). Rekursive Wortfunktionen X X I X 199. Review of Trahtenbrot ( X X X 259). AUSTIN, J . L.: Ifs and cans X X I I I 74. Reprinted ( X X X 110). AVEY, Albert E. : Recent schools of logic X X I V 184. Reprinted ( X X I X 67). AXT, Paul: 3-recursive functions X X I X 199. Enumeration and Grzegorczyk hierarchy X X X 90. AYER, A. J . : Language t ru th and logic (X 134). Excerpt therefrom (Elimination of metaphysics) ( X X X 110). Names and descriptions X X I X 197. Discussion by Russell, Dopp, Findlay, and Ayer X X I X 197. BAGEMIHL, F . : Review of Borel ( X X I X 217). BANKS, P. (pseudonym): Philosophical interpretation of logic XXVII 116. Reprinted X X X 363. Reviews by Lejewski ( X X X 400), Kamihski (Polish) ( X X X 400). B A R H I L L E L , Yehoshua: On syntactical categories XV 220. Reprinted X X X 382. Linguistic problems connected with machine translation X X 192. Partially reprinted (Intertranslatability of natural languages) X X X 383. Notation for syntactic description X X 193. Reprinted X X X 383. Logical syntax and semantics XX 290.\n\n",
                "DataExportTag": "AI546540",
                "QuestionID": "QID264",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X...",
                "Choices": {
                    "1": {
                        "Display": "\"Genomics and Genetic Sequencing\""
                    },
                    "2": {
                        "Display": "sequence, deoxyribonucleic_acid, genome, gene, ribonucleic_acid, genomic, motif, prediction, methylation, variant, promoter, transcription_factor, phylogenetic, nucleotide, enhancer"
                    },
                    "3": {
                        "Display": "\"Cancer Diagnosis and Gene Expression Analysis\""
                    },
                    "4": {
                        "Display": "gene, patient, cancer, tumor, biomarker, prediction, expression, gene_expression, diagnosis, signature, prognostic, analysis, survival, healthcare, cohort"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID330",
            "SecondaryAttribute": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X 214. ACKERMANN, WILHELM : See Hermes, Hans. ADAN, S. I . : Unsolvability of algorithmic problems in theory of groups (Russian) X X X 391. See Novikov, P. S., and S. I. Adan. ADDISON, J . W.: Separation principles in .hierarchies of classical and effective descriptive set theory X X I X 60. Theory of hierarchies X X I X 60. Problems of hierarchy theory X X I X 61. AJDUKIEWICZ, Kazimierz: See Kotarbinski, Tadeusz. [Ajdukiewicz ( X X X 110).] ALDRICH, Virgil C : Review of Ziff ( X X X 259). ALLEN, Layman E. : Symbolic logic X X I X 43. Propositional calculi X X I X 44. Alethic logic X X I X 44. Deontic logic X X I X 44. Simplifying the reiteration rule X X I X 44. Use of definitions to justify steps in proofs X X I X 44. Wff 'n proof X X X 105. Wff X X X 105. See Allen, Layman E., and Gabriel Orechkoff. ALLEN, Layman E., and Gabriel Orechkoff: More systematic drafting and interpreting of internal revenue code X X I X 44. ALSTON, William P . : Review of Ziff ( X X I X 216). AMEMIYA, Ichiro, and Israel Halperin: Complemented modular lattices ( X X X 402). ANDERSON, Alan Ross, and Nuel D. Belnap, J r . : Pure calculus of entailment X X X 240. Review by Orlovskij (Russian) ( X X X 259). ANDERSON, John M., and Henry W. Johnstone, J r . : Natural deduction X X I X 93. Review by Cameron ( X X X 401). ANDREE, Richard V.: Selections from modern abstract algebra ( X X I X 148). ANDREWS, P . : Reduction of axioms for theory of propositional types X X X 385. A P E L , Karl Otto: Sprache und Wahrheit ( X X X 111). APOSTEL, Leo: Game theory and interpretation of deontic logic X X X 242. Structure et gdnese ( X X X 402). ARNOLD, B. H. : Logic and Boolean algebra X X I X 95. ASMUS, V. F . : Intuition in philosophy and mathematics (Russian) ( X X X 258). ASSER, Giinter: Einfuhrung in mathematische Logik I, XXV 276. Review by Czajsner (Polish) ( X X I X 217). Review of Kobrinskij and Trahtenbrot ( X X I X 113). Review of Carnap ( X X I X 149). Rekursive Wortfunktionen X X I X 199. Review of Trahtenbrot ( X X X 259). AUSTIN, J . L.: Ifs and cans X X I I I 74. Reprinted ( X X X 110). AVEY, Albert E. : Recent schools of logic X X I V 184. Reprinted ( X X I X 67). AXT, Paul: 3-recursive functions X X I X 199. Enumeration and Grzegorczyk hierarchy X X X 90. AYER, A. J . : Language t ru th and logic (X 134). Excerpt therefrom (Elimination of metaphysics) ( X X X 110). Names and descriptions X X I X 197. Discussion by Russell, Dopp, Findlay, and Ayer X X I X 197. BAGEMIHL, F . : Review of Borel ( X X I X 217). BANKS, P. (pseudonym): Philosophical interpretation of logic XXVII 116. Reprinted X X X 363. Reviews by Lejewski ( X X X 400), Kamihski (Polish) ( X X X 400). B A R H I L L E L , Yehoshua: On syntactical categories XV 220. Reprinted X X X 382. Linguistic problems connected with machine translation X X 192. Partially reprinted (Intertranslatability of natural languages) X X X 383. Notation for syntactic description X X 193. Reprinted X X X 383. Logical syntax and semantics XX 290.\n\n",
                "DataExportTag": "AI546540",
                "QuestionID": "QID330",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Index of Reviews ABLONSKIJ, S. V.: Functional constructions in ft-valued logic (Russian) X X I X...",
                "Choices": {
                    "1": {
                        "Display": "\"Genomics and Genetic Sequencing\""
                    },
                    "2": {
                        "Display": "sequence, deoxyribonucleic_acid, genome, gene, ribonucleic_acid, genomic, motif, prediction, methylation, variant, promoter, transcription_factor, phylogenetic, nucleotide, enhancer"
                    },
                    "3": {
                        "Display": "\"Cancer Diagnosis and Gene Expression Analysis\""
                    },
                    "4": {
                        "Display": "gene, patient, cancer, tumor, biomarker, prediction, expression, gene_expression, diagnosis, signature, prognostic, analysis, survival, healthcare, cohort"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID406",
            "SecondaryAttribute": "INING IN H YPERMEDIA C ASE L IBRARIES The idea of case-based reasoning (CBR), came as a welcome a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "INING IN H YPERMEDIA C ASE L IBRARIES The idea of case-based reasoning (CBR), came as a welcome alternative to the rule-base paradigm, which was dominating knowledge-based design during the 80s. Attribute-value tables and object-oriented representations were the dominating approaches to case representation. Hypermedia added to the case representation a collection of \"natural\" descriptions represented as text in free or table format and other multimedia data, such as images, video, sound, etc. However, reasoning algorithms didn't take the advantage of this representation. In this paper, we present the idea of employing data mining techniques for extracting additional information from hypermedia cases and incorporating the knowledge into the case-based model. 1. Case-based design using informal representations to manage complexity Since the early seventies considerable research in artificial intelligence (AI) has been focused on the implementation of various knowledge-based computing models, which was inspired by a common initial idea, namely, to formalise and represent human expertise in machine-readable form, and employ it in computer support of human problem solving activities. Rule-based descriptions of human expertise, although difficult to obtain, and rule-based inference techniques were dominating the area during the 80s. Rule-based systems tended to be used to represent generalised expertise, or \u201crules-of-thumb\u201d. The idea of using analogy, recalling previous experience, came as a welcome alternative to the rule-base paradigm. Case-based reasoning (CBR) is a technique which implements this idea. A \"case\" represents the experience accumulated during the solution of relevant problems in the past (Aamodt and Plaza, 1994). The collection of data forms the case base or case library. Past experience can be described in a variety of forms, which span from a row or a number of rows in a database table to a number of volumes of documentation. Considering computer implementations of the CBR paradigm, the representation of cases requires an abstraction of the experience into a form that can be manipulated by the reasoner, where the reasoner comprises procedural or heuristic modules for retrieving and selecting relevant cases and for adapting a selected case for a new problem. Sometimes the reasoner is assumed to be the user, rather than a computational process. What exactly is denoted by a case and how it is represented are major stuctural issues in CBR. The application of case-based reasoning to structural design (Maher et al., 1995), has shown that in the early systems, attribute-value pairs and object-oriented representations were the dominating approaches to case representation. Case models based on hypermedia representations are alternatives to the strict format of object-oriented and attribute-value representations. The hypermedia representations comprise a collection of \"natural\" descriptions represented as text in free or table format and other multimedia data, such as images, video, sound, etc. Another characteristic of hypermedia is the use of links, where the links can connect information within a case, between different cases, or links to data that lies outside the case library. Case-based reasoning, as a design process, is illustrated in Figure 1. A case library provides several examples of designs and the basis for finding relevant designs to a new design problem. A new design problem provides some information that serves as the basis for recalling one or more design cases. A selected design case can then be adapted to be a new design. The resulting new design can be added to the case library, allowing the library to grow with use. This accumulation of experience is considered as the machine learning part of case-based design computing model. Case-based design, as an information rich process, readily made the shift towards hypermedia case representations. However, reasoning algorithms are based on attribute-value representations. Therefore, hypermedia design cases include an additional structured layer, and case indexing and selection is still based on the comparison of attribute values. Following this paradigm, we have developed a hypermedia case library of buildings that focus on structural design. The library is referred to as SAM, for its use in teaching Structures And Materials to undergraduate architecture students. In developing SAM, we consider the issues raised by the need to organise the material within a multimedia case library of structural designs, while presenting the material using multimedia. Specifically, we consider: \u2022 the need to represent and manage complex design cases, \u2022 the need to formalise a typically informal body of knowledge or experiences. CASE LIBRARY Design Case Recall Design Case Adaptation New Design Problem New Design Solution Accumulating experience Figure 1. Design process using case-based reasoning Design in any domain usually involves the development and understanding of complex systems. The complex representations needed to adequately capture a design case have introduced challenges to CBR systems. As mentioned earlier, the CBR paradigm assumes that there is a concept of \"a case\", but in most design domains this concept accommodates a set of experiences and decisions resulting in a complex system. Three approaches to addressing complexity are: 1. a scase is a hierarchy of concepts, or subcases 2. a case is represented by different views 3. a case is presented as hypermedia A general approach to addressing domain complexity is the representation and reuse of parts of cases, typically organised as hierarchies of \u201csubcases.\u201d This supports case-based reasoning because subdividing designs in this way allows reasoning to focus only on the relevant parts of a design. By processing only some of the knowledge associated with a case, reasoning can become more efficient. The development of a case-base that has a hierarchical structure usually requires defining a typical decomposition of a design experience. The use of different views of a design case recognises that a design can be understood from different perspectives. In this approach, a single, complex design project is represented as multiple cases. The use of multimedia can make it easier to understand complex systems icons, images, sketches, etc. can highlight and illustrate corresponding text or tabular information. The lack of formal knowledge in design affects both the ability to define a formal and consistent representation of design cases and the role of adaptation as a human-centred activity or an automated process. The development of CBR for design domains in which there is little formal or theoretical knowledge has been pursued by either formalising knowledge that previously was not formalised, eg by using an object-oriented representation, or by identifying a representation of the design cases to support human reasoning rather than automated reasoning, eg by creating a multimedia presentation. Resolving the issue of \u201cwhat is in a design case?\u201d is done in many different ways. Contrary to the initial observation that case acquisition should be straightforward, most design stories told by designers or found in design documents are not easily formed into cases that can be indexed and classified for reuse. A systematic approach is needed to identify a uniform representation and to parse design stories into these formats. The representation of design cases in SAM follows the structural design principals that are taught in the Structures and Materials course. The overall organisation of design information falls into three categories: 1. project information, 2. functional decomposition of the structural design, and 3. structural system types. These three categories are reflected in the navigation aids provided on each page as links to other parts of the case description. Figure 2 shows the navigational part of a typical presentation of the case description. The functional decomposition of the structure is reflected in the links to the vertical load system, the lateral load system, and the footings. The structural systems types are shown as links along the third row, in Capita Centre the primary structural systems used are truss, floor, and core. SAM Case Library: Capita Centre | Case Navigator Figure 2. Front page of Capita Centre in Sydney The project information is presented on the \"front page\" of each case, providing a tabular overview of the project. The information includes: Project data this describes the general information about the building including the people involved and the overall geometry. Design requirements this describes the context in which the building was designed considering the city planning issues, the architect\u2019s decisions about the overall shape and function of the building, the specifications for load and foundation conditions, etc. This information represents the considerations in the transition from the design brief to the structural design requirements. Design alternatives and solution this describes the different structural solutions considered by the engineers and the reasons for selecting the one implemented. Justification this describes the reasons for many of decisions that resulted in the final structural system. References acknowledges the sources of information. The functional decomposition of the structural design is reflected in the set of \"pages\" that refer to the vertical, lateral, and footings systems. Each \"page\" further decomposes the information related to each functional system, for example, the vertical load systems can be further decomposed into gravitational and uplift load systems. Structural efficiency issues are dealt with at a number of levels load transfer strategy, load paths, and structural actions, as illustrated for the lateral load resisting system in the Capita Centre in Figure 3. SAM Case Library: Capita Centre | Lateral Load Resisting System Figure 3. Lateral load resisting system in the Syd\n\n",
                "DataExportTag": "AI44150",
                "QuestionID": "QID406",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "INING IN H YPERMEDIA C ASE L IBRARIES The idea of case-based reasoning (CBR), came as a welcome a...",
                "Choices": {
                    "1": {
                        "Display": "\"Edge Computing and Deployment of Lightweight DNNs\""
                    },
                    "2": {
                        "Display": "dnn, deep_learning, pruning, energy, inference, gpu, dnns, deployment, mobile, automl, accelerator, deploy, latency, lightweight, edge"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and GPU Optimization\""
                    },
                    "4": {
                        "Display": "deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID352",
            "SecondaryAttribute": "Innovation Management Capacity Enhancement of SMEs in Saxony-Anhalt for the period 2020-21 \"The g...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Innovation Management Capacity Enhancement of SMEs in Saxony-Anhalt for the period 2020-21 \"The general objective of the described actions is to contribute to smart growth in the European Union by increasing theefficiency and effectiveness of investments in research, development and innovation and by contributing to a better andmore efficient connection of actors along the innovation chain.The specific action will deliver services designed to \"\"Enhance the Innovation management capacity of SMEs\"\" by providingindepth support services to SMEs. Furthermore, beneficiaries of the SME Instrument under Horizon 2020 will receivespecific targeted assistance to support coaching assignments addressing specific innovation weaknesses.To achieve the above mentioned objectives bottlenecks to the creation of economic impact in companies benefitting fromgrants by Horizon2020's SME instrument will be identified and these bottlenecks with the help of capable coachesaddressed. This service to beneficiaries of the SME instrument in the context of a giving project is called 'Key accountmanagement'. In addition the efficiency and effectiveness of innovation processes will be increased in small and mediumsized enterprises with significant innovation activities that could potentially become beneficiaries of European support toresearch and innovation but would not have effective access to consulting services for innovation management. This service,provided independently from a specific innovation project and focussing on a company's innovation management system, iscalled 'Enhancing the innovation management capacity of SMEs'.Direct impact from the two actions will be a significantly enhanced growth and profitability of SMEs receiving the services ascompared to a control group. Innovation processes will be conducted more efficient and more effectively.Indirect impact will arise from the introduction of high quality innovation management capacity assessment and supportservices in the region of Saxony-Anhalt.\"\n\n",
                "DataExportTag": "COR42382",
                "QuestionID": "QID352",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Innovation Management Capacity Enhancement of SMEs in Saxony-Anhalt for the period 2020-21 \"The g...",
                "Choices": {
                    "1": {
                        "Display": "\"Mobile Banking and E-commerce\""
                    },
                    "2": {
                        "Display": "mobile, smartphone, payment, ip, blockchain, biometric, purchase, store, transaction, insurance, retail, bank, retailer, artificial_intelligence, e_commerce"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and Cloud Systems\""
                    },
                    "4": {
                        "Display": "computation, heterogeneous, hpc, code, hardware, cps, processor, programming, cryptography, storage, layer, machine_learning, cloud_computing, compute, exascale"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID348",
            "SecondaryAttribute": "Innovative training network: Immunoprofile-directed stratification of patients with the autoimmun...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Innovative training network: Immunoprofile-directed stratification of patients with the autoimmune disorder thrombotic thrombocytopenic purpura The PROFILE Innovative European Industrial Doctoral programme will develop improved tools for stratification of patients with the autoimmune disorder acquired thrombotic thrombocytopenic purpura (TTP). Patient stratification is needed in order to further develop personalized medicine approaches for human disorders. Therefore, a detailed understanding of the disease at the molecular level is needed. Furthermore, implementation of a robust set of novel biomarkers is essential for stratification of patients. Also, highly flexible platforms to rapidly develop novel therapeutics are crucial to further boost the field of personalized medicine. The research program is embedded in a unique translational training program, which emphasizes clinical needs as a driving force for development of novel, innovative diagnostics and therapeutics (for personalized medicine). There is intersectoral knowledge transfer between academia and companies at the European level as 6 different EU Member states are involved in the network either as beneficiary or partner organization. The PROFILE network will allow 6 early stage researchers to follow this unique translational training. They will be trained in the clinic to identify clinical needs, in academia where they will contribute to a scientifically ambitious project and in the non-academic sector where they will be trained in a novel technologies and commercializing bioassays endowing them with an unique PROFILE for further advancing personalized medicine in Europe.\n\n",
                "DataExportTag": "COR15895",
                "QuestionID": "QID348",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Innovative training network: Immunoprofile-directed stratification of patients with the autoimmun...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Drug Development for Rare Diseases\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Nanomedicine and Drug Delivery Systems\""
                    },
                    "4": {
                        "Display": "drug, nanoparticle, compound, cellular, drug_delivery, therapeutic, drug_discovery, vivo, nanomedicine, toxicity, peptide, antibody, chemical, molecule, formulation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID171",
            "SecondaryAttribute": "Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean Prim...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean Primary production in the oceans is strongly limited by the availability of fixed nitrogen. In open ocean nutrient-impoverished areas, which make up ~50% of the global ocean surface, nitrogen is mainly provided through the process of biological atmospheric nitrogen (N2) fixation. N2 fixation is carried out by the so termed diazotrophs, marine microorganisms that may belong to the cyanobacteria, bacteria or archaea. For many years, autotrophic diazotrophs were thought to be the most abundant diazotrophs in the ocean. Autotrophic diazotrophs need light to fix carbon dioxide via photosynthesis, and therefore are constrained to the sunlit layer of the ocean, which is generally less than 100 m deep. Recent investigations have revealed that heterotrophic diazotrophs, which cannot photosynthesize, are present in greater abundance than autotrophic diazotrophs in the world\u00e2\u20ac\u2122s oceans. Heterotrophic diazotrophs are not constrained by the availability of light and therefore are able to live in the dark ocean, the largest and less studied habitat on Earth. This discovery significantly expands the boundaries where N2 fixation was though to be possible and theoretically increases the inputs of fixed nitrogen to the ocean, which remain unaccounted for. Because they are not photosynthetic, heterotrophic diazotrophs need an external source of dissolved organic matter (DOM) for their nutrition. However, the nature of this DOM and how it influences their activity is largely unknown. This project aims to cover this gap by studying their relationship with DOM in the ocean. Through shipboard experiments and use of cutting-edge analytical techniques we will explore the spatial distribution of heterotrophic diazotrophs\u00e2\u20ac\u2122 abundance, diversity and N2 fixation activity related to the in situ concentration and composition of DOM. The results will provide unique insights into the ecology of heterotrophic diazotrophs and their role in the oceanic nitrogen cycle.\n\n",
                "DataExportTag": "COR7096",
                "QuestionID": "QID171",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean Prim...",
                "Choices": {
                    "1": {
                        "Display": "\"Polymer and Composite Material Science\""
                    },
                    "2": {
                        "Display": "polymer, plastic, composite, textile, packaging, coating, fibre, film, printing, ink, adhesive, resin, surface, composite_material, biodegradable"
                    },
                    "3": {
                        "Display": "\"Environmental Science and Climate Change\""
                    },
                    "4": {
                        "Display": "soil, carbon_dioxide, microbial, ocean, carbon, bacterial, fate, sediment, flux, metabolic, atmospheric, microplastic, plant, climate, metal"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID232",
            "SecondaryAttribute": "Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean Prim...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean Primary production in the oceans is strongly limited by the availability of fixed nitrogen. In open ocean nutrient-impoverished areas, which make up ~50% of the global ocean surface, nitrogen is mainly provided through the process of biological atmospheric nitrogen (N2) fixation. N2 fixation is carried out by the so termed diazotrophs, marine microorganisms that may belong to the cyanobacteria, bacteria or archaea. For many years, autotrophic diazotrophs were thought to be the most abundant diazotrophs in the ocean. Autotrophic diazotrophs need light to fix carbon dioxide via photosynthesis, and therefore are constrained to the sunlit layer of the ocean, which is generally less than 100 m deep. Recent investigations have revealed that heterotrophic diazotrophs, which cannot photosynthesize, are present in greater abundance than autotrophic diazotrophs in the world\u00e2\u20ac\u2122s oceans. Heterotrophic diazotrophs are not constrained by the availability of light and therefore are able to live in the dark ocean, the largest and less studied habitat on Earth. This discovery significantly expands the boundaries where N2 fixation was though to be possible and theoretically increases the inputs of fixed nitrogen to the ocean, which remain unaccounted for. Because they are not photosynthetic, heterotrophic diazotrophs need an external source of dissolved organic matter (DOM) for their nutrition. However, the nature of this DOM and how it influences their activity is largely unknown. This project aims to cover this gap by studying their relationship with DOM in the ocean. Through shipboard experiments and use of cutting-edge analytical techniques we will explore the spatial distribution of heterotrophic diazotrophs\u00e2\u20ac\u2122 abundance, diversity and N2 fixation activity related to the in situ concentration and composition of DOM. The results will provide unique insights into the ecology of heterotrophic diazotrophs and their role in the oceanic nitrogen cycle.\n\n",
                "DataExportTag": "COR7096",
                "QuestionID": "QID232",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Interactions between nitrogen-fixers (diazotrophs) and dissolved organic matter in the ocean Prim...",
                "Choices": {
                    "1": {
                        "Display": "\"Polymer and Composite Material Science\""
                    },
                    "2": {
                        "Display": "polymer, plastic, composite, textile, packaging, coating, fibre, film, printing, ink, adhesive, resin, surface, composite_material, biodegradable"
                    },
                    "3": {
                        "Display": "\"Environmental Science and Climate Change\""
                    },
                    "4": {
                        "Display": "soil, carbon_dioxide, microbial, ocean, carbon, bacterial, fate, sediment, flux, metabolic, atmospheric, microplastic, plant, climate, metal"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID125",
            "SecondaryAttribute": "INTERVENTIONS Patients were assigned to undergo laparoscopic peritoneal lavage (n\u2009=\u2009101) or colo",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "INTERVENTIONS\nPatients were assigned to undergo laparoscopic peritoneal lavage (n\u2009=\u2009101) or colon resection (n\u2009=\u200998) based on a computer-generated, center-stratified block randomization. All patients with fecal peritonitis (15 patients in the laparoscopic peritoneal lavage group vs 13 in the colon resection group) underwent colon resection. Patients with a pathology requiring treatment beyond that necessary for perforated diverticulitis (12 in the laparoscopic lavage group vs 13 in the colon resection group) were also excluded from the protocol operations and treated as required for the pathology encountered.\n\n",
                "DataExportTag": "53",
                "QuestionID": "QID125",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "INTERVENTIONS Patients were assigned to undergo laparoscopic peritoneal lavage (n\u2009=\u2009101) or colon...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID146",
            "SecondaryAttribute": "INTRODUCTION CancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated t...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "INTRODUCTION\nCancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated to provide quality care to those who have been diagnosed and are living with cancer. MacCharles Chemotherapy unit is specially built to provide chemotherapy treatment to the cancer patients of Winnipeg. In order to maintain an excellent service, it tries to ensure that patients get their treatment in a timely manner. It is challenging to maintain that goal because of the lack of a proper roster, the workload distribution and inefficient resource allotment. In order to maintain the satisfaction of the patients and the healthcare providers, by serving the maximum number of patients in a timely manner, it is necessary to develop an efficient scheduling template that matches the required demand with the availability of resources. This goal can be reached using simulation modelling. Simulation has proven to be an excellent modelling tool. It can be defined as building computer models that represent real world or hypothetical systems, and hence experimenting with these models to study system behaviour under different scenarios.1, 2 A study was undertaken at the Children's Hospital of Eastern Ontario to identify the issues behind the long waiting time of a emergency room.3 A 20---day field observation revealed that the availability of the staff physician and interaction affects the patient wait time. Jyv\u00e4skyl\u00e4 et al.4 used simulation to test different process scenarios, allocate resources and perform activity---based cost analysis in the Emergency Department (ED) at the Central Hospital. The simulation also supported the study of a new operational method, named \"triage-team\" method without interrupting the main system. The proposed triage team method categorises the entire patient according to the urgency to see the doctor and allows the patient to complete the necessary test before being seen by the doctor for the first time. The simulation study showed that it will decrease the throughput time of the patient and reduce the utilisation of the specialist and enable the ordering all the tests the patient needs right after arrival, thus quickening the referral to treatment. Santib\u00e1\u00f1ez et al.5 developed a discrete event simulation model of British Columbia Cancer Agency\"s ambulatory care unit which was used to study the impact of scenarios considering different operational factors (delay in starting clinic), appointment schedule (appointment order, appointment adjustment, add---ons to the schedule) and resource allocation. It was found that the best outcomes were obtained when not one but multiple changes were implemented simultaneously. Sep\u00falveda et al.6 studied the M. D. Anderson Cancer Centre Orlando, which is a cancer treatment facility and built a simulation model to analyse and improve flow process and increase capacity in the main facility. Different scenarios were considered like, transferring laboratory and pharmacy areas, adding an extra blood draw room and applying different scheduling techniques of patients. The study shows that by increasing the number of short---term (four hours or less) patients in the morning could increase chair utilisation. Discrete event simulation also helps improve a service where staff are ignorant about the behaviour of the system as a whole; which can also be described as a real professional system. Niranjon et al.7 used simulation successfully where they had to face such constraints and lack of accessible data. Carlos et al. 8 used Total quality management and simulation - animation to improve the quality of the emergency room. Simulation was used to cover the key point of the emergency room and animation was used to indicate the areas of opportunity required. This study revealed that a long waiting time, overload personnel and increasing withdrawal rate of patients are caused by the lack of capacity in the emergency room. Baesler et al.9 developed a methodology for a cancer treatment facility to find stochastically a global optimum point for the control variables. A simulation model generated the output using a goal programming framework for all the objectives involved in the analysis. Later a genetic algorithm was responsible for performing the search for an improved solution. The control variables that were considered in this research are number of treatment chairs, number of drawing blood nurses, laboratory personnel, and pharmacy personnel. Guo et al. 10 presented a simulation framework considering demand for appointment, patient flow logic, distribution of resources, scheduling rules followed by the scheduler. The objective of the study was to develop a scheduling rule which will ensure that 95% of all the appointment requests should be seen within one week after the request is made to increase the level of patient satisfaction and balance the schedule of each doctor to maintain a fine harmony between \"busy clinic\" and \"quiet clinic\". Huschka et al.11 studied a healthcare system which was about to change their facility layout. In this case a simulation model study helped them to design a new healthcare practice by evaluating the change in layout before implementation. Historical data like the arrival rate of the patients, number of patients visited each day, patient flow logic, was used to build the current system model. Later, different scenarios were designed which measured the changes in the current layout and performance. Wijewickrama et al.12 developed a simulation model to evaluate appointment schedule (AS) for second time consultations and patient appointment sequence (PSEQ) in a multi---facility system. Five different appointment rule (ARULE) were considered: i) Baily; ii) 3Baily; iii) Individual (Ind); iv) two patients at a time (2AtaTime); v) Variable Interval and (V---I) rule. PSEQ is based on type of patients: Appointment patients (APs) and new patients (NPs). The different PSEQ that were studied in this study were: i) first--- come first---serve; ii) appointment patient at the beginning of the clinic (APBEG); iii) new patient at the beginning of the clinic (NPBEG); iv) assigning appointed and new patients in an alternating manner (ALTER); v) assigning a new patient after every five---appointment patients. Also patient no show (0% and 5%) and patient punctuality (PUNCT) (on---time and 10 minutes early) were also considered. The study found that ALTER---Ind. and ALTER5---Ind. performed best on 0% NOSHOW, on---time PUNCT and 5% NOSHOW, on---time PUNCT situation to reduce WT and IT per patient. As NOSHOW created slack time for waiting patients, their WT tends to reduce while IT increases due to unexpected cancellation. Earliness increases congestion whichin turn increases waiting time. Ramis et al.13 conducted a study of a Medical Imaging Center (MIC) to build a simulation model which was used to improve the patient journey through an imaging centre by reducing the wait time and making better use of the resources. The simulation model also used a Graphic User Interface (GUI) to provide the parameters of the centre, such as arrival rates, distances, processing times, resources and schedule. The simulation was used to measure the waiting time of the patients in different case scenarios. The study found that assigning a common function to the resource personnel could improve the waiting time of the patients. The objective of this study is to develop an efficient scheduling template that maximises the number of served patients and minimises the average patient's waiting time at the given resources availability. To accomplish this objective, we will build a simulation model which mimics the working conditions of the clinic. Then we will suggest different scenarios of matching the arrival pattern of the patients with the availability of the resources. Full experiments will be performed to evaluate these scenarios. Hence, a simple and practical scheduling template will be built based on the indentified best scenario. The developed simulation model is described in section 2, which consists of a description of the treatment room, and a description of the types of patients and treatment durations. In section 3, different improvement scenarios are described and their analysis is presented in section 4. Section 5 illustrates a scheduling template based on one of the improvement scenarios. Finally, the conclusion and future direction of our work is exhibited in section 6. 2.\n\n",
                "DataExportTag": "74",
                "QuestionID": "QID146",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "INTRODUCTION CancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated t...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID209",
            "SecondaryAttribute": "INTRODUCTION CancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated t...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "INTRODUCTION\nCancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated to provide quality care to those who have been diagnosed and are living with cancer. MacCharles Chemotherapy unit is specially built to provide chemotherapy treatment to the cancer patients of Winnipeg. In order to maintain an excellent service, it tries to ensure that patients get their treatment in a timely manner. It is challenging to maintain that goal because of the lack of a proper roster, the workload distribution and inefficient resource allotment. In order to maintain the satisfaction of the patients and the healthcare providers, by serving the maximum number of patients in a timely manner, it is necessary to develop an efficient scheduling template that matches the required demand with the availability of resources. This goal can be reached using simulation modelling. Simulation has proven to be an excellent modelling tool. It can be defined as building computer models that represent real world or hypothetical systems, and hence experimenting with these models to study system behaviour under different scenarios.1, 2 A study was undertaken at the Children's Hospital of Eastern Ontario to identify the issues behind the long waiting time of a emergency room.3 A 20---day field observation revealed that the availability of the staff physician and interaction affects the patient wait time. Jyv\u00e4skyl\u00e4 et al.4 used simulation to test different process scenarios, allocate resources and perform activity---based cost analysis in the Emergency Department (ED) at the Central Hospital. The simulation also supported the study of a new operational method, named \"triage-team\" method without interrupting the main system. The proposed triage team method categorises the entire patient according to the urgency to see the doctor and allows the patient to complete the necessary test before being seen by the doctor for the first time. The simulation study showed that it will decrease the throughput time of the patient and reduce the utilisation of the specialist and enable the ordering all the tests the patient needs right after arrival, thus quickening the referral to treatment. Santib\u00e1\u00f1ez et al.5 developed a discrete event simulation model of British Columbia Cancer Agency\"s ambulatory care unit which was used to study the impact of scenarios considering different operational factors (delay in starting clinic), appointment schedule (appointment order, appointment adjustment, add---ons to the schedule) and resource allocation. It was found that the best outcomes were obtained when not one but multiple changes were implemented simultaneously. Sep\u00falveda et al.6 studied the M. D. Anderson Cancer Centre Orlando, which is a cancer treatment facility and built a simulation model to analyse and improve flow process and increase capacity in the main facility. Different scenarios were considered like, transferring laboratory and pharmacy areas, adding an extra blood draw room and applying different scheduling techniques of patients. The study shows that by increasing the number of short---term (four hours or less) patients in the morning could increase chair utilisation. Discrete event simulation also helps improve a service where staff are ignorant about the behaviour of the system as a whole; which can also be described as a real professional system. Niranjon et al.7 used simulation successfully where they had to face such constraints and lack of accessible data. Carlos et al. 8 used Total quality management and simulation - animation to improve the quality of the emergency room. Simulation was used to cover the key point of the emergency room and animation was used to indicate the areas of opportunity required. This study revealed that a long waiting time, overload personnel and increasing withdrawal rate of patients are caused by the lack of capacity in the emergency room. Baesler et al.9 developed a methodology for a cancer treatment facility to find stochastically a global optimum point for the control variables. A simulation model generated the output using a goal programming framework for all the objectives involved in the analysis. Later a genetic algorithm was responsible for performing the search for an improved solution. The control variables that were considered in this research are number of treatment chairs, number of drawing blood nurses, laboratory personnel, and pharmacy personnel. Guo et al. 10 presented a simulation framework considering demand for appointment, patient flow logic, distribution of resources, scheduling rules followed by the scheduler. The objective of the study was to develop a scheduling rule which will ensure that 95% of all the appointment requests should be seen within one week after the request is made to increase the level of patient satisfaction and balance the schedule of each doctor to maintain a fine harmony between \"busy clinic\" and \"quiet clinic\". Huschka et al.11 studied a healthcare system which was about to change their facility layout. In this case a simulation model study helped them to design a new healthcare practice by evaluating the change in layout before implementation. Historical data like the arrival rate of the patients, number of patients visited each day, patient flow logic, was used to build the current system model. Later, different scenarios were designed which measured the changes in the current layout and performance. Wijewickrama et al.12 developed a simulation model to evaluate appointment schedule (AS) for second time consultations and patient appointment sequence (PSEQ) in a multi---facility system. Five different appointment rule (ARULE) were considered: i) Baily; ii) 3Baily; iii) Individual (Ind); iv) two patients at a time (2AtaTime); v) Variable Interval and (V---I) rule. PSEQ is based on type of patients: Appointment patients (APs) and new patients (NPs). The different PSEQ that were studied in this study were: i) first--- come first---serve; ii) appointment patient at the beginning of the clinic (APBEG); iii) new patient at the beginning of the clinic (NPBEG); iv) assigning appointed and new patients in an alternating manner (ALTER); v) assigning a new patient after every five---appointment patients. Also patient no show (0% and 5%) and patient punctuality (PUNCT) (on---time and 10 minutes early) were also considered. The study found that ALTER---Ind. and ALTER5---Ind. performed best on 0% NOSHOW, on---time PUNCT and 5% NOSHOW, on---time PUNCT situation to reduce WT and IT per patient. As NOSHOW created slack time for waiting patients, their WT tends to reduce while IT increases due to unexpected cancellation. Earliness increases congestion whichin turn increases waiting time. Ramis et al.13 conducted a study of a Medical Imaging Center (MIC) to build a simulation model which was used to improve the patient journey through an imaging centre by reducing the wait time and making better use of the resources. The simulation model also used a Graphic User Interface (GUI) to provide the parameters of the centre, such as arrival rates, distances, processing times, resources and schedule. The simulation was used to measure the waiting time of the patients in different case scenarios. The study found that assigning a common function to the resource personnel could improve the waiting time of the patients. The objective of this study is to develop an efficient scheduling template that maximises the number of served patients and minimises the average patient's waiting time at the given resources availability. To accomplish this objective, we will build a simulation model which mimics the working conditions of the clinic. Then we will suggest different scenarios of matching the arrival pattern of the patients with the availability of the resources. Full experiments will be performed to evaluate these scenarios. Hence, a simple and practical scheduling template will be built based on the indentified best scenario. The developed simulation model is described in section 2, which consists of a description of the treatment room, and a description of the types of patients and treatment durations. In section 3, different improvement scenarios are described and their analysis is presented in section 4. Section 5 illustrates a scheduling template based on one of the improvement scenarios. Finally, the conclusion and future direction of our work is exhibited in section 6. 2.\n\n",
                "DataExportTag": "49",
                "QuestionID": "QID209",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "INTRODUCTION CancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated t...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID270",
            "SecondaryAttribute": "INTRODUCTION CancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated t...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "INTRODUCTION\nCancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated to provide quality care to those who have been diagnosed and are living with cancer. MacCharles Chemotherapy unit is specially built to provide chemotherapy treatment to the cancer patients of Winnipeg. In order to maintain an excellent service, it tries to ensure that patients get their treatment in a timely manner. It is challenging to maintain that goal because of the lack of a proper roster, the workload distribution and inefficient resource allotment. In order to maintain the satisfaction of the patients and the healthcare providers, by serving the maximum number of patients in a timely manner, it is necessary to develop an efficient scheduling template that matches the required demand with the availability of resources. This goal can be reached using simulation modelling. Simulation has proven to be an excellent modelling tool. It can be defined as building computer models that represent real world or hypothetical systems, and hence experimenting with these models to study system behaviour under different scenarios.1, 2 A study was undertaken at the Children's Hospital of Eastern Ontario to identify the issues behind the long waiting time of a emergency room.3 A 20---day field observation revealed that the availability of the staff physician and interaction affects the patient wait time. Jyv\u00e4skyl\u00e4 et al.4 used simulation to test different process scenarios, allocate resources and perform activity---based cost analysis in the Emergency Department (ED) at the Central Hospital. The simulation also supported the study of a new operational method, named \"triage-team\" method without interrupting the main system. The proposed triage team method categorises the entire patient according to the urgency to see the doctor and allows the patient to complete the necessary test before being seen by the doctor for the first time. The simulation study showed that it will decrease the throughput time of the patient and reduce the utilisation of the specialist and enable the ordering all the tests the patient needs right after arrival, thus quickening the referral to treatment. Santib\u00e1\u00f1ez et al.5 developed a discrete event simulation model of British Columbia Cancer Agency\"s ambulatory care unit which was used to study the impact of scenarios considering different operational factors (delay in starting clinic), appointment schedule (appointment order, appointment adjustment, add---ons to the schedule) and resource allocation. It was found that the best outcomes were obtained when not one but multiple changes were implemented simultaneously. Sep\u00falveda et al.6 studied the M. D. Anderson Cancer Centre Orlando, which is a cancer treatment facility and built a simulation model to analyse and improve flow process and increase capacity in the main facility. Different scenarios were considered like, transferring laboratory and pharmacy areas, adding an extra blood draw room and applying different scheduling techniques of patients. The study shows that by increasing the number of short---term (four hours or less) patients in the morning could increase chair utilisation. Discrete event simulation also helps improve a service where staff are ignorant about the behaviour of the system as a whole; which can also be described as a real professional system. Niranjon et al.7 used simulation successfully where they had to face such constraints and lack of accessible data. Carlos et al. 8 used Total quality management and simulation - animation to improve the quality of the emergency room. Simulation was used to cover the key point of the emergency room and animation was used to indicate the areas of opportunity required. This study revealed that a long waiting time, overload personnel and increasing withdrawal rate of patients are caused by the lack of capacity in the emergency room. Baesler et al.9 developed a methodology for a cancer treatment facility to find stochastically a global optimum point for the control variables. A simulation model generated the output using a goal programming framework for all the objectives involved in the analysis. Later a genetic algorithm was responsible for performing the search for an improved solution. The control variables that were considered in this research are number of treatment chairs, number of drawing blood nurses, laboratory personnel, and pharmacy personnel. Guo et al. 10 presented a simulation framework considering demand for appointment, patient flow logic, distribution of resources, scheduling rules followed by the scheduler. The objective of the study was to develop a scheduling rule which will ensure that 95% of all the appointment requests should be seen within one week after the request is made to increase the level of patient satisfaction and balance the schedule of each doctor to maintain a fine harmony between \"busy clinic\" and \"quiet clinic\". Huschka et al.11 studied a healthcare system which was about to change their facility layout. In this case a simulation model study helped them to design a new healthcare practice by evaluating the change in layout before implementation. Historical data like the arrival rate of the patients, number of patients visited each day, patient flow logic, was used to build the current system model. Later, different scenarios were designed which measured the changes in the current layout and performance. Wijewickrama et al.12 developed a simulation model to evaluate appointment schedule (AS) for second time consultations and patient appointment sequence (PSEQ) in a multi---facility system. Five different appointment rule (ARULE) were considered: i) Baily; ii) 3Baily; iii) Individual (Ind); iv) two patients at a time (2AtaTime); v) Variable Interval and (V---I) rule. PSEQ is based on type of patients: Appointment patients (APs) and new patients (NPs). The different PSEQ that were studied in this study were: i) first--- come first---serve; ii) appointment patient at the beginning of the clinic (APBEG); iii) new patient at the beginning of the clinic (NPBEG); iv) assigning appointed and new patients in an alternating manner (ALTER); v) assigning a new patient after every five---appointment patients. Also patient no show (0% and 5%) and patient punctuality (PUNCT) (on---time and 10 minutes early) were also considered. The study found that ALTER---Ind. and ALTER5---Ind. performed best on 0% NOSHOW, on---time PUNCT and 5% NOSHOW, on---time PUNCT situation to reduce WT and IT per patient. As NOSHOW created slack time for waiting patients, their WT tends to reduce while IT increases due to unexpected cancellation. Earliness increases congestion whichin turn increases waiting time. Ramis et al.13 conducted a study of a Medical Imaging Center (MIC) to build a simulation model which was used to improve the patient journey through an imaging centre by reducing the wait time and making better use of the resources. The simulation model also used a Graphic User Interface (GUI) to provide the parameters of the centre, such as arrival rates, distances, processing times, resources and schedule. The simulation was used to measure the waiting time of the patients in different case scenarios. The study found that assigning a common function to the resource personnel could improve the waiting time of the patients. The objective of this study is to develop an efficient scheduling template that maximises the number of served patients and minimises the average patient's waiting time at the given resources availability. To accomplish this objective, we will build a simulation model which mimics the working conditions of the clinic. Then we will suggest different scenarios of matching the arrival pattern of the patients with the availability of the resources. Full experiments will be performed to evaluate these scenarios. Hence, a simple and practical scheduling template will be built based on the indentified best scenario. The developed simulation model is described in section 2, which consists of a description of the treatment room, and a description of the types of patients and treatment durations. In section 3, different improvement scenarios are described and their analysis is presented in section 4. Section 5 illustrates a scheduling template based on one of the improvement scenarios. Finally, the conclusion and future direction of our work is exhibited in section 6. 2.\n\n",
                "DataExportTag": "49",
                "QuestionID": "QID270",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "INTRODUCTION CancerCare Manitoba is a provincially mandated cancer care agency. It is dedicated t...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID207",
            "SecondaryAttribute": "IoT Based Smart Water Monitoring & Distribution System For An Apartments As we know water is so p...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "IoT Based Smart Water Monitoring & Distribution System For An Apartments As we know water is so precious for human being as well as for the complete nature without which it will not be possible to survive. Even though lot many efforts have been taken by government through various schemes and it is becoming difficult day by day to save water for future and make efficient utilization of it. In this proposed work, an IoT design for water monitoring and control approach which supports internet-based data collection on real time bases. This proposed system shall implement in highly populated residential buildings like hotels, lodge, hostels, dormitory, apartments, shopping malls etc. And also, this system can provide a complete survey and the usage of water by every individual room. This system addresses that the flow rate measuring and scheming the supply of water in order to limit the water wastage and approach the water conservation and also this system can measure the quality and quantity of water distributed to every household by using ph and flow rate sensors. The system has been designed in such a way that it will monitor the available water level continuously. System has been implemented by using embedded system and communication will takes. How to cite this paper: Dr. R. Mohana Priya | M. Sathyamoorthy | S. Surya | V. Vishnu Gopal \"IoT Based Smart Water Monitoring & Distribution System For An Apartments\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-3, April 2021, pp.1125-1127, URL: www.ijtsrd.com\/papers\/ijtsrd41133.pdf Copyright \u00a9 2021 by author (s) and International Journal of Trend in Scientific Research and Development Journal. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (CC BY 4.0) (http:\/\/creativecommons.org\/licenses\/by\/4.0) INTRODUCTION The internet of things (IoT) forms an important part of intelligent monitoring which connects people and devices using wireless sensor technology. It is a fast growing research area in the military, energy management, healthcare and many more. The concept of IoT was proposed by Kevin Ashton to demonstrate a set of interconnected devices. IoT makes it possible to transfer information between different electronic devices embedded with new technology. The energy management is possible using energy harvesting mechanisms, which is a method of collecting energy from natural sources such as light, vibration, pressure etc. The combination of technologies such as Wireless sensor network (WSN), Radio frequency identification (RFID), Energy harvesting (EH) and Artificial Intelligence (AI) helps IoT to flourish widely. Water distribution system (WDS) is a very important research area that affects the economic growth of our country. WDS mainly have two issues first is the water loss due to leakage and the second is that it is prone to contamination. It is affecting the health and safety of the people. According to the report of world health organization (WHO) in 2017, around 2.1 billion people around the world lack safe drinking water. So there is a need to ensure the water quality and wastage by using Iot to reduce such issue. There are different traditional methods to collect water datasets to measure its quality, but managing and monitoring the data from WDS in real time is challenging as the data is heterogeneous, data collection is time consuming, energy required for processing, coverage and connectivity of the nodes in the network. By using IoT and combining technologies such as WSN, AI and EH can be used to ensure the water quality in real time and alerts the users to take remedial measures. LITERATURE REVIEW: While over 15 million American households rely upon private well sources for water [3], the remaining 110 million households are connected to public water supplies. Likewise, most commercial and industrial applications use public water supplies. Public and municipal water utilities must carefully monitor the water they provide for public safety, billing, and resource management. Over the last few decades, water utility companies have begun installing automated meter reading (AMR) systems to further simplify the process of meter reading, decrease manual labor, and reduce transcription errors within collected data [4]. These systems allow more frequent reporting of measured demand at the individual customers, while simultaneously reducing the manual effort of physically looking at each meter to record the volume measured. In 2018, the American Water Works IJTSRD41133 International Journal of Trend in Scientific Research and Development (IJTSRD) @ www.ijtsrd.com eISSN: 2456-6470 @ IJTSRD | Unique Paper ID \u2013 IJTSRD41133 | Volume \u2013 5 | Issue \u2013 3 | March-April 2021 Page 1126 Association reported 37% of utilities in North America have fully implemented AMR systems, and another 24% are in the process of doing so [5]. Many of the AMR systems support quarter-hourly reads, but battery limitations and datarelated costs constrain the data collection to hourly or daily reads. It is from these AMR systems that the data for our proposed algorithm comes. While there is little work in customer water flow clustering, other research explores clustering of energy customer\u2019s using smart meter data. Panapakid is et al. [6], [7] implement clustering of electric smart meter data. As opposed to creating models such as our algorithm, their work clusters the daily typical load profiles within a customer\u2019s dataset. Representatives of those clusters are used to complete the second stage clustering across the population of all customers. Their work illustrates the complex problem of identifying the optimal number of clusters in a diverse dataset. In contrast to the Panapakid is work, the clustering method presented here does not require a definition of an optimal number of clusters. Bose and Chen [8], [9] track changing cluster populations over time using fuzzy c-means algorithms. Their work focuses upon migratory patterns of cellular phone customers, for the purposes of tracking dynamic market demands and customer retention. Their data exhibit not only customers who migrate from one cluster within the data to another, but also the formation of new clusters and dissolution of others as new behavior patterns emerge within the population. PROPOSED EXPLANATIONS: This system addresses that the flow rate measuring and scheming the supply of water in order to limit the water wastage and approach the water conservation and also this system can measure the quality and quantity of water distributed to every household by using ph and flow rate sensors. The system has been designed in such a way that it will monitor the available water level continuously. System has been implemented by using embedded system and communication will takes through IoT. Proposed smart management platform (hereinafter referred to as SmartWMP) is considering a supervisory and control both water and energy flows to improve water and energy efficiency offering simultaneously the possibility carrying out transactions directly between utilities and local renewable producers in order to provide a sustainable management of water supply systems. The structure of SmartWMP SmartWMP integrates water and energy nexus usage related information from smart meters, data analysis (profiling, modeling, simulation, and optimization) using AI techniques, DR programme, and services for peer-to-peer (P2P) transactions on basis of smart contracts between the water utilities and local renewable producers (inside micro grids). The integrated modules of software are the following: Integrated Water and Energy Database Management comprehensive analysis of integrated databases to describe different patterns to the level of pump stations (water and energy) and consumers (water). Water distribution system (WDS) is a very important research area that affects the economic growth of our country. WDS mainly have two issues first is the water loss due to leakage and the second is that it is prone to contamination. It is affecting the health and safety of the people. According to the report of world health organization (WHO) in 2017, around 2.1 billion people around the world lack safe drinking water. So there is a need to ensure the water quality and wastage by using Iot to reduce such issue. DR programme identifies solutions for the water pumps which could participate in a DR scheme and how the control of power consumption improves the potential of participation in a DR scheme. Blockchain based ICT topology analyses the potential impact of Blockchain technology on the integrated water and power sectors and explores what opportunities it may hold for water utilities. A vision of solution is presented. Integrated Profiling Concept to treat integrated of the profiling techniques to energy and water used in a smart management based on the correlation between data provided by the smart metering system. AI based techniques for modelling and simulation develop the mathematical models and computing algorithms based on meta-heuristic approaches for optimization and expert systems for decision making in pumping water management. ARDUINO UNO: The Arduino Integrated Development Environment (IDE) is a cross platform application (for Windows, macOS, Linux) that is written in functions from C and C++. It is used to write and upload programs to Arduino compatible boards, but also, with the help of third-party cores, other vendor development boards. The source code for the IDE is released under the GNU General Public License, version 2. The Arduino IDE supports the languages C and C++ using special rules of code structuring. The Arduino IDE supplies a software library from the Wiring project, which provides many common input and output procedures. User-written code only requires two basic functions, for starting the sketch and the main program loop, that are compiled and linked with a program stub main() into an ex\n\n",
                "DataExportTag": "AI900548",
                "QuestionID": "QID207",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "IoT Based Smart Water Monitoring & Distribution System For An Apartments As we know water is so p...",
                "Choices": {
                    "1": {
                        "Display": "\"Unmanned Aerial and Underwater Vehicles\""
                    },
                    "2": {
                        "Display": "unmanned_aerial_vehicles, flight, mission, drone, aircraft, vehicle, unmanned_aerial, ship, autonomous, underwater, spacecraft, unmanned, vehicle_uav, landing, vessel"
                    },
                    "3": {
                        "Display": "\"Industrial Automation and Robotics\""
                    },
                    "4": {
                        "Display": "robot, computer_vision, image, inspection, speech_recognition, industrial, remote, autonomous_robot, sensor, voice, autonomous, smart, microcontroller, raspberry_pi, wireless"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID122",
            "SecondaryAttribute": "Laparoscopic Lavage vs Primary Resection for Acute Perforated Diverticulitis: The SCANDIV Randomi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Laparoscopic Lavage vs Primary Resection for Acute Perforated Diverticulitis: The SCANDIV Randomized Clinical Trial. IMPORTANCE\nPerforated colonic diverticulitis usually requires surgical resection, which is associated with significant morbidity. Cohort studies have suggested that laparoscopic lavage may treat perforated diverticulitis with less morbidity than resection procedures.\n\n",
                "DataExportTag": "CAN1059326",
                "QuestionID": "QID122",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Laparoscopic Lavage vs Primary Resection for Acute Perforated Diverticulitis: The SCANDIV Randomi...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID201",
            "SecondaryAttribute": "Lipophilic Statins and Risk for Hepatocellular Carcinoma and Death in Patients With Chronic Viral...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Lipophilic Statins and Risk for Hepatocellular Carcinoma and Death in Patients With Chronic Viral Hepatitis: Results From a Nationwide Swedish Population Approximately 500000 cases of hepatocellular carcinoma (HCC) are diagnosed worldwide each year (1), related primarily to chronic infection with hepatitis B virus (HBV) or hepatitis C virus (HCV) (2). In the United States and Europe, the incidence of HCC has tripled since the 1970s, and mortality is increasing more rapidly for HCC than for any other type of cancer (1, 3). The disease carries a grim prognosis, with limited treatment options and median survival of less than 1 year (2, 3). Although HCC risk is reduced with HBV suppression or HCV eradication, it may persist in high-risk patients or in those with advanced fibrosis (2, 4). Consequently, there is an urgent need to identify effective primary prevention strategies to reduce HCC-related morbidity and mortality. Accumulating data suggest that in chronic liver disease, 3-hydroxy-3-methylglutaryl coenzyme A reductase inhibitors (statins) may improve clinical outcomes and reduce HCC risk (5, 6). However, the influence of different statin classes on HCC risk and survival is unknown. Recent experimental models (7) and observational studies (810) suggest that lipophilic statins (atorvastatin, simvastatin, fluvastatin, and lovastatin) may prevent hepatocarcinogenesis more effectively than hydrophilic statins (pravastatin or rosuvastatin). However, clinical evidence for the effectiveness of lipophilic or hydrophilic statins in chronic liver disease is scarce (815), and published studies have been limited by retrospective designs and a lack of detailed, updated data on statin class and dosage over time. As a result, the optimal statin type for effective HCC prevention remains unknown. We examined the influence of lipophilic and hydrophilic statins on risk for incident HCC and death in a nationwide cohort of patients with confirmed chronic HBV or HCV infection in Sweden. Methods Data Sources Hepatitis B and C are notifiable diseases under Swedish law, and the Register for Surveillance of Communicable Diseases maintains well-validated registers of all cases of acute and chronic HBV infection (since 1967) and HCV infection (since 1990) (16), including information on acute versus chronic infection (for HBV) and route of transmission. Cases are confirmed in duplicate by clinician report and serologic testing (hepatitis B surface antigen and HBV DNA, or HCV antibodies and HCV RNA) (16). We linked this database to the validated Patient Register, the Cause of Death Register, the Cancer Register, and the Prescribed Drug Register at the National Board of Health and Welfare, using the unique personal identity number assigned to Swedish residents. The Patient Register contains prospectively updated and detailed data on all hospitalizations (including surgeries and liver transplants), discharge diagnoses (since 1987), and specialty outpatient care (since 2001). All clinical diagnoses are coded using the International Classification of Diseases (ICD), with positive predictive values that exceed 85% in validation studies (17). This study was approved by the Ethics Review Board in Stockholm, Sweden (2017\/707-31\/2) on 15 June 2017. Study Population The population included patients aged 18 years or older with confirmed chronic HBV or HCV mono-infection who filled a first prescription for lipophilic or hydrophilic statins between 1 July 2005 and 31 December 2013. To ensure a new-user design (18), we required each statin user to fulfill a baseline period of at least 180 days between hepatitis B or C notification date and the date of the first filled statin prescription (index date). The index date for nonusers was randomly selected from recorded medical visits in the same calendar month and year as the matched statin user index date to ensure similar opportunities for care and treatment (19). We also required that nonusers fulfill the same entry criteria as statin users (19). We excluded patients prescribed a statin before diagnosis of HBV or HCV infection (n= 42 and 179, respectively), those with HCC diagnosed before or within the 180-day baseline period (n= 336), those with HIV infection (n= 4515) (8), and those who received lipophilic and hydrophilic statins (together or sequentially) during follow-up (n= 27), leaving 63279 eligible patients for analysis (Supplement Figures 1A and 1B). Supplement. Supplementary Material Exposures Statins available in Sweden between 2005 and 2013 included simvastatin, atorvastatin, pravastatin, rosuvastatin, fluvastatin, and lovastatin. However, fluvastatin and lovastatin were rarely prescribed (<1%), a pattern that has been observed in other European countries (2023). Our initial eligible population included no lovastatin users and 6 fluvastatin users, all of whom had previously used hydrophilic statins and therefore were excluded. Thus, statin exposures included simvastatin, atorvastatin, pravastatin, and rosuvastatin. The Prescribed Drug Register has prospectively recorded all dispensed prescriptions from Swedish pharmacies since 1 July 2005 and is at least 99.7% complete (24). Statins are classified by Anatomical Therapeutic Chemical classification system code C10AA (Supplement Table 1). Each record includes prescription dates; dosage; number of pills; and defined daily dose (DDD), a standardized statistical measure of average daily drug consumption used by the World Health Organization. A single statin DDD is equivalent to a daily 30-mg dose of simvastatin, 20-mg dose of atorvastatin, 30-mg dose of pravastatin, or 10-mg dose of rosuvastatin. The sum of the total DDD over monthly intervals is the cumulative DDD (cDDD), a widely used measure of cumulative dose and duration of use (8, 11). Statin use was defined as 30 or more cDDDs, consistent with prior studies (8, 11). For analyses of dose and duration, we also created predefined, a priori categories: 0 to fewer than 30 (reference), 30 to 299, 300 to 599, and 600 or more cDDDs. To construct the propensity score (PS), baseline demographic, clinical, and medication covariates were identified up to and including the index date (Supplement Methods and Supplement Table 1). For additional sensitivity analyses, we selected 10 prognostic covariates a priori on the basis of known or putative associations with study outcomes, including age (years); sex; duration of HBV or HCV infection (years); presence or absence of cirrhosis or diabetes; obesity; alcohol abuse; and prior use of antiviral therapy, aspirin, or metformin (Supplement Methods). Outcomes Study outcomes were ascertained from the validated Cancer Register and Cause of Death Register (25). The Cancer Register contains prospectively collected data from more than 96% of incident cancer cases, and HCC cases are confirmed by trained specialists using established pathologic or radiographic criteria (25). Secondary outcomes included all-cause and liver-specific mortality, ascertained from the Cause of Death Register, which is more than 99% complete. Liver-specific mortality was defined as a primary ICD code for chronic viral hepatitis, chronic liver disease, liver failure, variceal bleeding, spontaneous bacterial peritonitis, hepatorenal syndrome, or HCC. Statistical Analysis We used a PS-matched, prospective cohort design to balance baseline characteristics and minimize potential confounding (26). Exposure PSs were derived from the predicted probability of hydrophilic or lipophilic statin use, estimated in a logistic regression model, and conditioned on baseline covariates selected a priori (Supplement Methods). We used a published greedy nearest-neighbor within-caliper matching algorithm without replacement to select nonuser matches for each lipophilic or hydrophilic statin user in a 1:1 ratio (27) according to cohort (HBV or HCV) and index date (in the same month and year). Adequacy of matching was assessed by calculating standardized mean differences (28) and comparing PS distribution and balance (Supplement Figure 2 and Supplement Table 2). Follow-up time accrued from the index date to the first recorded date of HCC diagnosis, all-cause death, liver transplant, emigration, or 31 December 2015. No heterogeneity was found between statin type and HCC risk by HBV or HCV cohort (P for heterogeneity= 0.37), and data were therefore pooled. Proportional hazards regression models that accounted for competing risks (29) were used to estimate adjusted cumulative incidences (percentages), adjusted subdistribution hazard ratios (aHRs), and corresponding 95% CIs for each outcome according to statin type and cDDD. We defined death, liver transplant, and emigration as the competing events for HCC, and we defined liver transplant and emigration as the competing events for death (Supplement Table 3A). We conducted the primary analysis of statin type in the PS-matched cohort, in which lipophilic or hydrophilic statin use was defined over study follow-up. In addition, we conducted analyses of statin dose and duration in the unmatched population using time-varying cDDD exposures and covariates to minimize potential immortal time bias. The main model was stratified by cohort (HBV or HCV) and PS quintile, from which we estimated adjusted absolute risk differences (RDs), with 95% CIs calculated via bootstrapping (30). In the unmatched population, we constructed an additional model that further accounted for age, sex, region, continuous PS, and a priori time-varying covariates, updated over each month of follow-up. In stratified models, we assessed the relationship between statin exposure and outcomes according to prespecified HCC risk factors, and we tested the significance of interactions using the log-likelihood ratio test. Schoenfeld residual tests identified no violations of the proportional hazards assumption. We conducted several sensitivity analyses to test the robustness of our results. First, we evaluated statin use and type in the unmatched population using time-varying statin exposures with and without adjustment for PS, region, and time-updated covariates.\n\n",
                "DataExportTag": "CAN561422",
                "QuestionID": "QID201",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Lipophilic Statins and Risk for Hepatocellular Carcinoma and Death in Patients With Chronic Viral...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\""
                    },
                    "2": {
                        "Display": "mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "3": {
                        "Display": "\"Racial and Socioeconomic Disparities in Cancer Screening\""
                    },
                    "4": {
                        "Display": "screening, disparity, colorectal_cancer, black, white, woman, mammography, non_hispanic, ses, cervical, hispanic, african_american, racial_ethnic, socioeconomic, income"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID262",
            "SecondaryAttribute": "Lipophilic Statins and Risk for Hepatocellular Carcinoma and Death in Patients With Chronic Viral...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Lipophilic Statins and Risk for Hepatocellular Carcinoma and Death in Patients With Chronic Viral Hepatitis: Results From a Nationwide Swedish Population Approximately 500000 cases of hepatocellular carcinoma (HCC) are diagnosed worldwide each year (1), related primarily to chronic infection with hepatitis B virus (HBV) or hepatitis C virus (HCV) (2). In the United States and Europe, the incidence of HCC has tripled since the 1970s, and mortality is increasing more rapidly for HCC than for any other type of cancer (1, 3). The disease carries a grim prognosis, with limited treatment options and median survival of less than 1 year (2, 3). Although HCC risk is reduced with HBV suppression or HCV eradication, it may persist in high-risk patients or in those with advanced fibrosis (2, 4). Consequently, there is an urgent need to identify effective primary prevention strategies to reduce HCC-related morbidity and mortality. Accumulating data suggest that in chronic liver disease, 3-hydroxy-3-methylglutaryl coenzyme A reductase inhibitors (statins) may improve clinical outcomes and reduce HCC risk (5, 6). However, the influence of different statin classes on HCC risk and survival is unknown. Recent experimental models (7) and observational studies (810) suggest that lipophilic statins (atorvastatin, simvastatin, fluvastatin, and lovastatin) may prevent hepatocarcinogenesis more effectively than hydrophilic statins (pravastatin or rosuvastatin). However, clinical evidence for the effectiveness of lipophilic or hydrophilic statins in chronic liver disease is scarce (815), and published studies have been limited by retrospective designs and a lack of detailed, updated data on statin class and dosage over time. As a result, the optimal statin type for effective HCC prevention remains unknown. We examined the influence of lipophilic and hydrophilic statins on risk for incident HCC and death in a nationwide cohort of patients with confirmed chronic HBV or HCV infection in Sweden. Methods Data Sources Hepatitis B and C are notifiable diseases under Swedish law, and the Register for Surveillance of Communicable Diseases maintains well-validated registers of all cases of acute and chronic HBV infection (since 1967) and HCV infection (since 1990) (16), including information on acute versus chronic infection (for HBV) and route of transmission. Cases are confirmed in duplicate by clinician report and serologic testing (hepatitis B surface antigen and HBV DNA, or HCV antibodies and HCV RNA) (16). We linked this database to the validated Patient Register, the Cause of Death Register, the Cancer Register, and the Prescribed Drug Register at the National Board of Health and Welfare, using the unique personal identity number assigned to Swedish residents. The Patient Register contains prospectively updated and detailed data on all hospitalizations (including surgeries and liver transplants), discharge diagnoses (since 1987), and specialty outpatient care (since 2001). All clinical diagnoses are coded using the International Classification of Diseases (ICD), with positive predictive values that exceed 85% in validation studies (17). This study was approved by the Ethics Review Board in Stockholm, Sweden (2017\/707-31\/2) on 15 June 2017. Study Population The population included patients aged 18 years or older with confirmed chronic HBV or HCV mono-infection who filled a first prescription for lipophilic or hydrophilic statins between 1 July 2005 and 31 December 2013. To ensure a new-user design (18), we required each statin user to fulfill a baseline period of at least 180 days between hepatitis B or C notification date and the date of the first filled statin prescription (index date). The index date for nonusers was randomly selected from recorded medical visits in the same calendar month and year as the matched statin user index date to ensure similar opportunities for care and treatment (19). We also required that nonusers fulfill the same entry criteria as statin users (19). We excluded patients prescribed a statin before diagnosis of HBV or HCV infection (n= 42 and 179, respectively), those with HCC diagnosed before or within the 180-day baseline period (n= 336), those with HIV infection (n= 4515) (8), and those who received lipophilic and hydrophilic statins (together or sequentially) during follow-up (n= 27), leaving 63279 eligible patients for analysis (Supplement Figures 1A and 1B). Supplement. Supplementary Material Exposures Statins available in Sweden between 2005 and 2013 included simvastatin, atorvastatin, pravastatin, rosuvastatin, fluvastatin, and lovastatin. However, fluvastatin and lovastatin were rarely prescribed (<1%), a pattern that has been observed in other European countries (2023). Our initial eligible population included no lovastatin users and 6 fluvastatin users, all of whom had previously used hydrophilic statins and therefore were excluded. Thus, statin exposures included simvastatin, atorvastatin, pravastatin, and rosuvastatin. The Prescribed Drug Register has prospectively recorded all dispensed prescriptions from Swedish pharmacies since 1 July 2005 and is at least 99.7% complete (24). Statins are classified by Anatomical Therapeutic Chemical classification system code C10AA (Supplement Table 1). Each record includes prescription dates; dosage; number of pills; and defined daily dose (DDD), a standardized statistical measure of average daily drug consumption used by the World Health Organization. A single statin DDD is equivalent to a daily 30-mg dose of simvastatin, 20-mg dose of atorvastatin, 30-mg dose of pravastatin, or 10-mg dose of rosuvastatin. The sum of the total DDD over monthly intervals is the cumulative DDD (cDDD), a widely used measure of cumulative dose and duration of use (8, 11). Statin use was defined as 30 or more cDDDs, consistent with prior studies (8, 11). For analyses of dose and duration, we also created predefined, a priori categories: 0 to fewer than 30 (reference), 30 to 299, 300 to 599, and 600 or more cDDDs. To construct the propensity score (PS), baseline demographic, clinical, and medication covariates were identified up to and including the index date (Supplement Methods and Supplement Table 1). For additional sensitivity analyses, we selected 10 prognostic covariates a priori on the basis of known or putative associations with study outcomes, including age (years); sex; duration of HBV or HCV infection (years); presence or absence of cirrhosis or diabetes; obesity; alcohol abuse; and prior use of antiviral therapy, aspirin, or metformin (Supplement Methods). Outcomes Study outcomes were ascertained from the validated Cancer Register and Cause of Death Register (25). The Cancer Register contains prospectively collected data from more than 96% of incident cancer cases, and HCC cases are confirmed by trained specialists using established pathologic or radiographic criteria (25). Secondary outcomes included all-cause and liver-specific mortality, ascertained from the Cause of Death Register, which is more than 99% complete. Liver-specific mortality was defined as a primary ICD code for chronic viral hepatitis, chronic liver disease, liver failure, variceal bleeding, spontaneous bacterial peritonitis, hepatorenal syndrome, or HCC. Statistical Analysis We used a PS-matched, prospective cohort design to balance baseline characteristics and minimize potential confounding (26). Exposure PSs were derived from the predicted probability of hydrophilic or lipophilic statin use, estimated in a logistic regression model, and conditioned on baseline covariates selected a priori (Supplement Methods). We used a published greedy nearest-neighbor within-caliper matching algorithm without replacement to select nonuser matches for each lipophilic or hydrophilic statin user in a 1:1 ratio (27) according to cohort (HBV or HCV) and index date (in the same month and year). Adequacy of matching was assessed by calculating standardized mean differences (28) and comparing PS distribution and balance (Supplement Figure 2 and Supplement Table 2). Follow-up time accrued from the index date to the first recorded date of HCC diagnosis, all-cause death, liver transplant, emigration, or 31 December 2015. No heterogeneity was found between statin type and HCC risk by HBV or HCV cohort (P for heterogeneity= 0.37), and data were therefore pooled. Proportional hazards regression models that accounted for competing risks (29) were used to estimate adjusted cumulative incidences (percentages), adjusted subdistribution hazard ratios (aHRs), and corresponding 95% CIs for each outcome according to statin type and cDDD. We defined death, liver transplant, and emigration as the competing events for HCC, and we defined liver transplant and emigration as the competing events for death (Supplement Table 3A). We conducted the primary analysis of statin type in the PS-matched cohort, in which lipophilic or hydrophilic statin use was defined over study follow-up. In addition, we conducted analyses of statin dose and duration in the unmatched population using time-varying cDDD exposures and covariates to minimize potential immortal time bias. The main model was stratified by cohort (HBV or HCV) and PS quintile, from which we estimated adjusted absolute risk differences (RDs), with 95% CIs calculated via bootstrapping (30). In the unmatched population, we constructed an additional model that further accounted for age, sex, region, continuous PS, and a priori time-varying covariates, updated over each month of follow-up. In stratified models, we assessed the relationship between statin exposure and outcomes according to prespecified HCC risk factors, and we tested the significance of interactions using the log-likelihood ratio test. Schoenfeld residual tests identified no violations of the proportional hazards assumption. We conducted several sensitivity analyses to test the robustness of our results. First, we evaluated statin use and type in the unmatched population using time-varying statin exposures with and without adjustment for PS, region, and time-updated covariates.\n\n",
                "DataExportTag": "CAN561422",
                "QuestionID": "QID262",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Lipophilic Statins and Risk for Hepatocellular Carcinoma and Death in Patients With Chronic Viral...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\""
                    },
                    "2": {
                        "Display": "mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "3": {
                        "Display": "\"Racial and Socioeconomic Disparities in Cancer Screening\""
                    },
                    "4": {
                        "Display": "screening, disparity, colorectal_cancer, black, white, woman, mammography, non_hispanic, ses, cervical, hispanic, african_american, racial_ethnic, socioeconomic, income"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID215",
            "SecondaryAttribute": "Locating the Whole Pattern is Better than Locating its Pieces: A Geometric Explanation of an Empi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Locating the Whole Pattern is Better than Locating its Pieces: A Geometric Explanation of an Empirical Phenomenon In many practical problems, we must find a pattern in an image. For situations in which the desired pattern consists of several simple components, the traditional approach is first to look for such components, and then to see whether the relative locations of these components are consistent with the pattern. Recent experiments have shown that a much more efficient pattern recognition can be achieved if we look for the whole pattern (without decomposing it first). In this paper, we give a simple geometric explanation of this empirical fact. The practical problem. In many pattern recognition problems, we must locate a known simple pattern in a complicated black-andwhite image. For example: \u2022 in automatic analysis of electronic schemes, we must locate symbols of standard electronic components (such as \u2212| |\u2212); \u2022 in text recognition, we must find letters, \u2022 similar pattern matching problems arise in satellite imaging, etc. Traditional approach. Most traditional methods for solving this problem are based on the fact that the desired pattern consists of simple geometric components (straight line intervals, arcs, etc.) For example, the above symbol for capacitor consists of four straight line intervals \u2212, |, |, and \u2212. Traditional methods consist of two stages: \u2022 first, we try to locate each component of the desired pattern; \u2022 after all components are located, we check that their relative locations are close to the relative locations of these components in the desired pattern (to be more precise, we check that the difference between the observed and desired relative locations is within the limits set by the observation inaccuracy of component location). A new approach turns out to be better. The authors of [Murshed Bortolozzi 1998] propose to recognize the entire pattern (symbol) without first decomposing it into simple components. The resulting algorithm requires more computation time, but leads to much better recognition: namely, if we set up the parameters of both methods in such a way as to avoid false negatives (unrecognized symbols), then for the new method, the number of false positives (false recognitions of a pattern) is much smaller than for the traditional methods. In this paper, we give a simple geometric explanation of this empirical phenomenon. Geometric reformulation of the problem. We start with a sample pattern P which consists of several components Pi: P = P1 \u222a . . . \u222a Pn. Without losing generality, we can assume that P is a compact set. In the actual image, the actual pattern may be shifted relative to the standard one, so this actual pattern has the form TP for some shift (translation) T . For simplicity, we will assume that the pattern is surrounded by an empty space, i.e. (at least locally): \u2022 either the actual image I coincides with the shifted pattern TP , in which case the pattern is present, \u2022 or the actual image is different from the shifted pattern, in which case the pattern is not here. Description of measurement inaccuracy. Due to measurement inaccuracy, the observed image \u0128 is, in general, slightly different from the actual image I. Namely, due to this inaccuracy, for each point p from the original image, the corresponding observed point p\u0303 may be different from p. The observation inaccuracy can be characterized by the largest possible distance d(p\u0303, p) between the actual and the observed points. If this inaccuracy is \u03b5 > 0, this means that: \u2022 every point from I is \u03b5-close to some point from \u0128, and \u2022 every point from \u0128 is \u03b5-close to some point from I. In other words, this means that the Hausdorff distance between the actual and observed images does not exceed \u03b5: dH(I, \u0128) \u2264 \u03b5. New approach reformulated in geometric terms. If the desired pattern P is present in the image, i.e., if I = TP , then: There exists a T for which dH(TP, \u0128) \u2264 \u03b5. (1) Vice versa, if for an observed image \u0128, this condition holds, this means that there exists a pattern TP which is consistent with the observed image, and therefore, it is quite possible that the observed image contains a desired pattern. Thus, the condition (1) expresses the fact that the observed image \u0128 is consistent with the assumption that the actual image contains the desired pattern. Hence, if we want to avoid false negatives (i.e., un-recognized patterns), we must check the condition (1). This is what the new approach does. How good is the new approach. \u2022 If the result of the new approach is negative, this means that the observed image does not contain the pattern; \u2022 on the other hand, if the result of this approach is positive, this means that it is possible that the observed image contains the pattern (i.e., that the observed image is consistent with the assumption that it is actually the shifted standard pattern). We cannot get any better than that. Of course, due to the observation inaccuracy, without additional assumptions, we can never guarantee that the image is actually the desired pattern: the actual image could as well be a slightly distorted pattern, and because of the observation inaccuracy, we do not notice this distortion. With this comment in mind, we can see that we cannot get any better pattern recognition than by using the new approach. Traditional approach reformulated in geometric terms. In traditional approach, we first look for components, i.e., we look for the possibility for representing the observed image \u0128 as a union of n sets \u01281, . . . , \u0128n such that for every i, the i-th component \u0128i of the observed image is consistent with it being actually a shift TiPi of i-th component Pi of the desired pattern P . Similarly to the above argument, we can conclude that the possibility for \u0128i to be actually a shift of Pi can be described as follows: There exists a Ti for which dH(TiPi, \u0128i) \u2264 \u03b5. (2) Therefore, if we want to avoid false negatives (i.e., if we do not want un-recognized patterns), we should look for a partition \u0128 = \u01281 \u222a . . . \u222a \u0128n which satisfies the property (2) for all i = 1, . . . , n. This is the first stage of the traditional approach. As a result of this stage: \u2022 If such a partition is impossible, then, based on the observation \u0128, we can conclude that the actual (unknown) image I does not coincide with the desired pattern, and therefore, the desired pattern is not present here. \u2022 On the other hand, if the partition is possible, i.e., if \u0128 = \u01281 \u222a . . .\u222a \u0128n with dH(\u0128i, TiPi) \u2264 \u03b5 for some shifts Ti, then it is not necessarily true that \u0128 can contain the desired pattern: it may happen that the shifts are too far away from each other. If the actual image I is indeed a shift of the standard pattern P , i.e., if I = TP for some T , then, due to possible observation inaccuracy, dH(\u0128i, TPi) \u2264 \u03b5. Based on the observed components \u0128i, we select shifts Ti for which dH(\u0128i, TiPi) \u2264 \u03b5. Therefore, we can conclude that if the actual image is indeed the shift of the standard pattern, then dH(TiPi, TPi) \u2264 dH(TiPi, \u0128i) + dH(\u0128i, TPi) \u2264 2\u03b5. The Hausdorff distance between two shifts TiPi and TPi of the same set is equal to the distance between d(Ti, T ) these shifts, i.e., to the Euclidean distance between the vectors corresponding to these shifts. So, we can conclude that if the pattern is present, then all the shifts Ti generated on the first stage should be 2\u03b5-close to some (unknown) shift T . This means, in turn, that for every i and j, we have d(Ti, Tj) \u2264 d(Ti, T ) + d(T, Tj) \u2264 4\u03b5. So, on the second stage of the traditional method, we check the following condition: d(Ti, Tj) \u2264 4\u03b5 for all i and j. (3) How good is the traditional approach. \u2022 If the result of traditional approach is negative, this means that the observed image does not contain the pattern; \u2022 on the other hand, if the result of this approach is positive, this does not necessarily mean that it is possible that the observed image contains the pattern; it is quite possible that the observed image is inconsistent with the assumption that it is actually the shifted standard pattern. Let us give a simple example explaining why this can happen. Let us consider a 2-component pattern P = | consisting of a vertical component P1 of length 1 and a horizontal component P2 of the same length 1. If we take the angle of P as the origin (0, 0) of the coordinate system, then P1 = {0} \u00d7 [0, 1] and P2 = [0, 1] \u00d7 {0}. Let us take \u0128 = \u01281 \u222a \u01282, where \u01281 = {\u22122\u03b5} \u00d7 [0, 1] and \u01282 = [2\u03b5, 1 + 2\u03b5]\u00d7 {0}. \u2022 For this image, the traditional approach can lead to a positive answer: indeed, here: \u2022 dH(\u01281, T1P1) \u2264 \u03b5 for T1 = (\u2212\u03b5, 0), \u2022 dH(\u01282, T2P2) \u2264 \u03b5 for T2 = (\u03b5, 0), and \u2022 d(T1, T2) = 2\u03b5 < 4\u03b5. \u2022 On the other hand, the image \u0128 is not consistent with the pattern P because, as one can easily see, dH(\u0128 , TP ) \u2265 2\u03b5 for all possible shifts T . So, the traditional approach is indeed not perfect. Open problem. In the above text, we simply gave an example of when a traditional method leads to unnecessary false positives. It is desirable to have a general numerical estimate of the quality of the traditional approach. In precise terms, we have the following problem: We have n compact sets Pi, and n compact sets \u0128i. We know that for every i from 1 to n, dH(\u0128i, TiPi) \u2264 \u03b5 for some shifts Ti for which d(Ti, Tj) \u2264 4\u03b5 for all i and j. What is the smallest possible value of th Hausdorff distance dH(\u0128 , TP ) between the union \u0128 = \u01281 \u222a . . . \u222a \u0128n and a shift TP of the union P = P1 \u222a . . . \u222a Pn? Our guess is that this smallest possible value is 3\u03b5. Our argument in favor of this guess is as follows: it looks like, since the diameter of the set {T1, . . . , Tn} is \u2264 4\u03b5, that its radius will be \u2264 2\u03b5, i.e., that there should exist a shift T for which d(Ti, T ) \u2264 2\u03b5 for all i. For this shift T , we have dH(\u0128i, TPi) \u2264 dH(\u0128i, TiPi) + dH(TiPi, TPi) \u2264 \u03b5+ d(Ti, T ) \u2264 \u03b5+ 2\u03b5 = 3\u03b5. Acknowledgments. This work was supported in part by NASA under cooperative agreement NCC5-209, by NSF grants No. DUE9750858 and CDA-9522207, by United Space Alliance, grant No. NAS 9-20000 (PWO C0C67713A6), by the Future Aerospace Science and Technology Program (\n\n",
                "DataExportTag": "AI421469",
                "QuestionID": "QID215",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Locating the Whole Pattern is Better than Locating its Pieces: A Geometric Explanation of an Empi...",
                "Choices": {
                    "1": {
                        "Display": "\"Mathematical Geometry and Data Clustering\""
                    },
                    "2": {
                        "Display": "distance, similarity, geometry, measure, entropy, dimensional, curve, symmetry, topological, euclidean, invariant, divergence, projection, clustering, transformation"
                    },
                    "3": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "4": {
                        "Display": "prediction, causal, uncertainty, loss, classification, active, fairness, semi_supervised, imputation, bias, calibration, estimation, real_world, ranking, instance"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID276",
            "SecondaryAttribute": "Locating the Whole Pattern is Better than Locating its Pieces: A Geometric Explanation of an Empi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Locating the Whole Pattern is Better than Locating its Pieces: A Geometric Explanation of an Empirical Phenomenon In many practical problems, we must find a pattern in an image. For situations in which the desired pattern consists of several simple components, the traditional approach is first to look for such components, and then to see whether the relative locations of these components are consistent with the pattern. Recent experiments have shown that a much more efficient pattern recognition can be achieved if we look for the whole pattern (without decomposing it first). In this paper, we give a simple geometric explanation of this empirical fact. The practical problem. In many pattern recognition problems, we must locate a known simple pattern in a complicated black-andwhite image. For example: \u2022 in automatic analysis of electronic schemes, we must locate symbols of standard electronic components (such as \u2212| |\u2212); \u2022 in text recognition, we must find letters, \u2022 similar pattern matching problems arise in satellite imaging, etc. Traditional approach. Most traditional methods for solving this problem are based on the fact that the desired pattern consists of simple geometric components (straight line intervals, arcs, etc.) For example, the above symbol for capacitor consists of four straight line intervals \u2212, |, |, and \u2212. Traditional methods consist of two stages: \u2022 first, we try to locate each component of the desired pattern; \u2022 after all components are located, we check that their relative locations are close to the relative locations of these components in the desired pattern (to be more precise, we check that the difference between the observed and desired relative locations is within the limits set by the observation inaccuracy of component location). A new approach turns out to be better. The authors of [Murshed Bortolozzi 1998] propose to recognize the entire pattern (symbol) without first decomposing it into simple components. The resulting algorithm requires more computation time, but leads to much better recognition: namely, if we set up the parameters of both methods in such a way as to avoid false negatives (unrecognized symbols), then for the new method, the number of false positives (false recognitions of a pattern) is much smaller than for the traditional methods. In this paper, we give a simple geometric explanation of this empirical phenomenon. Geometric reformulation of the problem. We start with a sample pattern P which consists of several components Pi: P = P1 \u222a . . . \u222a Pn. Without losing generality, we can assume that P is a compact set. In the actual image, the actual pattern may be shifted relative to the standard one, so this actual pattern has the form TP for some shift (translation) T . For simplicity, we will assume that the pattern is surrounded by an empty space, i.e. (at least locally): \u2022 either the actual image I coincides with the shifted pattern TP , in which case the pattern is present, \u2022 or the actual image is different from the shifted pattern, in which case the pattern is not here. Description of measurement inaccuracy. Due to measurement inaccuracy, the observed image \u0128 is, in general, slightly different from the actual image I. Namely, due to this inaccuracy, for each point p from the original image, the corresponding observed point p\u0303 may be different from p. The observation inaccuracy can be characterized by the largest possible distance d(p\u0303, p) between the actual and the observed points. If this inaccuracy is \u03b5 > 0, this means that: \u2022 every point from I is \u03b5-close to some point from \u0128, and \u2022 every point from \u0128 is \u03b5-close to some point from I. In other words, this means that the Hausdorff distance between the actual and observed images does not exceed \u03b5: dH(I, \u0128) \u2264 \u03b5. New approach reformulated in geometric terms. If the desired pattern P is present in the image, i.e., if I = TP , then: There exists a T for which dH(TP, \u0128) \u2264 \u03b5. (1) Vice versa, if for an observed image \u0128, this condition holds, this means that there exists a pattern TP which is consistent with the observed image, and therefore, it is quite possible that the observed image contains a desired pattern. Thus, the condition (1) expresses the fact that the observed image \u0128 is consistent with the assumption that the actual image contains the desired pattern. Hence, if we want to avoid false negatives (i.e., un-recognized patterns), we must check the condition (1). This is what the new approach does. How good is the new approach. \u2022 If the result of the new approach is negative, this means that the observed image does not contain the pattern; \u2022 on the other hand, if the result of this approach is positive, this means that it is possible that the observed image contains the pattern (i.e., that the observed image is consistent with the assumption that it is actually the shifted standard pattern). We cannot get any better than that. Of course, due to the observation inaccuracy, without additional assumptions, we can never guarantee that the image is actually the desired pattern: the actual image could as well be a slightly distorted pattern, and because of the observation inaccuracy, we do not notice this distortion. With this comment in mind, we can see that we cannot get any better pattern recognition than by using the new approach. Traditional approach reformulated in geometric terms. In traditional approach, we first look for components, i.e., we look for the possibility for representing the observed image \u0128 as a union of n sets \u01281, . . . , \u0128n such that for every i, the i-th component \u0128i of the observed image is consistent with it being actually a shift TiPi of i-th component Pi of the desired pattern P . Similarly to the above argument, we can conclude that the possibility for \u0128i to be actually a shift of Pi can be described as follows: There exists a Ti for which dH(TiPi, \u0128i) \u2264 \u03b5. (2) Therefore, if we want to avoid false negatives (i.e., if we do not want un-recognized patterns), we should look for a partition \u0128 = \u01281 \u222a . . . \u222a \u0128n which satisfies the property (2) for all i = 1, . . . , n. This is the first stage of the traditional approach. As a result of this stage: \u2022 If such a partition is impossible, then, based on the observation \u0128, we can conclude that the actual (unknown) image I does not coincide with the desired pattern, and therefore, the desired pattern is not present here. \u2022 On the other hand, if the partition is possible, i.e., if \u0128 = \u01281 \u222a . . .\u222a \u0128n with dH(\u0128i, TiPi) \u2264 \u03b5 for some shifts Ti, then it is not necessarily true that \u0128 can contain the desired pattern: it may happen that the shifts are too far away from each other. If the actual image I is indeed a shift of the standard pattern P , i.e., if I = TP for some T , then, due to possible observation inaccuracy, dH(\u0128i, TPi) \u2264 \u03b5. Based on the observed components \u0128i, we select shifts Ti for which dH(\u0128i, TiPi) \u2264 \u03b5. Therefore, we can conclude that if the actual image is indeed the shift of the standard pattern, then dH(TiPi, TPi) \u2264 dH(TiPi, \u0128i) + dH(\u0128i, TPi) \u2264 2\u03b5. The Hausdorff distance between two shifts TiPi and TPi of the same set is equal to the distance between d(Ti, T ) these shifts, i.e., to the Euclidean distance between the vectors corresponding to these shifts. So, we can conclude that if the pattern is present, then all the shifts Ti generated on the first stage should be 2\u03b5-close to some (unknown) shift T . This means, in turn, that for every i and j, we have d(Ti, Tj) \u2264 d(Ti, T ) + d(T, Tj) \u2264 4\u03b5. So, on the second stage of the traditional method, we check the following condition: d(Ti, Tj) \u2264 4\u03b5 for all i and j. (3) How good is the traditional approach. \u2022 If the result of traditional approach is negative, this means that the observed image does not contain the pattern; \u2022 on the other hand, if the result of this approach is positive, this does not necessarily mean that it is possible that the observed image contains the pattern; it is quite possible that the observed image is inconsistent with the assumption that it is actually the shifted standard pattern. Let us give a simple example explaining why this can happen. Let us consider a 2-component pattern P = | consisting of a vertical component P1 of length 1 and a horizontal component P2 of the same length 1. If we take the angle of P as the origin (0, 0) of the coordinate system, then P1 = {0} \u00d7 [0, 1] and P2 = [0, 1] \u00d7 {0}. Let us take \u0128 = \u01281 \u222a \u01282, where \u01281 = {\u22122\u03b5} \u00d7 [0, 1] and \u01282 = [2\u03b5, 1 + 2\u03b5]\u00d7 {0}. \u2022 For this image, the traditional approach can lead to a positive answer: indeed, here: \u2022 dH(\u01281, T1P1) \u2264 \u03b5 for T1 = (\u2212\u03b5, 0), \u2022 dH(\u01282, T2P2) \u2264 \u03b5 for T2 = (\u03b5, 0), and \u2022 d(T1, T2) = 2\u03b5 < 4\u03b5. \u2022 On the other hand, the image \u0128 is not consistent with the pattern P because, as one can easily see, dH(\u0128 , TP ) \u2265 2\u03b5 for all possible shifts T . So, the traditional approach is indeed not perfect. Open problem. In the above text, we simply gave an example of when a traditional method leads to unnecessary false positives. It is desirable to have a general numerical estimate of the quality of the traditional approach. In precise terms, we have the following problem: We have n compact sets Pi, and n compact sets \u0128i. We know that for every i from 1 to n, dH(\u0128i, TiPi) \u2264 \u03b5 for some shifts Ti for which d(Ti, Tj) \u2264 4\u03b5 for all i and j. What is the smallest possible value of th Hausdorff distance dH(\u0128 , TP ) between the union \u0128 = \u01281 \u222a . . . \u222a \u0128n and a shift TP of the union P = P1 \u222a . . . \u222a Pn? Our guess is that this smallest possible value is 3\u03b5. Our argument in favor of this guess is as follows: it looks like, since the diameter of the set {T1, . . . , Tn} is \u2264 4\u03b5, that its radius will be \u2264 2\u03b5, i.e., that there should exist a shift T for which d(Ti, T ) \u2264 2\u03b5 for all i. For this shift T , we have dH(\u0128i, TPi) \u2264 dH(\u0128i, TiPi) + dH(TiPi, TPi) \u2264 \u03b5+ d(Ti, T ) \u2264 \u03b5+ 2\u03b5 = 3\u03b5. Acknowledgments. This work was supported in part by NASA under cooperative agreement NCC5-209, by NSF grants No. DUE9750858 and CDA-9522207, by United Space Alliance, grant No. NAS 9-20000 (PWO C0C67713A6), by the Future Aerospace Science and Technology Program (\n\n",
                "DataExportTag": "AI421469",
                "QuestionID": "QID276",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Locating the Whole Pattern is Better than Locating its Pieces: A Geometric Explanation of an Empi...",
                "Choices": {
                    "1": {
                        "Display": "\"Mathematical Geometry and Data Clustering\""
                    },
                    "2": {
                        "Display": "distance, similarity, geometry, measure, entropy, dimensional, curve, symmetry, topological, euclidean, invariant, divergence, projection, clustering, transformation"
                    },
                    "3": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "4": {
                        "Display": "prediction, causal, uncertainty, loss, classification, active, fairness, semi_supervised, imputation, bias, calibration, estimation, real_world, ranking, instance"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID126",
            "SecondaryAttribute": "MAIN OUTCOMES AND MEASURES The primary outcome was severe postoperative complications (Clavien-Di...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "MAIN OUTCOMES AND MEASURES\nThe primary outcome was severe postoperative complications (Clavien-Dindo score >IIIa) within 90 days. Secondary outcomes included other postoperative complications, reoperations, length of operating time, length of postoperative hospital stay, and quality of life.\n\n",
                "DataExportTag": "54",
                "QuestionID": "QID126",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "MAIN OUTCOMES AND MEASURES The primary outcome was severe postoperative complications (Clavien-Di...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID120",
            "SecondaryAttribute": "MAIN RESULTS Four RCTs were identified, each using a different image guided technique: 1. iMRI (5...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "MAIN RESULTS\nFour RCTs were identified, each using a different image guided technique: 1. iMRI (58 patients), 2. 5-aminolevulinic acid (5-ALA) fluorescence guided surgery (322 patients), 3. neuronavigation (45 patients) and 4. DTI-neuronavigation (238 patients). Meta-analysis was not appropriate due to differences in the tumours included (eloquent versus non-eloquent locations) and variations in the image guidance tools used in the control arms (usually selective utilisation of neuronavigation). There were significant concerns regarding risk of bias in all the included studies, especially for the study using DTI-neuronavigation. All studies included patients with high grade glioma, with one study also including patients with low grade glioma. The extent of resection was increased with iMRI (risk ratio (RR) (incomplete resection) 0.13, 95% CI 0.02 to 0.96, low quality evidence), 5-ALA (RR 0.55, 95% CI 0.42 to 0.71) and DTI-neuronavigation (RR 0.35, 95% CI 0.20 to 0.63, very low quality evidence). Insufficient data were available to evaluate the effects of neuronavigation on extent of resection. Reporting of adverse events was incomplete, with a suggestion of significant reporting bias. Overall, reported events were low in most studies, but there was concern that surgical resection using 5-ALA may lead to more frequent early neurological deficits. There was no clear evidence of improvement in overall survival (OS) with 5-ALA (hazard ratio (HR) 0.82, 95% CI 0.62 to 1.07) or DTI-neuronavigation (HR 0.57, 95% CI 0.32 to 1.00) in patients with high grade glioma. Progression-free survival (PFS) data were not available in the appropriate format for analysis.Data for quality of life (QoL) were only available for one study and suffered from significant attrition bias.\n\n",
                "DataExportTag": "48",
                "QuestionID": "QID120",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "MAIN RESULTS Four RCTs were identified, each using a different image guided technique: 1. iMRI (5...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID183",
            "SecondaryAttribute": "Major histocompatibility complex class i and tumour immuno-evasion: how to fool T cells and natur...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Major histocompatibility complex class i and tumour immuno-evasion: how to fool T cells and natural killer cells at one time. Cytotoxic T lymphocytes (ctls) and natural killer (nk) cells lyse tumours expressing and lacking, respectively, properly conformed class i molecules of the major histocompatibility complex [mhc-i (Figure 1)]. In keeping with the \u201cmissing self\u201d hypothesis 1, a logical extrapolation would be to postulate that the primary goal of a tumour is to elude both defense lines.\n\n",
                "DataExportTag": "CAN578782",
                "QuestionID": "QID183",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Major histocompatibility complex class i and tumour immuno-evasion: how to fool T cells and natur...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID244",
            "SecondaryAttribute": "Major histocompatibility complex class i and tumour immuno-evasion: how to fool T cells and natur...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Major histocompatibility complex class i and tumour immuno-evasion: how to fool T cells and natural killer cells at one time. Cytotoxic T lymphocytes (ctls) and natural killer (nk) cells lyse tumours expressing and lacking, respectively, properly conformed class i molecules of the major histocompatibility complex [mhc-i (Figure 1)]. In keeping with the \u201cmissing self\u201d hypothesis 1, a logical extrapolation would be to postulate that the primary goal of a tumour is to elude both defense lines.\n\n",
                "DataExportTag": "CAN578782",
                "QuestionID": "QID244",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Major histocompatibility complex class i and tumour immuno-evasion: how to fool T cells and natur...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID387",
            "SecondaryAttribute": "MATERIAL AND METHOD Retrospective study of the 15 patients with liver and kidney transplants perf...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "MATERIAL AND METHOD\nRetrospective study of the 15 patients with liver and kidney transplants performed in our Hospital. We have reviewed patients main characteristics, liver and renal failure causes, renal graft and patient outcome and complications relate to renal transplant.\n\n",
                "DataExportTag": "41",
                "QuestionID": "QID387",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "MATERIAL AND METHOD Retrospective study of the 15 patients with liver and kidney transplants perf...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID378",
            "SecondaryAttribute": "MATERIALS AND METHODS From July 1971 through December 1997, 1,054 patients underwent radical cyst...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "MATERIALS AND METHODS\nFrom July 1971 through December 1997, 1,054 patients underwent radical cystectomy and bilateral pelvic-iliac lymphadenectomy for high grade, invasive transitional cell carcinoma of the bladder. Of these patients 244 (23%) with a median age of 66 years (range 36 to 90) had pathological lymph node metastases. Overall 139 of the 244 patients (57%) received some form of chemotherapy. At a median followup of greater than 10 years (range 0 to 28) outcomes data were analyzed in univariate analysis according to tumor grade, carcinoma in situ, primary bladder tumor stage, pathological subgroups, total number of lymph nodes removed and involved with tumor, and lymph node density (total number of positive lymph nodes\/total number removed). In addition, the form of urinary diversion and the administration of chemotherapy were also evaluated. Multivariate analysis was then performed to analyze these variables independently.\n\n",
                "DataExportTag": "32",
                "QuestionID": "QID378",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "MATERIALS AND METHODS From July 1971 through December 1997, 1,054 patients underwent radical cyst...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID336",
            "SecondaryAttribute": "Maximal Consistent Interpretations of Errorful Data in Hierarchically Modeled Domains A method is...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Maximal Consistent Interpretations of Errorful Data in Hierarchically Modeled Domains A method is presented fo r c o n s t r u c t i n g maximal cons is ten t t n t e p r e t a t i o n s of e r r o r ! u l d a t a . The method appears a p p l i c a b l e to many tasks (speech unders tand ing , n a t u r a l language understanding;, v i s i o n , medical d iagnos is ) r e q u i r i n g p a r t i a l m a t c h i n g o f e r r o r f u l data aga inst complex, h i e r a r c h i c a l l y de f ined p a t t e r n s . The data is represented as symbolic s t r u c t u r e s (word sequences, l i n e segment c o n f i g u r a t i o n s , disease symptoms). Errors consist of missing data (unrecognized words, occluded l i n e s , undetected symptoms) and e x t r a (poss ib ly incons is ten t ) data ( i n c o r r e c t l y recognized words, v i s u a l n o i s e , spur ious symptoms). Data i n t e r p r e t a t i o n s correspond to subst ruc tures of a h ie rarchy of concepts . Cons t ra in ts on cons is ten t predef ined conceptual h i e r a r c h y . c o r r e c t l y fragments speech s t r u c t u r e s embedded t h e An imp1erne nta t ion of the me t hod has sets of sentence the HEARSAY-II system. The i n t e r p r e t e d e r r o r f u l recognized by understanding Implementat ion has a lso c o r r e c t l y i n t e r p r e t e d t y p e d i n ungrammatical sentences. D e t a i l e d examples i l l u s t r a t e o p e r a t i o n of the method on rea l d a t a . 0DUCT10N The a p p l i c a t i o n of Al methods to complex domains ( e . g . , spe ec h , v i s ion , medical d i agn os is ) has expanded the dimensions of data i n t e r p r e t a t i o n to incorpora te some novel phenomena. Two of these phenomena are data e r r o r and h i e r a r c h i c a l l y de f ined data p a t t e r n s . Many complex domains are c h a r a c t e r i z e d by e r r o r f u l d a t a . E r rors such as i n s e r t i o n , d e l e t i o n , s u b s t i t u t i o n , in f orina t ion incrcase as source data t r a n s d u c t i o n i n cr eases . D a t a ma y be in that two or more piece: be e x p l a i n e d c o n s i s t e n t l y , inconsis t enc i es in t he and r e p e t i t i o n of the u n c e r t a i n t y of and i n t e r p r e t a t i o n mut vial ly i nco n s i s t en t s of i n fo rmat ion cannot T o l e r a t i n g e r r o r and data r e q u i r e s robust methods that can not only f i n d the best i n t e r p r e t a t i o n but are able to d i s t i n g u i s h the incons is ten t and e r r o r f u l data from the cons is ten t d a t a . Another aspect of data i n t e r p r e t a t i o n in complex domains is that i n t e r p r e t a t i o n s represent complex, h i e r a r c h i c a l l y de f ined concepts ( i d e a s , r u l e s , p a t t e r n s ) r a t h e r than s i m p l e , independent concepts ( f e a t u r e s ) . Of ten the concepts used in i n t e r p r e t a t i o n s can be placed in a h ierarchy where each concept is de f ined in terms \"of i t s subconcepts. Th is s t r u c t u r e of concepts is c a l l e d a conceptual h i e r a r c h y . A c o l l e c t i o n oi data can then be i n t e r p r e t e d by the highest concept in the h ie ra rchy supported ( v a l i d a t e d ) by the d a t a . The i n t e r p r e t a t i o n of the data is de f ined by the concept 's descendants (subconcepts, subsubconcepts, e t c . ) and the data which supports them. These descendants form a s u b s t r u c t u r e of the conceptual h i e r a r c h y . The general data i n t e r p r e t a t i o n problem can now be r e s t a t e d as a search f o r the concept in the conceptual h ie ra rchy that e x p l a i n s ( i s supported by) the most d a t a . The data suppor t ing the s t r u c t u r e under ly ing t h i s maximal concept can be descr ibed as the maximal c o n s i s t e n t subset of d a t a . In t h i s paper we de f ine conceptual This work was supported in p a r t by the Defense Advanced Research P r o j e c t s Agency under c o n t r a c t no. F 4 4 6 2 0 7 3 O 0 0 7 4 and monitored by the A i r Force O f f i c e of S c i e n t i f i c Research. In a d d i t i o n , the f i r s t author was p a r t i a l l y supported by a N a t i o n a l Research Counci l of Canada Postgraduate Scholarship and the second author was p a r t i a l l y supported by a N a t i o n a l Science Foundation Graduate F e l l o w s h i p . h i e r a r c h i e s and maximal cons is ten t i n t e r p r e t a t i o n s . We then descr ibe a method f o r i n t e r p r e t i n g data in such an environment, i . e . , f i n d i n g maximal cons is ten t i n t e r p r e t a t i o n s in a conceptual h i e r a r c h y . Examples i l l u s t r a t i n g the method are shown. F i n a l l y , we show the ac tua l a p p l i c a t i o n of the method to the problem of i n t e r p r e t i n g e r r o r f u l sentence fragments recognized by the HEARSAY-II speech understanding system (Erman, 19 7 5 ) . 2. A REAL EXAMPLE The ma tch ing problem used as throughout t h i s paper is taken from speech understanding system. When unable to complete ly recogniz sentence ( u t t e r a n c e ) , i t generat'* sentence fragments (Hayes-Rotn et ai , must be i n t e r p r e t e d by the semanticmodule, named SGI ANT. The generat can be both e r r o r f u l a i n c o n s i s t e n t (Example 2 . 1 ) . A senten a chunk of cons is ten t data in that it grammat ica l ly p l a u s i b l e sequence words. HEARSAY-II mechanisms i d e n t i f y i n g such chunks are not s u i t e them i n t o an o v e r a l l cons is tent i n t tlie u t t e r a n c e . EXAMPLE 2. 1 an example the HEARSAY-II HEARSAY-II is e a spoken s a set of 19 76c) which i n t e r p r e t a t i o n ed fragments nd mutual ly ce fragment is consists of a of recognized e f f e c t i v e i n d to combining e r p r e t a t i o n of Fragment p o r t i o n of the 1-3 conta in 1 and 2 are they provide the over lapp ing 1 6. 3, 1 & 1: [ WHAT HAS HERBERT 2: PAPER ABOUT PATTERN MATCHING ] 3: IN LEARNING OR PATTERN MATCHING J 4: [ WHO Correct Sentence: [ WHO HAS WRITTEN ABOUT PATTERN MATCHING ] Example 2 . 1 shows four sentence fragments generated when HEARSAY-II was unable to recognize the sentence [ WHO HAS WRITTEN ABOUT PATTERN MATCHING ] . The square brackets denote the s t a r t and f i n i s h of the spoken u t t e r a n c e . The numbers enclosed in angle brackets s p e c i f y , in cent iseconds, how long a f t e r the s t a r t of the u t te rance each fragment begins and ends. 4 c o r r e c t l y matches the i n i t i a l spoken sentence. Fragments s u b s t i t u t i o n e r r o r s . Fragments mutual ly i n c o n s i s t e n t in that d i f f e r e n t i n t e r p r e t a t i o n s o f the time per iod . The fragment p a i r s 4. and 2 & 3 are i n c o n s i s t e n t for the same reason. A lso , Fragment I s p e c i f i c s a WHAT quest ion whereas fragment 4 s p e c i f i e s a WHO q u e s t i o n . Thus Fragments 1 and 4 are semant l e a l ly i n c o n s i s t e n t , I r r e g a r d less of t h e i r t imes . Each \" fragment is semant i c a l l y descr ibed by a h i e r a r c h i c a l l y s t r u c t u r e d c o l l e c t i o n of concepts. F igure 2 .1 shows a p o r t i o n of the conceptual h ie rarchy used by the SEMANT module in HEARSAY-II . F igure 2.2 shows the h i e r a r c h i c a l d e s c r i p t i o n of the cor rec t sentence. The problem of i n t e r p r e t i n g these fragments i l l u s t r a t e s the phenomena of data e r r o r and h i e r a r c h i c a l l y s t r u c t u r e d i n t e r p r e t a t i o n s . The method used f o r s o l v i n g t h i s problem appears a p p l i c a b l e to a s i g n i f i c a n t c lass of problems e x h i b i t i n g these two phenomena. 3. CONCEPTUAL HT FERARC.HIES A conceptual h ie ra rchy can be represented by a d i r e c t e d graph of concepts. Th is graph is trees t r u c t u r e d In tha t i t has a root at the to leaf nodes at the bottom; however p e r m i t t e d . The sons of a node subconcepts that compose the f a t h e r , of the graph de f ines the h ighest g e n e r a l ) i n t e r p r e t a t i o n o f a l l beneath i t . A g iven i n t e r p r e t a t i o n task has top and cycles are de f ine the The root l e v e l (most the concepts\n\n",
                "DataExportTag": "AI58829",
                "QuestionID": "QID336",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Maximal Consistent Interpretations of Errorful Data in Hierarchically Modeled Domains A method is...",
                "Choices": {
                    "1": {
                        "Display": "\"Hydrological Forecasting and Water Management\""
                    },
                    "2": {
                        "Display": "flood, rainfall, forecast, river, drought, runoff, hydrological, rmse, reservoir, precipitation, water, root_mean, temperature, dam, streamflow"
                    },
                    "3": {
                        "Display": "\"Water Management and Flood Risk Assessment\""
                    },
                    "4": {
                        "Display": "water, flood, river, groundwater, flow, runoff, reservoir, catchment, watershed, lake, basin, decision_making, discharge, hydrological, risk"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID405",
            "SecondaryAttribute": "Maximal Consistent Interpretations of Errorful Data in Hierarchically Modeled Domains A method is...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Maximal Consistent Interpretations of Errorful Data in Hierarchically Modeled Domains A method is presented fo r c o n s t r u c t i n g maximal cons is ten t t n t e p r e t a t i o n s of e r r o r ! u l d a t a . The method appears a p p l i c a b l e to many tasks (speech unders tand ing , n a t u r a l language understanding;, v i s i o n , medical d iagnos is ) r e q u i r i n g p a r t i a l m a t c h i n g o f e r r o r f u l data aga inst complex, h i e r a r c h i c a l l y de f ined p a t t e r n s . The data is represented as symbolic s t r u c t u r e s (word sequences, l i n e segment c o n f i g u r a t i o n s , disease symptoms). Errors consist of missing data (unrecognized words, occluded l i n e s , undetected symptoms) and e x t r a (poss ib ly incons is ten t ) data ( i n c o r r e c t l y recognized words, v i s u a l n o i s e , spur ious symptoms). Data i n t e r p r e t a t i o n s correspond to subst ruc tures of a h ie rarchy of concepts . Cons t ra in ts on cons is ten t predef ined conceptual h i e r a r c h y . c o r r e c t l y fragments speech s t r u c t u r e s embedded t h e An imp1erne nta t ion of the me t hod has sets of sentence the HEARSAY-II system. The i n t e r p r e t e d e r r o r f u l recognized by understanding Implementat ion has a lso c o r r e c t l y i n t e r p r e t e d t y p e d i n ungrammatical sentences. D e t a i l e d examples i l l u s t r a t e o p e r a t i o n of the method on rea l d a t a . 0DUCT10N The a p p l i c a t i o n of Al methods to complex domains ( e . g . , spe ec h , v i s ion , medical d i agn os is ) has expanded the dimensions of data i n t e r p r e t a t i o n to incorpora te some novel phenomena. Two of these phenomena are data e r r o r and h i e r a r c h i c a l l y de f ined data p a t t e r n s . Many complex domains are c h a r a c t e r i z e d by e r r o r f u l d a t a . E r rors such as i n s e r t i o n , d e l e t i o n , s u b s t i t u t i o n , in f orina t ion incrcase as source data t r a n s d u c t i o n i n cr eases . D a t a ma y be in that two or more piece: be e x p l a i n e d c o n s i s t e n t l y , inconsis t enc i es in t he and r e p e t i t i o n of the u n c e r t a i n t y of and i n t e r p r e t a t i o n mut vial ly i nco n s i s t en t s of i n fo rmat ion cannot T o l e r a t i n g e r r o r and data r e q u i r e s robust methods that can not only f i n d the best i n t e r p r e t a t i o n but are able to d i s t i n g u i s h the incons is ten t and e r r o r f u l data from the cons is ten t d a t a . Another aspect of data i n t e r p r e t a t i o n in complex domains is that i n t e r p r e t a t i o n s represent complex, h i e r a r c h i c a l l y de f ined concepts ( i d e a s , r u l e s , p a t t e r n s ) r a t h e r than s i m p l e , independent concepts ( f e a t u r e s ) . Of ten the concepts used in i n t e r p r e t a t i o n s can be placed in a h ierarchy where each concept is de f ined in terms \"of i t s subconcepts. Th is s t r u c t u r e of concepts is c a l l e d a conceptual h i e r a r c h y . A c o l l e c t i o n oi data can then be i n t e r p r e t e d by the highest concept in the h ie ra rchy supported ( v a l i d a t e d ) by the d a t a . The i n t e r p r e t a t i o n of the data is de f ined by the concept 's descendants (subconcepts, subsubconcepts, e t c . ) and the data which supports them. These descendants form a s u b s t r u c t u r e of the conceptual h i e r a r c h y . The general data i n t e r p r e t a t i o n problem can now be r e s t a t e d as a search f o r the concept in the conceptual h ie ra rchy that e x p l a i n s ( i s supported by) the most d a t a . The data suppor t ing the s t r u c t u r e under ly ing t h i s maximal concept can be descr ibed as the maximal c o n s i s t e n t subset of d a t a . In t h i s paper we de f ine conceptual This work was supported in p a r t by the Defense Advanced Research P r o j e c t s Agency under c o n t r a c t no. F 4 4 6 2 0 7 3 O 0 0 7 4 and monitored by the A i r Force O f f i c e of S c i e n t i f i c Research. In a d d i t i o n , the f i r s t author was p a r t i a l l y supported by a N a t i o n a l Research Counci l of Canada Postgraduate Scholarship and the second author was p a r t i a l l y supported by a N a t i o n a l Science Foundation Graduate F e l l o w s h i p . h i e r a r c h i e s and maximal cons is ten t i n t e r p r e t a t i o n s . We then descr ibe a method f o r i n t e r p r e t i n g data in such an environment, i . e . , f i n d i n g maximal cons is ten t i n t e r p r e t a t i o n s in a conceptual h i e r a r c h y . Examples i l l u s t r a t i n g the method are shown. F i n a l l y , we show the ac tua l a p p l i c a t i o n of the method to the problem of i n t e r p r e t i n g e r r o r f u l sentence fragments recognized by the HEARSAY-II speech understanding system (Erman, 19 7 5 ) . 2. A REAL EXAMPLE The ma tch ing problem used as throughout t h i s paper is taken from speech understanding system. When unable to complete ly recogniz sentence ( u t t e r a n c e ) , i t generat'* sentence fragments (Hayes-Rotn et ai , must be i n t e r p r e t e d by the semanticmodule, named SGI ANT. The generat can be both e r r o r f u l a i n c o n s i s t e n t (Example 2 . 1 ) . A senten a chunk of cons is ten t data in that it grammat ica l ly p l a u s i b l e sequence words. HEARSAY-II mechanisms i d e n t i f y i n g such chunks are not s u i t e them i n t o an o v e r a l l cons is tent i n t tlie u t t e r a n c e . EXAMPLE 2. 1 an example the HEARSAY-II HEARSAY-II is e a spoken s a set of 19 76c) which i n t e r p r e t a t i o n ed fragments nd mutual ly ce fragment is consists of a of recognized e f f e c t i v e i n d to combining e r p r e t a t i o n of Fragment p o r t i o n of the 1-3 conta in 1 and 2 are they provide the over lapp ing 1 6. 3, 1 & 1: [ WHAT HAS HERBERT 2: PAPER ABOUT PATTERN MATCHING ] 3: IN LEARNING OR PATTERN MATCHING J 4: [ WHO Correct Sentence: [ WHO HAS WRITTEN ABOUT PATTERN MATCHING ] Example 2 . 1 shows four sentence fragments generated when HEARSAY-II was unable to recognize the sentence [ WHO HAS WRITTEN ABOUT PATTERN MATCHING ] . The square brackets denote the s t a r t and f i n i s h of the spoken u t t e r a n c e . The numbers enclosed in angle brackets s p e c i f y , in cent iseconds, how long a f t e r the s t a r t of the u t te rance each fragment begins and ends. 4 c o r r e c t l y matches the i n i t i a l spoken sentence. Fragments s u b s t i t u t i o n e r r o r s . Fragments mutual ly i n c o n s i s t e n t in that d i f f e r e n t i n t e r p r e t a t i o n s o f the time per iod . The fragment p a i r s 4. and 2 & 3 are i n c o n s i s t e n t for the same reason. A lso , Fragment I s p e c i f i c s a WHAT quest ion whereas fragment 4 s p e c i f i e s a WHO q u e s t i o n . Thus Fragments 1 and 4 are semant l e a l ly i n c o n s i s t e n t , I r r e g a r d less of t h e i r t imes . Each \" fragment is semant i c a l l y descr ibed by a h i e r a r c h i c a l l y s t r u c t u r e d c o l l e c t i o n of concepts. F igure 2 .1 shows a p o r t i o n of the conceptual h ie rarchy used by the SEMANT module in HEARSAY-II . F igure 2.2 shows the h i e r a r c h i c a l d e s c r i p t i o n of the cor rec t sentence. The problem of i n t e r p r e t i n g these fragments i l l u s t r a t e s the phenomena of data e r r o r and h i e r a r c h i c a l l y s t r u c t u r e d i n t e r p r e t a t i o n s . The method used f o r s o l v i n g t h i s problem appears a p p l i c a b l e to a s i g n i f i c a n t c lass of problems e x h i b i t i n g these two phenomena. 3. CONCEPTUAL HT FERARC.HIES A conceptual h ie ra rchy can be represented by a d i r e c t e d graph of concepts. Th is graph is trees t r u c t u r e d In tha t i t has a root at the to leaf nodes at the bottom; however p e r m i t t e d . The sons of a node subconcepts that compose the f a t h e r , of the graph de f ines the h ighest g e n e r a l ) i n t e r p r e t a t i o n o f a l l beneath i t . A g iven i n t e r p r e t a t i o n task has top and cycles are de f ine the The root l e v e l (most the concepts\n\n",
                "DataExportTag": "AI58829",
                "QuestionID": "QID405",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Maximal Consistent Interpretations of Errorful Data in Hierarchically Modeled Domains A method is...",
                "Choices": {
                    "1": {
                        "Display": "\"Hydrological Forecasting and Water Management\""
                    },
                    "2": {
                        "Display": "flood, rainfall, forecast, river, drought, runoff, hydrological, rmse, reservoir, precipitation, water, root_mean, temperature, dam, streamflow"
                    },
                    "3": {
                        "Display": "\"Water Management and Flood Risk Assessment\""
                    },
                    "4": {
                        "Display": "water, flood, river, groundwater, flow, runoff, reservoir, catchment, watershed, lake, basin, decision_making, discharge, hydrological, risk"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID103",
            "SecondaryAttribute": "MEASURES Content analysis was used to analyze the interview data.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "MEASURES\nContent analysis was used to analyze the interview data.\n\n",
                "DataExportTag": "31",
                "QuestionID": "QID103",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "MEASURES Content analysis was used to analyze the interview data.",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID191",
            "SecondaryAttribute": "METHODS Between September 2011 and September 2014, 13 patients with refractory esophageal anastom...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "METHODS\nBetween September 2011 and September 2014, 13 patients with refractory esophageal anastomotic strictures were treated with endoscopic incision. Their clinical data were retrospectively collected to evaluate the efficacy and safety of the technique.\n\n",
                "DataExportTag": "31",
                "QuestionID": "QID191",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "METHODS Between September 2011 and September 2014, 13 patients with refractory esophageal anastom...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID252",
            "SecondaryAttribute": "METHODS Between September 2011 and September 2014, 13 patients with refractory esophageal anastom...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "METHODS\nBetween September 2011 and September 2014, 13 patients with refractory esophageal anastomotic strictures were treated with endoscopic incision. Their clinical data were retrospectively collected to evaluate the efficacy and safety of the technique.\n\n",
                "DataExportTag": "31",
                "QuestionID": "QID252",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "METHODS Between September 2011 and September 2014, 13 patients with refractory esophageal anastom...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID315",
            "SecondaryAttribute": "METHODS Proton beams of 155- and 200-MeV were used to irradiate a variety of phantom materials an...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "METHODS\nProton beams of 155- and 200-MeV were used to irradiate a variety of phantom materials and secondary particles were detected using organic liquid scintillators. These detectors are sensitive to fast neutrons and gamma rays: pulse shape discrimination was used to classify each detected pulse as either a neutron or a gamma ray. The mcnpx-PoliMi code was used to simulate the secondary neutron field produced during proton irradiation of the same tissue-equivalent phantom materials.\n\n",
                "DataExportTag": "33",
                "QuestionID": "QID315",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "METHODS Proton beams of 155- and 200-MeV were used to irradiate a variety of phantom materials an...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID311",
            "SecondaryAttribute": "METHODS Select ACS and internal quality indicators were implemented system wide in 2014. We compa...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "METHODS\nSelect ACS and internal quality indicators were implemented system wide in 2014. We compared compliance with these measures before and after their implementation at the main hospital and two new affiliate institutions.\n\n",
                "DataExportTag": "29",
                "QuestionID": "QID311",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "METHODS Select ACS and internal quality indicators were implemented system wide in 2014. We compa...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID318",
            "SecondaryAttribute": "Multifrequency ultrasound transducers for conformal interstitial thermal therapy Control over the...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Multifrequency ultrasound transducers for conformal interstitial thermal therapy Control over the pattern of thermal damage generated by interstitial ultrasound heating applicators can be enhanced by changing the ultrasound frequency during heating. The ability to change transmission frequency from a single transducer through the use of high impedance front layers was investigated in this study. The transmission spectrum of multifrequency transducers was calculated using the KLM equivalent circuit model and verified with experimental measurements on prototype transducers. The addition of a quarter-wavelength thick PZT (unpoled) front layer enabled the transmission of ultrasound at two discrete frequencies, 4.7 and 9.7 MHz, from a transducer with an original resonant frequency of 8.4 MHz. Three frequency transmission at 3.3, 8.4, and 10.8 MHz was possible for a transducer with a half-wavelength thick front layer. Calculations of the predicted thermal lesion size at each transmission frequency indicated that the depth of thermal lesion could be varied by a factor of 1.6 for the quarter-wavelength front layer. Heating experiments performed in excised liver tissue with a dual-frequency applicator confirmed this ability to control the shape of thermal lesions during heating to generate a desired geometry. Practical interstitial Designs that enable the generation of shaped thermal lesions are feasible.\n\n",
                "DataExportTag": "CAN138574",
                "QuestionID": "QID318",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Multifrequency ultrasound transducers for conformal interstitial thermal therapy Control over the...",
                "Choices": {
                    "1": {
                        "Display": "\"Medical Imaging and Therapeutic Techniques\""
                    },
                    "2": {
                        "Display": "imaging, temperature, ultrasound, optical, oct, tissue, hifu, vivo, heating, optical_coherence, probe, hyperthermia, phantom, real_time, laser"
                    },
                    "3": {
                        "Display": "\"Radiation Therapy and Dosimetry Simulation\""
                    },
                    "4": {
                        "Display": "beam, energy, monte_carlo, radiotherapy, dosimetry, phantom, proton, photon, electron, irradiation, detector, absorb_dose, film, simulation, neutron"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID205",
            "SecondaryAttribute": "Multiple Agent Event Detection and Representation in Videos Paper We propose a novel method to de...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Multiple Agent Event Detection and Representation in Videos Paper We propose a novel method to detect events involving multiple agents in a video and to learn their structure in terms of temporally related chain of sub-events. The proposed method has three significant contributions over existing frameworks. First, we present the concept of avideo event graph , to learn the event structure from training videos. The video event graph is composed of temporally correlated sub-events, which is used to automatically encode thevent correlation graph . The event correlation graph signifies the frequency of occurrence of conditionally dependent sub-events. Second, we pose the problem of event detection in novel videos as clustering the maximally correlated sub-events, and use normalized cuts to determine these clusters. The principal assumption made in this work is that the events are composed of highly correlated chain of sub-events, that have high weights (association) within the cluster and relatively low weights (disassociation) between clusters. Last, we recognize the importance of representing the variations (in the temporal order of sub-events) occurring in a event and encode the probabilities directly into our representation. We show results of our learning and detection of events for videos in the meeting, surveillance, and railroad monitoring domains. Introduction The world that we live in is a complex network of agents and their interactions which we term events. These interactions can be visualized in the form of a hierarchy of events and sub-events. An instance of an event is a composition of directly measurable low-level actions (which we term sub-events) having a temporal order. For example, a voting event is composed of a sequence of move, raise and lower hand sub-events. Also, the agents can act independently (e.g. voting) as well as collectively (e.g. touchdown in a football game) to perform certain events. Hence, in the enterprise of machine vision, the ability to detect and learn the observed events must be one of the ultimate goals. In literature, a variety of approaches have been proposed for the detection of events in video sequences. Most of these approaches can be arranged into three categories based on their approach to event detection. First, approaches where event models are pre-defined include force dynamics Copyright c \u00a9 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. (Siskind 2000), stochastic context free grammars (Bobick and Ivanov 1998), state machines (Koller, Heinze, and Nagel 1991), and PNF Networks (Pinhanez and Bobick 1998). These approaches either manually encode the event models or provide constraints (grammar or rules) to detect events in novel videos. Second, approaches that learn the event models such as Hidden Markov Models (HMMs) (Ivanov and Bobick 2000, Brand and Kettnaker 2000), Coupled HMMs (Oliver, Rosario, and Pentland 1999), and Dynamic Bayesian Networks (Friedman, Murphy, and Russell 1998) have been widely used in the area of activity recognition. The above learning methods either model single person activities or require prior knowledge about the number of people involved in the events and variation in data may require complete re-training, so as to modify the model structure and parameters to accommodate those variations. Similarly, there is no straight-forward method of expanding the domain to other events, once training has been completed. Third, approaches that do not model the events, but utilize clustering methods for event detection include co-embedding prototypes (Zhong, Shi, Visontai 2004), and spatio-temporal derivatives (Zelnik-Manor and Irani 2001). These methods find event segments by spectral graph partitioning (e.g. normalized cut) of the weight (similarity) matrix. These methods assume maximum length of an event and are restricted to single person non-interactive event detection. What is missing in these approaches is ability to model long complex events involving multiple agents performing multiple actions simultaneously. Can these approaches be used to automatically learn events involving unknown number of agents? Will the learnt event model still hold for a novel video, in case of interfering events from an independent agent? Can these approaches extend their abstract event model to representations related to human understanding of events? Can a human communicate his or her observation of an event to a computer or vice versa? These questions are addressed in this paper, where event models are learnt from training data, and are used for event detection in novel videos.Event learningis formulated in a probabilistic framework whileevent detectionis treated as a graphtheoretic clustering problem. The primary objective of this work is to detect and learn the complex interactions of the multiple agents performing multiple actions in the form of domain events, without prior knowledge about the number Figure 1: Automated detection of sub-events for stealing video. Using the tracked trajectories, the sub-events of each agent are detected, and frames 37, 119, 127, and 138 of the video are shown. of agents involved in the interaction and length of the event. Another objective is to present a coherent representation of these domain events, as a means to encode the relationships between agents and objects participating in a domain event. Formally, adomain eventis defined as a collection of actions performed by one or more agents. Also, we term these actions asvideo events , since they are directly measurable from the video (e.g. move, pick, enter, etc.). In this paper, events refer todomain events , and sub-events refer to video events, unless otherwise stated. Although CASE (Hakeem, Sheikh, Shah 2004) is an existing multiple agent event representation, the proposed method caters for three of its shortcomings. Firstly, we automatically learn the domain event structure from training videos and encode the domain event ontology . This has a significant advantage, since the domain experts need not go through the tedious task of determining the structure of events by browsing all the videos in the domain. Secondly, we recognize the importance of representing the variations in the temporal order of the sub-events occurring in a domain event and encode it directly into our representation. These variations in the temporal order of sub-events occur due to the style of execution of events for different agents. Finally, we present the concept of a video event graph (instead of event-tree) for event detection in videos. The reason for departing from the temporal event-tree representation of the video is that it fails to detect events when there are interfering sub-events from an independent agent, present in the tree structure of the novel video, which were not present in the actual event tree structure. Also, it fails to represent the complete temporal order between sub-events, which can easily be represented by video event graphs. For learning the domain events from training videos, firstly, we introduce the notion of video event graph, which is aDirected Acyclic Graph(DAG) for representing the temporal relationship of sub-events in a video. In the video event graph each vertexrepresents a sub-event and each directed edgeprovides the temporal relationship between two sub-events. These temporal relationships are based on the interval algebra in (Allen and Ferguson 1994), which is a more descriptive model of relationships compared to the low level abstract relationship model of HMMs. Secondly, using the video event graph, we determine the event correlamoves enters moves raises DURING\n\n",
                "DataExportTag": "AI1037345",
                "QuestionID": "QID205",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Multiple Agent Event Detection and Representation in Videos Paper We propose a novel method to de...",
                "Choices": {
                    "1": {
                        "Display": "\"Operational Management and Automated Manufacturing Planning\""
                    },
                    "2": {
                        "Display": "management, planning, plan, decision_making, manufacturing, workflow, automation, production, business, planner, support, automate, military, operational, framework"
                    },
                    "3": {
                        "Display": "\"Healthcare Expert System and Patient Diagnosis\""
                    },
                    "4": {
                        "Display": "expert, diagnosis, healthcare, fault, acquisition, es, inference, reasoning, maintenance, kbs, design, inference_engine, patient, expertise, user"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID358",
            "SecondaryAttribute": "Multiple manifestations of genetic and non-genetic factors in Multiple Sclerosis disentangled wit...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Multiple manifestations of genetic and non-genetic factors in Multiple Sclerosis disentangled with a multi-omics approach to accelerate personalised medicine The complex interactions between genetic and non-genetic factors produce heterogeneities in patients as reflected in the diversity of pathophysiology, clinical manifestations, response to therapies, disease development and progression. Yet, the full potential of personalized medicine entails biomarker-guided delivery of efficient therapies in stratified patient populations. MultipleMS will therefore develop, validate, and exploit methods for patient stratification in Multiple Sclerosis, a chronic inflammatory disease and a leading causes of non-traumatic disability in young adults, with an estimated cost of \u20ac37 000 per patient per year over a duration of 30 years. Here we benefit from several large clinical cohorts with multiple data types, including genetic and lifestyle information. This in combination with publically available multi-omics maps enables us to identify biomarkers of the clinical course and the response to existing therapies in a real-world setting, and to gain in-depth knowledge of distinct pathogenic pathways setting the stage for development of new interventions.To create strategic global synergies, MultipleMS includes 21 partners and covers not only the necessary clinical, biological, and computational expertise, but also includes six industry partners ensuring dissemination and exploitation of the methods and clinical decision support system. Moreover, the pharmaceutical industry partners provide expertise to ensure optimal selection and validation of clinically relevant biomarkers and new targets. Our conceptual personalized approach can readily be adapted to other immune-mediated diseases with a complex gene-lifestyle background and broad clinical spectrum with heterogeneity in treatment response. MultipleMS therefore goes significantly beyond current state-of-the-art thereby broadly affecting European policies, healthcare systems, innovation in translating big data and basic research into evidence-based personalized clinical applications.\n\n",
                "DataExportTag": "COR44141",
                "QuestionID": "QID358",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Multiple manifestations of genetic and non-genetic factors in Multiple Sclerosis disentangled wit...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Drug Development for Rare Diseases\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Gene Therapy and Immune Response in Cardiac and Metabolic Disorders\""
                    },
                    "4": {
                        "Display": "cellular, therapeutic, immune, genetic, inflammation, metabolic, gene_therapy, mechanism, liver, ra, cardiac, vivo, heart_failure, inflammatory, molecule"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID181",
            "SecondaryAttribute": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel Prize in Chemistry was awarded to the scientists who discovered the ubiquitin proteasome system (UPS), the major pathway for regulated degradation of intracellular proteins in eukaryotes (1, 2). In the UPS, intracellular proteins marked with ubiquitin are degraded by the proteasome. One important function of the system is to maintain cell quality control by removing damaged and pathologic accumulations of proteins. Various diseases are characterized by the formation of such proteins; among these diseases are several neurodegenerative disorders (such as Alzheimer disease), cystic fibrosis, and multiple myeloma. Thus, the UPS seems to provide a broad platform for drug targeting. Bortezomib, used for the treatment of multiple myeloma, is the first of a new class of anticancer drugs that acts through inhibition of the UPS. Multiple myeloma is characterized by malignant proliferation of plasma cells in bone marrow, suppression of formation of blood cell progenitors, bone destruction, and paraprotein formation with secondary renal damage. Of approximately 570000 expected deaths due to cancer in the United States in 2005, 11000 will be caused by multiple myeloma (3). At present, more than 50000 patients in the United States have received a diagnosis of this disease. The effectiveness of the different treatments for multiple myeloma is limited, and all patients with multiple myeloma eventually die of the disease. The U.S. Food and Drug Administration recently approved bortezomib for the treatment of multiple myeloma. The goal of this review is to present the logical progression from basic biological research to the discovery of the UPS and the mechanisms of its action and regulation through the unraveling of aberrations in the system that result in disease, and finally to the development of rational, mechanism-based drugs. Better understanding of the mechanisms that underlie the activity of the UPS holds promise for the development of more efficient and less toxic treatments of various diseases. This review is not intended to cover all that is currently known of the UPS. Rather, we will briefly describe the general components of the system, explain their mode of action, and highlight, through several specific examples, the involvement of the system in the pathogenesis of human disease. Recent detailed reviews on the UPS have been published elsewhere (46). Multiple Pathways for Degradation of Proteins Proteins in mammalian cells are always in a dynamic state; they are continuously degraded and replaced by newly synthesized proteins. Protein turnover (the degradation and resynthesis of proteins) is a highly regulated process. Synthesis of new proteins requires a continuous source of amino acids. Amino acids are generated in the body by 3 different processes: 1) digestion of dietary proteins in the gut and the absorption of the released amino acids; 2) degradation of extracellular proteins; and 3) degradation of intracellular proteins (6). Degradation of Dietary and Extracellular Proteins Mammals catalyze proteolysis using several proteolytic systems that serve distinct functions. The degradation of dietary proteins occurs in the lumen of the gut, an extracorporeal compartment. This process is mediated by nonspecific proteases, such as pepsin, trypsin, and chymotrypsin, that degrade proteins into single, nonantigenic amino acids that are then absorbed. Thermodynamically, degradation of high-energy proteins into low-energy amino acids releases energy (exergonic process). Thus, the purpose of degradation of proteins in the gastrointestinal tract is dual: to remove antigenicity and to generate energy. The body's own proteins are degraded by 2 major pathways. The first involves lysosomal degradation of such extracellular proteins as the low-density lipoprotein core particle and peptide hormones (for example, insulin). Before the degradation of these proteins by nonspecific lysosomal proteases, they are internalized into the cell and routed to the lysosome by a series of intracellular vesicles. Lysosomal degradation accounts for approximately 20% of normal protein turnover. In addition, the lysosome also degrades cell surface membrane proteins, such as the low-density lipoprotein receptor. These proteins are targeted to the lysosome as membrane protein components in the same vesicles that route soluble internalized proteins. Lysosomal proteases are nonspecific; all proteins exposed to them are degraded at the same rate. In contrast, intracellular proteins, especially those that regulate vital metabolic pathways, have half-lives that vary from a few minutes to many days. Many are degraded in a timed manner or following specific signals. Also, specific inhibitors of lysosomal proteases have a minimal effect on degradation of intracellular proteins. These observations and findings made it clear that the lysosome cannot be the organelle responsible for specific and timed regulated intracellular proteolysis and prompted researchers to look for a more selective system that targets cellular proteins. These efforts culminated in the discovery of the UPS, the second major pathway responsible for degrading the body's own proteins (6). Intracellular Protein Degradation The roles of intracellular proteolysis are clearly distinct from those of degradation of dietary and extracellular proteins. A major function of intracellular proteolysis is to maintain the cell's quality control by removing misfolded, mutated, or otherwise damaged proteins. Another important role is controlling basic cellular processes, including diverse metabolic pathways, cell cycle, and transcription, through the removal of key regulatory proteins, such as key enzymes and cell-cycle and transcriptional regulators. Thus, intracellular proteolysis must be extremely specific and selective. To attain this high degree of specificity, the process consumes rather than produces metabolic energy. Degradation of Proteins by the UPS Degradation of proteins by the UPS is a complex, tightly regulated, and highly specific process that degrades numerous intracellular proteins (6). How does the cell select, in a highly specific manner, proteins that are destined for degradation? It does so by attaching a small protein tag (ubiquitin) to them that marks them for destruction (Figure 1). Degradation of a target substrate by the UPS involves 2 successive steps. The first step is the conjugation of several ubiquitin molecules, a process that is catalyzed by 3 enzymes that act in concert: E1 (ubiquitin-activating enzyme), E2 (ubiquitin-conjugating enzyme), and a substrate-specific E3 (ubiquitin-protein ligase). The second step is the degradation of the tagged substrate by the 26S proteasome complex; 26S recognizes only ubiquitin-conjugated proteins and degrades them to small peptides, releasing free, reusable ubiquitin (Figure 2). The derived peptides are further degraded into individual amino acids by downstream cytosolic proteases. The UPS is hierarchical: Mammalian cells express a single E1 enzyme that transfers ubiquitin to approximately 50 distinct E2 enzymes. Each E2 enzyme appears to interact with several E3 enzymes. The many E3 enzymes (approximately 1000) are responsible for the specific recognition of the numerous substrates of the ubiquitin system. In some cases, the substrates are recognized without any previous alteration and are degraded at the same rate under different physiologic conditions (constitutive degradation). In many other cases, however, the substrate must undergo a post-translational modification (for example, phosphorylation or oxidation) or an alteration in its structure (for example, denaturation or misfolding) to be recognized by its cognate E3 enzyme (6). Figure 1. Conjugation of ubiquitin to the protein substrate. Ubiquitin is activated by the ubiquitin-activating enzyme, E1 [1], in a process that requires energy (adenosine triphosphate [ATP]). Ubiquitin is then transferred to a ubiquitin-conjugating enzyme, E2 [2]. E2 transfers the activated ubiquitin moiety to the protein substrate that is bound specifically to a unique ubiquitin ligase, E3 [3]. Successive conjugation of ubiquitin moieties to one another generates a polyubiquitin chain [4]. Figure 2. Degradation of the ubiquitin-tagged substrate by the 26S proteasome. The polyubiquitin chain serves as a binding and degradation signal for the 26S proteasome [1]. The substrate is degraded to short peptides [2], followed by a release of free and reusable ubiquitin molecules [3]. Proteasomal degradation also requires energy. ATP = adenosine triphosphate. Diseases Linked to the UPS The pathologic states associated with the UPS can be classified into 2 mechanism-based groups: 1) those that result from a mutation in a UPS enzyme or a target substrate (loss of function), leading to stabilization of certain proteins, and 2) those that result from accelerated degradation of the target protein (gain of function). These states are depicted in Figure 3 . This section presents several well-studied examples of aberrations in the UPS that are directly linked to human diseases. The Table presents a nonexhaustive list of such diseases. Figure 3. Aberrations in the ubiquitin proteasome system and pathogenesis of human diseases. Normal degradation of cellular proteins maintains them in a steady-state level [1]. When degradation is accelerated because of an increase in the level of an E3 (Skp2 in the case of p27, for example) or an ancillary protein that binds the substrate and targets it for degradation (for example, the human papillomavirus E6 oncoprotein that associates with p53 and targets it for degradation), the steady-state level of the protein decreases [2]. A mutation in an E3 enzyme or in the substrate's recognition motif (such as occurs in the Liddle syndrome) will result in decreased degradation and accumulation of the target substrate [3]. Table. Human Disorders Related to Aberrant Function of the\n\n",
                "DataExportTag": "CAN715071",
                "QuestionID": "QID181",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetic Research in Aging and Cancer\""
                    },
                    "2": {
                        "Display": "mouse, senescence, pten, transgenic_mouse, wild, deletion, senescent, nan_value, ras_mutations, arf, deficient, cellular_senescence, hh, oncogene, deficiency"
                    },
                    "3": {
                        "Display": "\"Cell Adhesion and Migration Mechanisms\""
                    },
                    "4": {
                        "Display": "membrane, actin, plasma_membrane, integrin, adhesion, migration, surface, rhoa, rho, trafficking, lipid_raft, estrogen_receptor, actin_cytoskeleton, endocytosis, motility"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID242",
            "SecondaryAttribute": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel Prize in Chemistry was awarded to the scientists who discovered the ubiquitin proteasome system (UPS), the major pathway for regulated degradation of intracellular proteins in eukaryotes (1, 2). In the UPS, intracellular proteins marked with ubiquitin are degraded by the proteasome. One important function of the system is to maintain cell quality control by removing damaged and pathologic accumulations of proteins. Various diseases are characterized by the formation of such proteins; among these diseases are several neurodegenerative disorders (such as Alzheimer disease), cystic fibrosis, and multiple myeloma. Thus, the UPS seems to provide a broad platform for drug targeting. Bortezomib, used for the treatment of multiple myeloma, is the first of a new class of anticancer drugs that acts through inhibition of the UPS. Multiple myeloma is characterized by malignant proliferation of plasma cells in bone marrow, suppression of formation of blood cell progenitors, bone destruction, and paraprotein formation with secondary renal damage. Of approximately 570000 expected deaths due to cancer in the United States in 2005, 11000 will be caused by multiple myeloma (3). At present, more than 50000 patients in the United States have received a diagnosis of this disease. The effectiveness of the different treatments for multiple myeloma is limited, and all patients with multiple myeloma eventually die of the disease. The U.S. Food and Drug Administration recently approved bortezomib for the treatment of multiple myeloma. The goal of this review is to present the logical progression from basic biological research to the discovery of the UPS and the mechanisms of its action and regulation through the unraveling of aberrations in the system that result in disease, and finally to the development of rational, mechanism-based drugs. Better understanding of the mechanisms that underlie the activity of the UPS holds promise for the development of more efficient and less toxic treatments of various diseases. This review is not intended to cover all that is currently known of the UPS. Rather, we will briefly describe the general components of the system, explain their mode of action, and highlight, through several specific examples, the involvement of the system in the pathogenesis of human disease. Recent detailed reviews on the UPS have been published elsewhere (46). Multiple Pathways for Degradation of Proteins Proteins in mammalian cells are always in a dynamic state; they are continuously degraded and replaced by newly synthesized proteins. Protein turnover (the degradation and resynthesis of proteins) is a highly regulated process. Synthesis of new proteins requires a continuous source of amino acids. Amino acids are generated in the body by 3 different processes: 1) digestion of dietary proteins in the gut and the absorption of the released amino acids; 2) degradation of extracellular proteins; and 3) degradation of intracellular proteins (6). Degradation of Dietary and Extracellular Proteins Mammals catalyze proteolysis using several proteolytic systems that serve distinct functions. The degradation of dietary proteins occurs in the lumen of the gut, an extracorporeal compartment. This process is mediated by nonspecific proteases, such as pepsin, trypsin, and chymotrypsin, that degrade proteins into single, nonantigenic amino acids that are then absorbed. Thermodynamically, degradation of high-energy proteins into low-energy amino acids releases energy (exergonic process). Thus, the purpose of degradation of proteins in the gastrointestinal tract is dual: to remove antigenicity and to generate energy. The body's own proteins are degraded by 2 major pathways. The first involves lysosomal degradation of such extracellular proteins as the low-density lipoprotein core particle and peptide hormones (for example, insulin). Before the degradation of these proteins by nonspecific lysosomal proteases, they are internalized into the cell and routed to the lysosome by a series of intracellular vesicles. Lysosomal degradation accounts for approximately 20% of normal protein turnover. In addition, the lysosome also degrades cell surface membrane proteins, such as the low-density lipoprotein receptor. These proteins are targeted to the lysosome as membrane protein components in the same vesicles that route soluble internalized proteins. Lysosomal proteases are nonspecific; all proteins exposed to them are degraded at the same rate. In contrast, intracellular proteins, especially those that regulate vital metabolic pathways, have half-lives that vary from a few minutes to many days. Many are degraded in a timed manner or following specific signals. Also, specific inhibitors of lysosomal proteases have a minimal effect on degradation of intracellular proteins. These observations and findings made it clear that the lysosome cannot be the organelle responsible for specific and timed regulated intracellular proteolysis and prompted researchers to look for a more selective system that targets cellular proteins. These efforts culminated in the discovery of the UPS, the second major pathway responsible for degrading the body's own proteins (6). Intracellular Protein Degradation The roles of intracellular proteolysis are clearly distinct from those of degradation of dietary and extracellular proteins. A major function of intracellular proteolysis is to maintain the cell's quality control by removing misfolded, mutated, or otherwise damaged proteins. Another important role is controlling basic cellular processes, including diverse metabolic pathways, cell cycle, and transcription, through the removal of key regulatory proteins, such as key enzymes and cell-cycle and transcriptional regulators. Thus, intracellular proteolysis must be extremely specific and selective. To attain this high degree of specificity, the process consumes rather than produces metabolic energy. Degradation of Proteins by the UPS Degradation of proteins by the UPS is a complex, tightly regulated, and highly specific process that degrades numerous intracellular proteins (6). How does the cell select, in a highly specific manner, proteins that are destined for degradation? It does so by attaching a small protein tag (ubiquitin) to them that marks them for destruction (Figure 1). Degradation of a target substrate by the UPS involves 2 successive steps. The first step is the conjugation of several ubiquitin molecules, a process that is catalyzed by 3 enzymes that act in concert: E1 (ubiquitin-activating enzyme), E2 (ubiquitin-conjugating enzyme), and a substrate-specific E3 (ubiquitin-protein ligase). The second step is the degradation of the tagged substrate by the 26S proteasome complex; 26S recognizes only ubiquitin-conjugated proteins and degrades them to small peptides, releasing free, reusable ubiquitin (Figure 2). The derived peptides are further degraded into individual amino acids by downstream cytosolic proteases. The UPS is hierarchical: Mammalian cells express a single E1 enzyme that transfers ubiquitin to approximately 50 distinct E2 enzymes. Each E2 enzyme appears to interact with several E3 enzymes. The many E3 enzymes (approximately 1000) are responsible for the specific recognition of the numerous substrates of the ubiquitin system. In some cases, the substrates are recognized without any previous alteration and are degraded at the same rate under different physiologic conditions (constitutive degradation). In many other cases, however, the substrate must undergo a post-translational modification (for example, phosphorylation or oxidation) or an alteration in its structure (for example, denaturation or misfolding) to be recognized by its cognate E3 enzyme (6). Figure 1. Conjugation of ubiquitin to the protein substrate. Ubiquitin is activated by the ubiquitin-activating enzyme, E1 [1], in a process that requires energy (adenosine triphosphate [ATP]). Ubiquitin is then transferred to a ubiquitin-conjugating enzyme, E2 [2]. E2 transfers the activated ubiquitin moiety to the protein substrate that is bound specifically to a unique ubiquitin ligase, E3 [3]. Successive conjugation of ubiquitin moieties to one another generates a polyubiquitin chain [4]. Figure 2. Degradation of the ubiquitin-tagged substrate by the 26S proteasome. The polyubiquitin chain serves as a binding and degradation signal for the 26S proteasome [1]. The substrate is degraded to short peptides [2], followed by a release of free and reusable ubiquitin molecules [3]. Proteasomal degradation also requires energy. ATP = adenosine triphosphate. Diseases Linked to the UPS The pathologic states associated with the UPS can be classified into 2 mechanism-based groups: 1) those that result from a mutation in a UPS enzyme or a target substrate (loss of function), leading to stabilization of certain proteins, and 2) those that result from accelerated degradation of the target protein (gain of function). These states are depicted in Figure 3 . This section presents several well-studied examples of aberrations in the UPS that are directly linked to human diseases. The Table presents a nonexhaustive list of such diseases. Figure 3. Aberrations in the ubiquitin proteasome system and pathogenesis of human diseases. Normal degradation of cellular proteins maintains them in a steady-state level [1]. When degradation is accelerated because of an increase in the level of an E3 (Skp2 in the case of p27, for example) or an ancillary protein that binds the substrate and targets it for degradation (for example, the human papillomavirus E6 oncoprotein that associates with p53 and targets it for degradation), the steady-state level of the protein decreases [2]. A mutation in an E3 enzyme or in the substrate's recognition motif (such as occurs in the Liddle syndrome) will result in decreased degradation and accumulation of the target substrate [3]. Table. Human Disorders Related to Aberrant Function of the\n\n",
                "DataExportTag": "CAN715071",
                "QuestionID": "QID242",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetic Research in Aging and Cancer\""
                    },
                    "2": {
                        "Display": "mouse, senescence, pten, transgenic_mouse, wild, deletion, senescent, nan_value, ras_mutations, arf, deficient, cellular_senescence, hh, oncogene, deficiency"
                    },
                    "3": {
                        "Display": "\"Cell Adhesion and Migration Mechanisms\""
                    },
                    "4": {
                        "Display": "membrane, actin, plasma_membrane, integrin, adhesion, migration, surface, rhoa, rho, trafficking, lipid_raft, estrogen_receptor, actin_cytoskeleton, endocytosis, motility"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID365",
            "SecondaryAttribute": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel Prize in Chemistry was awarded to the scientists who discovered the ubiquitin proteasome system (UPS), the major pathway for regulated degradation of intracellular proteins in eukaryotes (1, 2). In the UPS, intracellular proteins marked with ubiquitin are degraded by the proteasome. One important function of the system is to maintain cell quality control by removing damaged and pathologic accumulations of proteins. Various diseases are characterized by the formation of such proteins; among these diseases are several neurodegenerative disorders (such as Alzheimer disease), cystic fibrosis, and multiple myeloma. Thus, the UPS seems to provide a broad platform for drug targeting. Bortezomib, used for the treatment of multiple myeloma, is the first of a new class of anticancer drugs that acts through inhibition of the UPS. Multiple myeloma is characterized by malignant proliferation of plasma cells in bone marrow, suppression of formation of blood cell progenitors, bone destruction, and paraprotein formation with secondary renal damage. Of approximately 570000 expected deaths due to cancer in the United States in 2005, 11000 will be caused by multiple myeloma (3). At present, more than 50000 patients in the United States have received a diagnosis of this disease. The effectiveness of the different treatments for multiple myeloma is limited, and all patients with multiple myeloma eventually die of the disease. The U.S. Food and Drug Administration recently approved bortezomib for the treatment of multiple myeloma. The goal of this review is to present the logical progression from basic biological research to the discovery of the UPS and the mechanisms of its action and regulation through the unraveling of aberrations in the system that result in disease, and finally to the development of rational, mechanism-based drugs. Better understanding of the mechanisms that underlie the activity of the UPS holds promise for the development of more efficient and less toxic treatments of various diseases. This review is not intended to cover all that is currently known of the UPS. Rather, we will briefly describe the general components of the system, explain their mode of action, and highlight, through several specific examples, the involvement of the system in the pathogenesis of human disease. Recent detailed reviews on the UPS have been published elsewhere (46). Multiple Pathways for Degradation of Proteins Proteins in mammalian cells are always in a dynamic state; they are continuously degraded and replaced by newly synthesized proteins. Protein turnover (the degradation and resynthesis of proteins) is a highly regulated process. Synthesis of new proteins requires a continuous source of amino acids. Amino acids are generated in the body by 3 different processes: 1) digestion of dietary proteins in the gut and the absorption of the released amino acids; 2) degradation of extracellular proteins; and 3) degradation of intracellular proteins (6). Degradation of Dietary and Extracellular Proteins Mammals catalyze proteolysis using several proteolytic systems that serve distinct functions. The degradation of dietary proteins occurs in the lumen of the gut, an extracorporeal compartment. This process is mediated by nonspecific proteases, such as pepsin, trypsin, and chymotrypsin, that degrade proteins into single, nonantigenic amino acids that are then absorbed. Thermodynamically, degradation of high-energy proteins into low-energy amino acids releases energy (exergonic process). Thus, the purpose of degradation of proteins in the gastrointestinal tract is dual: to remove antigenicity and to generate energy. The body's own proteins are degraded by 2 major pathways. The first involves lysosomal degradation of such extracellular proteins as the low-density lipoprotein core particle and peptide hormones (for example, insulin). Before the degradation of these proteins by nonspecific lysosomal proteases, they are internalized into the cell and routed to the lysosome by a series of intracellular vesicles. Lysosomal degradation accounts for approximately 20% of normal protein turnover. In addition, the lysosome also degrades cell surface membrane proteins, such as the low-density lipoprotein receptor. These proteins are targeted to the lysosome as membrane protein components in the same vesicles that route soluble internalized proteins. Lysosomal proteases are nonspecific; all proteins exposed to them are degraded at the same rate. In contrast, intracellular proteins, especially those that regulate vital metabolic pathways, have half-lives that vary from a few minutes to many days. Many are degraded in a timed manner or following specific signals. Also, specific inhibitors of lysosomal proteases have a minimal effect on degradation of intracellular proteins. These observations and findings made it clear that the lysosome cannot be the organelle responsible for specific and timed regulated intracellular proteolysis and prompted researchers to look for a more selective system that targets cellular proteins. These efforts culminated in the discovery of the UPS, the second major pathway responsible for degrading the body's own proteins (6). Intracellular Protein Degradation The roles of intracellular proteolysis are clearly distinct from those of degradation of dietary and extracellular proteins. A major function of intracellular proteolysis is to maintain the cell's quality control by removing misfolded, mutated, or otherwise damaged proteins. Another important role is controlling basic cellular processes, including diverse metabolic pathways, cell cycle, and transcription, through the removal of key regulatory proteins, such as key enzymes and cell-cycle and transcriptional regulators. Thus, intracellular proteolysis must be extremely specific and selective. To attain this high degree of specificity, the process consumes rather than produces metabolic energy. Degradation of Proteins by the UPS Degradation of proteins by the UPS is a complex, tightly regulated, and highly specific process that degrades numerous intracellular proteins (6). How does the cell select, in a highly specific manner, proteins that are destined for degradation? It does so by attaching a small protein tag (ubiquitin) to them that marks them for destruction (Figure 1). Degradation of a target substrate by the UPS involves 2 successive steps. The first step is the conjugation of several ubiquitin molecules, a process that is catalyzed by 3 enzymes that act in concert: E1 (ubiquitin-activating enzyme), E2 (ubiquitin-conjugating enzyme), and a substrate-specific E3 (ubiquitin-protein ligase). The second step is the degradation of the tagged substrate by the 26S proteasome complex; 26S recognizes only ubiquitin-conjugated proteins and degrades them to small peptides, releasing free, reusable ubiquitin (Figure 2). The derived peptides are further degraded into individual amino acids by downstream cytosolic proteases. The UPS is hierarchical: Mammalian cells express a single E1 enzyme that transfers ubiquitin to approximately 50 distinct E2 enzymes. Each E2 enzyme appears to interact with several E3 enzymes. The many E3 enzymes (approximately 1000) are responsible for the specific recognition of the numerous substrates of the ubiquitin system. In some cases, the substrates are recognized without any previous alteration and are degraded at the same rate under different physiologic conditions (constitutive degradation). In many other cases, however, the substrate must undergo a post-translational modification (for example, phosphorylation or oxidation) or an alteration in its structure (for example, denaturation or misfolding) to be recognized by its cognate E3 enzyme (6). Figure 1. Conjugation of ubiquitin to the protein substrate. Ubiquitin is activated by the ubiquitin-activating enzyme, E1 [1], in a process that requires energy (adenosine triphosphate [ATP]). Ubiquitin is then transferred to a ubiquitin-conjugating enzyme, E2 [2]. E2 transfers the activated ubiquitin moiety to the protein substrate that is bound specifically to a unique ubiquitin ligase, E3 [3]. Successive conjugation of ubiquitin moieties to one another generates a polyubiquitin chain [4]. Figure 2. Degradation of the ubiquitin-tagged substrate by the 26S proteasome. The polyubiquitin chain serves as a binding and degradation signal for the 26S proteasome [1]. The substrate is degraded to short peptides [2], followed by a release of free and reusable ubiquitin molecules [3]. Proteasomal degradation also requires energy. ATP = adenosine triphosphate. Diseases Linked to the UPS The pathologic states associated with the UPS can be classified into 2 mechanism-based groups: 1) those that result from a mutation in a UPS enzyme or a target substrate (loss of function), leading to stabilization of certain proteins, and 2) those that result from accelerated degradation of the target protein (gain of function). These states are depicted in Figure 3 . This section presents several well-studied examples of aberrations in the UPS that are directly linked to human diseases. The Table presents a nonexhaustive list of such diseases. Figure 3. Aberrations in the ubiquitin proteasome system and pathogenesis of human diseases. Normal degradation of cellular proteins maintains them in a steady-state level [1]. When degradation is accelerated because of an increase in the level of an E3 (Skp2 in the case of p27, for example) or an ancillary protein that binds the substrate and targets it for degradation (for example, the human papillomavirus E6 oncoprotein that associates with p53 and targets it for degradation), the steady-state level of the protein decreases [2]. A mutation in an E3 enzyme or in the substrate's recognition motif (such as occurs in the Liddle syndrome) will result in decreased degradation and accumulation of the target substrate [3]. Table. Human Disorders Related to Aberrant Function of the\n\n",
                "DataExportTag": "CAN249766",
                "QuestionID": "QID365",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Narrative Review: Protein Degradation and Human Diseases: The Ubiquitin Connection The 2004 Nobel...",
                "Choices": {
                    "1": {
                        "Display": "\"Cancer Drug Research and Development\""
                    },
                    "2": {
                        "Display": "mtor, xenograft, kinase, metformin, combination, rapamycin, antitumor, drug, resistance, celecoxib, target_rapamycin, akt_mtor, synergistic, inhibitor, anticancer"
                    },
                    "3": {
                        "Display": "\"Cancer Cell Research and Treatment\""
                    },
                    "4": {
                        "Display": "bax_activation, proliferation, apoptotic, flow_cytometry, gastric, resveratrol, viability, mtt_assay, cervical, phase, annexin_v, arrest, cycle_arrest, quercetin, western_blot"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID170",
            "SecondaryAttribute": "New, realistic and robust models for cryptocurrency volatility Forecasting cryptocurrency volatil...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "New, realistic and robust models for cryptocurrency volatility Forecasting cryptocurrency volatility is a topic of interest in quantitative finance. A growing number of studies argue that compared to equty price cryptocurrency prices are to a large and perhaps abnormal degree driven by sentiments. However, econometric studies focus on forcing conditional volatility models developed for equity return volatility to fit on cryptocurrency data despite being aware that estimation techniques developed for analyzing equity price or commodity price volatility lack robustness and do not work as intended. Is it possible to propose solutions to deal with the mentioned shortcomings? Is it possible to suggest a new family of models? If so, how? The purpose of New, realistic and robust models for cryptocurrency volatility is to answer these questions by suggesting new and more realistic conditional volatility models accompanied with reliable cross-disciplinary estimation techniques to forecast cryptocurrency price volatility. What is novel and innovative about the suggested framework is that contrary to the current literature our point of departure is the empirical features observed in cryptocurrency prices combined with a useful tool, namely, artificial neural networks used to measure sentiments. Our aim is to build a machine that produces discrete sentiment phases each day using news articles and internet search data. Once we have identified the number of phases and determined, which phase an observation at a given time-period belongs to following neural network estimation, we can estimate the model parameters, jumps and filter out the continuous conditional volatility process contemporaneously using particle filtering techniques. Besides academics, this proposal is also relevant for regulators and investors as they can learn a great deal by understanding how cryptocurrency volatility actually behaves. Regulators can use sentiment labels from the neural network to design policies to contrast and overcome financial crises in the future.\n\n",
                "DataExportTag": "COR48680",
                "QuestionID": "QID170",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "New, realistic and robust models for cryptocurrency volatility Forecasting cryptocurrency volatil...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\""
                    },
                    "2": {
                        "Display": "child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "3": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\""
                    },
                    "4": {
                        "Display": "farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID231",
            "SecondaryAttribute": "New, realistic and robust models for cryptocurrency volatility Forecasting cryptocurrency volatil...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "New, realistic and robust models for cryptocurrency volatility Forecasting cryptocurrency volatility is a topic of interest in quantitative finance. A growing number of studies argue that compared to equty price cryptocurrency prices are to a large and perhaps abnormal degree driven by sentiments. However, econometric studies focus on forcing conditional volatility models developed for equity return volatility to fit on cryptocurrency data despite being aware that estimation techniques developed for analyzing equity price or commodity price volatility lack robustness and do not work as intended. Is it possible to propose solutions to deal with the mentioned shortcomings? Is it possible to suggest a new family of models? If so, how? The purpose of New, realistic and robust models for cryptocurrency volatility is to answer these questions by suggesting new and more realistic conditional volatility models accompanied with reliable cross-disciplinary estimation techniques to forecast cryptocurrency price volatility. What is novel and innovative about the suggested framework is that contrary to the current literature our point of departure is the empirical features observed in cryptocurrency prices combined with a useful tool, namely, artificial neural networks used to measure sentiments. Our aim is to build a machine that produces discrete sentiment phases each day using news articles and internet search data. Once we have identified the number of phases and determined, which phase an observation at a given time-period belongs to following neural network estimation, we can estimate the model parameters, jumps and filter out the continuous conditional volatility process contemporaneously using particle filtering techniques. Besides academics, this proposal is also relevant for regulators and investors as they can learn a great deal by understanding how cryptocurrency volatility actually behaves. Regulators can use sentiment labels from the neural network to design policies to contrast and overcome financial crises in the future.\n\n",
                "DataExportTag": "COR48680",
                "QuestionID": "QID231",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "New, realistic and robust models for cryptocurrency volatility Forecasting cryptocurrency volatil...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\""
                    },
                    "2": {
                        "Display": "child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "3": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\""
                    },
                    "4": {
                        "Display": "farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID391",
            "SecondaryAttribute": "Nongenomic actions of L-thyroxine and 3,5,3'-triiodo-L-thyronine. Focus on \"L-Thyroxine vs. 3,5,3...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Nongenomic actions of L-thyroxine and 3,5,3'-triiodo-L-thyronine. Focus on \"L-Thyroxine vs. 3,5,3'-triiodo-L-thyronine and cell proliferation: activation of mitogen-activated protein kinase and phosphatidylinositol 3-kinase\". the molecular mechanisms of the numerous cellular actions of thyroid hormone have been widely studied. The classical mechanism of thyroid hormone action occurs by uptake of l-thyroxine (T4) or 3,5,3\u2032 triiodo-l-thyronine (T3) into cells, transport into the cell nucleus, binding with a thyroid receptor (TR), recruitment of coactivators, and regulation of gene transcription via thyroid response elements (TRE). T3 is more potent in these actions than T4. These genomic actions require access of the hormone to the cell interior, translocation to the nucleus, alteration of the rate of gene transcription, and translation of the specific gene product; thus, the overall response generally requires several hours to become manifest. Over the past decade, many actions of thyroid hormone have been described that do not involve initial nuclear action of thyroid receptors and\/or gene transcription; therefore they are considered \u201cnongenomic\u201d (6). Davis and colleagues (6, 7) have described both TR-dependent and TR-independent novel nongenomic actions that involve cell surface receptors and signal transduction pathways. Some actions that begin nongenomically at the cell surface may ultimately become nuclear and cellular events. One example is the phosphorylation of the TR\u03b2 by T4 that results in derepression of the transcriptional activity of SMRT (silencing mediator of retinoid and thyroid hormone receptor) by dissociation of TR and SMRT (7). Another example is that thyroid hormone promotes cell proliferation via nongenomic actions in the chick chorioallantoic membrane model (4) and in glioma cells (5).\n\n",
                "DataExportTag": "CAN1028263",
                "QuestionID": "QID391",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Nongenomic actions of L-thyroxine and 3,5,3'-triiodo-L-thyronine. Focus on \"L-Thyroxine vs. 3,5,3...",
                "Choices": {
                    "1": {
                        "Display": "The nongenomic actions of thyroid hormone include generation of second messengers directly involved in signaling pathways that include the phosphatidylinositol 3-kinase (PI3K) (2, 12, 15, 16) or mitogen-activated protein kinase (MAPK) (11, 13, 17, 20) pathways. In a number of studies, T3 acted by stimulating the PI3K pathway, although this has not been demonstrated for T4. In human skin fibroblasts, Cao et al. (2) elucidated a T3-dependent signaling cascade leading to ZAKI-4\u03b1 expression via mammalian target of rapamycin (mTOR) activation. The mTOR activation was mediated by a PI3K-Akt\/PKB signaling cascade, because T3 induced phosphorylation of Akt\/PKB more rapidly than that of mTOR. The T3-dependent phosphorylations were blocked both by PI3K inhibitors and by expression of a dominant-negative PI3K. The regulation of PI3K pathway by T3 was altered in gastric cancer, raising the possibility that changes in nongenomic signaling by T3 play a potential role in disease states (15). Lei and colleagues (12) demonstrated that T3 stimulated the PI3K\/PKB pathway via the Src family of tyrosine kinases. In this system, activation of both Src kinase and PI3K was required for the T3-induced stimulation of Na-K-ATPase activity and its cell surface expression in adult rat alveolar epithelial cells (12)."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "Both T4 and T3 thyroid hormones nongenomically regulate signal transduction pathways other than the PI3K pathway, including MAP kinases. For example, T3 activates MAPK\/ERK1\/2 in alveolar epithelial cells, stimulating the sodium pump in a dose- and time-dependent manner (11). In prior work, Lin et al. (13) showed that thyroid hormone-enhanced IFN-\u03b3 induced antiviral activity in HeLa cells, which lack thyroid hormone receptors. This effect was activated by T4, T4-agarose, and, to a lesser extent, T3. This effect also required activation of the MAPK cascade and an interaction with the STAT1\u03b1 pathway that is activated by the IFN-\u03b3 (13). Proangiogenic effects of thyroid hormone and its analogs also depend on ERK1\/2 signaling in a chick chorioallantoic membrane model (17). T4, T4-agarose, and the thyroid hormone analog 3,5-diiodothyropropionic acid (DITPA) stimulated angiogenesis in this model, and the magnitude of the angiogenic effect was similar to that of VEGF and basic FGF. Either tetraiodothyroacetic acid (tetrac), a known inhibitor of binding of T4 to plasma membrane integrins, or a MAPK pathway inhibitor inhibited DITPA-induced angiogenesis. In human osteoblast-like cells, both T3 and T4 activated ERK, which resulted in DNA synthesis and cell proliferation (20). Thus there is accumulating evidence that thyroid hormones and their analogs can have rapid nongenomic effects and stimulate more than one signal transduction pathway. It is uncertain whether there is cross talk between different pathways that are stimulated by thyroid hormones."
                    },
                    "2": {
                        "Display": "T4 and T3 are able to activate other intracellular signal transduction cascades beyond PI3K and MAPK. Acting independently of TR, thyroid hormone modulates the activity of the plasma membrane Na+\/H+ exchanger (10), Ca2+-dependent stimulation of adenosine triphosphatase (23), and other ion pumps or channels [inward potassium channel (19) and sodium current (9) in cardiac myocytes]. They also stimulate the guanosine triphosphatase activity of synaptosomes (8)."
                    },
                    "3": {
                        "Display": "Studies of thyroid hormone action on cell surface events, such as calcium efflux (3, 18) or glucose uptake (21, 22), several decades ago implied the existence of one or more plasma membrane receptors for T3 or T4. Recently, integrin-\u03b1v\u03b23 has been reported as a cell surface receptor for T4 in CV-1 cells, a monkey fibroblast cell line that lacks functional thyroid hormone receptors (1). Inhibition of the proangiogenic effects of thyroid hormone in chick chorioallantoic membrane model by LM609, a monoclonal antibody directed against \u03b1v\u03b23-integrin, suggests the involvement of \u03b1v\u03b23 as a surface receptor (4)."
                    },
                    "4": {
                        "Display": "Lin et al. (14) studied the role of l-thyroxine and 3,5,3\u2032 triiodo-l-thyronine in cell proliferation of human glioma cells and the contributions of MAPK (ERK1\/2) and PI3K pathways in the actions of T3 and T4 (Fig. 1). They demonstrate that T3 and T4 activate ERK1\/2 and proliferating cell nuclear antigen (PCNA) accumulation in a concentration-dependent manner. Although ERK activation occurred within 30 min, PCNA accumulation was seen at 24 h. In contrast, activation of PI3K with phosphorylation of its p85 subunit occurred in the U-87 MG cells treated with T3, but not with T4. The T3-induced activation of PI3K was blocked by Arg-Gly-Asp, indicating a role of integrin receptors. While the activation of the ERK1\/2 pathway was necessary for thyroid hormone-induced cell proliferation in these glioma cells, the T3-induced PI3K activation caused nuclear accumulation of TR\u03b1, but not TR\u03b21. PI3K activation by T3 was required for T3-induced expression of hypoxia-inducible factor-1\u03b1. Taken in combination, their results suggest that there are two different receptor sites for thyroid hormones on one integrin molecule that cause downstream activation of ERK and\/or PI3K. T3 binds to both sites and activates both the ERK and PI3K pathways, whereas T4 only activates ERK1\/2 after binding to only one of the two surface integrin sites."
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID74",
            "SecondaryAttribute": "Novel Devulcanization Machine for Industrial and Tyre Rubber Recycling DEVULC is a novel rubber r...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Novel Devulcanization Machine for Industrial and Tyre Rubber Recycling DEVULC is a novel rubber recycling technology based on a cost-effective devulcanization process which uses supercritical CO2 instead of chemical solvents banned by the REACH directive. DEVULC can reclaim valuable manufacturing scrap such as tyre rubber crumb, EPDM (ethylene propylene diene monomer), and Silicone rubber, and it can deal with spent and used rubber materials (rubber seals, used tyres, etc.) that are sufficiently clean, and with waste (scrap cut-off) when new rubber products are manufactured create a new 100% recycled material: DEVULC TPV. The process is socially responsible, environmentally sustainable and economically viable. No chemical solvents are used, devulcanization takes 1 min, and there is a 99% crumb conservation. DEVULC TPV is a versatile material that can be engineered to provide a range of properties. It has a consistent quality and can be incorporated in up to 25% content for the production of new rubber products. DEVULC will positively impact the industrial rubber and the tyre rubber markets. It will develop a recycling method which complies with the environmental standards imposed in the EU. It results in a recycled material fully compliant with the restrictive standards for automotive industry. It will impact large rubber manufacturers which will built inside their installations a DEVULC recycling plant working in continuous with the rubber production chain for the recycling of the scrap cut-offs. The recycled material could be entyrely used by this large company. Plants located near a pole of rubber producers, to jointly manage all residues, can benefit from the scale-size effect. Technically, different lines could be operated for different rubber types. Finally, waste sorting plants could benefit from a DEVULC plant built in their facilities. With this installations, rubber cemeteries could become sources of recycled rubber, turning rubber waste management into an economically profitable activity.\n\n",
                "DataExportTag": "COR57236",
                "QuestionID": "QID74",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Novel Devulcanization Machine for Industrial and Tyre Rubber Recycling DEVULC is a novel rubber r...",
                "Choices": {
                    "1": {
                        "Display": "\"Biofuel Production and Waste Management\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Renewable Energy and Carbon Capture Technology\""
                    },
                    "4": {
                        "Display": "catalysis, carbon_dioxide, membrane, reaction, fuel, solar, reactor, nanoparticle, mof, conversion, synthesis, selectivity, hydrogen, separation, electrochemical"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID347",
            "SecondaryAttribute": "Novel Thermoresponsive Organic Nanogels for Topical Gene Delivery of RNA-Based Drugs \"The scienti...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Novel Thermoresponsive Organic Nanogels for Topical Gene Delivery of RNA-Based Drugs \"The scientific aim of this project is to develop and characterize a selected set of novel intelligent nanogels, designed to be able to cross the Stratum Corneum of the skin, and to study their suitability as drug delivery systems in inflammatory skin diseases.Ultrasmall nanogels will be synthesized using high dilution radical polymerization, a technique well established in the host\u00e2\u20ac\u2122s laboratory, which allows the control of the particle size and polydispersity. Three different groups of nanogels will be prepared: 1) fluorescent nanogels 2) molecular imprinting nanogels 3) thermoresponsive nanogels.The first group will be used to study the distribution and localization of nanogels in normal human skin model reproduced in vitro by organotype cell co-culture. The second group will be used to evaluate the molecular imprinting approach as a tool to obtain very selective delivery system with high recognition characteristics. This has not been studied before and will provide a unique approach, when coupled with high permeation characteristics. The last group, the thermoresponsive nanogels, will combine good permeation with ability to release the drug following a change in temperature and will be compared with more traditional systems.The project will explore the use of each nanogels set to complex and deliver (a) small anti-inflammatory drugs, and (b) large molecules, in particular siRNA, given the strong expertise of the applicant in this area and the emerging interest for these new therapeutics in topical administration.Penetration and pharmacological effects of the drug-nanogels complexes will be assessed in pathological skin in vitro model by the improvement of the disease phenotype.The most significant novelty of the project will be the development of new organic polymeric nanogels able to cross the SC of the skin, providing a new non-invasive gene delivery technology system, that could bring very important applications  in dermathology as well as in other fields.\"\n\n",
                "DataExportTag": "COR46495",
                "QuestionID": "QID347",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Novel Thermoresponsive Organic Nanogels for Topical Gene Delivery of RNA-Based Drugs \"The scienti...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Drug Development for Rare Diseases\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Nanomedicine and Drug Delivery Systems\""
                    },
                    "4": {
                        "Display": "drug, nanoparticle, compound, cellular, drug_delivery, therapeutic, drug_discovery, vivo, nanomedicine, toxicity, peptide, antibody, chemical, molecule, formulation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID123",
            "SecondaryAttribute": "OBJECTIVE To compare the outcomes from laparoscopic lavage with those for colon resection for per...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "OBJECTIVE\nTo compare the outcomes from laparoscopic lavage with those for colon resection for perforated diverticulitis.\n\n",
                "DataExportTag": "51",
                "QuestionID": "QID123",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "OBJECTIVE To compare the outcomes from laparoscopic lavage with those for colon resection for per...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID116",
            "SecondaryAttribute": "OBJECTIVES To compare image guided surgery with surgery either not using any image guidance or to...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "OBJECTIVES\nTo compare image guided surgery with surgery either not using any image guidance or to compare surgery using two different forms of image guidance. The primary outcome criteria was extent of resection and adverse events. Other outcome criteria were overall survival; progression free survival; and quality of life (QoL).\n\n",
                "DataExportTag": "44",
                "QuestionID": "QID116",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "OBJECTIVES To compare image guided surgery with surgery either not using any image guidance or to...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID386",
            "SecondaryAttribute": "OBJECTIVES To review our results in renal transplants in those patients with liver and kidney tra...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "OBJECTIVES\nTo review our results in renal transplants in those patients with liver and kidney transplants.\n\n",
                "DataExportTag": "40",
                "QuestionID": "QID386",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "OBJECTIVES To review our results in renal transplants in those patients with liver and kidney tra...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID182",
            "SecondaryAttribute": "Oncogene addiction versus oncogene amnesia: perhaps more than just a bad habit? Cancer is a multi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Oncogene addiction versus oncogene amnesia: perhaps more than just a bad habit? Cancer is a multistep process whereby genetic events that result in the activation of proto-oncogenes or the inactivation of tumor suppressor genes usurp physiologic programs mandating relentless proliferation and growth. Experimental evidence surprisingly illustrates that the inactivation of even a single oncogene can be sufficient to induce sustained tumor regression. These observations suggest the hypothesis that tumors become irrevocably addicted to the oncogenes that initiated tumorigenesis. The proposed explanation for this phenomenon is that activated oncogenes result in a signaling state in which the sudden abatement of oncogene activity balances towards proliferative arrest and apoptosis. Indeed, substantial evidence supports this hypothesis. Here, we propose an alternative, although not necessarily mutually exclusive, explanation for how oncogenes initiate and sustain tumorigenesis. We suggest that oncogene activation initiates tumorigenesis precisely because it directly overrides physiologic programs inducing a state of cellular amnesia, not only inducing relentless cellular proliferation, but also bypassing checkpoint mechanisms that are essential for cellular mortality, self-renewal, and genomic integrity. Because no single oncogenic lesion is sufficient to overcome all of these physiologic barriers, oncogenes are restrained from inducing tumorigenesis. Correspondingly, in a tumor that has acquired the complete complement of oncogenic lesions required to overcome all of these safety mechanisms, the inactivation of a single oncogene can restore some of these pathways resulting in proliferative arrest, differentiation, cellular senescence, and\/or apoptosis. Thus, oncogenes induce cancer because they induce a cellular state of enforced oncogenic amnesia in which, only upon oncogene inactivation, the tumor becomes aware of its transgression.\n\n",
                "DataExportTag": "CAN557079",
                "QuestionID": "QID182",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Oncogene addiction versus oncogene amnesia: perhaps more than just a bad habit? Cancer is a multi...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetic Research in Aging and Cancer\""
                    },
                    "2": {
                        "Display": "mouse, senescence, pten, transgenic_mouse, wild, deletion, senescent, nan_value, ras_mutations, arf, deficient, cellular_senescence, hh, oncogene, deficiency"
                    },
                    "3": {
                        "Display": "\"Cell Adhesion and Migration Mechanisms\""
                    },
                    "4": {
                        "Display": "membrane, actin, plasma_membrane, integrin, adhesion, migration, surface, rhoa, rho, trafficking, lipid_raft, estrogen_receptor, actin_cytoskeleton, endocytosis, motility"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID243",
            "SecondaryAttribute": "Oncogene addiction versus oncogene amnesia: perhaps more than just a bad habit? Cancer is a multi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Oncogene addiction versus oncogene amnesia: perhaps more than just a bad habit? Cancer is a multistep process whereby genetic events that result in the activation of proto-oncogenes or the inactivation of tumor suppressor genes usurp physiologic programs mandating relentless proliferation and growth. Experimental evidence surprisingly illustrates that the inactivation of even a single oncogene can be sufficient to induce sustained tumor regression. These observations suggest the hypothesis that tumors become irrevocably addicted to the oncogenes that initiated tumorigenesis. The proposed explanation for this phenomenon is that activated oncogenes result in a signaling state in which the sudden abatement of oncogene activity balances towards proliferative arrest and apoptosis. Indeed, substantial evidence supports this hypothesis. Here, we propose an alternative, although not necessarily mutually exclusive, explanation for how oncogenes initiate and sustain tumorigenesis. We suggest that oncogene activation initiates tumorigenesis precisely because it directly overrides physiologic programs inducing a state of cellular amnesia, not only inducing relentless cellular proliferation, but also bypassing checkpoint mechanisms that are essential for cellular mortality, self-renewal, and genomic integrity. Because no single oncogenic lesion is sufficient to overcome all of these physiologic barriers, oncogenes are restrained from inducing tumorigenesis. Correspondingly, in a tumor that has acquired the complete complement of oncogenic lesions required to overcome all of these safety mechanisms, the inactivation of a single oncogene can restore some of these pathways resulting in proliferative arrest, differentiation, cellular senescence, and\/or apoptosis. Thus, oncogenes induce cancer because they induce a cellular state of enforced oncogenic amnesia in which, only upon oncogene inactivation, the tumor becomes aware of its transgression.\n\n",
                "DataExportTag": "CAN557079",
                "QuestionID": "QID243",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Oncogene addiction versus oncogene amnesia: perhaps more than just a bad habit? Cancer is a multi...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetic Research in Aging and Cancer\""
                    },
                    "2": {
                        "Display": "mouse, senescence, pten, transgenic_mouse, wild, deletion, senescent, nan_value, ras_mutations, arf, deficient, cellular_senescence, hh, oncogene, deficiency"
                    },
                    "3": {
                        "Display": "\"Cell Adhesion and Migration Mechanisms\""
                    },
                    "4": {
                        "Display": "membrane, actin, plasma_membrane, integrin, adhesion, migration, surface, rhoa, rho, trafficking, lipid_raft, estrogen_receptor, actin_cytoskeleton, endocytosis, motility"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID110",
            "SecondaryAttribute": "Operative time was 190 minutes, and blood loss was 450\u2009mL. No complications occurred, and dischar.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Operative time was 190 minutes, and blood loss was 450\u2009mL. No complications occurred, and discharge was on postoperative day 2. Histologic evaluation confirmed clear-cell renal-cell carcinoma, Fuhrman grade 3 with negative surgical margins, pT3bN0Mx (Fig. 3).\n\n",
                "DataExportTag": "38",
                "QuestionID": "QID110",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Operative time was 190 minutes, and blood loss was 450\u2009mL. No complications occurred, and dischar...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID368",
            "SecondaryAttribute": "Optimal extent of surgery for early gallbladder cancer with regard to long\u2010term survival: a meta",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Optimal extent of surgery for early gallbladder cancer with regard to long\u2010term survival: a meta\u2010analysis The optimal surgical extent for T1 gallbladder cancer (GBC) remains controversial. Simple cholecystectomy is routinely performed for T1 GBC while several guidelines recommend extended cholecystectomy for T1b GBC. However, evidence regarding the optimal surgical extent for T1 GBC is lacking. This study aims to systematically evaluate the optimal surgical extent for early GBC with regard to long\u2010term survival.\n\n",
                "DataExportTag": "CAN1431677",
                "QuestionID": "QID368",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Optimal extent of surgery for early gallbladder cancer with regard to long\u2010term survival: a meta\u2010...",
                "Choices": {
                    "1": {
                        "Display": "\"Gallbladder Cancer Prognosis and Treatment\""
                    },
                    "2": {
                        "Display": "recurrence, prognosis, metastasis, gallbladder, staging, survival, preoperative, adjuvant, resection, adjuvant_chemotherapy, invasion, gbc, lr, local_recurrence, lymph_node"
                    },
                    "3": {
                        "Display": "\"Cancer Treatment and Survival Rate Analysis\""
                    },
                    "4": {
                        "Display": "overall_survival, lung, rectal, radiotherapy, chemotherapy, non_small_cell_lung_cancer, heart_rate, adenocarcinoma, dfs, adjuvant_chemotherapy, adjuvant, survival, neoadjuvant, squamous_cell_carcinoma, ac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID177",
            "SecondaryAttribute": "Optimized Industrial HIPIMS system for coating Plastic Injection Moulds In recent years an innova...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Optimized Industrial HIPIMS system for coating Plastic Injection Moulds In recent years an innovative magnetron sputtering technique named high power pulsed\/impulse magnetron sputtering (HPPMS\/HIPIMS) has emerged. It is characterized by a high degree of ionization of the sputtered material which makes it a very attractive deposition technique for enhancing and tailoring coating properties. Extremely dense, hard, well adherent and smooth coatings are obtained.However, after fourteen years of developments, the real up-scaling and industrialization of this promising technology has not been achieved. The main problem is the mistaken assumption based just on including new HIPIMS power supplies into conventional sputtering systems.This project aims to develop new HIPIMS deposition equipment capable to control process parameters (magnetic field strength and unbalance and pulsing configuration) in order to demonstrate that process-designed HIPIMS for specific application can result in superior coatings. The target market for the developed coatings is the plastic injection moulding sector where there are high demands on surface quality of moulds. The main degradation mechanisms involved are adhesion, abrasion and corrosion of the surface; moreover, low adhesion of the plastic melt is necessary for reduction of release forces. HIPIMS technology offers outstanding advantages with respect to conventional coating technologies currently applied on moulds (PVD and electroplating) offering optimized performance against corrosion, sticking and wear.This project gives FLUBETECH the opportunity to evaluate the requirements and benefits (both technical and economical) that the launch of an optimized concept of HIPIMS, repetitive and easy to scale-up, will entail for them. Phase 1 of this project will assess the technical feasibility and commercial potential of the breakthrough innovation that Flubetech wants to exploit and commercialize. The outcome of this study will be a detailed business plan.\n\n",
                "DataExportTag": "COR25343",
                "QuestionID": "QID177",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Optimized Industrial HIPIMS system for coating Plastic Injection Moulds In recent years an innova...",
                "Choices": {
                    "1": {
                        "Display": "\"Biofuel Production and Waste Management\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID238",
            "SecondaryAttribute": "Optimized Industrial HIPIMS system for coating Plastic Injection Moulds In recent years an innova...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Optimized Industrial HIPIMS system for coating Plastic Injection Moulds In recent years an innovative magnetron sputtering technique named high power pulsed\/impulse magnetron sputtering (HPPMS\/HIPIMS) has emerged. It is characterized by a high degree of ionization of the sputtered material which makes it a very attractive deposition technique for enhancing and tailoring coating properties. Extremely dense, hard, well adherent and smooth coatings are obtained.However, after fourteen years of developments, the real up-scaling and industrialization of this promising technology has not been achieved. The main problem is the mistaken assumption based just on including new HIPIMS power supplies into conventional sputtering systems.This project aims to develop new HIPIMS deposition equipment capable to control process parameters (magnetic field strength and unbalance and pulsing configuration) in order to demonstrate that process-designed HIPIMS for specific application can result in superior coatings. The target market for the developed coatings is the plastic injection moulding sector where there are high demands on surface quality of moulds. The main degradation mechanisms involved are adhesion, abrasion and corrosion of the surface; moreover, low adhesion of the plastic melt is necessary for reduction of release forces. HIPIMS technology offers outstanding advantages with respect to conventional coating technologies currently applied on moulds (PVD and electroplating) offering optimized performance against corrosion, sticking and wear.This project gives FLUBETECH the opportunity to evaluate the requirements and benefits (both technical and economical) that the launch of an optimized concept of HIPIMS, repetitive and easy to scale-up, will entail for them. Phase 1 of this project will assess the technical feasibility and commercial potential of the breakthrough innovation that Flubetech wants to exploit and commercialize. The outcome of this study will be a detailed business plan.\n\n",
                "DataExportTag": "COR25343",
                "QuestionID": "QID238",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Optimized Industrial HIPIMS system for coating Plastic Injection Moulds In recent years an innova...",
                "Choices": {
                    "1": {
                        "Display": "\"Biofuel Production and Waste Management\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID194",
            "SecondaryAttribute": "PatientPhysician Connectedness and Quality of Primary Care Context Continuity of care is a basic...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "PatientPhysician Connectedness and Quality of Primary Care Context Continuity of care is a basic tenet of high-quality primary care, but the relationship between quality of care and the connection between patient and physician has not been rigorously studied. Contribution The researchers defined whether 155590 adults in a primary care network received most of their care from a specific physician, practice, or neither. Patients who were connected to a particular physician were more likely to have received recommended care than patients who were connected to a practice but not a physician. Caution The study involved only 1 network, which is one of many potential definitions of continuity, and selected quality measures. The Editors Persistent deficiencies exist in the quality of health care in the United States (14). Because primary care physicians are the first source of health care for most patients to receive preventive and chronic illness care, efforts to measure and improve quality of care have often focused on these physicians (57). In practice, however, many patients receive episodic care from different physicians (812). Patients without a regular source of care are less likely to receive care consistent with guidelines (1320). Continuity of care is a shared responsibility between physicians and patients. Even if physicians or practices treated all patients similarly, patients vary in their ability and willingness to adhere to recommendations. Performance measures originally designed for use in large populations are increasingly used to assess the quality of practices and individual physicians. One concern with this approach is that physicians who care for patients who are less willing or able to adhere to recommendations will seem to perform less well. To investigate this possibility, we developed the concept of physicianpatient connectedness. We use the term connectedness to describe the closeness of the relationship between a patient and an individual physician on the basis of a model predicting how likely a physician is to identify a patient as my patient. We hypothesized that patients highly connected to a specific physician would be more likely to receive care consistent with guidelines, according to common performance measures. We further hypothesized that differences in connectedness may contribute to health care disparities to the extent that connectedness is correlated with race or ethnicity and insurance status. We investigated these hypotheses in a network of primary care physicians affiliated with a large teaching hospital. We used a previously developed and validated algorithm (21, 22) to determine the connectedness of more than 150000 patients with a specific physician. The algorithm used the designated primary care physician field from the practice registration system along with patient age, time since most recent visit, and in-state residence. We then examined variation in the proportion of connected patients among practices and the association of connectedness with the performance of commonly used measures of health care quality. Methods Study Setting and Sample The Massachusetts General Hospital (Boston, Massachusetts) adult primary care network includes 181 primary care physicians working in 13 clinically and demographically diverse practices (4 community health centers and 9 hospital-affiliated practices). The practices use the same electronic billing and scheduling systems, and physicians have the same compensation plan and staffing resources. Patients must designate a primary care physician when registering for care. We identified all patients with a visit to 1 of these practices from 1 January 2003 to 31 December 2005 by using electronic billing records. During this time, 169024 unique patients were seen for 994431 visits. We excluded patients if they were younger than 18 years (n= 1924), had died (determined on the basis of review of social security records) (n= 2817), or were registered as having a primary care physician outside of the Massachusetts General Hospital network (n= 8693). The Massachusetts General Hospital institutional review board approved the study. Connecting Patients With Primary Care Physicians and Practices Figure 1 shows the process used to connect patients with a specific physician or practice. We previously developed and validated an algorithm to connect patients with a specific physician by having 18 primary care physicians review a list of all patients seen over 3 years (mean, 1029 patients per physician; range, 226 to 2372 patients per physician) and designate which patients they considered to be my patient (21, 22). The algorithm primarily uses the primary care physician designee field from the hospital registration system. However, as a stand-alone variable, its specificity (84.9%) would result in too many patients on a list being incorrectly identified as being connected to that physician (21). As a result, the final algorithm combined the primary care physician designee field with a logistic regression model that included patient age, time since most recent visit, in-state residence, and physician practice style (21). We defined the physician practice style variable according to the proportion of all visits by patients registered to the physician. Thus, physicians who were the registered provider for at least 70% of the patients they saw were categorized as following a solo-practice style, whereas physicians who were the registered provider for fewer than 70% of the patients they saw were designated as having a collaborative-practice style. The model variables were designed to provide a highly specific list of patients for a given physician (overall specificity, 93.7%; positive predictive value, 96.5% [range, 90.1% to 100%]) (21). Figure 1. Method of connecting patients with specific primary care physicians or practices. MGH = Massachusetts General Hospital; PCP = primary care physician. The square boxes represent the patient population seen in the MGH primary care network and their initial assessment based on listed provider. The hexagonal boxes represent the algorithms that connect patients to a specific physician or practice. The rounded boxes represent the disposition of the primary care population based on patientphysician connectedness. * Patients younger than 18 years and those who were deceased are also included in this category. Patients who could not be connected to a specific physician were connected to the primary care practice in which they received most of their care. Patients were not connected to a specific physician because they had a primary care physician in a given practice but did not meet threshold criteria (using the patientphysician connectedness algorithm), were only seen by physicians other than their registered primary care physician, were followed by a resident physician, or received care in a given practice but were not registered with a primary care physician in that practice. Patients who were followed by a resident physician were assigned to the practice in which the resident provided care. We developed criteria for connecting patients to individual practices by consensus in collaboration with physician practice representatives (Table 1). Patients who could not be assigned to either a physician or a practice with these methods were designated as unconnected. Table 1. Criteria Used to Define Whether Patients Not Connected to a Specific Physician Were Connected to a Specific Primary Care Practice Patient and Provider Characteristics and Performance Measures We obtained data from an electronic record repository for Massachusetts General Hospital and affiliated institutions (23). Available patient characteristics included date of birth, sex, race or ethnicity, primary language spoken, insurance status, number of outpatient office visits during the previous 3 years, and months since most recent outpatient visit. We obtained physician characteristics (age, sex, practice location, and years since medical school graduation) from the hospital registrar database. Physician performance measures focused on cancer screening and chronic disease management. Cancer screening measures were mammography for women age 42 to 69 years in the previous 2 years and without previous bilateral mastectomy; Papanicolaou cervical screening in the previous 3 years for women age 21 to 64 years without hysterectomy; and colonoscopy within 10 years, sigmoidoscopy or double-contrast barium enema within 5 years, or home fecal occult blood testing within 1 year for patients age 52 to 69 years without total colectomy. For patients with diabetes, we assessed 2 measures: hemoglobin A1c (HbA1c) and low-density lipoprotein cholesterol measured in the previous year (24). For patients with coronary artery disease, we assessed low-density lipoprotein cholesterol measured in the previous year (25). For persons who had HbA1c and low-density lipoprotein cholesterol testing, we also assessed the most recent value available and categorized HbA1c level as less than 8.0% or not and low-density lipoprotein cholesterol level as less than 2.59 mmol\/L (<100 mg\/dL) or not (26). We extracted data for these measures from electronic laboratory and imaging reports or billing data within the Partners Healthcare System on the basis of Healthcare Effectiveness Data and Information Set criteria (27). Statistical Analysis We first grouped patients by connectedness status and compared characteristics of physician-connected, practice-connected, and unconnected patients. To account for the repeated measures of patients from the same physician, we used generalized estimating equations techniques with compound symmetry correlation structure (PROC GENMOD [SAS, version 9.1.3, SAS Institute, Cary, North Carolina]) (28) in all statistical analyses for clustering effects. The physician was considered as the unit of cluster for physician-connected patients, and each patient was considered as an individual cluster for practice-connected patients. Becaus\n\n",
                "DataExportTag": "CAN536241",
                "QuestionID": "QID194",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "PatientPhysician Connectedness and Quality of Primary Care Context Continuity of care is a basic...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Development and Prevention Strategies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Cancer Screening and Prevention\""
                    },
                    "4": {
                        "Display": "screening, woman, colorectal_cancer, gene, breast, cervical, behavior, human_papillomavirus, cutaneous, physician, belief, prevention, prostate, sun_protection, perception"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID255",
            "SecondaryAttribute": "PatientPhysician Connectedness and Quality of Primary Care Context Continuity of care is a basic...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "PatientPhysician Connectedness and Quality of Primary Care Context Continuity of care is a basic tenet of high-quality primary care, but the relationship between quality of care and the connection between patient and physician has not been rigorously studied. Contribution The researchers defined whether 155590 adults in a primary care network received most of their care from a specific physician, practice, or neither. Patients who were connected to a particular physician were more likely to have received recommended care than patients who were connected to a practice but not a physician. Caution The study involved only 1 network, which is one of many potential definitions of continuity, and selected quality measures. The Editors Persistent deficiencies exist in the quality of health care in the United States (14). Because primary care physicians are the first source of health care for most patients to receive preventive and chronic illness care, efforts to measure and improve quality of care have often focused on these physicians (57). In practice, however, many patients receive episodic care from different physicians (812). Patients without a regular source of care are less likely to receive care consistent with guidelines (1320). Continuity of care is a shared responsibility between physicians and patients. Even if physicians or practices treated all patients similarly, patients vary in their ability and willingness to adhere to recommendations. Performance measures originally designed for use in large populations are increasingly used to assess the quality of practices and individual physicians. One concern with this approach is that physicians who care for patients who are less willing or able to adhere to recommendations will seem to perform less well. To investigate this possibility, we developed the concept of physicianpatient connectedness. We use the term connectedness to describe the closeness of the relationship between a patient and an individual physician on the basis of a model predicting how likely a physician is to identify a patient as my patient. We hypothesized that patients highly connected to a specific physician would be more likely to receive care consistent with guidelines, according to common performance measures. We further hypothesized that differences in connectedness may contribute to health care disparities to the extent that connectedness is correlated with race or ethnicity and insurance status. We investigated these hypotheses in a network of primary care physicians affiliated with a large teaching hospital. We used a previously developed and validated algorithm (21, 22) to determine the connectedness of more than 150000 patients with a specific physician. The algorithm used the designated primary care physician field from the practice registration system along with patient age, time since most recent visit, and in-state residence. We then examined variation in the proportion of connected patients among practices and the association of connectedness with the performance of commonly used measures of health care quality. Methods Study Setting and Sample The Massachusetts General Hospital (Boston, Massachusetts) adult primary care network includes 181 primary care physicians working in 13 clinically and demographically diverse practices (4 community health centers and 9 hospital-affiliated practices). The practices use the same electronic billing and scheduling systems, and physicians have the same compensation plan and staffing resources. Patients must designate a primary care physician when registering for care. We identified all patients with a visit to 1 of these practices from 1 January 2003 to 31 December 2005 by using electronic billing records. During this time, 169024 unique patients were seen for 994431 visits. We excluded patients if they were younger than 18 years (n= 1924), had died (determined on the basis of review of social security records) (n= 2817), or were registered as having a primary care physician outside of the Massachusetts General Hospital network (n= 8693). The Massachusetts General Hospital institutional review board approved the study. Connecting Patients With Primary Care Physicians and Practices Figure 1 shows the process used to connect patients with a specific physician or practice. We previously developed and validated an algorithm to connect patients with a specific physician by having 18 primary care physicians review a list of all patients seen over 3 years (mean, 1029 patients per physician; range, 226 to 2372 patients per physician) and designate which patients they considered to be my patient (21, 22). The algorithm primarily uses the primary care physician designee field from the hospital registration system. However, as a stand-alone variable, its specificity (84.9%) would result in too many patients on a list being incorrectly identified as being connected to that physician (21). As a result, the final algorithm combined the primary care physician designee field with a logistic regression model that included patient age, time since most recent visit, in-state residence, and physician practice style (21). We defined the physician practice style variable according to the proportion of all visits by patients registered to the physician. Thus, physicians who were the registered provider for at least 70% of the patients they saw were categorized as following a solo-practice style, whereas physicians who were the registered provider for fewer than 70% of the patients they saw were designated as having a collaborative-practice style. The model variables were designed to provide a highly specific list of patients for a given physician (overall specificity, 93.7%; positive predictive value, 96.5% [range, 90.1% to 100%]) (21). Figure 1. Method of connecting patients with specific primary care physicians or practices. MGH = Massachusetts General Hospital; PCP = primary care physician. The square boxes represent the patient population seen in the MGH primary care network and their initial assessment based on listed provider. The hexagonal boxes represent the algorithms that connect patients to a specific physician or practice. The rounded boxes represent the disposition of the primary care population based on patientphysician connectedness. * Patients younger than 18 years and those who were deceased are also included in this category. Patients who could not be connected to a specific physician were connected to the primary care practice in which they received most of their care. Patients were not connected to a specific physician because they had a primary care physician in a given practice but did not meet threshold criteria (using the patientphysician connectedness algorithm), were only seen by physicians other than their registered primary care physician, were followed by a resident physician, or received care in a given practice but were not registered with a primary care physician in that practice. Patients who were followed by a resident physician were assigned to the practice in which the resident provided care. We developed criteria for connecting patients to individual practices by consensus in collaboration with physician practice representatives (Table 1). Patients who could not be assigned to either a physician or a practice with these methods were designated as unconnected. Table 1. Criteria Used to Define Whether Patients Not Connected to a Specific Physician Were Connected to a Specific Primary Care Practice Patient and Provider Characteristics and Performance Measures We obtained data from an electronic record repository for Massachusetts General Hospital and affiliated institutions (23). Available patient characteristics included date of birth, sex, race or ethnicity, primary language spoken, insurance status, number of outpatient office visits during the previous 3 years, and months since most recent outpatient visit. We obtained physician characteristics (age, sex, practice location, and years since medical school graduation) from the hospital registrar database. Physician performance measures focused on cancer screening and chronic disease management. Cancer screening measures were mammography for women age 42 to 69 years in the previous 2 years and without previous bilateral mastectomy; Papanicolaou cervical screening in the previous 3 years for women age 21 to 64 years without hysterectomy; and colonoscopy within 10 years, sigmoidoscopy or double-contrast barium enema within 5 years, or home fecal occult blood testing within 1 year for patients age 52 to 69 years without total colectomy. For patients with diabetes, we assessed 2 measures: hemoglobin A1c (HbA1c) and low-density lipoprotein cholesterol measured in the previous year (24). For patients with coronary artery disease, we assessed low-density lipoprotein cholesterol measured in the previous year (25). For persons who had HbA1c and low-density lipoprotein cholesterol testing, we also assessed the most recent value available and categorized HbA1c level as less than 8.0% or not and low-density lipoprotein cholesterol level as less than 2.59 mmol\/L (<100 mg\/dL) or not (26). We extracted data for these measures from electronic laboratory and imaging reports or billing data within the Partners Healthcare System on the basis of Healthcare Effectiveness Data and Information Set criteria (27). Statistical Analysis We first grouped patients by connectedness status and compared characteristics of physician-connected, practice-connected, and unconnected patients. To account for the repeated measures of patients from the same physician, we used generalized estimating equations techniques with compound symmetry correlation structure (PROC GENMOD [SAS, version 9.1.3, SAS Institute, Cary, North Carolina]) (28) in all statistical analyses for clustering effects. The physician was considered as the unit of cluster for physician-connected patients, and each patient was considered as an individual cluster for practice-connected patients. Becaus\n\n",
                "DataExportTag": "CAN536241",
                "QuestionID": "QID255",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "PatientPhysician Connectedness and Quality of Primary Care Context Continuity of care is a basic...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Development and Prevention Strategies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Cancer Screening and Prevention\""
                    },
                    "4": {
                        "Display": "screening, woman, colorectal_cancer, gene, breast, cervical, behavior, human_papillomavirus, cutaneous, physician, belief, prevention, prostate, sun_protection, perception"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID339",
            "SecondaryAttribute": "Performance Analysis of DMD and SURF Methods for Texture Classification The Texture is the crucia...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Performance Analysis of DMD and SURF Methods for Texture Classification The Texture is the crucial attribute in every image classification method as it describes the appearance of object. Today, different approaches of texture classification have been developed which focus on acquisition of image features from its texture and categorize them into different classes by using a particular classifier. This paper gives a state-of-the-art texture classification technique called Dense MicroBlock Difference (DMD). In this concept, image data representation is accomplished by capturing features in the form of micro-blocks. This method gives superior performance over already established methods in terms of processing time, accuracy and robustness and able to obtain whole image information. In this paper, we have taken UMD dataset for processing and calculated different performance parameters which gives excellent results than comparative method SURF. International Journal of Engineering and Techniques Volume 3 Issue 3, May-June 2017 ISSN: 2395-1303 http:\/\/www.ijetjournal.org Page 153 region to be described. The major advantage of this method is that the relative distribution of local orientation is not lost and these LBP-HF descriptors also outperform the MR8 descriptors [8]. L.D.Griffiu and M. Crosier, in 2008, proposed texture classification with a dictionary of basic image features. In this method, they had represented a multi-scale texture classification algorithm which produces state-of-art results on widely used texture dataset.BIF columns were used here to describe larger local regions of image. They have used geometrically derived dictionary of features over which images are represented, rather than pre-training of dictionary of textons. This results in simpler and more general approach. Here, more sophisticated classifiers are not used for ease of implementation. So, we can improve this method by using SVM classifier as it has higher accuracy [9]. D. Nghi and L. Chi Mai invented method, training data selection for support vector machines model in 2011. When parameters of SVM are applied to a large dataset, it requires a long time for training so the model selection task and its performance can be degraded. To reduce the time for model selection this paper has proposed a training data selection method then applied the model selection on reduced training set. Results showed that a significant amount of time for model selection can be saved without degrading the performance [10]. Efficient and robust image descriptors for GUI object classification invented by A. Dubrovina et al. in 2011.In this paper, a advanced image descriptor is implemented specifically for GUI objects which is robust to various changes in the appearance of GUI objects like various screen resolution as well as various operating system related issues. This image descriptor is further used with SVM and experiments have shown the descriptor robustness to the above transformations and its superior performance compared to existing image descriptors [11] A. Wojnar and A. Pinheiro presented annotation of medical images using the SURF descriptors in 2012. Here, Fast Hessian matrix is used to extract features and classification is given by SVM with a quadratic kernels. The testing of developed system was performed on IRMA radiographic images. Then results of SURF features are compared with SIFT features and results showed that SURF features has better accuracy of 96%. The annotation performance will be increased by implementing multiple classifiers [12] J. Sanchez et al. proposed image classification using Fisher vector in 2013.It is patch encoding strategy which has advantages like efficiency in computing, excellent results, and minimal loss of accuracy. Within Fisher vector framework, images are defined by first extracting a set of low level patch and then computing their deviations from a universal generative model. However being very high-dimensional and dense, the Fisher vector becomes impractical for large scale application due to storage limitation [13]. III. PROPOSED METHODOLOGIES Proposed classification methodology is studied by two different ways which includes features extracted using DMD features extracted using SURF as follows: i] Texture Classification using DMD feature extraction method ii] Texture Classification using SURF feature extraction method A. Texture Classification using DMD feature extraction method: Figure shows block diagram of classification of texture using DMD and SVM. Input images used for experimentation are taken from UMD dataset available free on internet. Then DMD features are extracted for input image [6]. The dimensions of DMD features are reduced using Random Projection technique. Further these low dimensional features are converted into descriptors in encoding block and finally classified by SVM classifier as shown in Fig. 1. Fig.1Texture classification using DMD with SVM 1. Input Image Input image is taken from UMD dataset for processing. The size of each image is 1280x960. All images from the dataset are taken with different viewpoint changes and scale. 2. Extract DMD Features DMD features captures information by working on micro-blocks of input texture image. DMD uses intensity difference from image patch to capture the variations in it. In this method, we use small blocks in image patch instead of single pixel because individual pixels are more adaptive to noise and do not capture entire information. Here, average intensity is considered to capture data [6]. 3. Dimensionality Reduction using Random Projection (RP) To efficiently capture the intensity difference, it is necessary to select a large number of sampling points which further increases dimensionality. This puts limitation on the size of image patch. But images are sparse in nature so we can apply compression sensing techniques to it. For reduction on dimensionality and to make it more compressed Random Projections technique is used which reduces dimensionality by preserving required information. International Journal of Engineering and Techniques Volume 3 Issue 3, May-June 2017 ISSN: 2395-1303 http:\/\/www.ijetjournal.org Page 154 4. Encoding To convert features into descriptors encoding is used. Fisher vector technique is used for encoding [6]. Generative model for feature extraction is used by representing data by means of gradient of data loglikelihood with respect to model parameters. The Fisher vector uses the Gaussian Mixture Model (GMM) to obtain representation of compressed features. The first order and second order differences between the image descriptors and the GMM centers are obtained by encoding. 5. SVM Classifier Final stage of DMD algorithm is classification. Here, we have used SVM classifier which converts image features into texture classes. It compares testing set with the training set and gives proper classification of objects. SVM divides two classes by using a hyperplane. SVM gives best accuracy performance compared to ANN and nearest neighbors. B. Texture Classification using SURF feature extraction method: Figure shows block diagram of classification of texture using SURF and SVM. Input images used for experimentation are taken from UMD dataset available free on internet. Then SURF features are extracted for input image and feature vector is formed and finally classification is done with SVM classifier [7]. Fig. 2 Texture Classification using SURF and SVM In this method, SURF feature extraction algorithm consists of four parts. First part is integral image computation which is very important to boost the performance speed of SURF method. Each input image is first converted into integral image and then passed for further processing. Second part is interest point detection in which point of interests are located by using determinant of Hessian matrix and points are obtained where this determinant is maximum. Third part is description in which Haar wavelet responses are used to give best description and finally these descriptors are categorized using SM classifier. V. RESULTS The DMD method classification is presented by following stepsStep 1: Test Input Image Input image is taken from first texture class for processing. Size of an image is 1280 x 960. The result is shown below in the form of GUI. Fig.3 Input image for DMD method Step 3: Obtain DMD features DMD features of selected image are extracted by taking image patch of size 4 x 4. Number of sample points selected are 100 and number of co-ordinate points selected are 60. Block radius is 2. Hence output DMD vector is of size 60 x 800. This local feature vector is encoded by using Fisher encoding to obtain descriptors. Encoding uses GMM to derive probabilistic representation of local features. Here first and second order differences between the image descriptors and the GMM centers are captured. At the end of encoding, we get vector of 1x 300. Finally, these descriptors are fed to classifier to perform categorization. Step 4: Classification Final stage of proposed method is classification in which multilevel classifier is used which converts obtained image descriptors into texture classes. We have taken input test image from texture class 1, so at the end of classification selected image is correctly classified as \u2018Test set is Class 1\u2019. This result is shown in following Fig. 4 Fig.4 Texture classification of input image using DMD method The SURF method classification is presented by following stepsStep 1: Test Input Image Input image is taken from texture class 1 of UMD dataset which is of size 1280 x 960. International Journal of Engineering and Techniques Volume 3 Issue 3, May-June 2017 ISSN: 2395-1303 http:\/\/www.ijetjournal.org Page 155 Fig.5 Input image for SURF method Step 2: SURF feature extraction To obtain SURF features, first stage is integral image computation which is useful for boosting the speed of performance. Further interest points are located by using Hessian matrix determinant. Extracted SURF features appear on the image as red ci\n\n",
                "DataExportTag": "AI113515",
                "QuestionID": "QID339",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Performance Analysis of DMD and SURF Methods for Texture Classification The Texture is the crucia...",
                "Choices": {
                    "1": {
                        "Display": "\"Self-Supervised Learning and Data Augmentation\""
                    },
                    "2": {
                        "Display": "transfer, self_supervised, augmentation, shot, annotation, deep_learning, contrastive, semi_supervised, unlabele, unlabeled, ssl, adaptation, unsupervised, pseudo_label, classification"
                    },
                    "3": {
                        "Display": "\"Deep Learning for Image and Speech Recognition\""
                    },
                    "4": {
                        "Display": "cnn, deep_learning, classification, speech_recognition, image, transfer, convolution, traffic_sign, capsule, augmentation, dcnn, networks, rnn, imagenet, extraction"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID321",
            "SecondaryAttribute": "Pericyte-endothelial cell interaction Newly formed blood vessels recruit pericytes that provide t...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Pericyte-endothelial cell interaction Newly formed blood vessels recruit pericytes that provide the vasculature with stability. However the exact molecular mechanisms involved in conferring this stability are mostly unknown. Platelet-derived growth factor (PDGF) secreted by the endothelial cells of the newly formed blood vessels attracts the pericytes toward the endothelial cells. PDGF\/PDGFR signaling has been shown to assist in recruitment of pericytes to endothelial cells. Mice lacking either PDGF or its receptor show abnormal vasculature due to defective pericyte coverage.1-3 In a recent report, Franco et al. show that the pericyte-endothelial interaction results in a pericyte-induced survival advantage to the endothelial cells of the tumor vessels.4\n\n",
                "DataExportTag": "CAN21715",
                "QuestionID": "QID321",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "Pericyte-endothelial cell interaction Newly formed blood vessels recruit pericytes that provide t...",
                "Choices": {
                    "1": {
                        "Display": "Angiogenesis involves endothelial cell proliferation, migration, and tube formation. However, without proper pericyte coverage, these blood vessels remain immature and appear abnormal in nature. The pericytes in turn provide paracrine signals that promote vascular stability. The Ang-Tie receptor system has been implicated for vascular maturation by recruiting pericytes.5 The Tie receptors are mainly found on endothelial cells whereas Ang-1 is mainly expressed by pericytes. Ang-1 supports pericyte recruitment and vessel stability, whereas Ang-2, which is expressed by both endothelial cells and smooth muscle cells, seems to function in an antagonistic manner, causing loss of pericyte attachment to endothelial cells.6 In addition to vessel stability, pericyte coverage is said to also provide endothelial survival signals to the tumor endothelial cells.7"
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "By using the RIP1-Tag2 mouse model of pancreatic neuroendocrine tumorigenesis (PNET), Franco et al. were able to study the association between pericytes and endothelial cells. They uncovered the mechanism through which pericytes confer vascular stability to the tumor endothelial cells.4 RIP1-Tag2 mice treated with tyrosine kinase inhibitors imatinib or CP-673,451, which blocked PDGFR\u03b2, but not VEGFR kinases, caused partial detachment of pericytes from tumor blood vessels. Thus they were able to isolate tumor endothelial cells that had poor pericyte coverage by treating the tumors with imatinib. Gene expression studies performed with these poor pericyte-covered endothelial cells showed a consistent reduction in expression of Bcl-w, an anti-apoptotic gene, compared with control endothelial cells that had abundant pericyte coverage. This suggested that a close association between pericytes and endothelial cells is essential for endothelial cell survival. Co-culture studies of pericytes and endothelial cells further demonstrated a need for physical association between the two cell types for upregulation of Bcl-w as well as VEGF-A in the endothelial cells. VEGF\/VEGFR-2 signaling has been involved in cell proliferation and migration as well as pro-survival pathways. VEGF-A is also known to upregulate Bcl-2 expression in cancer cells and endothelial cells that lead to cell survival.8,9 The finding that the pericyte-dependent VEGF upregulation in the tumor endothelial cells is novel. Furthermore, the authors, using sFlt1, which blocks paracrine VEGF, and AG-02826, a small molecule kinase inhibitor that blocks both internal and external sources of VEGFR activation, have shown that Bcl-w expression in endothelial cells by association with pericytes is dependent on the autocrine signaling through VEGF\/VEGFR. The authors further show that reduced pericyte association with endothelial cells results in a decrease in expression of a number of NF\u03baB target genes, including VEGF-A.10 Since NF\u03baB is activated downstream of integrin signaling, the authors used neutralizing antibodies against \u03b1v integrins to block the expression of VEGF and Bcl-w in a pericyte-endothelial cell co-culture system. Thus the data provided by Franco et al. strongly suggests a signaling crosstalk initiated through the interaction between pericytes and endothelial cells, which renders a survival advantage to the endothelial cells. Taken together, their results provide a pathway wherein PDGF-B expressed by endothelial cells attracts pericytes in a paracrine manner to the newly formed vessel. The recruited pericytes deposit vitronectin, which is recognized by \u03b1v integrins on the endothelial cells. The integrin interaction with vitronectin leads to NF\u03baB activation and VEGF gene transcription. Increased expression of VEGF results in activation of VEGFR-2 through autocrine signaling and Bcl-w expression in endothelial cells (Fig.\u00a01)."
                    }
                },
                "AnswerOrder": [
                    "1"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID320",
            "SecondaryAttribute": "Personal Time: The Patient's Experience It is one of the peculiarities of Time that it is intense...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Personal Time: The Patient's Experience It is one of the peculiarities of Time that it is intensely private and yet so widely shared. We could put it like this: that superficially, in the world of clocks and watches and appointments, we share Time; then, on a deeper level, it seems intensely private; and then, on a still deeper level, perhaps we begin to share it again, in ways we cannot fully understand. J.B. Priestley, Man and Time, 1964 (1) My patient Margaret regularly keeps a journal, the story of her lifelong journey with multiple sclerosis; she allows me to read it on each visit. She has reconciled herself to the loss of some abilities and activities and to her altered life plans, but she doesn't dwell on these. Instead, she writes about the many things she can do, her family and friends, her garden, the autumn colors. She writes that she has a new life now, with a different view of the future and, in many ways, an enhanced appreciation of experiences and of time. Her sense of time and future changes depending on whether she is in an attack or in remission. One of the difficulties of her disease is the requirement to adjust anew after an attack; once she has adapted to a new level and way of life, the next attack leaves further deficit, demanding a new adjustment. Margaret comments that her sense of the passage of time changes as her disease and her life change. Life seemed to stop for her when she had her first attack and learned the diagnosis. Slowly she got her life back on track, but recovery seemed to take forever, and it dragged on through a hot, humid summer. She had sick leave from her job only until mid-September and made a mental resolution to be back to normal activities by that time. Recovery seemed agonizingly slow, while at the same time her self-imposed deadline approached quickly (Figure 1). Figure 1. Sink. As her physician, I had to learn to relate to her sense of the flow of her life. Rushed consultation, rapid changes in medications for each new symptom, and concentration on the disease would not help her cope with her challenges. Understanding that she wanted to concentrate on her life and how it could be altered and reshaped to deal with the disease helped me to help her. She did not want discussion of mechanisms of demyelination and a prescription. She wanted encouragement to keep going, to continue gardening despite numbness in her legs, to travel across the country to a wedding. Her disease had changed her view of time and the journey of life, and she wanted permission and encouragement to get on with it. Of Time and Illness Patients generally have variable perceptions of how time moves, depending on the circumstances and the illness. A painful procedure can seem to last longer, but apprehension of a coming event that they dread can make time seem to move too fast. Fever increases the rate of activity in the brain and our sense of passing time. In hypothyroidism the days slow, as do bodily movements and thinking. Hyperactive children feel that time passes more quickly (2). Our bodies have their own sense of keeping time with internal clocks, circadian rhythms, and rhythms in non-nervous system organs, noted in our cardiac pulse, renal output, menstrual cycle, sleep-wake cycles, and brain rhythms. Regardless of whether we are well or ill, time can seem to pass too slowly or too quickly, no matter what the Newtonian clock on the wall tells us. Time seems slow in childhood, but we perceive it as moving by faster as we age. Sir Roger Bannister described the last 4 seconds of his historic under-four-minute mile as seemingly never ending. Senator Robert Kennedy, commenting on the shortened life of his brother, said that some people live their lives faster than others. Personality characteristics can modify how time is experienced: Some people feel that time is always interminably slow, others that it's always speeded up, and highly integrated people experience it with great variation (3). The task we undertake can change how we perceive time moving. Time flies when we are working on something we enjoy and to which we are committed and enthusiastic, but it slows down, moving at a snail's pace, when the job is ponderous, depressing, or difficult. Illumination, barometric pressure, anxiety, stress, pain, anticipation, and feeling overwhelmed by tasks and deadlines can all affect our sense of time. Drugs can affect time: Caffeine and thyroxine decrease the estimation of time, and amphetamines, marijuana, and opium increase it. Busy days, constructed around our watches, our pocket organizer outlining the daily sequence of meetings and duties, and meals and TV schedules, make it seem as if time flows in a straight line; Dossey referred to this experience as the devouring tyrant of linear time (4). Because earlier cultures saw life as continuous cycles of the days, months, and seasons, speculation has arisen about when modern societies began to see time as linear. Shallis (5) felt it was the development of the clock that yoked mankind to a linear, countable time and lost for the people of modern Western culture that organic feel for time's endless patterns; McLuhan [6] believed that the construct of linear time began with the formulation of modern language, with its linear form); and Szamosi (7) related it to the invention of polyphony in music during the Middle Ages. These notions are more than academic in this discussion; my patients see life in cycles when an illness strikes, a return to the circular, eternal braid described by Hofstadter (8). When life-threatening disease looms over us, everything changes. Patients often talk of the moment when they heard they had cancer, amyotrophic lateral sclerosis, multiple sclerosis, or leukemia as a heart-stopping thud, a Wagnerian chord, as if time stopped. Up until then, our fast-forwarded lives allowed us to ignore and repress the mental and physical bumps in the road of life, expecting that time will heal (the proverbial tincture of time) so that we could continue with our busy schedules. Serious illness changes all of that. Because most of my patients have multiple sclerosis, I have become aware of how this particular chronic disease changes their sense of time, and I find that I can be a better advisor for them if I learn how they see their new life with illness. It is often complex; they may experience a variable sense of time, with different time lines running simultaneously like cars on a highway, some moving annoyingly slowly, others dangerously fast. Episodes of symptoms drag on, progression moves quickly, the memory of a past healthy and vigorous life compresses and fades, and the future seems to shorten. None of these are in chronological time; rather, they exist in a very personal, intensely felt dimension, each with its own characteristic shape and size and speed. When patients ask, How much time do I have, Doc? or Will I be in a wheelchair? time again slows as they wait for the answer. Suddenly, the way they saw time, how time was intricately entwined with their lives and went off in a line forever into the future, was changed forever, as suddenly as if a distant thread was cut. Although physicians are not comfortable with such questions, patients must ask them to reorient life to a new sense of time. The Physician's Sense of Time As a physician, I am aware that my sense of time in my life has changed as the years have passed. When I began my medical studies, I was in a hurry. The years ahead seemed to stretch on forever, but my studies toward graduation seemed to proceed at a snail's pace. The clinical years seemed to go faster, and residency even faster again. By the time I was in general medical practice, it all changed. What was the hurry? Why didn't I spend more time on other things in my studies? Why didn't I take a year off to travel or spend a year in a developing country? During subspecialization later, time seemed more even, linked, and continuous. Now that I am in the last decade of my practice, my sense of the time in medicine has changed again. I have learned to see as my patients see, with life in cycles, seasons, and renewal through my children and my grandchildren. I believe that if physicians contemplate their own perception of time in their lives and experience, it will probably lead to a more sensitive appreciation of how the patient's sense of time affects him or her. Certainly the physician who becomes ill will recognize the reality of the patient's sense of time and time change. Because I work with patients who have a chronic disease, I have learned to work in a sphere where everything is seen in long periods or in terms of a lifetime but with episodes of sudden change, necessitating repeated adaptation. My patients have taught me to see on a larger landscape and to in turn help them day by day with this broader and more cyclical view. Africa Time During my time in Africa some years ago, I was told repeatedly to learn how to organize my days, my activities, my expectations of others according to Africa time, which meant that things happened when people got around to it, that things happened when they should. Soon I released myself from the yoke of thinking in hourly and half-hourly appointments in an electronic palm computer and lived life as it naturally unfolded. I became acutely aware of the difference from my sense of time back home in how my life and my schedule were organized. I've had the same feeling while on sabbatical, unfettered by much of the time-linked clutter of the average day of an academic physician. I see my patients adapting to a sense of Africa time, with things being done when they feel they should be done, without the yoke of clock or calendar. The Art of Robert Pope Artist Robert Pope said that he had two birthdays, the second being the anniversary of the diagnosis of his cancer; a new and different life began with the advent of a threat that changed all that came before (9). Robert was a young artist who, after undergoing rep\n\n",
                "DataExportTag": "CAN1354713",
                "QuestionID": "QID320",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Personal Time: The Patient's Experience It is one of the peculiarities of Time that it is intense...",
                "Choices": {
                    "1": {
                        "Display": "\"Survival Analysis and Prognosis in Medical Studies\""
                    },
                    "2": {
                        "Display": "overall_survival, heart_rate, prognosis, dfs, hazard_ratio, survival, progression_free_survival, rfs, kaplan_meier, css, log_rank, cox_proportional, independent_prognostic, multivariate_analysis, dss"
                    },
                    "3": {
                        "Display": "\"Cancer Treatment and Survival Analysis\""
                    },
                    "4": {
                        "Display": "overall_survival, lung, rectal, radiotherapy, chemotherapy, non_small_cell_lung_cancer, heart_rate, adenocarcinoma, dfs, adjuvant_chemotherapy, adjuvant, survival, neoadjuvant, squamous_cell_carcinoma, ac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID346",
            "SecondaryAttribute": "PSS8 COST EFFICACY OF USTEKINUMAB IN TREATMENT OF MODERATE TO SEVERE PLAQUE PSORIASIS IN TURKEY s...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "PSS8 COST EFFICACY OF USTEKINUMAB IN TREATMENT OF MODERATE TO SEVERE PLAQUE PSORIASIS IN TURKEY s A147 body weight less or equal to 100 kg who received 45 mg. Additionally, PASI responses from ACCEPT between ustekinumab and etanercept were used. Costs in the model included drug acquisition costs only. RESULTS: In the weight-based efficacy analysis, ustekinumab 45 mg treatment had the highest PASI 75 response. Under a fixed budget of TL 1,000,000, it is possible to treat more patients successfully (achieving a PASI75) with ustekinumab 45 mg than with etanercept 50 mg biweekly. In both the first year of therapy and in the maintenance year, ustekinumab 45 mg is more cost-effective compared to etanercept 50 mg biweekly. CONCLUSIONS: According to the results of the cost per responder model, ustekinumab 45 mg is more cost-effective than etanercept 50 mg biweekly and therefore a preferable alternative in the treatment of moderate to severe plaque psoriasis in Turkey. PSS9 EFFICIENCY (COST\/EFFICACY) OF BIOLOGIC AGENTS IN THE TREATMENT OF MODERATE TO SEVERE PSORIASIS L&aacute;zaro P, Blasco AJ, Ferr&aacute;ndiz C, Garc&iacute;a A, Liso J Advanced Techniques in Health Services Research (TAISS), Madrid, Spain, Universidad Aut&oacute;noma de Barcelona, Badalona, Barcelona, Spain, Universidad Aut&oacute;noma de Madrid, Madrid, Spain, Complejo Hospitalario de Badajoz, Badajoz, Spain OBJECTIVES: To estimate the cost\/efficacy ratios of biologics authorized in Spain in 2009 (adalimumab, etanercept, infliximab and ustekinumab) in the management of moderate-severe psoriasis. METHODS: A model for economic evaluation (decision tree) was built for the treatments according to the available scientific evidence. The payer perspective (National Health System) was used, only considering drug cost and assuming zero cost for placebo. In the case of weight-dependent dosing, the weight of the study participants was adjusted by age and sex to the standard Spanish population corrected by the weight increment in individuals with psoriasis. The Psoriasis Area Severity Index (PASI) 75 criterion (improvement of 75% from baseline PASI) was used as indicator of efficacy. The incremental efficacy (calculated as the proportion of patients responding with PASI 75 criterion in the biologic group minus the proportion who respond in the placebo group) was assigned according to the outcomes of clinical trials at the period of time defined in the primary efficacy outcome. When more than one trial was available per treatment, a meta-analysis was undertaken (DerSimonianLaird method). Uncertainty was tested by deterministic sensitivity analysis, building scenarios with the confidence intervals at 95% for costs and efficacy. RESULTS: The incremental efficacy in the baseline scenario ranged from 31.19 % (etanercept: 25 mg twice a week at 12 weeks of treatment) to 78.35% (infliximab: 5 mg\/Kg at 24 weeks of treatment). The efficiency in terms of cost\/efficay, in the baseline scenario, ranged from 38,013 (adalimumab at 16 weeks) and 317,981 (ustekinumab: 90 mg at 12 weeks) per PASI 75 responder. In the sensitivity analysis, adalimumab remains as the most efficient biologic on the most and least favourable scenarios. CONCLUSIONS: Of the biologic agents authorized in Spain for treating moderate-severe psoriasis, the most efficient in terms of cost\/efficacy is adalimumab. PSS10 COST PER RESPONDER OF USTEKINUMAB VERSUS ETANERCEPT IN PATIENTS WITH MODERATE-TO-SEVERE PLAQUE PSORIASIS: ANALYSIS FROM THE ACCEPT TRIAL Feldman SR, Augustin M, Martin S, Szapary P, Schenkel B Wake Forest University, Winston-Salem, NC, USA, University Clinics of Hamburg, Hamburg, Germany, Centocor Ortho Biotech Services, LLC, Horsham, PA, USA, Centocor Research &amp; Development, Inc., Malvern, PA, USA, Johnson &amp; Johnson Pharmaceutical Services, LLC, Horsham, PA, USA OBJECTIVES: To compare the cost per responder of ustekinumab (UST) versus etanercept (ETN) based on head-to-head data from the ACCEPT trial, which demonstrated greater efficacy of two doses of UST, 45 mg and 90 mg at weeks 0 and 4, versus ETN, 50 mg twice weekly through week 12, in patients with moderate-tosevere plaque psoriasis (PsO). METHODS: Efficacy results (proportion of patients achieving at least 75% improvement in the Psoriasis Area and Severity Index [PASI75]) were obtained from the ACCEPT trial (n = 903). Given the unique dosing of UST (weeks 0, 4, 16, and q12 weeks thereafter), we determined the cost per PASI75 response at week 16, the appropriate decision point for determining whether to proceed with a third dose. Week 16 PASI75 results were assumed to be equal to week 12 efficacy from ACCEPT; previously published randomized controlled trials have reported similar observations for both drugs. Dosing through week 12 was per ACCEPT. Dosing for weeks 13&ndash;16 was assumed to be per labeled indication in PsO. US wholesale acquisition cost (WAC) was used for calculating costs. The analyses used weight-based efficacy results for UST (45 mg &le;100 kg and 90 mg &gt;100 kg) and overall efficacy for ETN to align with the respective approved labels for each drug. RESULTS: In ACCEPT, 209 patients received UST 45 mg, 347 received UST 90 mg, and 347 received ETN. Baseline demographics and disease characteristics were comparable between groups. Twenty-eight percent of patients were &gt;100 kg. The PASI75 responses at week 12 were 72% for UST 45 mg in patients &le;100 kg and 65% for UST 90 mg in patients &gt;100 kg, compared with 57% for the ETN group. At week 16, the WAC per PASI75 response was $17,009 for UST-treated patients and $19,140 for ETN-treated patients. CONCLUSIONS: WAC per PASI75 response was lower for UST relative to ETN through 16 weeks in PsO patients. PSS11 COST-UTILITY ANALYSIS OF MAINTENANCE TREATMENT WITH TACROLIMUS OINTMENT IN ADULTS AND CHILDREN WITH MODERATE AND SEVERE ATOPIC DERMATITIS Chambers C, Bentley A Astellas Pharma Europe Ltd, Staines, UK, Abacus International, Bicester, Oxfordshire, UK OBJECTIVES: A twice weekly maintenance treatment strategy with tacrolimus ointment for atopic dermatitis significantly delayed and reduced the number of disease flares over a 12-month period compared with the standard reactive tacrolimus treatment strategy. The aim of this post hoc analysis was to evaluate the cost-effectiveness of the maintenance strategy versus the reactive strategy in adults and children with moderate and severe atopic dermatitis (AD). METHODS: The evaluation was performed using a decision analytic model based on the results of two pivotal phase III trials that were conducted in adults and children receiving 0.1% and 0.03% tacrolimus ointment, respectively. Clinical data were taken from the clinical trials and utility data were derived from a published source. The time horizon was 12 months; costs and utilities were applied to the treatment period and to any remaining days in the 12-month period post-tacrolimus discontinuation. Sensitivity analyses assessed the degree of uncertainty around the results. The analysis was conducted from the perspective of the UK National Health Service. RESULTS: In the base-case analysis for both adults and children with moderate and severe AD, the maintenance treatment strategy with tacrolimus ointment was dominant over the reactive treatment strategy in that it was more effective and less costly. In univariate sensitivity analyses, for all patient groups, few parameters when varied between the value of their upper and lower confidence interval resulted in incremental cost-effectiveness ratios (ICERs) above zero. Probabilistic sensitivity analyses demonstrated that the probability of tacrolimus maintenance treatment being dominant over the reactive treatment strategy was; 76% for adults with moderate AD, 89% for adults with severe AD, 75% for children with moderate AD and 54% for children with severe AD. CONCLUSIONS: Maintenance use of tacrolimus ointment is a dominant treatment strategy compared with reactive use, providing incremental health benefits at a lower cost. PSS12 MODELING THE COST-EFFECTIVENESS OF USTEKINUMAB FOR MODERATE TO SEVERE PLAQUE PSORIASIS IN US Verma S, Dharmarajan S, Yang Y University of Mississippi, University, MS, USA OBJECTIVES: To determine cost-effectiveness of Ustekinumab in patients with moderate-to-severe plaque psoriasis in comparison with Etanercept from third-party payer perspective. METHODS: A cost-utility analysis was performed using a Markov model which compared cost per QALY of Ustekinumab (45 mg at week 0 and 4, then every 12 weeks thereafter) and Etanercept (50 mg twice weekly for the first 12 weeks and then once a week). The probabilities of treatment response were taken from the ACCEPT trial (which compared both the drugs); while utility values for different stages were obtained from published studies. A 12 week paradigm for the base case of each agent was developed on the basis of dosage administration, laboratory monitoring utilized in the Randomized Clinical Trials and manufacturer&rsquo;s published guidelines. The cost of therapies included 2009 AWP (average wholesale price) of both the drugs, and cost of physician visits and lab were inflated to 2009 from 2006 Medicare clinical laboratory fee schedule and physician reimbursement schedule (which used mean US reimbursement). Since the time frame of the analysis was only 12 weeks, the costs of long-term side effects and adverse events were not included. Extrapolations were made to evaluate the cost-effectiveness of two drugs over a period of five years, with costs and benefits discounted at 3.5% per annum. Various sensitivity analysis were carried out to test the robustness of the model. RESULTS: The QALYs gained by Ustekinumab in comparison to Etanercept over a period of 5 years were 0.23, at an incremental cost-effectiveness ratio (ICER) of $65,693.59 per QALY gained. Further sensitivity analysis confirmed the robustness of results. CONCLUSIONS: Although as per the present analysis, Ustekinumab might not appear to be more cost effective than Etanercept, but it may be recommended due to modest increase in QALYs and convenient dos",
                "DataExportTag": "AI789374",
                "QuestionID": "QID346",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "PSS8 COST EFFICACY OF USTEKINUMAB IN TREATMENT OF MODERATE TO SEVERE PLAQUE PSORIASIS IN TURKEY s...",
                "Choices": {
                    "1": {
                        "Display": "\"Healthcare Cost-Effectiveness and Drug Therapy Management\""
                    },
                    "2": {
                        "Display": "treatment, patient, cost, therapy, dose, effectiveness, trial, drug, clinical_trial, antibiotic, chemotherapy, treat, intervention, incremental, decision_tree"
                    },
                    "3": {
                        "Display": "\"Healthcare Diagnosis and Disease Prediction\""
                    },
                    "4": {
                        "Display": "diagnosis, disease, prediction, expert, heart_disease, symptom, patient, healthcare, doctor, pandemic, parkinson_disease, classification, data_mining, fuzzy_logic, world"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 5,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID220",
            "SecondaryAttribute": "Quadratic Randomized Lower Boundfor the Knapsack Problem We prove (n2) complexity lower bound for...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Quadratic Randomized Lower Boundfor the Knapsack Problem We prove (n2) complexity lower bound for the general model of randomized computation trees solving the Knapsack Problem, and more generally Restricted Integer Programming. This is also the rst nontrivial lower bound for randomized computation trees. The method of the proof depends crucially on the new technique for proving lower bounds on the border complexity of a polynomial which could be of independent interest. 3 0 Introduction We prove for the rst time nonlinear complexity lower bounds for randomized computation trees (RCT s) (see e.g. [MT82], [S83]) recognizing languages like unions of hyperplanes (i.e. linear arrangements) or intersections of halfspaces (polyhedra). As an application we prove a quadratic lower bound on RCT s solving the knapsack problem, or more general, the restricted integer programming. Obtaining general lower bounds for randomized computations was an open question for a long time (see e.g. [M85a, b, c] and [KV88]). Only recently, a nonlinear lower bound was proven in [GKMS96] for a weaker model of randomized d-decision trees (d-RDT s), in which the testing polynomials have degrees at most d (for 2-dimensional case the lower bound was proven in [GK93] and for the generic arrangements a lower bound was proved in [GK94]). In particular, for d-RDT s in [GKMS96] the lower bound (n logn) was proven for the Element Distinctness Problem (i.e. whether all the numbers x1; : : : ; xn are pairwise distinct), and the lower bound (n2) was proved for the Knapsack problem. Usually, the bound d on the degree in d-RDT is small enough, and the main di culty while considering RCT is that the degree of testing polynomials in principle could be exponential in the depth. Therefore, we develop in the present paper a new method for obtaining complexity lower bounds for RCT s. The method developed in the present paper is not applicable to the element distinctness problem. In [BKL93], [GKMS96] a linear depth RCT was constructed for a similar problem (permutation problem) beating its deterministic (n logn)2 lower bound [B83]). This example shows that the still open problem of complexity of an RCT for the element distinctness problem is quite delicate. We also mention that a linear n4 lower bound for an RCT recognizing the arrangement S1 i nfXi = 0g or the \\orthant\" T1 i nfXi 0g was proved 4 in [GKMS96]. For a stronger model of randomized analytic decision trees (RADT ) a complexity upper bound O(log2 n) for testing T1 i nfXi 0g was proven in [GKS96] (for deterministic analytic decision trees the exact complexity bound n was proved in [R72], [MPR94]). Besides, in [GKS96] for RADT a sublinear lower bound (n1=2) was proved for the union of orthants Sf iXi 0; 1 i ng where i 2 f 1; 1g and the number of negative among i is divided by a xed q such that q 6= 2s for any s. For deterministic models of the computation and decision trees several methods for obtaining complextity lower bounds were developed earlier. The \\topological\" methods based on the number of connected components ([SY82], [B83]), or more general, on the sum of Betti numbers ([BLY92], [Y94]), provide the lower bound (n2) for the knapsack problem and the lower bound (n logn) for the distinctness or the permutation problem. The already mentioned example from [BKL93] shows that these \\topological\" bounds cannot be directly extended to RCT . For testing a polyhedron (to which the topological methods are not applicable), the di erential-geometric method (involving the curvature) for obtaining complextity lower bounds for deterministic computations was developped in [GKV96], which provides (logN) lower bound for decision trees (see also [GKV95]) and (logN= log logN) for computation trees, where N is the number of all faces of the polyhedron. We now brie y describe the content of the paper. In section 1 we introduce the notion of the border complexity, for the similar notations cf. [S90] [B79] [BCLR79], of a polynomial and prove a lower bound on it which is of independent interest, in terms of the number of connected components. In section 2 we prove the main theorem which provides a complexity lower bound for RCT testing an arrangement or a polyhedron. For that purpose we use some tools (in particular, the tree of ags) from [GKMS96], but the proof di ers from the one in [GKMS96] since the degree of RCT s could be exponential as we already mentioned.5 In section 3 as an application of the main theorem we give a complexity quadratic lower bound (n2 log j) for RCT testing the Restricted Integer Programming Ln;j = [ a 2 f0;:::;j 1gnfaX = 1g (which is an arrangement consisting of jn hyperplanes). Notice that for j = 2 this problem coincides with the Knapsack Problem. In particular, in section 3 we give a lower bound j (n2) on the number of faces of Ln;j (and thereby, on the number of the connected components of the complement of Ln;j, which was also ascertained in [YI65], [M85b], [GKMS96], and [DL78]). Moreover, in section 3 we provide a stronger lower bound on the number of faces of subarrangements of Ln;j (under a subarrangement we understand the restriction of a subset of hyperplanes from Ln;j on a face of Ln;j). The analogue of this bound for subarrangements of the distinctness problem is wrong, that is why we cannot get a nonlinear complexity lower bound for RCT , solving the distinctness problem. In the last section 4 we state the complexity lower bound for the deterministic computation trees recognizing a polyhedron under less restrictive conditions than for the randomized computation trees as in the theorem from section 2. 1 Lower bound on the border complexity We start now with the technical development leading to the crucial lower bound on the border complexity of a polynomial. Let H1; : : : ; Hn k IRn be hyperplanes such that their intersection = H1 \\ \\ Hn k has the dimension dim = k. Fix arbitrary coordinates Z1; : : : ; Zk in . Then treating H1; : : : ; Hn k as the coordinate hyperplanes of the coordinates Y1; : : : ; Yn k, one gets the coordinates Z1; : : : ; Zk; Y1; : : : ; Yn k in IRn. 6 For any polynomial f 2 IR[X1; : : : ; Xn] rewrite it in the coordinates f(Z1; : : : ; Zk; Y1; : : : ; Yn k) and following [GKMS96], de ne its leading term lm(f) = Zm01 1 Zm0k k Y m1 1 Y mn k n k 0 6= 2 IR (with respect to the coordinate system Z1; : : : ; Zk; Y1; : : : ; Yn k) as follows. First, take the minimal integer mn k such that Y mn k n k occurs in the terms of f . Consider the polynomial 0 6 f (1) = f Y mn k n k ! (Z1; : : : ; Zk; Y1; : : : ; Yn k 1; 0) 2 IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 1] which could be viwed as a polynomial on the hyperplane Hn k. Observe that mn k depends only on Hn k and not on Z1; : : : ; Zk; Y1; : : : ; Yn k 1, since a linear transformation of the coordinates Z1; : : : ; Zk; Y1; : : : ; Yn k 1 changes the coe cients (being the polynomials from IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 1]) of the expansion of f in the variable Yn k, and a coe cient vanishes identically if and only if it vanishes identically after the transformation. Then f (1) is the coe cient of the expansion of f at the power Y mn k n k . Second, take the minimal integer mn k 1 such that Y mn k 1 n k 1 occurs in the terms of f (1). In other words, Y mn k 1 n k 1 is the minimal power of Yn k 1 occurring in the terms of f in which occurs the power Y mn k n k . Therefore, mn k, mn k 1 depend only on the hyperplanes Hn k, Hn k 1 and not on Z1; : : : ; Zk; Y1; : : : ; Yn k 2, since (as above) a linear transformation of the coordinates Z1; : : : ; Zk; Y1; : : : ; Yn k 2 changes the coe cients (being the polynomials from IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 2]) of the expansion of f in the variables Yn k, Yn k 1 and a coe cient vanishes identically if and only if it vanishes identically after the transformation. Denote by 0 6 f (2) 2 IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 2] the coe cient of the expansion of f at the monomial Y mn k 1 n k 1 Y mn k n k . Obviously f (2) = f (1) Y mn k 1 n k 1 ! (Z1; : : : ; Zk; Y1; : : : ; Yn k 2; 0) 7 One could view f (2) as a polynomial on the (n 2)-dimensional plane Hn k \\ Hn k 1. Continuing in the similar way, we obtain consecutively the (non-negative) integers mn k; mn k 1; : : : ; m1 and the polynomials 0 6 f (l) 2 IR[Z1; : : : ; Zk; Y1; : : : ; Yn k l] 1 l n k, by induction on l. Herewith, Y mn k l+1 n k l+1 is the minimal power of Yn k l+1 occurring in the terms of f , in which occurs the monomial Y mn k l+2 n k l+2 Y mn k n k for each 1 l n k. Notice that mn k; : : : ; mn k l depend only on the hyperplanes Hn k; : : : ; Hn k l and not on Z1; : : : ; Zk; Y1; : : : ; Yn k l 1. Then f (l) is the coe cient of the expansion of f at the monomial Y mn k l+1 n k l+1 Y mn k n k and f (l+1) = f (l) Y mn k l n k l ! (Z1; : : : ; Zk; Y1; : : : ; Yn k l 1; 0) Thus, f (l) depends only on Hn k; : : : ; Hn k l and not on Z1; : : : ; Zk; Y1; : : : ; Yn k l 1. One could view f (l) as a polynomial on the (n l)-dimensional plane Hn k \\ \\Hn k l+1. Continuing, we de ne also m0k; : : : ; m01. Finally, the leading term lm(f) = Zm01 1 Zm0k k Y m1 1 Y mn k n k is the minimal term of f in the lexicographical ordering with respect to the ordering Z1 > > Zk > Y1 > > Yn k. The leading term lm(f (l)) = Zm01 1 Zm0k k Y m1 1 Y mn k l n k l , we refer to this equality as the maintenance property (see also [GKMS96]). Denote by V ar(f) = V ar(H1;:::;Hn k) (f) the number of positive (i.e. nonzero) integers among mn k; : : : ; m1. As we have shown above, V ar(f) is independent from the coordinates Z1; : : : ; Zk of . Obviously, V ar(f) coincides with the number of 1 l n k such that Yn k l j f (l), the latter condition is equivalent to that the variety ff (l) = 0gT(Hn k\\ \\Hn k l+1) contains the plane Hn k \\ \\ Hn k l+1 \\ Hn k l (being a hyperplane in Hn k \\ \\Hn k l+1). 8 It is convenient (see also [GKMS96]) to reformulate the introduced concepts by means of in nitesimals. Namely for a real closed eld F (see e.g. [L65]) we say that an element \" transcendental over F is an in n\n\n",
                "DataExportTag": "AI1031345",
                "QuestionID": "QID220",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Quadratic Randomized Lower Boundfor the Knapsack Problem We prove (n2) complexity lower bound for...",
                "Choices": {
                    "1": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "2": {
                        "Display": "prediction, bias, uncertainty, meta, generalization, semi_supervised, active, instance, fairness, loss, calibration, deep_learning, interpretability, surrogate, hyperparameter"
                    },
                    "3": {
                        "Display": "\"Quantum Computing and Machine Learning Theory\""
                    },
                    "4": {
                        "Display": "decision_tree, bound, quantum, probabilistic, query, polynomial, perceptron, automata, temporal, boolean, binary, finite, theorem, bind, pac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID281",
            "SecondaryAttribute": "Quadratic Randomized Lower Boundfor the Knapsack Problem We prove (n2) complexity lower bound for...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Quadratic Randomized Lower Boundfor the Knapsack Problem We prove (n2) complexity lower bound for the general model of randomized computation trees solving the Knapsack Problem, and more generally Restricted Integer Programming. This is also the rst nontrivial lower bound for randomized computation trees. The method of the proof depends crucially on the new technique for proving lower bounds on the border complexity of a polynomial which could be of independent interest. 3 0 Introduction We prove for the rst time nonlinear complexity lower bounds for randomized computation trees (RCT s) (see e.g. [MT82], [S83]) recognizing languages like unions of hyperplanes (i.e. linear arrangements) or intersections of halfspaces (polyhedra). As an application we prove a quadratic lower bound on RCT s solving the knapsack problem, or more general, the restricted integer programming. Obtaining general lower bounds for randomized computations was an open question for a long time (see e.g. [M85a, b, c] and [KV88]). Only recently, a nonlinear lower bound was proven in [GKMS96] for a weaker model of randomized d-decision trees (d-RDT s), in which the testing polynomials have degrees at most d (for 2-dimensional case the lower bound was proven in [GK93] and for the generic arrangements a lower bound was proved in [GK94]). In particular, for d-RDT s in [GKMS96] the lower bound (n logn) was proven for the Element Distinctness Problem (i.e. whether all the numbers x1; : : : ; xn are pairwise distinct), and the lower bound (n2) was proved for the Knapsack problem. Usually, the bound d on the degree in d-RDT is small enough, and the main di culty while considering RCT is that the degree of testing polynomials in principle could be exponential in the depth. Therefore, we develop in the present paper a new method for obtaining complexity lower bounds for RCT s. The method developed in the present paper is not applicable to the element distinctness problem. In [BKL93], [GKMS96] a linear depth RCT was constructed for a similar problem (permutation problem) beating its deterministic (n logn)2 lower bound [B83]). This example shows that the still open problem of complexity of an RCT for the element distinctness problem is quite delicate. We also mention that a linear n4 lower bound for an RCT recognizing the arrangement S1 i nfXi = 0g or the \\orthant\" T1 i nfXi 0g was proved 4 in [GKMS96]. For a stronger model of randomized analytic decision trees (RADT ) a complexity upper bound O(log2 n) for testing T1 i nfXi 0g was proven in [GKS96] (for deterministic analytic decision trees the exact complexity bound n was proved in [R72], [MPR94]). Besides, in [GKS96] for RADT a sublinear lower bound (n1=2) was proved for the union of orthants Sf iXi 0; 1 i ng where i 2 f 1; 1g and the number of negative among i is divided by a xed q such that q 6= 2s for any s. For deterministic models of the computation and decision trees several methods for obtaining complextity lower bounds were developed earlier. The \\topological\" methods based on the number of connected components ([SY82], [B83]), or more general, on the sum of Betti numbers ([BLY92], [Y94]), provide the lower bound (n2) for the knapsack problem and the lower bound (n logn) for the distinctness or the permutation problem. The already mentioned example from [BKL93] shows that these \\topological\" bounds cannot be directly extended to RCT . For testing a polyhedron (to which the topological methods are not applicable), the di erential-geometric method (involving the curvature) for obtaining complextity lower bounds for deterministic computations was developped in [GKV96], which provides (logN) lower bound for decision trees (see also [GKV95]) and (logN= log logN) for computation trees, where N is the number of all faces of the polyhedron. We now brie y describe the content of the paper. In section 1 we introduce the notion of the border complexity, for the similar notations cf. [S90] [B79] [BCLR79], of a polynomial and prove a lower bound on it which is of independent interest, in terms of the number of connected components. In section 2 we prove the main theorem which provides a complexity lower bound for RCT testing an arrangement or a polyhedron. For that purpose we use some tools (in particular, the tree of ags) from [GKMS96], but the proof di ers from the one in [GKMS96] since the degree of RCT s could be exponential as we already mentioned.5 In section 3 as an application of the main theorem we give a complexity quadratic lower bound (n2 log j) for RCT testing the Restricted Integer Programming Ln;j = [ a 2 f0;:::;j 1gnfaX = 1g (which is an arrangement consisting of jn hyperplanes). Notice that for j = 2 this problem coincides with the Knapsack Problem. In particular, in section 3 we give a lower bound j (n2) on the number of faces of Ln;j (and thereby, on the number of the connected components of the complement of Ln;j, which was also ascertained in [YI65], [M85b], [GKMS96], and [DL78]). Moreover, in section 3 we provide a stronger lower bound on the number of faces of subarrangements of Ln;j (under a subarrangement we understand the restriction of a subset of hyperplanes from Ln;j on a face of Ln;j). The analogue of this bound for subarrangements of the distinctness problem is wrong, that is why we cannot get a nonlinear complexity lower bound for RCT , solving the distinctness problem. In the last section 4 we state the complexity lower bound for the deterministic computation trees recognizing a polyhedron under less restrictive conditions than for the randomized computation trees as in the theorem from section 2. 1 Lower bound on the border complexity We start now with the technical development leading to the crucial lower bound on the border complexity of a polynomial. Let H1; : : : ; Hn k IRn be hyperplanes such that their intersection = H1 \\ \\ Hn k has the dimension dim = k. Fix arbitrary coordinates Z1; : : : ; Zk in . Then treating H1; : : : ; Hn k as the coordinate hyperplanes of the coordinates Y1; : : : ; Yn k, one gets the coordinates Z1; : : : ; Zk; Y1; : : : ; Yn k in IRn. 6 For any polynomial f 2 IR[X1; : : : ; Xn] rewrite it in the coordinates f(Z1; : : : ; Zk; Y1; : : : ; Yn k) and following [GKMS96], de ne its leading term lm(f) = Zm01 1 Zm0k k Y m1 1 Y mn k n k 0 6= 2 IR (with respect to the coordinate system Z1; : : : ; Zk; Y1; : : : ; Yn k) as follows. First, take the minimal integer mn k such that Y mn k n k occurs in the terms of f . Consider the polynomial 0 6 f (1) = f Y mn k n k ! (Z1; : : : ; Zk; Y1; : : : ; Yn k 1; 0) 2 IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 1] which could be viwed as a polynomial on the hyperplane Hn k. Observe that mn k depends only on Hn k and not on Z1; : : : ; Zk; Y1; : : : ; Yn k 1, since a linear transformation of the coordinates Z1; : : : ; Zk; Y1; : : : ; Yn k 1 changes the coe cients (being the polynomials from IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 1]) of the expansion of f in the variable Yn k, and a coe cient vanishes identically if and only if it vanishes identically after the transformation. Then f (1) is the coe cient of the expansion of f at the power Y mn k n k . Second, take the minimal integer mn k 1 such that Y mn k 1 n k 1 occurs in the terms of f (1). In other words, Y mn k 1 n k 1 is the minimal power of Yn k 1 occurring in the terms of f in which occurs the power Y mn k n k . Therefore, mn k, mn k 1 depend only on the hyperplanes Hn k, Hn k 1 and not on Z1; : : : ; Zk; Y1; : : : ; Yn k 2, since (as above) a linear transformation of the coordinates Z1; : : : ; Zk; Y1; : : : ; Yn k 2 changes the coe cients (being the polynomials from IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 2]) of the expansion of f in the variables Yn k, Yn k 1 and a coe cient vanishes identically if and only if it vanishes identically after the transformation. Denote by 0 6 f (2) 2 IR[Z1; : : : ; Zk; Y1; : : : ; Yn k 2] the coe cient of the expansion of f at the monomial Y mn k 1 n k 1 Y mn k n k . Obviously f (2) = f (1) Y mn k 1 n k 1 ! (Z1; : : : ; Zk; Y1; : : : ; Yn k 2; 0) 7 One could view f (2) as a polynomial on the (n 2)-dimensional plane Hn k \\ Hn k 1. Continuing in the similar way, we obtain consecutively the (non-negative) integers mn k; mn k 1; : : : ; m1 and the polynomials 0 6 f (l) 2 IR[Z1; : : : ; Zk; Y1; : : : ; Yn k l] 1 l n k, by induction on l. Herewith, Y mn k l+1 n k l+1 is the minimal power of Yn k l+1 occurring in the terms of f , in which occurs the monomial Y mn k l+2 n k l+2 Y mn k n k for each 1 l n k. Notice that mn k; : : : ; mn k l depend only on the hyperplanes Hn k; : : : ; Hn k l and not on Z1; : : : ; Zk; Y1; : : : ; Yn k l 1. Then f (l) is the coe cient of the expansion of f at the monomial Y mn k l+1 n k l+1 Y mn k n k and f (l+1) = f (l) Y mn k l n k l ! (Z1; : : : ; Zk; Y1; : : : ; Yn k l 1; 0) Thus, f (l) depends only on Hn k; : : : ; Hn k l and not on Z1; : : : ; Zk; Y1; : : : ; Yn k l 1. One could view f (l) as a polynomial on the (n l)-dimensional plane Hn k \\ \\Hn k l+1. Continuing, we de ne also m0k; : : : ; m01. Finally, the leading term lm(f) = Zm01 1 Zm0k k Y m1 1 Y mn k n k is the minimal term of f in the lexicographical ordering with respect to the ordering Z1 > > Zk > Y1 > > Yn k. The leading term lm(f (l)) = Zm01 1 Zm0k k Y m1 1 Y mn k l n k l , we refer to this equality as the maintenance property (see also [GKMS96]). Denote by V ar(f) = V ar(H1;:::;Hn k) (f) the number of positive (i.e. nonzero) integers among mn k; : : : ; m1. As we have shown above, V ar(f) is independent from the coordinates Z1; : : : ; Zk of . Obviously, V ar(f) coincides with the number of 1 l n k such that Yn k l j f (l), the latter condition is equivalent to that the variety ff (l) = 0gT(Hn k\\ \\Hn k l+1) contains the plane Hn k \\ \\ Hn k l+1 \\ Hn k l (being a hyperplane in Hn k \\ \\Hn k l+1). 8 It is convenient (see also [GKMS96]) to reformulate the introduced concepts by means of in nitesimals. Namely for a real closed eld F (see e.g. [L65]) we say that an element \" transcendental over F is an in n\n\n",
                "DataExportTag": "AI1031345",
                "QuestionID": "QID281",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Quadratic Randomized Lower Boundfor the Knapsack Problem We prove (n2) complexity lower bound for...",
                "Choices": {
                    "1": {
                        "Display": "\"Machine Learning and Predictive Analysis\""
                    },
                    "2": {
                        "Display": "prediction, bias, uncertainty, meta, generalization, semi_supervised, active, instance, fairness, loss, calibration, deep_learning, interpretability, surrogate, hyperparameter"
                    },
                    "3": {
                        "Display": "\"Quantum Computing and Machine Learning Theory\""
                    },
                    "4": {
                        "Display": "decision_tree, bound, quantum, probabilistic, query, polynomial, perceptron, automata, temporal, boolean, binary, finite, theorem, bind, pac"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID200",
            "SecondaryAttribute": "Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening Contex...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening Context Repeated digital mammography examinations expose women to ionizing radiation that can increase breast cancer risk. Contribution This modeling study found that annual mammography screening of 100000 women aged 40 to 74 years might induce 125 breast cancer cases and 16 deaths but avert 968 breast cancer deaths because of early detection. Factors associated with increased risk for radiation-induced cancer included large breasts requiring extra views, higher-than-average doses per view, beginning screening at younger ages, and annual screening. Caution The model had several assumptions. Implication Biennial mammography screening starting at age 50 years and use of the fewest number of views possible would decrease risk for radiation-induced breast cancer. Exposure to ionizing radiation from repeated mammography examinations may increase breast cancer risk (1, 2). Radiation-induced breast cancer incidence and mortality associated with recommended screening strategies are suggested to be low relative to breast cancer deaths prevented (35). However, prior projected population risks were based on exposure from screening only and assumed only 4 standard views per screening examination at the mean radiation dose. Evaluations of screening programs should consider full episodes of care, including diagnostic work-up prompted by an abnormal screening result (6). False-positive recalls, breast biopsies, and short-interval follow-up examinations are relatively common in the United States and add radiation exposure from diagnostic mammography (7). Some subgroups of women, such as obese women and those with dense breasts, are more likely to have additional evaluations (79), which may increase their risk for radiation-induced cancer. When risk for radiation-induced breast cancer is being evaluated, it may also be important to consider variation in radiation dose from a single examination. Examinations vary in the number of views performed and dose per view; therefore, some women receive more than the mean dose. The American College of Radiology Imaging Network DMIST (Digital Mammographic Imaging Screening Trial) found an average radiation dose of 1.86 mGy to the breast from a single digital mammography screening view (10), but dose per view varied from 0.15 to 13.4 mGy (Supplement), and 21% of digital screening examinations used more than 4 views (10). Radiation dose is strongly correlated with compressed breast thickness; thus, women with large breasts tend to receive greater doses per view and may require more than 4 views for complete examination (10, 11). Women with breast augmentation receive implant-displacement views in addition to standard screening views, which doubles their radiation dose (12). Women may have repeated views because of movement artifacts or improper breast positioning. Supplement. Supplemental material. We estimated the distribution of cumulative radiation dose and associated breast cancer risk from full screening episodes to identify subgroups of women who may have a greater risk for radiation-induced cancer because they have factors contributing to greater doses per examination or frequent false-positive screening results that lead to additional radiation exposure from subsequent diagnostic work-up. Using population-based data from the Breast Cancer Surveillance Consortium (BCSC) (13), we estimated the probability of a false-positive screening result followed by additional imaging evaluation, short-interval follow-up, or biopsy. We used data from the BCSC, DMIST, and other sources in 2 simulation models to estimate radiation exposure and radiation-induced breast cancer incidence and mortality associated with 8 potential screening strategies with different starting ages (40, 45, or 50 years) and screening intervals (annual, biennial, or a hybrid strategy). Methods Screening Strategies We used 2 complementary stochastic modeling approaches to evaluate the following 8 strategies for screening with digital mammography: annual screening from age 40 to 74, 45 to 74, or 50 to 74 years; biennial screening from age 40 to 74, 45 to 74, or 50 to 74 years; or a hybrid strategy of annual screening from age 40 to 49 or 45 to 49 years followed by biennial screening from age 50 to 74 years. We included the hybrid strategies because more frequent screening has been advocated for younger and premenopausal women due to their greater prevalence of dense breasts and more aggressive tumors, resulting in a greater risk for interval cancer, than older women (1417). Outcomes were breast cancer deaths averted (benefits) and radiation-induced breast cancer incidence and mortality (harms) associated with a lifetime of mammography screening relative to no screening. Simulation-Modeling Approaches Figure 1 summarizes our approach. We used 2 complementary stochastic modeling approaches to simulate mammography events associated with radiation exposure and outcomes for a population adherent with each of the 8 screening strategies. The first approach used the Microsimulation of Screening AnalysisFatal Diameter (MISCAN-Fadia) model (18), which is a detailed natural history model of breast cancer. This approach provided estimates of breast cancer incidence and mortality with and without screening to contextualize estimates of radiation-induced breast cancer cases. Although MISCAN-Fadia models the average effects of screening on a population level, it does not model correlation among repeated mammography results in individual women or the specific types of work-up after an abnormal screening result; thus, it cannot be used to estimate the distribution of cumulative radiation exposure from both screening mammography and subsequent diagnostic work-up among women. Therefore, we developed a new simulation model that provides woman-level exposure histories that were not available from the MISCAN-Fadia model. This new model captures exposure heterogeneity by simulating mammography results and subsequent work-up in each woman and allowing for variability in radiation exposure and breast size. Figure 1. Schematic of 2 modeling approaches used to simulate mammography events and outcomes associated with 8 screening strategies. Estimates of the number of screening examinations and false-positive results from the MISCAN-Fadia model were combined with the mean radiation dose from the radiation exposure model to estimate mean incidence of radiation-induced breast cancer. Estimates of the probability distribution of cumulative radiation dose at each age among women from the radiation exposure model were used to estimate the probability distribution of radiation-induced breast cancer incidence. Radiation-induced breast cancer incidence was combined with breast cancer survival estimates from the MISCAN-Fadia model to estimate radiation-induced breast cancer mortality. BCSC = Breast Cancer Surveillance Consortium; DMIST = Digital Mammographic Imaging Screening Trial; MISCAN-Fadia = Microsimulation of Screening AnalysisFatal Diameter. MISCAN-Fadia Model The MISCAN-Fadia model simulates individual life histories of women with and without breast cancer in the presence and absence of screening from birth to death from breast cancer or other causes. The model has been described in detail elsewhere (18), information about the model can be found online (http:\/\/cisnet.cancer.gov), and inputs and assumptions are described in our report for the draft U.S. Preventive Services Task Force recommendations (19). In brief, on the basis of BCSC data on sensitivity of digital mammography screening, cancer detection rates, and cancer stage at detection, we estimated thresholds at which tumors become screen-detectable. Screening sensitivity and specificity depended on age, breast density, and screening interval. Breast cancer risk depended on age and breast density. The effect of screening on breast cancer natural history was assessed by modeling continuous tumor growth, in which tumors detected before they reached their fatal diameter were cured and those detected past their fatal diameter led to breast cancer death. We assumed that all women received the mean dose per screening examination and, if recalled, the mean dose associated with diagnostic work-up after a false-positive screening result, both of which were estimated from the radiation exposure model. We also projected breast cancer incidence and mortality with and without screening. Radiation Exposure Simulation Model Full details, including approach, data sources, and assumptions, are available in the Supplement. In brief, for each of the 8 screening strategies, we simulated woman-level factors and screening-related events for 100000 women. Woman-Level Factors. Each woman was assigned a compressed breast thickness from the DMIST distribution (Appendix Table 1). Women with a compressed breast thickness of 7.5 cm or greater (8% of DMIST population) were assumed to have large breasts that required extra views for complete examination. On the basis of distributions seen in the BCSC, each woman was assigned a baseline Breast Imaging Reporting and Data System (12) density at the start of screening, which could potentially decrease by 1 category at ages 50 and 65 years (20) (Appendix Table 2). Appendix Table 1. Distribution of Compressed Breast Thickness on Digital Mammography From ACRIN DMIST* Appendix Table 2. Prevalence of BI-RADS Breast Density (by Age) and Probability of Changing Density Category at Age 50 and 65 Years, Estimated From the Breast Cancer Surveillance Consortium* Evaluation of a Positive Screening Result. For each screening strategy, we simulated events after a positive screening result that did not lead to a diagnosis of breast cancer (Figure 2) to focus on risk for first breast cancer induced by radiation. We modeled the probability of each event by using data from digital mammography done at BCSC facilities from 2003 to 2011 on women aged 40 to 74 years without a history of breast cancer or\n\n",
                "DataExportTag": "CAN614423",
                "QuestionID": "QID200",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening Contex...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\""
                    },
                    "2": {
                        "Display": "mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "3": {
                        "Display": "\"Racial and Socioeconomic Disparities in Cancer Screening\""
                    },
                    "4": {
                        "Display": "screening, disparity, colorectal_cancer, black, white, woman, mammography, non_hispanic, ses, cervical, hispanic, african_american, racial_ethnic, socioeconomic, income"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID261",
            "SecondaryAttribute": "Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening Contex...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening Context Repeated digital mammography examinations expose women to ionizing radiation that can increase breast cancer risk. Contribution This modeling study found that annual mammography screening of 100000 women aged 40 to 74 years might induce 125 breast cancer cases and 16 deaths but avert 968 breast cancer deaths because of early detection. Factors associated with increased risk for radiation-induced cancer included large breasts requiring extra views, higher-than-average doses per view, beginning screening at younger ages, and annual screening. Caution The model had several assumptions. Implication Biennial mammography screening starting at age 50 years and use of the fewest number of views possible would decrease risk for radiation-induced breast cancer. Exposure to ionizing radiation from repeated mammography examinations may increase breast cancer risk (1, 2). Radiation-induced breast cancer incidence and mortality associated with recommended screening strategies are suggested to be low relative to breast cancer deaths prevented (35). However, prior projected population risks were based on exposure from screening only and assumed only 4 standard views per screening examination at the mean radiation dose. Evaluations of screening programs should consider full episodes of care, including diagnostic work-up prompted by an abnormal screening result (6). False-positive recalls, breast biopsies, and short-interval follow-up examinations are relatively common in the United States and add radiation exposure from diagnostic mammography (7). Some subgroups of women, such as obese women and those with dense breasts, are more likely to have additional evaluations (79), which may increase their risk for radiation-induced cancer. When risk for radiation-induced breast cancer is being evaluated, it may also be important to consider variation in radiation dose from a single examination. Examinations vary in the number of views performed and dose per view; therefore, some women receive more than the mean dose. The American College of Radiology Imaging Network DMIST (Digital Mammographic Imaging Screening Trial) found an average radiation dose of 1.86 mGy to the breast from a single digital mammography screening view (10), but dose per view varied from 0.15 to 13.4 mGy (Supplement), and 21% of digital screening examinations used more than 4 views (10). Radiation dose is strongly correlated with compressed breast thickness; thus, women with large breasts tend to receive greater doses per view and may require more than 4 views for complete examination (10, 11). Women with breast augmentation receive implant-displacement views in addition to standard screening views, which doubles their radiation dose (12). Women may have repeated views because of movement artifacts or improper breast positioning. Supplement. Supplemental material. We estimated the distribution of cumulative radiation dose and associated breast cancer risk from full screening episodes to identify subgroups of women who may have a greater risk for radiation-induced cancer because they have factors contributing to greater doses per examination or frequent false-positive screening results that lead to additional radiation exposure from subsequent diagnostic work-up. Using population-based data from the Breast Cancer Surveillance Consortium (BCSC) (13), we estimated the probability of a false-positive screening result followed by additional imaging evaluation, short-interval follow-up, or biopsy. We used data from the BCSC, DMIST, and other sources in 2 simulation models to estimate radiation exposure and radiation-induced breast cancer incidence and mortality associated with 8 potential screening strategies with different starting ages (40, 45, or 50 years) and screening intervals (annual, biennial, or a hybrid strategy). Methods Screening Strategies We used 2 complementary stochastic modeling approaches to evaluate the following 8 strategies for screening with digital mammography: annual screening from age 40 to 74, 45 to 74, or 50 to 74 years; biennial screening from age 40 to 74, 45 to 74, or 50 to 74 years; or a hybrid strategy of annual screening from age 40 to 49 or 45 to 49 years followed by biennial screening from age 50 to 74 years. We included the hybrid strategies because more frequent screening has been advocated for younger and premenopausal women due to their greater prevalence of dense breasts and more aggressive tumors, resulting in a greater risk for interval cancer, than older women (1417). Outcomes were breast cancer deaths averted (benefits) and radiation-induced breast cancer incidence and mortality (harms) associated with a lifetime of mammography screening relative to no screening. Simulation-Modeling Approaches Figure 1 summarizes our approach. We used 2 complementary stochastic modeling approaches to simulate mammography events associated with radiation exposure and outcomes for a population adherent with each of the 8 screening strategies. The first approach used the Microsimulation of Screening AnalysisFatal Diameter (MISCAN-Fadia) model (18), which is a detailed natural history model of breast cancer. This approach provided estimates of breast cancer incidence and mortality with and without screening to contextualize estimates of radiation-induced breast cancer cases. Although MISCAN-Fadia models the average effects of screening on a population level, it does not model correlation among repeated mammography results in individual women or the specific types of work-up after an abnormal screening result; thus, it cannot be used to estimate the distribution of cumulative radiation exposure from both screening mammography and subsequent diagnostic work-up among women. Therefore, we developed a new simulation model that provides woman-level exposure histories that were not available from the MISCAN-Fadia model. This new model captures exposure heterogeneity by simulating mammography results and subsequent work-up in each woman and allowing for variability in radiation exposure and breast size. Figure 1. Schematic of 2 modeling approaches used to simulate mammography events and outcomes associated with 8 screening strategies. Estimates of the number of screening examinations and false-positive results from the MISCAN-Fadia model were combined with the mean radiation dose from the radiation exposure model to estimate mean incidence of radiation-induced breast cancer. Estimates of the probability distribution of cumulative radiation dose at each age among women from the radiation exposure model were used to estimate the probability distribution of radiation-induced breast cancer incidence. Radiation-induced breast cancer incidence was combined with breast cancer survival estimates from the MISCAN-Fadia model to estimate radiation-induced breast cancer mortality. BCSC = Breast Cancer Surveillance Consortium; DMIST = Digital Mammographic Imaging Screening Trial; MISCAN-Fadia = Microsimulation of Screening AnalysisFatal Diameter. MISCAN-Fadia Model The MISCAN-Fadia model simulates individual life histories of women with and without breast cancer in the presence and absence of screening from birth to death from breast cancer or other causes. The model has been described in detail elsewhere (18), information about the model can be found online (http:\/\/cisnet.cancer.gov), and inputs and assumptions are described in our report for the draft U.S. Preventive Services Task Force recommendations (19). In brief, on the basis of BCSC data on sensitivity of digital mammography screening, cancer detection rates, and cancer stage at detection, we estimated thresholds at which tumors become screen-detectable. Screening sensitivity and specificity depended on age, breast density, and screening interval. Breast cancer risk depended on age and breast density. The effect of screening on breast cancer natural history was assessed by modeling continuous tumor growth, in which tumors detected before they reached their fatal diameter were cured and those detected past their fatal diameter led to breast cancer death. We assumed that all women received the mean dose per screening examination and, if recalled, the mean dose associated with diagnostic work-up after a false-positive screening result, both of which were estimated from the radiation exposure model. We also projected breast cancer incidence and mortality with and without screening. Radiation Exposure Simulation Model Full details, including approach, data sources, and assumptions, are available in the Supplement. In brief, for each of the 8 screening strategies, we simulated woman-level factors and screening-related events for 100000 women. Woman-Level Factors. Each woman was assigned a compressed breast thickness from the DMIST distribution (Appendix Table 1). Women with a compressed breast thickness of 7.5 cm or greater (8% of DMIST population) were assumed to have large breasts that required extra views for complete examination. On the basis of distributions seen in the BCSC, each woman was assigned a baseline Breast Imaging Reporting and Data System (12) density at the start of screening, which could potentially decrease by 1 category at ages 50 and 65 years (20) (Appendix Table 2). Appendix Table 1. Distribution of Compressed Breast Thickness on Digital Mammography From ACRIN DMIST* Appendix Table 2. Prevalence of BI-RADS Breast Density (by Age) and Probability of Changing Density Category at Age 50 and 65 Years, Estimated From the Breast Cancer Surveillance Consortium* Evaluation of a Positive Screening Result. For each screening strategy, we simulated events after a positive screening result that did not lead to a diagnosis of breast cancer (Figure 2) to focus on risk for first breast cancer induced by radiation. We modeled the probability of each event by using data from digital mammography done at BCSC facilities from 2003 to 2011 on women aged 40 to 74 years without a history of breast cancer or\n\n",
                "DataExportTag": "CAN614423",
                "QuestionID": "QID261",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Radiation-Induced Breast Cancer Incidence and Mortality From Digital Mammography Screening Contex...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\""
                    },
                    "2": {
                        "Display": "mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "3": {
                        "Display": "\"Racial and Socioeconomic Disparities in Cancer Screening\""
                    },
                    "4": {
                        "Display": "screening, disparity, colorectal_cancer, black, white, woman, mammography, non_hispanic, ses, cervical, hispanic, african_american, racial_ethnic, socioeconomic, income"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID294",
            "SecondaryAttribute": "Radically innovative technology for efficient manufacturing of preforms for ceramic matrix compos...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Radically innovative technology for efficient manufacturing of preforms for ceramic matrix composite (CMC) brake disks Light-weight materials lead to a drastic weight reduction in vehicle components such as the braking system, improving fuel efficiency and vehicle performance. The low density of Carbon Matrix Composites materials makes them ideal for light weight braking systems. CMC brakes are 50% lighter than traditional cast iron discs and their adoption could save up to 25kg\/car. They also reduce the braking distance by > 40%. However, they are very expensive. Their production cost (\u20ac500) and sales price (\u20ac700) is attributed to the multi-step batch production process that includes >7 manual steps done by specialized operators. That is why CMC brake discs accounting for only 1% of the global market. Product quality is also not guaranteed in the current manufacturing process because of inefficient control of the Carbon Fibre (CF) topology which is only achieved by hand or machine positioning.  Through this innovation project, we will make CMC brake discs affordable for more customer segments while improving their quality. ROLL-IT is a fully automated process that combines advanced additive manufacturing techniques with traditional sheet molding to enable a continuous forming process of near-to-shape preforms for CMC brake disks, exploiting an affordable processing route on the same machine. ROLL-IT will enable control of the CF topology and that of the material properties that mainly affect the disc performance.  CMC manufacturer will benefit from a 600% increase in throughput, 40% decrease in carbo fiber content, 50% reduction in productions costs and a 70% reduction in scrap whilst car owners will benefit from improved mechanical properties of the disc and enhanced safety through a 40% reduction in braking speed. This innovation project will play a significant role in advancing our commercialization strategy. We anticipate a high uptake of ROLL-IT which will boost our sales and elevate PETROCERAMICS from a start-up status to a fully-fledged company.\n\n",
                "DataExportTag": "COR31268",
                "QuestionID": "QID294",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Radically innovative technology for efficient manufacturing of preforms for ceramic matrix compos...",
                "Choices": {
                    "1": {
                        "Display": "\"Electric Vehicle Manufacturing and Assembly\""
                    },
                    "2": {
                        "Display": "electric, battery, electric_vehicle, assembly, energy, automotive, vehicle, sensor, motor, packaging, pilot_line, modular, component, weight, welding"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and Cloud Systems\""
                    },
                    "4": {
                        "Display": "computation, heterogeneous, hpc, code, hardware, cps, processor, programming, cryptography, storage, layer, machine_learning, cloud_computing, compute, exascale"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID174",
            "SecondaryAttribute": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingeri...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingering question in geoscience is whether earthquakes show a precursory nucleation process. Precursory signals from well-recorded large earthquakes displayed widely different amplitude or duration, and some large earthquakes displayed no detectable precursors. The main objectives of QUAKE-HUNTER are (1) to determine the most effective approaches to detect fault-related transients preceding moderate to large earthquakes, and (2) to monitor seismic and aseismic processes and infer from them the fault conditions under which earthquake nucleation processes emerge, and the optimal instrumentation required to capture them. To achieve these, we will develop different methodologies based on a combination of supervised and unsupervised artificial intelligence to identify retrospectively earthquake nucleation processes for active faults near earthquake-threatened urban areas. The ultimate goal will be to test the performance of this novel earthquake forecasting methodology in near-real time. We will analyze data from north-western Turkey, where the North Anatolian Fault is overdue for a magnitude M>7 earthquake directly adjacent to the Istanbul megalopolis with its >15M inhabitants. The groundbreaking part of QUAKE-HUNTER is that if earthquake nucleation processes could be discerned prior to large earthquakes in the Marmara region, then automated near-real-time detection could provide extended warning and preparation time. If successful, this could become in the future an essential ingredient for activating civil protection protocols to mitigate seismic risk. QUAKE-HUNTER aims at having a strong scientific impact on earthquake physics: we will be able to refine our knowledge on the physics of earthquakes shortly before their start, as well as the fault conditions favoring the identification of earthquake precursors. The first-time testing of such methodology in real-time will have a strong societal impact, potentially advancing earthquakes forecasting.\n\n",
                "DataExportTag": "COR26059",
                "QuestionID": "QID174",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingeri...",
                "Choices": {
                    "1": {
                        "Display": "\"Seismic Activity and Earthquake Mechanics\""
                    },
                    "2": {
                        "Display": "fluid, earth, flow, earthquake, seismic, deformation, turbulence, multiscale, mantle, mechanical, microstructure, wave, rock, fracture, mechanic"
                    },
                    "3": {
                        "Display": "\"Nanotechnology and Solar Cell Fabrication\""
                    },
                    "4": {
                        "Display": "electric, graphene, solar_cell, semiconductor, magnetic, spin, organic, layer, fabrication, thin_film, silicon, spintronic, nanostructure, oxide, perovskite"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID235",
            "SecondaryAttribute": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingeri...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingering question in geoscience is whether earthquakes show a precursory nucleation process. Precursory signals from well-recorded large earthquakes displayed widely different amplitude or duration, and some large earthquakes displayed no detectable precursors. The main objectives of QUAKE-HUNTER are (1) to determine the most effective approaches to detect fault-related transients preceding moderate to large earthquakes, and (2) to monitor seismic and aseismic processes and infer from them the fault conditions under which earthquake nucleation processes emerge, and the optimal instrumentation required to capture them. To achieve these, we will develop different methodologies based on a combination of supervised and unsupervised artificial intelligence to identify retrospectively earthquake nucleation processes for active faults near earthquake-threatened urban areas. The ultimate goal will be to test the performance of this novel earthquake forecasting methodology in near-real time. We will analyze data from north-western Turkey, where the North Anatolian Fault is overdue for a magnitude M>7 earthquake directly adjacent to the Istanbul megalopolis with its >15M inhabitants. The groundbreaking part of QUAKE-HUNTER is that if earthquake nucleation processes could be discerned prior to large earthquakes in the Marmara region, then automated near-real-time detection could provide extended warning and preparation time. If successful, this could become in the future an essential ingredient for activating civil protection protocols to mitigate seismic risk. QUAKE-HUNTER aims at having a strong scientific impact on earthquake physics: we will be able to refine our knowledge on the physics of earthquakes shortly before their start, as well as the fault conditions favoring the identification of earthquake precursors. The first-time testing of such methodology in real-time will have a strong societal impact, potentially advancing earthquakes forecasting.\n\n",
                "DataExportTag": "COR26059",
                "QuestionID": "QID235",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingeri...",
                "Choices": {
                    "1": {
                        "Display": "\"Seismic Activity and Earthquake Mechanics\""
                    },
                    "2": {
                        "Display": "fluid, earth, flow, earthquake, seismic, deformation, turbulence, multiscale, mantle, mechanical, microstructure, wave, rock, fracture, mechanic"
                    },
                    "3": {
                        "Display": "\"Nanotechnology and Solar Cell Fabrication\""
                    },
                    "4": {
                        "Display": "electric, graphene, solar_cell, semiconductor, magnetic, spin, organic, layer, fabrication, thin_film, silicon, spintronic, nanostructure, oxide, perovskite"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID300",
            "SecondaryAttribute": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingeri...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingering question in geoscience is whether earthquakes show a precursory nucleation process. Precursory signals from well-recorded large earthquakes displayed widely different amplitude or duration, and some large earthquakes displayed no detectable precursors. The main objectives of QUAKE-HUNTER are (1) to determine the most effective approaches to detect fault-related transients preceding moderate to large earthquakes, and (2) to monitor seismic and aseismic processes and infer from them the fault conditions under which earthquake nucleation processes emerge, and the optimal instrumentation required to capture them. To achieve these, we will develop different methodologies based on a combination of supervised and unsupervised artificial intelligence to identify retrospectively earthquake nucleation processes for active faults near earthquake-threatened urban areas. The ultimate goal will be to test the performance of this novel earthquake forecasting methodology in near-real time. We will analyze data from north-western Turkey, where the North Anatolian Fault is overdue for a magnitude M>7 earthquake directly adjacent to the Istanbul megalopolis with its >15M inhabitants. The groundbreaking part of QUAKE-HUNTER is that if earthquake nucleation processes could be discerned prior to large earthquakes in the Marmara region, then automated near-real-time detection could provide extended warning and preparation time. If successful, this could become in the future an essential ingredient for activating civil protection protocols to mitigate seismic risk. QUAKE-HUNTER aims at having a strong scientific impact on earthquake physics: we will be able to refine our knowledge on the physics of earthquakes shortly before their start, as well as the fault conditions favoring the identification of earthquake precursors. The first-time testing of such methodology in real-time will have a strong societal impact, potentially advancing earthquakes forecasting.\n\n",
                "DataExportTag": "COR26059",
                "QuestionID": "QID300",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Real-time monitoring of earthquake nucleation for faults near urban areas A longstanding, lingeri...",
                "Choices": {
                    "1": {
                        "Display": "\"Seismic Activity and Earthquake Mechanics\""
                    },
                    "2": {
                        "Display": "fluid, earth, flow, earthquake, seismic, deformation, turbulence, multiscale, mantle, mechanical, microstructure, wave, rock, fracture, mechanic"
                    },
                    "3": {
                        "Display": "\"Nanotechnology and Solar Cell Fabrication\""
                    },
                    "4": {
                        "Display": "electric, graphene, solar_cell, semiconductor, magnetic, spin, organic, layer, fabrication, thin_film, silicon, spintronic, nanostructure, oxide, perovskite"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID260",
            "SecondaryAttribute": "Reproductive toxicants have a threshold of adversity This paper surveys the scientific basis for...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Reproductive toxicants have a threshold of adversity This paper surveys the scientific basis for the current threshold approach for reproductive hazard and risk assessment. In some regulatory areas it was recently suggested to consider reproductive toxicants under the stringent linear extrapolation risk assessment paradigm that was developed for genotoxic carcinogens. First, the current risk assessment paradigm for genotoxic carcinogens is addressed, followed by an overview of reproductive toxicology and its threshold dose approach for hazard and risk assessment, the testing procedures for assessing the reproductive toxicity of chemicals, and the derivation of conclusions on their risk assessment and Classification, Labelling and Packaging (CLP). Relevant details of testing methodologies are discussed, such as exposure time windows, parameters determined, and the coverage of the entire reproductive cycle. In addition, the dose-response relationship is considered, illustrated with several examples. It is concluded that the current risk assessment methodology for genotoxic carcinogens is a debatable worst-case scenario and that for risk assessment of reproductive toxicants the threshold dose approach remains valid.\n\n",
                "DataExportTag": "CAN810728",
                "QuestionID": "QID260",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Reproductive toxicants have a threshold of adversity This paper surveys the scientific basis for...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Discovery and Prevention Technologies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Childhood Illness and Family Psychosocial Experience\""
                    },
                    "4": {
                        "Display": "child, parent, survivor, family, pediatric, woman, experience, adolescent, qualitative, life, illness, childhood, interview, psychosocial, mother"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID289",
            "SecondaryAttribute": "Reputation Matters in the Regulatory State: Re-thinking the Fundamentals of Regulatory Independen...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Reputation Matters in the Regulatory State: Re-thinking the Fundamentals of Regulatory Independence, Credibility and Accountability RICA will fundamentally challenge, reconceptualise and redefine core foundational assumptions of the regulatory state. Arm\u2019s length governance and regulatory independence have become the cornerstone normative and institutional design principles of the European regulatory state. Regulatory credibility is premised on agency insulation from politics with major systemic implications in the form of a rise in non-majoritarianism and a parallel devaluation of the political process. Efforts to compensate through alternative sources of input legitimacy have proven challenging. Conceptualised in a rational-choice tradition, accountability is seen to be in a zero-sum relationship with regulatory autonomy. Bolstering accountability is said to threaten autonomy, undermining the system\u2019s normative foundations. Informed by insights from the political science literature on reputation, RICA profoundly questions these assumptions. It does so in two separate and interlocking tracks. Module 1 theorises about, and tests, the established wisdom of a wholesale positive relation between agency insulation and credibility. To the contrary, from a reputational perspective, a strategy of insulation is expected to have a negative effect on regulatory credibility, with overall delegitimising effects. Similarly informed by reputation insights, Module 2 challenges dominant understandings of accountability and puts forward the building blocks of a new theory of public accountability. The module draws out and studies empirically how reputation shapes the accountability behaviour of regulatory actors, both account-holders and account-givers, and their interactions. It will re-conceptualise the role played by accountability in the regulatory state as well as its relationship with autonomy. In contrast to dominant understandings, as a mechanism of reputation-building, accountability can reinforce autonomy with sweeping implications for regulatory control strategies and institutional design choices.\n\n",
                "DataExportTag": "COR31028",
                "QuestionID": "QID289",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Reputation Matters in the Regulatory State: Re-thinking the Fundamentals of Regulatory Independen...",
                "Choices": {
                    "1": {
                        "Display": "\"Research Funding and University Administration\""
                    },
                    "2": {
                        "Display": "cofund, national_contact, ncps, preparatory_phase, university, coordinator, administrative, smart_specialization, health, critical_mass, centres, website, strategic_partnership, preparation, increase_visibility"
                    },
                    "3": {
                        "Display": "\"Philosophy, Ethics and Psychology in Politics and Judiciary\""
                    },
                    "4": {
                        "Display": "philosophy, ethic, belief, logic, election, criminal, emotion, psychological, electoral, epistemic, opinion, responsibility, truth, news, judicial"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID312",
            "SecondaryAttribute": "RESULTS A total of 506 patients with pancreatic cancer were included. At the main hospital in the...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nA total of 506 patients with pancreatic cancer were included. At the main hospital in the pre-QA period, 11 (12.6%) of 87 patients were discussed at our institutional multidisciplinary tumor board meeting; this number rose to 89 (49.7%) of 179 patients (P < .001) after the implementation. Clinical TNM stage was documented in the electronic medical record in 75 patients (86.2%) in the pre-QA group; this number rose to 170 (94.9%; P = .026) in the post-QA era. External imaging of patients undergoing resection was uploaded in 10 (66.7%) of 15 patient cases in the pre-QA period; this number rose to 33 (93.4%) of 35 (P = .020). Rates of time from diagnosis to treatment initiation shorter than 60 days were similar between eras (n = 26, 96.3% v n = 57, 95%; P = XXXX). Implementation of QA measures at the affiliate institutions did not result in measurable differences (P = XXXX). There were no differences in margin-negative resection rate, lymph node yield, or perioperative mortality after QA implementation at any site (P = XXXX).\n\n",
                "DataExportTag": "30",
                "QuestionID": "QID312",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS A total of 506 patients with pancreatic cancer were included. At the main hospital in the...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID192",
            "SecondaryAttribute": "RESULTS All the 13 patients underwent the procedure successfully with median operation duration o...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nAll the 13 patients underwent the procedure successfully with median operation duration of 15 minutes. A total of 27 sessions were necessary to maintain lumen patency until September 2015, and 7 patients needed retreatment. The symptoms relieved in all the cases, and the median dysphagia score decreased from 4 to 1 during a median follow-up of 25 months. The median diameter of stricture was enlarged from 4 mm to 12 mm. As a short-term effect, dysphagia symptoms improved in 100% (13\/13), 84.6% (11\/13) and 76.9% (10\/13) of the patients one, three and six months after a single treatment. As long-term effect, the dysphagia improved in 61.5% (8\/13), 63.6% (7\/11) and 60% (6\/10) of the patients 12, 18 and 24 months after a single treatment.\n\n",
                "DataExportTag": "32",
                "QuestionID": "QID192",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS All the 13 patients underwent the procedure successfully with median operation duration o...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID253",
            "SecondaryAttribute": "RESULTS All the 13 patients underwent the procedure successfully with median operation duration o...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nAll the 13 patients underwent the procedure successfully with median operation duration of 15 minutes. A total of 27 sessions were necessary to maintain lumen patency until September 2015, and 7 patients needed retreatment. The symptoms relieved in all the cases, and the median dysphagia score decreased from 4 to 1 during a median follow-up of 25 months. The median diameter of stricture was enlarged from 4 mm to 12 mm. As a short-term effect, dysphagia symptoms improved in 100% (13\/13), 84.6% (11\/13) and 76.9% (10\/13) of the patients one, three and six months after a single treatment. As long-term effect, the dysphagia improved in 61.5% (8\/13), 63.6% (7\/11) and 60% (6\/10) of the patients 12, 18 and 24 months after a single treatment.\n\n",
                "DataExportTag": "32",
                "QuestionID": "QID253",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS All the 13 patients underwent the procedure successfully with median operation duration o...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID139",
            "SecondaryAttribute": "RESULTS Among 125 patients, 53 cases were node negative. Ipsilateral nodal metastasis was seen in...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nAmong 125 patients, 53 cases were node negative. Ipsilateral nodal metastasis was seen in 44\/125 (35.2%) patients, 26\/125 (20.8%) had bilateral neck node metastasis, and 2\/125 (1.6%) had isolated contralateral nodal metastasis. Among these 28 patients with contralateral nodal metastasis, 26 patients had ipsilateral nodal metastasis. Ipsilateral nodal metastasis and skin involvement were independently predictive of contralateral nodal metastasis.\n\n",
                "DataExportTag": "67",
                "QuestionID": "QID139",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS Among 125 patients, 53 cases were node negative. Ipsilateral nodal metastasis was seen in...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID316",
            "SecondaryAttribute": "RESULTS An experiment was performed at the Loma Linda University Medical Center proton therapy re...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nAn experiment was performed at the Loma Linda University Medical Center proton therapy research beam line and corresponding models were created using the mcnpx-PoliMi code. The authors' analysis showed agreement between the simulations and the measurements. The simulated detector response can be used to validate the simulations of neutron and gamma doses on a particular beam line with or without a phantom.\n\n",
                "DataExportTag": "34",
                "QuestionID": "QID316",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS An experiment was performed at the Loma Linda University Medical Center proton therapy re...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID388",
            "SecondaryAttribute": "RESULTS Between 1975 and December 2006 we performed 1483 kidney transplants and between 1991 and...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nBetween 1975 and December 2006 we performed 1483 kidney transplants and between 1991 and December 2006, 409 liver transplants. We performed multiorganic liver and kidney transplants to 15 patients (4 women and 11 men). The average for liver transplant recipients was 52.5+\/-9.3 years (range 37-61) and for kidney transplant recipients was 51+\/-12.5 years (35-66). Cold ischemia was 6.4+\/-5.4 hours (6-8) in simultaneous liver-kidney transplant and 20.5+\/-5.4 (8-27 hours) in non-simultaneous ones. Three patients had a renal transplant before the liver one (two functioning which had no changes after hepatic transplant but the other was lost due to IgA glomeruloneprhitis relapse and received a simultaneous kidney-liver transplant). Six patients received a simultaneous kidney-liver transplant and eight patients a renal transplant between 16 and 83 months (x=50.5+\/-25.9 months) after the liver transplant. A renal graft was lost due to renal vein thrombosis and two due to IgA relapse; the others were functioning between 6 and 264 months of follow-up (x=92.5+\/-66.7) with creatinine levels of 1.86+\/-mg\/100, (range 1-4.5). Four patients died due to hepatic failure between 8 months and 21 years after renal transplant and another died of oesophagus cancer 14 years after the kidney transplant, in all cases with functioning renal graft. There were no cases of kidney graft acute rejection in simultaneous transplants but there were five in non-simultaneous ones. Immunotherapy was based on steroids and tacrolimus.\n\n",
                "DataExportTag": "42",
                "QuestionID": "QID388",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS Between 1975 and December 2006 we performed 1483 kidney transplants and between 1991 and...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID379",
            "SecondaryAttribute": "RESULTS The incidence of positive lymph nodes increased with higher p stage and pathological subg...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nThe incidence of positive lymph nodes increased with higher p stage and pathological subgroups. Of 669 patients 75 (11%) with organ confined primary tumors and 169 of 385 (44%) with extravesical tumor extension had involved lymph nodes. The median number of lymph nodes removed in the 244 lymph node positive cases was 30 (range 1 to 96), while the median number of positive lymph nodes was 2 (range 1 to 63). Overall recurrence-free survival at 5 and 10 years for the 244 patients with lymph node positive disease was 35% and 34%, respectively. Patients with lymph node positive disease and an organ confined primary bladder tumor had significantly improved 10-year recurrence-free survival compared with those with extravesical tumor extension (44% vs 30%, p = 0.003). The total number of lymph nodes removed at surgery was also prognostic. Patients with 15 or less lymph nodes removed had 25% 10-year recurrence-free survival compared with 36% when greater than 15 lymph nodes were removed. Recurrence-free survival at 10 years for patients with 8 or less positive lymph nodes was significantly higher than in those with greater than 8 positive lymph nodes (40% vs 10%, p <0.001). The novel concept of lymph node density was also a significant prognostic factor. Patients with a lymph node density of 20% or less had 43% 10-year recurrence-free survival compared with only 17% survival at 10 years when lymph node density was greater than 20% (p <0.001). On multivariate analysis the total number of lymph nodes involved, pathological subgroups of the primary bladder tumor, lymph node density and adjuvant chemotherapy remained significant and independent risk factors for recurrence-free and overall survival.\n\n",
                "DataExportTag": "33",
                "QuestionID": "QID379",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS The incidence of positive lymph nodes increased with higher p stage and pathological subg...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID127",
            "SecondaryAttribute": "RESULTS The primary outcome was observed in 31 of 101 patients (30.7%) in the laparoscopic lavage...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nThe primary outcome was observed in 31 of 101 patients (30.7%) in the laparoscopic lavage group and 25 of 96 patients (26.0%) in the colon resection group (difference, 4.7% [95% CI, -7.9% to 17.0%]; P\u2009=\u2009.53). Mortality at 90 days did not significantly differ between the laparoscopic lavage group (14 patients [13.9%]) and the colon resection group (11 patients [11.5%]; difference, 2.4% [95% CI, -7.2% to 11.9%]; P\u2009=\u2009.67). The reoperation rate was significantly higher in the laparoscopic lavage group (15 of 74 patients [20.3%]) than in the colon resection group (4 of 70 patients [5.7%]; difference, 14.6% [95% CI, 3.5% to 25.6%]; P\u2009=\u2009.01) for patients who did not have fecal peritonitis. The length of operating time was significantly shorter in the laparoscopic lavage group; whereas, length of postoperative hospital stay and quality of life did not differ significantly between groups. Four sigmoid carcinomas were missed with laparoscopic lavage.\n\n",
                "DataExportTag": "55",
                "QuestionID": "QID127",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS The primary outcome was observed in 31 of 101 patients (30.7%) in the laparoscopic lavage...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID104",
            "SecondaryAttribute": "RESULTS The two health systems used similar processes for (a) early designation of program person...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "RESULTS\nThe two health systems used similar processes for (a) early designation of program personnel, (b) developing SCP templates, (c) provider\/staff input, and (d) identifying\/tracking eligible patients. In contrast, they developed differing processes for SCP completion and delivery. The two health systems also identified effective strategies and challenges in SCP development and implementation.\n\n",
                "DataExportTag": "32",
                "QuestionID": "QID104",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "RESULTS The two health systems used similar processes for (a) early designation of program person...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID357",
            "SecondaryAttribute": "Revealing the contribution of liver macrophage populations to NASH in insulin resistance Non-alco...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Revealing the contribution of liver macrophage populations to NASH in insulin resistance Non-alcoholic steatohepatitis (NASH), the most common chronic liver disease worldwide, is an unmet medical need with no approved therapies and debilitating consequences for patients. Obesity-associated insulin resistance is a high-risk factor for the development of NASH. The prevailing paradigm is a multiple hit process, whereby lipid accumulation in the liver of obese patients leads to oxidative stress and increased production of inflammatory cytokines by macrophages. However, my research group's comprehensive investigations in mice and humans have revealed that liver macrophages (LMs) contribute to insulin resistance and oxidative stress independently\u00a0of their inflammatory status. I thereby propose that LMs predispose insulin resistant patients to NASH independently\u00a0of their inflammatory status.\u00a0 In this ambitious multidisciplinary project, we will use a novel platform encompassing multiple single cell and in situ omics technologies tailored by my research group to characterize the phenotype of LM populations in healthy individuals and insulin resistant patients with or without NASH. We will strengthen this approach with functional validation in animal models as well as human liver organoids using a patented technology that I have developed to specifically manipulate gene expression in macrophages. We will then decipher how hepatic insulin resistance creates a spatiotemporal environment facilitating NASH. My group's unique access to patient material combined with cutting-edge methodologies to reveal the phenotype of single LMs provides an exceptional starting point from which to identify genes and pathways involved in the development of NASH in obese insulin resistant patients. This project will set the stage for a paradigm-shift in studying and treating life-threatening liver diseases.\n\n",
                "DataExportTag": "COR2821",
                "QuestionID": "QID357",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Revealing the contribution of liver macrophage populations to NASH in insulin resistance Non-alco...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Drug Development for Rare Diseases\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Gene Therapy and Immune Response in Cardiac and Metabolic Disorders\""
                    },
                    "4": {
                        "Display": "cellular, therapeutic, immune, genetic, inflammation, metabolic, gene_therapy, mechanism, liver, ra, cardiac, vivo, heart_failure, inflammatory, molecule"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID377",
            "SecondaryAttribute": "Risk factors for patients with pelvic lymph node metastases following radical cystectomy with en...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Risk factors for patients with pelvic lymph node metastases following radical cystectomy with en bloc pelvic lymphadenectomy: concept of lymph node density. PURPOSE\nWe evaluated the clinical outcomes and risk factors for progression in a large cohort of patients with lymph node metastases following en bloc radical cystectomy and bilateral pelvic lymphadenectomy.\n\n",
                "DataExportTag": "CAN922811",
                "QuestionID": "QID377",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Risk factors for patients with pelvic lymph node metastases following radical cystectomy with en...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID381",
            "SecondaryAttribute": "Risk for Hospital Contact With Infection in Patients With Splenectomy Context Splenectomy is asso...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Risk for Hospital Contact With Infection in Patients With Splenectomy Context Splenectomy is associated with infection, but the magnitude of risk is unclear. Contribution This study compared the risk for infections that required hospital care in all 3812 patients in Denmark who underwent splenectomy from 1996 to 2005 with that of comparison groups. Infection risk was highest in the 90 days after splenectomy. The risk for infection was 4.6 times higher in splenectomized patients than in the general population. The risk was only modestly higher than that of patients who had no splenectomy but did have the conditions that can lead to splenectomy. Implication Although splenectomized patients have a high risk for infection, some of this risk is due to underlying conditions and not to splenectomy alone. The Editors The spleen hosts important immune cells that are essential to the elimination of bloodborne pathogens, particularly encapsulated bacteria (1, 2). Medically indicated splenectomy is associated with pneumonia, bacteremia, severe sepsis, and other severe infections (39). The magnitude and duration of increased infection risk, however, is debated (10). Few studies have followed splenectomized patients for more than 1 year (3, 5, 6, 8, 9), and confounding by splenectomy indications and other comorbid conditions could have affected the findings. In addition, the longer-term risk for infection in splenectomized patients has not been quantified. Such data are important for evaluating splenectomy-associated risks and for better understanding patients' clinical course (10). We conducted a population-based assessment in Denmark of the risk for hospital contacts involving infection among splenectomized patients compared with the general population and with similar groups of unsplenectomized patients. We also examined variations in the risk for infections by splenectomy indication and by time since surgical procedure. Methods Cohort of Splenectomized Patients We identified all splenectomized patients from the Danish National Registry of Patients (NRP), which covers the entire Danish population of 5.4 million persons. The NRP has tracked 99.4% of all discharges from Danish acute care, nonpsychiatric hospitals since 1977 and all hospital outpatient and emergency department visits since 1995 (11). Records include dates of admission and discharge; surgical procedure codes; and up to 20 discharge diagnoses, classified according to the International Classification of Disease, Eighth Edition, until 31 December 1993 and the Tenth Edition thereafter. In all Danish registries, patients are identified by their civil registration number. These unique identifiers are stored in the Danish Civil Registration System (CRS) along with birth date; residency status; and dates of immigration, emigration, and death (if applicable) (12). From the NRP, we identified all surgical splenectomy procedures from 1 January 1996 to 31 December 2005. We also searched the NRP for patients with any previous medical indication for splenectomy, including diagnoses made during the hospitalization with splenectomy. We classified splenectomized patients into 8 groups according to indication. In cases of multiple indications, the category was determined by using the following hierarchy, regardless of other indications (13): traumatic rupture of spleen, immune thrombocytopenic purpura (ITP), other or unspecific thrombocytopenia, hematopoietic cancer, hereditary hemolytic anemia, abdominal cancer, splenomegaly or other splenic diseases only, and no diagnosis recorded (Appendix). General Population Comparison Cohort From the CRS, we randomly chose 10 members of the general population, matched by age and sex, for each splenectomized patient. The general population members had to be alive without history of splenectomy as of the splenectomy date of their matched patient (the index date). For the subset of splenectomized patients living in former North Jutland County (about 500000 inhabitants), for whom detailed data on bacteremia episodes were available (14, 15), we randomly chose 10 age- and sex-matched population comparisons living in the same county. Patient Comparison Cohorts Patients undergoing splenectomy are exposed to surgery and may have other diseases that increase the risk for infection or that involve immunosuppressive therapies. To separate potential effects of those conditions from effects of splenectomy on the risk for infection, we assembled 2 additional comparison groups: patients who had appendectomy (the appendectomized comparison cohort, for whom the index date was the date of appendectomy) and, for splenectomized patients with a recorded indication for splenectomy, patients who had the same condition diagnosed (for instance, myeloid leukemia) but without splenectomy (the matched-indication comparison cohort). For these comparisons, up to 5 matched patients without splenectomy on the index date were identified by using the CRS and NRP. In the matched-indication-cohort, we also matched on year of diagnosis of the medical condition. We then calculated the index date in each matched-indication patient by adding the time between diagnosis of the medical condition and splenectomy in the splenectomized patient to the date of the diagnosis. For patients who had splenectomy because of trauma, we selected trauma patient comparisons who underwent surgery for acute injury of the spleen, liver, or gallbladder but not splenectomy (the Appendix, provides the codes used). Hospital Contacts Involving Infection All participants' records were linked to the CRS and the NRP to identify all hospital contacts involving bacterial and viral infections, pneumonia, and infections other than pneumonia after the index date. We analyzed for bacteremia episodes by using the subset of persons from North Jutland County, whose population-based bacteremia research registry enables identification of all clinically significant bacteremia episodes and all available blood culture samples (14, 15). Other Covariates To control for confounding by conditions associated with both splenectomy and risk for infection, we retrieved data on several comorbid conditions, identified as conditions recorded in the NRP before each patient's index date (Appendix). Statistical Analysis We assessed the association between splenectomy for any indication and subsequent infection and for each splenectomy indication versus matched comparisons in the general population, appendectomy, and matched-indication comparison cohorts. We observed the patients from the index date until a hospital contact involving an outcome of interest (any infection, pneumonia, or nonpneumonia infection), death, emigration, or 31 December 2006, whichever came first. Referent patients who underwent splenectomy during the follow-up were censored at the time of splenectomy. We first compared overall rates of infectious events in the cohorts during total follow-up. We then compared occurrence of infection requiring hospitalization in patients with and without splenectomy within 90 days, from 91 to 365 days, and more than 365 days after the index date. Because the NRP does not list the exact date of the infectious event during the hospitalization in which the splenectomy was performed (4, 6), we estimated odds ratios with 95% CIs, by using logistic regression as the measure of relative risk for infections within 90 days of the index date. We counted all discharge diagnoses of infection documented either for ongoing hospitalizations ending within 90 days or for new hospital contacts occurring within 90 days after the surgery date. For infections more than 90 days after splenectomy, we used Cox proportional hazards regression to compute hazard ratios as measures of relative risk separately for the intermediate (91 to 365 days) and long-term (>365 days) follow-up. Thus, for each period we computed the time to the first infectious event among all patients alive and at risk at days 91 and 366, respectively, also including participants who had a hospital contact with infection during a previous period. We conducted similar analyses for bacteremia in the North Jutland County subcohort (6, 16). We adjusted for age (0 to 39, 40 to 59, 60 to 69, and 70 years), sex, and comorbid conditions (Appendix) as a priori confounders. In the matched-indication cohort analysis, presence of comorbid conditions was dichotomized (yes or no) because the sample was small. We used stratified regression models to account for matching. The traumatic rupture comparisons were unmatched because of sparse data; therefore, the overall estimates of splenectomy versus indication comparisons were calculated as an inverse variance weighted average of the individual indication estimates. We analyzed data by using SAS software, version 9.2 (SAS Institute, Cary, North Carolina). The Danish Data Protection Agency and Aarhus University Hospital Registry Board approved the study. Role of the Funding Source This study was supported in part by a grant from Amgen to Aarhus University, by the Clinical Epidemiological Research Foundation at Aarhus University, and by the Karen Elise Jensen Foundation. In collaboration with the investigators, Amgen designed the study. Amgen representatives participated in the interpretation of the data, which Aarhus University holds, and in the writing of this report. Results Descriptive Data We identified 3812 splenectomized persons, 38120 general population members, 16962 appendectomized cohort members, and 8310 matched-indication patients for the 2394 splenectomized patients with recorded indication. The median patient age was 60 years (interquartile range, 41 to 72 years). Splenectomized patients and matched-indication comparisons had a higher burden of comorbid conditions than the general population or appendectomized comparison group (Table 1). Table 1. Characteristics of Study Sample at Baseline The most common medical indications for splenectomy were traumatic rupture of the spleen (20.1%) and abdominal cancer\n\n",
                "DataExportTag": "CAN551193",
                "QuestionID": "QID381",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Risk for Hospital Contact With Infection in Patients With Splenectomy Context Splenectomy is asso...",
                "Choices": {
                    "1": {
                        "Display": "\"Gallbladder Cancer Prognosis and Treatment\""
                    },
                    "2": {
                        "Display": "recurrence, prognosis, metastasis, gallbladder, staging, survival, preoperative, adjuvant, resection, adjuvant_chemotherapy, invasion, gbc, lr, local_recurrence, lymph_node"
                    },
                    "3": {
                        "Display": "\"Cancer Progression and Prognosis\""
                    },
                    "4": {
                        "Display": "metastasis, lymph_node, invasion, cervical, colorectal_cancer, liver, lnm, endometrial_cancer, involvement, lymphatic, papillary_thyroid_carcinoma, recurrence, depth, peritoneal, prognosis"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID106",
            "SecondaryAttribute": "Robotic nephrectomy for kidney cancer in a horseshoe kidney with renal vein tumor thrombus: novel...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Robotic nephrectomy for kidney cancer in a horseshoe kidney with renal vein tumor thrombus: novel technique for thrombectomy. Surgical management of a renal vein thrombus during radical nephrectomy can be challenging under laparoscopic conditions. Laparoscopic radical nephrectomy in the setting of a renal vein tumor thrombus has been described using various GIA stapling, Statinsky clamping, or hand-assist techniques to milk back the renal vein thrombus.1 These techniques for milking the tumor thrombus have potential limitations, such as the challenge of placing a wide GIA stapler between the inferior vena cava (IVC) and thrombus while avoiding a positive margin at the staple line, the potential for injury to the renal vein when a Statinsky clamp is used to milk the thrombus, or the need for a hand port.\n\n",
                "DataExportTag": "CAN56122",
                "QuestionID": "QID106",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Robotic nephrectomy for kidney cancer in a horseshoe kidney with renal vein tumor thrombus: novel...",
                "Choices": {
                    "1": {
                        "Display": "We present that case of a 63-year-old woman with an 11-cm left renal mass in a horseshoe kidney and a renal vein tumor thrombus that extended to the IVC (Fig. 1) that was managed using robotic assistance for radical heminephrectomy and renal vein thrombectomy. We describe a novel Hem-o-Lok clip technique for management of a renal vein thrombus during radical nephrectomy with a minimally invasive approach."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID283",
            "SecondaryAttribute": "Scaling up the WHO-PEN package for diabetes and hypertension in Swaziland: a nation-wide cluster-...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Scaling up the WHO-PEN package for diabetes and hypertension in Swaziland: a nation-wide cluster-randomised evaluation of three strategies in Swaziland (WHO-PEN@Scale) Swaziland has a very high prevalence of both diabetes and hypertension, with the majority of those affected being undiagnosed. Like many other countries in sub-Saharan Africa (SSA), however, Swaziland provides care for diabetes and hypertension only through physician-led teams in hospitals. The country has recently conducted a successful feasibility pilot of the World Health Organisation\u2019s Package of Essential NCD Interventions for Primary Health Care in low-resource settings (WHO-PEN). The WHO-PEN@Scale project aims to improve diabetes and hypertension control at the population-level by helping Swaziland identify and scale up the most effective healthcare delivery model for WHO-PEN implementation at scale. Specifically, we will conduct an innovative nation-wide two-phased adaptive randomised study to i) rigorously assess the effectiveness and cost-effectiveness of four novel community-based healthcare delivery models to scale up WHO-PEN for diabetes and hypertension nationally, ii) study in-depth the real-life implementation of the WHO-PEN national scale-up in Swaziland to generate lessons for other countries, and iii) actively disseminate the Swazi experience along with an open-source toolkit for WHO-PEN scale-up to policy makers in SSA. WHO-PEN@Scale will therefore provide a blueprint of primary health system strengthening for diabetes and hypertension \u2013 and ultimately non-communicable diseases (NCDs) more generally - for other countries in the region. WHO-PEN@Scale is highly relevant to this work programme because i) the interventions contained in WHO-PEN have been shown to be cost-effective, ii) the scale-up is government-led, and iii) we assess the impact of WHO-PEN@Scale on diabetes and hypertension control at the population-level.\n\n",
                "DataExportTag": "COR39862",
                "QuestionID": "QID283",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Scaling up the WHO-PEN package for diabetes and hypertension in Swaziland: a nation-wide cluster-...",
                "Choices": {
                    "1": {
                        "Display": "\"Clinical Trials and Drug Development for Rare Diseases\""
                    },
                    "2": {
                        "Display": "biomarker, patient, cohort, drug, diagnosis, clinical, ra, translational, disease, trial, rare_disease, therapeutic, medicine, ms, sample"
                    },
                    "3": {
                        "Display": "\"Public Health and Preventive Medicine\""
                    },
                    "4": {
                        "Display": "health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID186",
            "SecondaryAttribute": "Screening for Breast Cancer: Recommendations and Rationale Summary of the Recommendations The U.S...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Screening for Breast Cancer: Recommendations and Rationale Summary of the Recommendations The U.S. Preventive Services Task Force (USPSTF) recommends screening mammography, with or without clinical breast examination (CBE), every 1 to 2 years for women aged 40 and older. This is a grade B recommendation. (See Appendix Table 1 for a description of the USPSTF classification of recommendations.) The USPSTF found fair evidence that mammography screening every 12 to 33 months significantly reduces mortality from breast cancer. (See Appendix Table 2 for a description of the USPSTF classification of levels of evidence.) Evidence is strongest for women aged 50 to 69, the age group generally included in screening trials. For women aged 40 to 49, the evidence that screening mammography reduces mortality from breast cancer is weaker, and the absolute benefit of mammography is smaller, than it is for older women. Most, but not all, studies indicate a mortality benefit for women undergoing mammography at ages 40 to 49, but the delay in observed benefit in women younger than 50 makes it difficult to determine the incremental benefit of beginning screening at age 40 rather than at age 50. The absolute benefit is smaller because the incidence of breast cancer is lower among women in their 40s than it is among older women. The USPSTF concluded that the evidence is also generalizable to women aged 70 and older (who face a higher absolute risk of breast cancer) if their life expectancy is not compromised by comorbid disease. The absolute probability of benefits of regular mammography increases along a continuum with age, whereas the likelihood of harms from screening (false-positive results and unnecessary anxiety, biopsies, and cost) diminishes from ages 40 to 70. The balance of benefits and potential harms, therefore, grows more favorable as women age. The precise age at which the potential benefits of mammography justify the possible harms is a subjective choice. The USPSTF did not find sufficient evidence to specify the optimal screening interval for women aged 40 to 49 (see Clinical Considerations). The USPSTF concludes that the evidence is insufficient to recommend for or against routine CBE alone to screen for breast cancer. This is a grade I recommendation. No screening trial has examined the benefits of CBE alone (without accompanying mammography) compared to no screening, and design characteristics limit the generalizability of studies that have examined CBE. The USPSTF could not determine the benefits of CBE alone or the incremental benefit of adding CBE to mammography. The USPSTF therefore could not determine whether potential benefits of routine CBE outweigh the potential harms. The USPSTF concludes that the evidence is insufficient to recommend for or against teaching or performing routine breast self-examination (BSE). This is a grade I recommendation. The USPSTF found poor evidence to determine whether BSE reduces breast cancer mortality. The USPSTF found fair evidence that BSE is associated with an increased risk of false-positive results and biopsies. Because of design limitations of published and ongoing studies of BSE, the USPSTF could not determine the balance of benefits and potential harms of BSE. Clinical Considerations The precise age at which the benefits from screening mammography justify the potential harms is a subjective judgment and should take into account patient preferences. Clinicians should inform women about the potential benefits (reduced chance of dying from breast cancer), potential harms (for example, false-positive results, unnecessary biopsies), and limitations of the test that apply to women their age. Clinicians should tell women that the balance of benefits and potential harms of mammography improves with increasing age for women between the ages of 40 and 70. Women who are at increased risk for breast cancer (for example, those with a family history of breast cancer in a mother or sister, a previous breast biopsy revealing atypical hyperplasia, or first childbirth after age 30) are more likely to benefit from regular mammography than women at lower risk. The recommendation for women to begin routine screening in their 40s is strengthened by a family history of breast cancer having been diagnosed before menopause. The USPSTF did not examine whether women should be screened for genetic mutations (BRCA1 and BRCA2) that increase the risk of developing breast cancer, or whether women with genetic mutations might benefit from earlier or more frequent screening for breast cancer. In the trials that demonstrated the effectiveness of mammography in lowering breast cancer mortality, screening was performed every 12 to 33 months. For women aged 50 and older, there is little evidence to suggest that annual mammography is more effective than mammography done every other year. For women aged 40 to 49, available trials also have not reported a clear advantage of annual mammography over biennial mammography. Nevertheless, some experts recommend annual mammography based on the lower sensitivity of the test and on evidence that tumors grow more rapidly in this age group. The precise age at which to discontinue screening mammography is uncertain. Only two randomized, controlled trials enrolled women older than 69, and no trials enrolled women older than 74. Older women face a higher probability of developing and dying of breast cancer but also have a greater chance of dying of other causes. Women with comorbid conditions that limit their life expectancy are unlikely to benefit from screening. Clinicians should refer patients to mammography screening centers with proper accreditation and quality assurance standards to ensure accurate imaging and radiographic interpretation. Clinicians should adopt office systems to ensure timely and adequate follow-up of abnormal results. A listing of accredited facilities is available at www.fda.gov\/cdrh\/mammography\/certified.html. Clinicians who advise women to perform BSE or who perform routine CBE to screen for breast cancer should understand that there is currently insufficient evidence to determine whether these practices affect breast cancer mortality and that they are likely to increase the incidence of clinical assessments and biopsies. The brief review of the evidence that is normally included in USPSTF recommendations is available in the complete recommendation and rationale statement on the USPSTF Web site (www.preventiveservices.ahrq.gov). Recommendations of Others Nearly all North American organizations support mammography screening, although groups vary in the recommended age to begin screening, the interval for screening, and the role of CBE. The American Medical Association (AMA) (1), the American College of Radiology (ACR) (2), and the American Cancer Society (ACS) (3) all support screening with mammography and CBE beginning at age 40. The American College of Obstetricians and Gynecologists (ACOG) (4) supports screening with mammography beginning at age 40 and CBE beginning at age 19. The Canadian Task Force on Preventive Health Care (CTFPHC) (5), the American Academy of Family Physicians (AAFP) (6), and the American College of Preventive Medicine (ACPM) (7) recommend beginning mammography for average-risk women at age 50. The AAFP and ACPM recommend that mammography in high-risk women begin at age 40, and the AAFP recommends that all women aged 40 to 49 be counseled about the risks and benefits of mammography before making decisions about screening (6, 7). A 1997 Consensus Development Panel convened by the U.S. National Institutes of Health concluded that the evidence was insufficient to determine the benefits of mammography among women aged 40 to 49. This panel recommended that women aged 40 to 49 should be counseled about potential benefits and harms before making decisions about mammography (8). In 2001, the CTFPHC concluded there was insufficient evidence to recommend for or against mammography in women 40 to 49 (9). Organizations differ on their recommendations for the appropriate interval for mammography. Annual mammography is recommended by the AMA, ACR, and ACS (1-3). Mammography every 1 to 2 years is recommended by the CTFPHC, AAFP, and ACPM (5-7). The ACOG recommends mammography every 1 to 2 years for women aged 40 to 49 and annually for women aged 50 and older (4). In its 2001 report, the CTFPHC recommends against teaching BSE to women aged 40 to 69 (10). The AMA, ACS, ACOG, and AAFP support teaching BSE (1, 3, 4, 6).\n\n",
                "DataExportTag": "CAN969355",
                "QuestionID": "QID186",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Screening for Breast Cancer: Recommendations and Rationale Summary of the Recommendations The U.S...",
                "Choices": {
                    "1": {
                        "Display": "\"Medical Imaging and Therapeutic Techniques\""
                    },
                    "2": {
                        "Display": "imaging, temperature, ultrasound, optical, oct, tissue, hifu, vivo, heating, optical_coherence, probe, hyperthermia, phantom, real_time, laser"
                    },
                    "3": {
                        "Display": "\"Medical Imaging and Cancer Treatment\""
                    },
                    "4": {
                        "Display": "imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID247",
            "SecondaryAttribute": "Screening for Breast Cancer: Recommendations and Rationale Summary of the Recommendations The U.S...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Screening for Breast Cancer: Recommendations and Rationale Summary of the Recommendations The U.S. Preventive Services Task Force (USPSTF) recommends screening mammography, with or without clinical breast examination (CBE), every 1 to 2 years for women aged 40 and older. This is a grade B recommendation. (See Appendix Table 1 for a description of the USPSTF classification of recommendations.) The USPSTF found fair evidence that mammography screening every 12 to 33 months significantly reduces mortality from breast cancer. (See Appendix Table 2 for a description of the USPSTF classification of levels of evidence.) Evidence is strongest for women aged 50 to 69, the age group generally included in screening trials. For women aged 40 to 49, the evidence that screening mammography reduces mortality from breast cancer is weaker, and the absolute benefit of mammography is smaller, than it is for older women. Most, but not all, studies indicate a mortality benefit for women undergoing mammography at ages 40 to 49, but the delay in observed benefit in women younger than 50 makes it difficult to determine the incremental benefit of beginning screening at age 40 rather than at age 50. The absolute benefit is smaller because the incidence of breast cancer is lower among women in their 40s than it is among older women. The USPSTF concluded that the evidence is also generalizable to women aged 70 and older (who face a higher absolute risk of breast cancer) if their life expectancy is not compromised by comorbid disease. The absolute probability of benefits of regular mammography increases along a continuum with age, whereas the likelihood of harms from screening (false-positive results and unnecessary anxiety, biopsies, and cost) diminishes from ages 40 to 70. The balance of benefits and potential harms, therefore, grows more favorable as women age. The precise age at which the potential benefits of mammography justify the possible harms is a subjective choice. The USPSTF did not find sufficient evidence to specify the optimal screening interval for women aged 40 to 49 (see Clinical Considerations). The USPSTF concludes that the evidence is insufficient to recommend for or against routine CBE alone to screen for breast cancer. This is a grade I recommendation. No screening trial has examined the benefits of CBE alone (without accompanying mammography) compared to no screening, and design characteristics limit the generalizability of studies that have examined CBE. The USPSTF could not determine the benefits of CBE alone or the incremental benefit of adding CBE to mammography. The USPSTF therefore could not determine whether potential benefits of routine CBE outweigh the potential harms. The USPSTF concludes that the evidence is insufficient to recommend for or against teaching or performing routine breast self-examination (BSE). This is a grade I recommendation. The USPSTF found poor evidence to determine whether BSE reduces breast cancer mortality. The USPSTF found fair evidence that BSE is associated with an increased risk of false-positive results and biopsies. Because of design limitations of published and ongoing studies of BSE, the USPSTF could not determine the balance of benefits and potential harms of BSE. Clinical Considerations The precise age at which the benefits from screening mammography justify the potential harms is a subjective judgment and should take into account patient preferences. Clinicians should inform women about the potential benefits (reduced chance of dying from breast cancer), potential harms (for example, false-positive results, unnecessary biopsies), and limitations of the test that apply to women their age. Clinicians should tell women that the balance of benefits and potential harms of mammography improves with increasing age for women between the ages of 40 and 70. Women who are at increased risk for breast cancer (for example, those with a family history of breast cancer in a mother or sister, a previous breast biopsy revealing atypical hyperplasia, or first childbirth after age 30) are more likely to benefit from regular mammography than women at lower risk. The recommendation for women to begin routine screening in their 40s is strengthened by a family history of breast cancer having been diagnosed before menopause. The USPSTF did not examine whether women should be screened for genetic mutations (BRCA1 and BRCA2) that increase the risk of developing breast cancer, or whether women with genetic mutations might benefit from earlier or more frequent screening for breast cancer. In the trials that demonstrated the effectiveness of mammography in lowering breast cancer mortality, screening was performed every 12 to 33 months. For women aged 50 and older, there is little evidence to suggest that annual mammography is more effective than mammography done every other year. For women aged 40 to 49, available trials also have not reported a clear advantage of annual mammography over biennial mammography. Nevertheless, some experts recommend annual mammography based on the lower sensitivity of the test and on evidence that tumors grow more rapidly in this age group. The precise age at which to discontinue screening mammography is uncertain. Only two randomized, controlled trials enrolled women older than 69, and no trials enrolled women older than 74. Older women face a higher probability of developing and dying of breast cancer but also have a greater chance of dying of other causes. Women with comorbid conditions that limit their life expectancy are unlikely to benefit from screening. Clinicians should refer patients to mammography screening centers with proper accreditation and quality assurance standards to ensure accurate imaging and radiographic interpretation. Clinicians should adopt office systems to ensure timely and adequate follow-up of abnormal results. A listing of accredited facilities is available at www.fda.gov\/cdrh\/mammography\/certified.html. Clinicians who advise women to perform BSE or who perform routine CBE to screen for breast cancer should understand that there is currently insufficient evidence to determine whether these practices affect breast cancer mortality and that they are likely to increase the incidence of clinical assessments and biopsies. The brief review of the evidence that is normally included in USPSTF recommendations is available in the complete recommendation and rationale statement on the USPSTF Web site (www.preventiveservices.ahrq.gov). Recommendations of Others Nearly all North American organizations support mammography screening, although groups vary in the recommended age to begin screening, the interval for screening, and the role of CBE. The American Medical Association (AMA) (1), the American College of Radiology (ACR) (2), and the American Cancer Society (ACS) (3) all support screening with mammography and CBE beginning at age 40. The American College of Obstetricians and Gynecologists (ACOG) (4) supports screening with mammography beginning at age 40 and CBE beginning at age 19. The Canadian Task Force on Preventive Health Care (CTFPHC) (5), the American Academy of Family Physicians (AAFP) (6), and the American College of Preventive Medicine (ACPM) (7) recommend beginning mammography for average-risk women at age 50. The AAFP and ACPM recommend that mammography in high-risk women begin at age 40, and the AAFP recommends that all women aged 40 to 49 be counseled about the risks and benefits of mammography before making decisions about screening (6, 7). A 1997 Consensus Development Panel convened by the U.S. National Institutes of Health concluded that the evidence was insufficient to determine the benefits of mammography among women aged 40 to 49. This panel recommended that women aged 40 to 49 should be counseled about potential benefits and harms before making decisions about mammography (8). In 2001, the CTFPHC concluded there was insufficient evidence to recommend for or against mammography in women 40 to 49 (9). Organizations differ on their recommendations for the appropriate interval for mammography. Annual mammography is recommended by the AMA, ACR, and ACS (1-3). Mammography every 1 to 2 years is recommended by the CTFPHC, AAFP, and ACPM (5-7). The ACOG recommends mammography every 1 to 2 years for women aged 40 to 49 and annually for women aged 50 and older (4). In its 2001 report, the CTFPHC recommends against teaching BSE to women aged 40 to 69 (10). The AMA, ACS, ACOG, and AAFP support teaching BSE (1, 3, 4, 6).\n\n",
                "DataExportTag": "CAN969355",
                "QuestionID": "QID247",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Screening for Breast Cancer: Recommendations and Rationale Summary of the Recommendations The U.S...",
                "Choices": {
                    "1": {
                        "Display": "\"Medical Imaging and Therapeutic Techniques\""
                    },
                    "2": {
                        "Display": "imaging, temperature, ultrasound, optical, oct, tissue, hifu, vivo, heating, optical_coherence, probe, hyperthermia, phantom, real_time, laser"
                    },
                    "3": {
                        "Display": "\"Medical Imaging and Cancer Treatment\""
                    },
                    "4": {
                        "Display": "imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID133",
            "SecondaryAttribute": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the American College of Physicians Recommendations Recommendation 1: In women 40 to 49 years of age, clinicians should periodically perform individualized assessment of risk for breast cancer to help guide decisions about screening mammography. A careful assessment of a woman's risk for breast cancer is important. The 5-year breast cancer risk can vary from 0.4% for a woman age 40 years with no risk factors to 6.0% for a woman age 49 years with several risk factors (1). Factors that increase the risk for breast cancer include older age, family history of breast cancer, older age at the time of first birth, younger age at menarche, and history of breast biopsy. Women 40 to 49 years of age who have any of the following risk factors have a higher risk for breast cancer than the average 50-year-old woman: 2 first-degree relatives with breast cancer; 2 previous breast biopsies; 1 first-degree relative with breast cancer and 1 previous breast biopsy; previous diagnosis of breast cancer, ductal carcinoma in situ (DCIS), or atypical hyperplasia; previous chest irradiation (1); or BRCA1 or BRCA2 mutation (2, 3). A family history can also help identify women who may have BRCA mutations that place them at substantially higher risk for breast and other types of cancer (Table). These women should be referred for counseling and recommendations specific to this population, as recommended by the U.S. Preventive Services Task Force (USPSTF) (4). Risk assessments should be updated periodically, particularly in women whose family history changes (for example, a relative receives a diagnosis of breast or ovarian cancer) and in women who choose not to have regular screening mammography. Although no evidence supports specific intervals, we encourage clinicians to update the woman's risk assessment every 1 to 2 years. Table. Family History Patterns Associated with an Increased Risk for BRCA1 or BRCA2Gene Mutations* The risk for invasive breast cancer can be estimated quantitatively by using the Web site calculator provided by the National Institutes of Health (NIH) (bcra.nci.nih.gov\/brc\/q1.htm) (1). This calculator is based on the Gail model, which takes into account many of the risk factors previously mentioned. However, clinicians who use the Gail model should be aware of its limitations. Although the model accurately predicts the risk for cancer for groups of women, its ability to discriminate between higher and lower risk for an individual woman is limited (5, 6). This limitation occurs because many women have similar, relatively low absolute risks for invasive breast cancer over 5 years, which makes discrimination among levels of risk difficult for an individual woman. Recommendation 2: Clinicians should inform women 40 to 49 years of age about the potential benefits and harms of screening mammography. Screening mammography for women 40 to 49 years of age is associated with both benefits and potential harms. The most important benefit of screening mammography every 1 to 2 years in women 40 to 49 years of age is a potential decrease in breast cancer mortality. A recent meta-analysis estimated the relative reduction in the breast cancer mortality rate to be 15% after 14 years of follow-up (relative risk, 0.85 [95% credible interval {CrI}, 0.73 to 0.99]) (7). An additional large randomized clinical trial of screening mammography in women 40 to 49 years of age found a similar decrease in the risk for death due to breast cancer, although the decrease did not reach statistical significance (relative risk, 0.83 [95% CI, 0.66 to 1.04]) (8). Potential risks of mammography include false-positive results, diagnosis and treatment for cancer that would not have become clinically evident during the patient's lifetime, radiation exposure, false reassurance, and procedure-associated pain. False-positive mammography can lead to increased anxiety and to feelings of increased susceptibility to breast cancer, but most studies found that anxiety resolved quickly after the evaluation. Recommendation 3: For women 40 to 49 years of age, clinicians should base screening mammography decisions on benefits and harms of screening, as well as on a woman's preferences and breast cancer risk profile. Because the evidence shows variation in risk for breast cancer and benefits and harms of screening mammography based on an individual woman's risk profile, a personalized screening strategy based on a discussion of the benefits and potential harms of screening and an understanding of a woman's preferences will help identify those who will most benefit from screening mammography. For many women, the potential reduction in breast cancer mortality rate associated with screening mammography will outweigh other considerations. For women who do not wish to discuss the screening decision, screening mammography every 1 to 2 years in women 40 to 49 years of age is reasonable. Important factors in the decision to undergo screening mammography are women's preferences for screening and the associated outcomes. Concerns about risks for breast cancer or its effect on quality of life will vary greatly among women. Some women may also be particularly concerned about the potential harms of screening mammography, such as false-positive mammograms and the resulting diagnostic work-up. When feasible, clinicians should explore women's concerns about breast cancer and screening mammography to help guide decision making about mammography. The relative balance of benefits and harms depends on women's concerns and preferences and on their risk for breast cancer. Clinicians should help women to judge the balance of benefits and harms from screening mammography. Women who are at greater-than-average absolute risk for breast cancer and who are concerned that breast cancer would have a severely adverse effect on quality of life may derive a greater-than-average benefit from screening mammography. Women who are at substantially lower-than-average risk for breast cancer or who are concerned about potential risks of mammography may derive a less-than-average benefit from screening mammography. If a woman decides to forgo mammography, clinicians should readdress the decision to have screening every 1 to 2 years. Recommendation 4: We recommend further research on the net benefits and harms of breast cancer screening modalities for women 40 to 49 years of age. Methodological issues associated with existing breast cancer screening trials, such as compliance with screening, lack of statistical power, and inadequate information about inclusion or exclusion criteria and study population, heighten the need for high-quality trials to confirm the effectiveness of screening mammography in women in this age group. Furthermore, harms of screening in this age group, such as pain, radiation exposure, and adverse outcomes related to false-positive results, should also be studied. Introduction Breast cancer is the second leading cause of cancer-related death among women in the United States. In 2005, an estimated 211240 new cases of invasive breast cancer will be diagnosed, and 40410 women will die of the disease (9). Screening mammography reduces breast cancer mortality in women 50 to 70 years of age. Although 25% of all diagnosed cases are among women younger than 50 years of age (9), screening mammography in this age group has remained a topic of debate because of the difficulty in determining the benefit of mammography in this age group. A meta-analysis performed for the USPSTF estimated that screening mammography every 1 to 2 years in women 40 to 49 years of age resulted in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (7). However, the 95% credible interval for this estimate is wide and indicates that the reduction could be as much as 27% or as little as 1%. This relative risk reduction corresponds to about 5.6 deaths prevented per 10000 women screened (95% CrI, 0.9 to 13.1 deaths prevented per 10000 women screened). Because screening mammography is also associated with potential harms, a discussion of risks (biopsies, surgery, radiation exposure, false-positive results, and false reassurance), benefits (early detection of breast cancer), and patient preferences should be the basis for screening decisions. The purpose of this guideline is to present the available evidence and to increase clinicians' understanding of the benefits and risks of screening mammography in women 40 to 49 years of age. The target audience is clinicians who are caring for women in this age group. The target patient population is all women 40 to 49 years of age. These recommendations are based on the systematic review of the evidence in the background paper in this issue (6). The systematic evidence review does not include breast cancer risk in men and genetic risk markers, such as BRCA. The goal for this guideline was to answer the following questions: 1. What are the benefits of screening mammography in women 40 to 49 years of age? 2. What are the risks associated with screening mammography in women 40 to 49 years of age? 3. Does the balance of risks and benefits vary according the individual woman's characteristics? 4. What are the methodological issues that affect the interpretation of the results of previous meta-analyses? Benefits Of the 8 currently published meta-analyses, 7 estimated that screening women 40 to 49 years of age reduced breast cancer mortality rates, but only 3 of these found a statistically significant reduction (7). The most recent meta-analysis found that screening mammography every 1 to 2 years in women 40 to 49 years of age results in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (relative risk, 0.85 [95% CI, 0.73 to 0.99]) (7). However, concerns about study quality and whether some of the observed benefit may be due to screening that occurred after the women turned 50 years of age complicate interpretation of the evidence. The use of death due to breast cancer as an end point can\n\n",
                "DataExportTag": "CAN987623",
                "QuestionID": "QID133",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
                "Choices": {
                    "1": {
                        "Display": "\"Gastrointestinal and Salivary Gland Tumors Diagnosis and Treatment\""
                    },
                    "2": {
                        "Display": "gastrointestinal_stromal_tumour, salivary_gland, gastrointestinal_stromal, acc, spindle, parotid_gland, kit_kinase_inhibitor, c_kit, smooth_muscle, pleomorphic_adenoma, parotid, adenoid_cystic, vimentin, ameloblastoma, immunohistochemical"
                    },
                    "3": {
                        "Display": "\"Skin Cancer and Dermatology\""
                    },
                    "4": {
                        "Display": "cutaneous, squamous_cell_carcinoma, basal_cell_carcinoma, basal, sebaceous, merkel, eyelid, mcc, adnexal, rare, ak, bccs, dfsp, epidermal, vulva"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID188",
            "SecondaryAttribute": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the American College of Physicians Recommendations Recommendation 1: In women 40 to 49 years of age, clinicians should periodically perform individualized assessment of risk for breast cancer to help guide decisions about screening mammography. A careful assessment of a woman's risk for breast cancer is important. The 5-year breast cancer risk can vary from 0.4% for a woman age 40 years with no risk factors to 6.0% for a woman age 49 years with several risk factors (1). Factors that increase the risk for breast cancer include older age, family history of breast cancer, older age at the time of first birth, younger age at menarche, and history of breast biopsy. Women 40 to 49 years of age who have any of the following risk factors have a higher risk for breast cancer than the average 50-year-old woman: 2 first-degree relatives with breast cancer; 2 previous breast biopsies; 1 first-degree relative with breast cancer and 1 previous breast biopsy; previous diagnosis of breast cancer, ductal carcinoma in situ (DCIS), or atypical hyperplasia; previous chest irradiation (1); or BRCA1 or BRCA2 mutation (2, 3). A family history can also help identify women who may have BRCA mutations that place them at substantially higher risk for breast and other types of cancer (Table). These women should be referred for counseling and recommendations specific to this population, as recommended by the U.S. Preventive Services Task Force (USPSTF) (4). Risk assessments should be updated periodically, particularly in women whose family history changes (for example, a relative receives a diagnosis of breast or ovarian cancer) and in women who choose not to have regular screening mammography. Although no evidence supports specific intervals, we encourage clinicians to update the woman's risk assessment every 1 to 2 years. Table. Family History Patterns Associated with an Increased Risk for BRCA1 or BRCA2Gene Mutations* The risk for invasive breast cancer can be estimated quantitatively by using the Web site calculator provided by the National Institutes of Health (NIH) (bcra.nci.nih.gov\/brc\/q1.htm) (1). This calculator is based on the Gail model, which takes into account many of the risk factors previously mentioned. However, clinicians who use the Gail model should be aware of its limitations. Although the model accurately predicts the risk for cancer for groups of women, its ability to discriminate between higher and lower risk for an individual woman is limited (5, 6). This limitation occurs because many women have similar, relatively low absolute risks for invasive breast cancer over 5 years, which makes discrimination among levels of risk difficult for an individual woman. Recommendation 2: Clinicians should inform women 40 to 49 years of age about the potential benefits and harms of screening mammography. Screening mammography for women 40 to 49 years of age is associated with both benefits and potential harms. The most important benefit of screening mammography every 1 to 2 years in women 40 to 49 years of age is a potential decrease in breast cancer mortality. A recent meta-analysis estimated the relative reduction in the breast cancer mortality rate to be 15% after 14 years of follow-up (relative risk, 0.85 [95% credible interval {CrI}, 0.73 to 0.99]) (7). An additional large randomized clinical trial of screening mammography in women 40 to 49 years of age found a similar decrease in the risk for death due to breast cancer, although the decrease did not reach statistical significance (relative risk, 0.83 [95% CI, 0.66 to 1.04]) (8). Potential risks of mammography include false-positive results, diagnosis and treatment for cancer that would not have become clinically evident during the patient's lifetime, radiation exposure, false reassurance, and procedure-associated pain. False-positive mammography can lead to increased anxiety and to feelings of increased susceptibility to breast cancer, but most studies found that anxiety resolved quickly after the evaluation. Recommendation 3: For women 40 to 49 years of age, clinicians should base screening mammography decisions on benefits and harms of screening, as well as on a woman's preferences and breast cancer risk profile. Because the evidence shows variation in risk for breast cancer and benefits and harms of screening mammography based on an individual woman's risk profile, a personalized screening strategy based on a discussion of the benefits and potential harms of screening and an understanding of a woman's preferences will help identify those who will most benefit from screening mammography. For many women, the potential reduction in breast cancer mortality rate associated with screening mammography will outweigh other considerations. For women who do not wish to discuss the screening decision, screening mammography every 1 to 2 years in women 40 to 49 years of age is reasonable. Important factors in the decision to undergo screening mammography are women's preferences for screening and the associated outcomes. Concerns about risks for breast cancer or its effect on quality of life will vary greatly among women. Some women may also be particularly concerned about the potential harms of screening mammography, such as false-positive mammograms and the resulting diagnostic work-up. When feasible, clinicians should explore women's concerns about breast cancer and screening mammography to help guide decision making about mammography. The relative balance of benefits and harms depends on women's concerns and preferences and on their risk for breast cancer. Clinicians should help women to judge the balance of benefits and harms from screening mammography. Women who are at greater-than-average absolute risk for breast cancer and who are concerned that breast cancer would have a severely adverse effect on quality of life may derive a greater-than-average benefit from screening mammography. Women who are at substantially lower-than-average risk for breast cancer or who are concerned about potential risks of mammography may derive a less-than-average benefit from screening mammography. If a woman decides to forgo mammography, clinicians should readdress the decision to have screening every 1 to 2 years. Recommendation 4: We recommend further research on the net benefits and harms of breast cancer screening modalities for women 40 to 49 years of age. Methodological issues associated with existing breast cancer screening trials, such as compliance with screening, lack of statistical power, and inadequate information about inclusion or exclusion criteria and study population, heighten the need for high-quality trials to confirm the effectiveness of screening mammography in women in this age group. Furthermore, harms of screening in this age group, such as pain, radiation exposure, and adverse outcomes related to false-positive results, should also be studied. Introduction Breast cancer is the second leading cause of cancer-related death among women in the United States. In 2005, an estimated 211240 new cases of invasive breast cancer will be diagnosed, and 40410 women will die of the disease (9). Screening mammography reduces breast cancer mortality in women 50 to 70 years of age. Although 25% of all diagnosed cases are among women younger than 50 years of age (9), screening mammography in this age group has remained a topic of debate because of the difficulty in determining the benefit of mammography in this age group. A meta-analysis performed for the USPSTF estimated that screening mammography every 1 to 2 years in women 40 to 49 years of age resulted in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (7). However, the 95% credible interval for this estimate is wide and indicates that the reduction could be as much as 27% or as little as 1%. This relative risk reduction corresponds to about 5.6 deaths prevented per 10000 women screened (95% CrI, 0.9 to 13.1 deaths prevented per 10000 women screened). Because screening mammography is also associated with potential harms, a discussion of risks (biopsies, surgery, radiation exposure, false-positive results, and false reassurance), benefits (early detection of breast cancer), and patient preferences should be the basis for screening decisions. The purpose of this guideline is to present the available evidence and to increase clinicians' understanding of the benefits and risks of screening mammography in women 40 to 49 years of age. The target audience is clinicians who are caring for women in this age group. The target patient population is all women 40 to 49 years of age. These recommendations are based on the systematic review of the evidence in the background paper in this issue (6). The systematic evidence review does not include breast cancer risk in men and genetic risk markers, such as BRCA. The goal for this guideline was to answer the following questions: 1. What are the benefits of screening mammography in women 40 to 49 years of age? 2. What are the risks associated with screening mammography in women 40 to 49 years of age? 3. Does the balance of risks and benefits vary according the individual woman's characteristics? 4. What are the methodological issues that affect the interpretation of the results of previous meta-analyses? Benefits Of the 8 currently published meta-analyses, 7 estimated that screening women 40 to 49 years of age reduced breast cancer mortality rates, but only 3 of these found a statistically significant reduction (7). The most recent meta-analysis found that screening mammography every 1 to 2 years in women 40 to 49 years of age results in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (relative risk, 0.85 [95% CI, 0.73 to 0.99]) (7). However, concerns about study quality and whether some of the observed benefit may be due to screening that occurred after the women turned 50 years of age complicate interpretation of the evidence. The use of death due to breast cancer as an end point can\n\n",
                "DataExportTag": "CAN836987",
                "QuestionID": "QID188",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
                "Choices": {
                    "1": {
                        "Display": "\"Reproductive Health and Ovarian Disorders\""
                    },
                    "2": {
                        "Display": "ovarian_cancer, follicle, fertility, reproductive, fibroid, pregnancy, oocyte, uterine_fibroid, gnrh, leiomyoma, uterine_leiomyoma, infertility, follicular, amh, granulosa"
                    },
                    "3": {
                        "Display": "\"Liver Cancer Treatment and Management\""
                    },
                    "4": {
                        "Display": "hepatocellular_carcinoma, liver, transplant, tace, lt, recurrence, portal_vein, sorafenib, hepatectomy, resection, transarterial_chemoembolization, transcatheter_arterial, milan_criterion, chemoembolization, rfa"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID249",
            "SecondaryAttribute": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the American College of Physicians Recommendations Recommendation 1: In women 40 to 49 years of age, clinicians should periodically perform individualized assessment of risk for breast cancer to help guide decisions about screening mammography. A careful assessment of a woman's risk for breast cancer is important. The 5-year breast cancer risk can vary from 0.4% for a woman age 40 years with no risk factors to 6.0% for a woman age 49 years with several risk factors (1). Factors that increase the risk for breast cancer include older age, family history of breast cancer, older age at the time of first birth, younger age at menarche, and history of breast biopsy. Women 40 to 49 years of age who have any of the following risk factors have a higher risk for breast cancer than the average 50-year-old woman: 2 first-degree relatives with breast cancer; 2 previous breast biopsies; 1 first-degree relative with breast cancer and 1 previous breast biopsy; previous diagnosis of breast cancer, ductal carcinoma in situ (DCIS), or atypical hyperplasia; previous chest irradiation (1); or BRCA1 or BRCA2 mutation (2, 3). A family history can also help identify women who may have BRCA mutations that place them at substantially higher risk for breast and other types of cancer (Table). These women should be referred for counseling and recommendations specific to this population, as recommended by the U.S. Preventive Services Task Force (USPSTF) (4). Risk assessments should be updated periodically, particularly in women whose family history changes (for example, a relative receives a diagnosis of breast or ovarian cancer) and in women who choose not to have regular screening mammography. Although no evidence supports specific intervals, we encourage clinicians to update the woman's risk assessment every 1 to 2 years. Table. Family History Patterns Associated with an Increased Risk for BRCA1 or BRCA2Gene Mutations* The risk for invasive breast cancer can be estimated quantitatively by using the Web site calculator provided by the National Institutes of Health (NIH) (bcra.nci.nih.gov\/brc\/q1.htm) (1). This calculator is based on the Gail model, which takes into account many of the risk factors previously mentioned. However, clinicians who use the Gail model should be aware of its limitations. Although the model accurately predicts the risk for cancer for groups of women, its ability to discriminate between higher and lower risk for an individual woman is limited (5, 6). This limitation occurs because many women have similar, relatively low absolute risks for invasive breast cancer over 5 years, which makes discrimination among levels of risk difficult for an individual woman. Recommendation 2: Clinicians should inform women 40 to 49 years of age about the potential benefits and harms of screening mammography. Screening mammography for women 40 to 49 years of age is associated with both benefits and potential harms. The most important benefit of screening mammography every 1 to 2 years in women 40 to 49 years of age is a potential decrease in breast cancer mortality. A recent meta-analysis estimated the relative reduction in the breast cancer mortality rate to be 15% after 14 years of follow-up (relative risk, 0.85 [95% credible interval {CrI}, 0.73 to 0.99]) (7). An additional large randomized clinical trial of screening mammography in women 40 to 49 years of age found a similar decrease in the risk for death due to breast cancer, although the decrease did not reach statistical significance (relative risk, 0.83 [95% CI, 0.66 to 1.04]) (8). Potential risks of mammography include false-positive results, diagnosis and treatment for cancer that would not have become clinically evident during the patient's lifetime, radiation exposure, false reassurance, and procedure-associated pain. False-positive mammography can lead to increased anxiety and to feelings of increased susceptibility to breast cancer, but most studies found that anxiety resolved quickly after the evaluation. Recommendation 3: For women 40 to 49 years of age, clinicians should base screening mammography decisions on benefits and harms of screening, as well as on a woman's preferences and breast cancer risk profile. Because the evidence shows variation in risk for breast cancer and benefits and harms of screening mammography based on an individual woman's risk profile, a personalized screening strategy based on a discussion of the benefits and potential harms of screening and an understanding of a woman's preferences will help identify those who will most benefit from screening mammography. For many women, the potential reduction in breast cancer mortality rate associated with screening mammography will outweigh other considerations. For women who do not wish to discuss the screening decision, screening mammography every 1 to 2 years in women 40 to 49 years of age is reasonable. Important factors in the decision to undergo screening mammography are women's preferences for screening and the associated outcomes. Concerns about risks for breast cancer or its effect on quality of life will vary greatly among women. Some women may also be particularly concerned about the potential harms of screening mammography, such as false-positive mammograms and the resulting diagnostic work-up. When feasible, clinicians should explore women's concerns about breast cancer and screening mammography to help guide decision making about mammography. The relative balance of benefits and harms depends on women's concerns and preferences and on their risk for breast cancer. Clinicians should help women to judge the balance of benefits and harms from screening mammography. Women who are at greater-than-average absolute risk for breast cancer and who are concerned that breast cancer would have a severely adverse effect on quality of life may derive a greater-than-average benefit from screening mammography. Women who are at substantially lower-than-average risk for breast cancer or who are concerned about potential risks of mammography may derive a less-than-average benefit from screening mammography. If a woman decides to forgo mammography, clinicians should readdress the decision to have screening every 1 to 2 years. Recommendation 4: We recommend further research on the net benefits and harms of breast cancer screening modalities for women 40 to 49 years of age. Methodological issues associated with existing breast cancer screening trials, such as compliance with screening, lack of statistical power, and inadequate information about inclusion or exclusion criteria and study population, heighten the need for high-quality trials to confirm the effectiveness of screening mammography in women in this age group. Furthermore, harms of screening in this age group, such as pain, radiation exposure, and adverse outcomes related to false-positive results, should also be studied. Introduction Breast cancer is the second leading cause of cancer-related death among women in the United States. In 2005, an estimated 211240 new cases of invasive breast cancer will be diagnosed, and 40410 women will die of the disease (9). Screening mammography reduces breast cancer mortality in women 50 to 70 years of age. Although 25% of all diagnosed cases are among women younger than 50 years of age (9), screening mammography in this age group has remained a topic of debate because of the difficulty in determining the benefit of mammography in this age group. A meta-analysis performed for the USPSTF estimated that screening mammography every 1 to 2 years in women 40 to 49 years of age resulted in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (7). However, the 95% credible interval for this estimate is wide and indicates that the reduction could be as much as 27% or as little as 1%. This relative risk reduction corresponds to about 5.6 deaths prevented per 10000 women screened (95% CrI, 0.9 to 13.1 deaths prevented per 10000 women screened). Because screening mammography is also associated with potential harms, a discussion of risks (biopsies, surgery, radiation exposure, false-positive results, and false reassurance), benefits (early detection of breast cancer), and patient preferences should be the basis for screening decisions. The purpose of this guideline is to present the available evidence and to increase clinicians' understanding of the benefits and risks of screening mammography in women 40 to 49 years of age. The target audience is clinicians who are caring for women in this age group. The target patient population is all women 40 to 49 years of age. These recommendations are based on the systematic review of the evidence in the background paper in this issue (6). The systematic evidence review does not include breast cancer risk in men and genetic risk markers, such as BRCA. The goal for this guideline was to answer the following questions: 1. What are the benefits of screening mammography in women 40 to 49 years of age? 2. What are the risks associated with screening mammography in women 40 to 49 years of age? 3. Does the balance of risks and benefits vary according the individual woman's characteristics? 4. What are the methodological issues that affect the interpretation of the results of previous meta-analyses? Benefits Of the 8 currently published meta-analyses, 7 estimated that screening women 40 to 49 years of age reduced breast cancer mortality rates, but only 3 of these found a statistically significant reduction (7). The most recent meta-analysis found that screening mammography every 1 to 2 years in women 40 to 49 years of age results in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (relative risk, 0.85 [95% CI, 0.73 to 0.99]) (7). However, concerns about study quality and whether some of the observed benefit may be due to screening that occurred after the women turned 50 years of age complicate interpretation of the evidence. The use of death due to breast cancer as an end point can\n\n",
                "DataExportTag": "CAN836987",
                "QuestionID": "QID249",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
                "Choices": {
                    "1": {
                        "Display": "\"Reproductive Health and Ovarian Disorders\""
                    },
                    "2": {
                        "Display": "ovarian_cancer, follicle, fertility, reproductive, fibroid, pregnancy, oocyte, uterine_fibroid, gnrh, leiomyoma, uterine_leiomyoma, infertility, follicular, amh, granulosa"
                    },
                    "3": {
                        "Display": "\"Liver Cancer Treatment and Management\""
                    },
                    "4": {
                        "Display": "hepatocellular_carcinoma, liver, transplant, tace, lt, recurrence, portal_vein, sorafenib, hepatectomy, resection, transarterial_chemoembolization, transcatheter_arterial, milan_criterion, chemoembolization, rfa"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID326",
            "SecondaryAttribute": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the American College of Physicians Recommendations Recommendation 1: In women 40 to 49 years of age, clinicians should periodically perform individualized assessment of risk for breast cancer to help guide decisions about screening mammography. A careful assessment of a woman's risk for breast cancer is important. The 5-year breast cancer risk can vary from 0.4% for a woman age 40 years with no risk factors to 6.0% for a woman age 49 years with several risk factors (1). Factors that increase the risk for breast cancer include older age, family history of breast cancer, older age at the time of first birth, younger age at menarche, and history of breast biopsy. Women 40 to 49 years of age who have any of the following risk factors have a higher risk for breast cancer than the average 50-year-old woman: 2 first-degree relatives with breast cancer; 2 previous breast biopsies; 1 first-degree relative with breast cancer and 1 previous breast biopsy; previous diagnosis of breast cancer, ductal carcinoma in situ (DCIS), or atypical hyperplasia; previous chest irradiation (1); or BRCA1 or BRCA2 mutation (2, 3). A family history can also help identify women who may have BRCA mutations that place them at substantially higher risk for breast and other types of cancer (Table). These women should be referred for counseling and recommendations specific to this population, as recommended by the U.S. Preventive Services Task Force (USPSTF) (4). Risk assessments should be updated periodically, particularly in women whose family history changes (for example, a relative receives a diagnosis of breast or ovarian cancer) and in women who choose not to have regular screening mammography. Although no evidence supports specific intervals, we encourage clinicians to update the woman's risk assessment every 1 to 2 years. Table. Family History Patterns Associated with an Increased Risk for BRCA1 or BRCA2Gene Mutations* The risk for invasive breast cancer can be estimated quantitatively by using the Web site calculator provided by the National Institutes of Health (NIH) (bcra.nci.nih.gov\/brc\/q1.htm) (1). This calculator is based on the Gail model, which takes into account many of the risk factors previously mentioned. However, clinicians who use the Gail model should be aware of its limitations. Although the model accurately predicts the risk for cancer for groups of women, its ability to discriminate between higher and lower risk for an individual woman is limited (5, 6). This limitation occurs because many women have similar, relatively low absolute risks for invasive breast cancer over 5 years, which makes discrimination among levels of risk difficult for an individual woman. Recommendation 2: Clinicians should inform women 40 to 49 years of age about the potential benefits and harms of screening mammography. Screening mammography for women 40 to 49 years of age is associated with both benefits and potential harms. The most important benefit of screening mammography every 1 to 2 years in women 40 to 49 years of age is a potential decrease in breast cancer mortality. A recent meta-analysis estimated the relative reduction in the breast cancer mortality rate to be 15% after 14 years of follow-up (relative risk, 0.85 [95% credible interval {CrI}, 0.73 to 0.99]) (7). An additional large randomized clinical trial of screening mammography in women 40 to 49 years of age found a similar decrease in the risk for death due to breast cancer, although the decrease did not reach statistical significance (relative risk, 0.83 [95% CI, 0.66 to 1.04]) (8). Potential risks of mammography include false-positive results, diagnosis and treatment for cancer that would not have become clinically evident during the patient's lifetime, radiation exposure, false reassurance, and procedure-associated pain. False-positive mammography can lead to increased anxiety and to feelings of increased susceptibility to breast cancer, but most studies found that anxiety resolved quickly after the evaluation. Recommendation 3: For women 40 to 49 years of age, clinicians should base screening mammography decisions on benefits and harms of screening, as well as on a woman's preferences and breast cancer risk profile. Because the evidence shows variation in risk for breast cancer and benefits and harms of screening mammography based on an individual woman's risk profile, a personalized screening strategy based on a discussion of the benefits and potential harms of screening and an understanding of a woman's preferences will help identify those who will most benefit from screening mammography. For many women, the potential reduction in breast cancer mortality rate associated with screening mammography will outweigh other considerations. For women who do not wish to discuss the screening decision, screening mammography every 1 to 2 years in women 40 to 49 years of age is reasonable. Important factors in the decision to undergo screening mammography are women's preferences for screening and the associated outcomes. Concerns about risks for breast cancer or its effect on quality of life will vary greatly among women. Some women may also be particularly concerned about the potential harms of screening mammography, such as false-positive mammograms and the resulting diagnostic work-up. When feasible, clinicians should explore women's concerns about breast cancer and screening mammography to help guide decision making about mammography. The relative balance of benefits and harms depends on women's concerns and preferences and on their risk for breast cancer. Clinicians should help women to judge the balance of benefits and harms from screening mammography. Women who are at greater-than-average absolute risk for breast cancer and who are concerned that breast cancer would have a severely adverse effect on quality of life may derive a greater-than-average benefit from screening mammography. Women who are at substantially lower-than-average risk for breast cancer or who are concerned about potential risks of mammography may derive a less-than-average benefit from screening mammography. If a woman decides to forgo mammography, clinicians should readdress the decision to have screening every 1 to 2 years. Recommendation 4: We recommend further research on the net benefits and harms of breast cancer screening modalities for women 40 to 49 years of age. Methodological issues associated with existing breast cancer screening trials, such as compliance with screening, lack of statistical power, and inadequate information about inclusion or exclusion criteria and study population, heighten the need for high-quality trials to confirm the effectiveness of screening mammography in women in this age group. Furthermore, harms of screening in this age group, such as pain, radiation exposure, and adverse outcomes related to false-positive results, should also be studied. Introduction Breast cancer is the second leading cause of cancer-related death among women in the United States. In 2005, an estimated 211240 new cases of invasive breast cancer will be diagnosed, and 40410 women will die of the disease (9). Screening mammography reduces breast cancer mortality in women 50 to 70 years of age. Although 25% of all diagnosed cases are among women younger than 50 years of age (9), screening mammography in this age group has remained a topic of debate because of the difficulty in determining the benefit of mammography in this age group. A meta-analysis performed for the USPSTF estimated that screening mammography every 1 to 2 years in women 40 to 49 years of age resulted in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (7). However, the 95% credible interval for this estimate is wide and indicates that the reduction could be as much as 27% or as little as 1%. This relative risk reduction corresponds to about 5.6 deaths prevented per 10000 women screened (95% CrI, 0.9 to 13.1 deaths prevented per 10000 women screened). Because screening mammography is also associated with potential harms, a discussion of risks (biopsies, surgery, radiation exposure, false-positive results, and false reassurance), benefits (early detection of breast cancer), and patient preferences should be the basis for screening decisions. The purpose of this guideline is to present the available evidence and to increase clinicians' understanding of the benefits and risks of screening mammography in women 40 to 49 years of age. The target audience is clinicians who are caring for women in this age group. The target patient population is all women 40 to 49 years of age. These recommendations are based on the systematic review of the evidence in the background paper in this issue (6). The systematic evidence review does not include breast cancer risk in men and genetic risk markers, such as BRCA. The goal for this guideline was to answer the following questions: 1. What are the benefits of screening mammography in women 40 to 49 years of age? 2. What are the risks associated with screening mammography in women 40 to 49 years of age? 3. Does the balance of risks and benefits vary according the individual woman's characteristics? 4. What are the methodological issues that affect the interpretation of the results of previous meta-analyses? Benefits Of the 8 currently published meta-analyses, 7 estimated that screening women 40 to 49 years of age reduced breast cancer mortality rates, but only 3 of these found a statistically significant reduction (7). The most recent meta-analysis found that screening mammography every 1 to 2 years in women 40 to 49 years of age results in a 15% decrease in breast cancer mortality rate after 14 years of follow-up (relative risk, 0.85 [95% CI, 0.73 to 0.99]) (7). However, concerns about study quality and whether some of the observed benefit may be due to screening that occurred after the women turned 50 years of age complicate interpretation of the evidence. The use of death due to breast cancer as an end point can\n\n",
                "DataExportTag": "CAN1063853",
                "QuestionID": "QID326",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Screening Mammography for Women 40 to 49 Years of Age: A Clinical Practice Guideline from the Ame...",
                "Choices": {
                    "1": {
                        "Display": "\"Women's Reproductive Health and Gynecological Disorders\""
                    },
                    "2": {
                        "Display": "endometrial_cancer, uterine, endometriosis, estrogen, progesterone, progesterone_receptor, aromatase, uterus, epirubicin_cyclophosphamide, estradiol, endometrial_hyperplasia, hysterectomy, endometrioid, progestin, vaginal"
                    },
                    "3": {
                        "Display": "\"Liver Cancer Treatment and Management\""
                    },
                    "4": {
                        "Display": "hepatocellular_carcinoma, liver, transplant, tace, lt, recurrence, portal_vein, sorafenib, hepatectomy, resection, transarterial_chemoembolization, transcatheter_arterial, milan_criterion, chemoembolization, rfa"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID117",
            "SecondaryAttribute": "SEARCH METHODS The following databases were searched, the Cochrane Central Register of Controlled...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SEARCH METHODS\nThe following databases were searched, the Cochrane Central Register of Controlled Trials (CENTRAL) (Issue 1, 2013), MEDLINE (1948 to March, week 10, 2013) and EMBASE (1970 to 2013, week 10). Reference lists of all identified studies were searched. Two journals, the Journal of Neuro-Oncology and Neuro-oncology, were handsearched from 1991 to 2013, including all conference abstracts. Neuro-oncologists, trial authors and manufacturers were contacted regarding ongoing and unpublished trials.\n\n",
                "DataExportTag": "45",
                "QuestionID": "QID117",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SEARCH METHODS The following databases were searched, the Cochrane Central Register of Controlled...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID118",
            "SecondaryAttribute": "SELECTION CRITERIA Study participants included patients of all ages with a presumed new or recurr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SELECTION CRITERIA\nStudy participants included patients of all ages with a presumed new or recurrent brain tumour (any location or histology) from clinical examination and imaging (computed tomography (CT), magnetic resonance imaging (MRI) or both). Image guidance interventions included intra-operative MRI (iMRI); fluorescence guided surgery; neuronavigation including diffusion tensor imaging (DTI); and ultrasonography. Included studies had to be randomised controlled trials (RCTs) with comparisons made either with patients having surgery without the image guidance tool in question or with another type of image guidance tool. Subgroups were to include high grade glioma; low grade glioma; brain metastasis; skull base meningiomas; and sellar or parasellar tumours.\n\n",
                "DataExportTag": "46",
                "QuestionID": "QID118",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SELECTION CRITERIA Study participants included patients of all ages with a presumed new or recurr...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID136",
            "SecondaryAttribute": "SETTINGS AND DESIGN A retrospective audit of 125 patients with squamous cell carcinoma of buccal...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SETTINGS AND DESIGN\nA retrospective audit of 125 patients with squamous cell carcinoma of buccal mucosa at a tertiary cancer center.\n\n",
                "DataExportTag": "64",
                "QuestionID": "QID136",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SETTINGS AND DESIGN A retrospective audit of 125 patients with squamous cell carcinoma of buccal...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID165",
            "SecondaryAttribute": "Signaturit We want to change the way we sign contracts and other documents, and in doing so, the...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Signaturit We want to change the way we sign contracts and other documents, and in doing so, the whole process that accompanies them. Currently, when documents need signatures, we use the post\/ courier to send them and then wait, sometimes days, to get the signed copies back. This is a manual process, slow, costly, prone to human error, with a negative impact on the environment and on the bottom line.Research shows that companies are not ready to switch to e-signature solutions for 3 reasons: (a) lack of security , (b) usability \u2013and (c) add minimal value. Our e-signature solution is:(a) very secure \u2013 we use a unique, proprietary, multi-authentication process, including biometric data, encrypting and sending material using a secure channel and logging every action,(b) easy to use from any device \u2013 the user can sign with a finger, mouse or stylus and send the signed material via regular email, with no special hardware or software installation and(c) provides significant value added through Smart contracts that take out the inefficient administrative processes around contract implementation.Our solution will finally allow companies to stop using a manual process, reducing paper use, improving companies\u2019 competitiveness and boosting the economy. Signaturit is the first e-signature service that guarantees 100% legal validity in business digital transactions, minimises the risk of cybercrimes and drastically improves productivity. During this project we aim to add unique advanced biometric and blockchain encryption tools that ensure an unreachable level of cyber security in e-mail transactions, while Smart contracts will ensure that productivity is optimised.We are Signaturit Solutions S.L., an SME that recently obtained the SME Instr. Phase 1 funding. After completion of the SME Inst. Phase 2 project, we aim to aggressively market our innovation to 5 European countries, with projections to gain an annual net profit of \u20ac17M by 2024 while directly creating 30 new jobs.\n\n",
                "DataExportTag": "COR36983",
                "QuestionID": "QID165",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Signaturit We want to change the way we sign contracts and other documents, and in doing so, the...",
                "Choices": {
                    "1": {
                        "Display": "\"Nuclear Security and Digital Infrastructure Management\""
                    },
                    "2": {
                        "Display": "nuclear, cybersecurity, critical_infrastructure, interoperable, blockchain, certification, marketplace, toolbox, federation, digital_twin, factory, security_privacy, experimentation, deploy, cross"
                    },
                    "3": {
                        "Display": "\"Mobile Banking and E-commerce\""
                    },
                    "4": {
                        "Display": "mobile, blockchain, payment, personal, store, bank, transaction, insurance, biometric, smartphone, retailer, purchase, retail, pay, e_commerce"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID147",
            "SecondaryAttribute": "SIMULATION MODEL A simulation model represents the actual system and assists in visualising and e...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SIMULATION MODEL\nA simulation model represents the actual system and assists in visualising and evaluating the performance of the system under different scenarios without interrupting the actual system. Building a proper simulation model of a system consists of the following steps. Observing the system to understand the flow of the entities, key players, availability of resources and overall generic framework.Collecting the data on the number and type of entities, time consumed by the entities at each step of their journey, and availability of resources.After building the simulation model it is necessary to confirm that the model is valid. (ABSTRACT TRUNCATED)\n\n",
                "DataExportTag": "75",
                "QuestionID": "QID147",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SIMULATION MODEL A simulation model represents the actual system and assists in visualising and e...",
                "Choices": {
                    "1": {
                        "Display": "\"FPGA and Chip Design\""
                    },
                    "2": {
                        "Display": "fpga, chip, processor, accelerator, field_programmable, reconfigurable, dsp, gate_array, power_consumption, circuit, clock, instruction, parallel, soc, energy"
                    },
                    "3": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "4": {
                        "Display": "cloud, edge, server, scheduling, job, cloud_computing, workload, mobile, distribute, deep_learning, scheduler, iot, edge_computing, user, center"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID210",
            "SecondaryAttribute": "SIMULATION MODEL A simulation model represents the actual system and assists in visualising and e...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SIMULATION MODEL\nA simulation model represents the actual system and assists in visualising and evaluating the performance of the system under different scenarios without interrupting the actual system. Building a proper simulation model of a system consists of the following steps. Observing the system to understand the flow of the entities, key players, availability of resources and overall generic framework.Collecting the data on the number and type of entities, time consumed by the entities at each step of their journey, and availability of resources.After building the simulation model it is necessary to confirm that the model is valid. (ABSTRACT TRUNCATED)\n\n",
                "DataExportTag": "50",
                "QuestionID": "QID210",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SIMULATION MODEL A simulation model represents the actual system and assists in visualising and e...",
                "Choices": {
                    "1": {
                        "Display": "\"FPGA and Chip Design\""
                    },
                    "2": {
                        "Display": "fpga, chip, processor, accelerator, field_programmable, reconfigurable, dsp, gate_array, power_consumption, circuit, clock, instruction, parallel, soc, energy"
                    },
                    "3": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "4": {
                        "Display": "cloud, edge, server, scheduling, job, cloud_computing, workload, mobile, distribute, deep_learning, scheduler, iot, edge_computing, user, center"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID271",
            "SecondaryAttribute": "SIMULATION MODEL A simulation model represents the actual system and assists in visualising and e...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SIMULATION MODEL\nA simulation model represents the actual system and assists in visualising and evaluating the performance of the system under different scenarios without interrupting the actual system. Building a proper simulation model of a system consists of the following steps. Observing the system to understand the flow of the entities, key players, availability of resources and overall generic framework.Collecting the data on the number and type of entities, time consumed by the entities at each step of their journey, and availability of resources.After building the simulation model it is necessary to confirm that the model is valid. (ABSTRACT TRUNCATED)\n\n",
                "DataExportTag": "50",
                "QuestionID": "QID271",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SIMULATION MODEL A simulation model represents the actual system and assists in visualising and e...",
                "Choices": {
                    "1": {
                        "Display": "\"FPGA and Chip Design\""
                    },
                    "2": {
                        "Display": "fpga, chip, processor, accelerator, field_programmable, reconfigurable, dsp, gate_array, power_consumption, circuit, clock, instruction, parallel, soc, energy"
                    },
                    "3": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "4": {
                        "Display": "cloud, edge, server, scheduling, job, cloud_computing, workload, mobile, distribute, deep_learning, scheduler, iot, edge_computing, user, center"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID134",
            "SecondaryAttribute": "Skin involvement and ipsilateral nodal metastasis as a predictor of contralateral nodal metastasi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Skin involvement and ipsilateral nodal metastasis as a predictor of contralateral nodal metastasis in buccal mucosa cancers. CONTEXT\nIn view of low incidence of contralateral nodal metastasis and increase in the morbidity, the opposite neck is not routinely addressed. However, contralateral nodal metastasis is seen frequently in a certain group of patients. Identifying those factors associated with higher chances of contralateral nodal metastasis may help in optimizing the treatment.\n\n",
                "DataExportTag": "CAN689839",
                "QuestionID": "QID134",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Skin involvement and ipsilateral nodal metastasis as a predictor of contralateral nodal metastasi...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID361",
            "SecondaryAttribute": "SMART LIVING HOMES - WHOLE INTERVENTIONS DEMONSTRATOR FOR PEOPLE AT HEALTH AND SOCIAL RISKS The m...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SMART LIVING HOMES - WHOLE INTERVENTIONS DEMONSTRATOR FOR PEOPLE AT HEALTH AND SOCIAL RISKS The main objective of the Project is to create a GATEKEEPER, that connects healthcare providers, businesses, entrepreneurs, elderly citizens and the communities they live in, in order to originate an open, trust-based arena for matching ideas, technologies, user needs and processes, aimed at ensuring healthier independent lives for the ageing populations. By 2022, GATEKEEPER will be embodied in an open source, European, standard-based, interoperable and secure framework available to all developers, for creating combined digital solutions for personalised early detection and interventions that (i) harness the next generation of healthcare and wellness innovations; (ii) cover the whole care continuum for elderly citizens, including primary, secondary and tertiary prevention, chronic diseases and co-morbidities; (iii) straightforwardly fit \u201cby design\u201d with European regulations, on data protection, consumer protection and patient protection (iv) are subjected to trustable certification processes; (iv) support value generation through the deployment of advanced business models based on the VBHC paradigm.GATEKEEPER will demonstrate its value by scaling up, during a 42-months work plan, towards the deployment of solutions that will involve ca 40.000 elderly citizens, supply and demand side (authorities, institutions, companies, associations, academies) in 8 regional communities, from 7 EU member states.\n\n",
                "DataExportTag": "COR21586",
                "QuestionID": "QID361",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SMART LIVING HOMES - WHOLE INTERVENTIONS DEMONSTRATOR FOR PEOPLE AT HEALTH AND SOCIAL RISKS The m...",
                "Choices": {
                    "1": {
                        "Display": "\"Medical Diagnosis and Healthcare Screening\""
                    },
                    "2": {
                        "Display": "diagnosis, detection, biomarker, point_care, patient, blood, hospital, sample, assay, medical_device, medical, detect, healthcare, screening, biosensor"
                    },
                    "3": {
                        "Display": "\"Public Health and Preventive Medicine\""
                    },
                    "4": {
                        "Display": "health, healthcare, cohort, biomarker, exposure, prevention, genetic, age, child, patient, mental_health, pregnancy, risk_factor, population, lifestyle"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID308",
            "SecondaryAttribute": "Smoking and Deaths between 40 and 70 Years of Age in Women and Men Context The best way to estima...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Smoking and Deaths between 40 and 70 Years of Age in Women and Men Context The best way to estimate deaths from smoking is to observe people over many decades, as done in a 50-year study of male British physicians. No such study exists for women. Content A 25-year study of residents of rural Norway included 24505 women. Of these women, 2333 women died at age 40 to 70 years (9.4% of never smokers and 18.7% of continuing smokers), mostly from cardiovascular disease and cancer. Continuing heavy smokers survived 1.4 fewer years than never smokers. Mortality rates in smokers and nonsmokers were lower in women than in men. Cautions Participants were white and lived in rural areas. Implications Active smoking has a large effect on women's longevity in middle age. The Editors An early report on smoking and longevity appeared more than 60 years ago (1), and it suggested a clear survival advantage of nonsmokers over heavy smokers in midlife. More than a half century later, another report presented precise survival curves for men by smoking habits (2). In the time between these reports, the historic dimension of the health consequences of smoking was increasingly realized (3-9). The World Health Organization now recognizes tobacco use as the major preventable cause of adult death, and about 5 million deaths worldwide each year (8.8% of all deaths annually) are attributed to smoking (10). More than half of smoking-attributable deaths worldwide (56%) occur in people younger than 70 years of age and account for 13% of deaths in people 30 to 69 years of age (9). On a global scale, the death toll from tobacco use is increasing (9, 11), and accurately quantifying and updating the number of premature deaths due to smoking is an important task. Estimated numbers of deaths caused by smoking on a national, international, or global level have been calculated by combining relative risks for smokers versus nonsmokers for different causes of death from the second large prospective American Cancer Society Cancer Prevention Study (CPS-II) (12, 13) with direct or indirect (via lung cancer mortality rates) estimates of smoking histories and national mortality statistics (6, 9, 14-17). However, estimating the number of smoking-attributable deaths that occur in a population in a given year depends on the maturity of the smoking epidemic and will predict the lifetime mortality experience of smoking men and women only under certain conditions (18). To predict individual risks precisely, follow-up studies of individuals through their lifetime are needed. Such studies are rare and have not always estimated direct long-term risk for death. Among studies of survival and smoking (1, 19, 20), the British doctors' study (2, 21) has provided the longest follow-up (50 years) and has allowed for direct and precise calculation of survival of men through middle age and beyond. Using data from nearly 50000 Norwegian adults who were born in the second quarter of the 20th century and were followed in the last quarter, we studied smoking, death in middle age (40 to 70 years of age), and causes of death in both women and men. Methods Study Sample Between 1974 and 1978, all men and women 35 to 49 years of age who were residing in the 3 rural Norwegian counties of Oppland, Sogn og Fjordane, and Finnmark were invited to a cardiovascular health screening examination. Excluding 783 men and 215 women who were temporarily absent from their residence, 91.4% of the men and 94.2% of the women gave a self-report of their past and current smoking habits and were screened for cardiovascular risk factors (22, 23). Thus, our report is based on mortality follow-up between 40 and 70 years of age (middle age) of 24505 women and 25034 men (who were born between 1925 and 1941). Approximately 92% of the respondents attended a second survey and 65% attended a third survey about 5 years and 10 years after the first examination, respectively. We grouped participants into never smokers (no report of smoking at any examination); former smokers; or continuing smokers of 1 to 9 cigarettes, 10 to 19 cigarettes, or 20 or more cigarettes per day (heavy smokers). As a supplementary analysis to facilitate comparison with the 40-year follow-up of the British doctors' study (2), we also studied men who reported smoking 25 or more cigarettes daily. We classified persons as continuing smokers on the basis of the information from the last of up to 3 examinations. On the basis of information given about time since smoking cessation and changes among the 3 examinations, we separated the former smokers into 3 groups on the basis of their age when they stopped smoking (<40 years, 40 to 49 years, or 50 to 59 years). Mortality Follow-up We performed mortality follow-up by record linkage using the Norwegian 11-digit birth number (date of birth plus a 5-digit identifier), which is unique to each person residing in Norway, to obtain the date and underlying cause of death kept by Statistics Norway. Loss to follow-up was due only to emigration, and we censored 93 men and 99 women (0.4%) on their registered date of taking residence abroad. Our report is based on mortality follow-up through the year 2000. We classified the 7013 deaths in middle age into 7 groups on the basis of the underlying cause of death coded at Statistics Norway by using the 8th, 9th, or 10th revisions of the International Classification of Diseases (ICD) and European shortlist categories (24). The groups were deaths due to 1) lung cancer (151 deaths in women and 316 deaths in men); 2) other types of smoking-related cancer, including cancer of the lip, oral cavity, pharynx, larynx, esophagus, stomach, liver, pancreas, cervix uteri, kidney, bladder, and acute myelogenous leukemia (262 deaths in women and 386 deaths in men); 3) other types of cancer (796 deaths in women and 648 deaths in men); 4) cardiovascular disease, including sudden death (624 deaths in women and 2252 deaths in men); 5) other medical causes (358 deaths in women and 505 deaths in men); 6) alcohol abuse and chronic liver disease (20 deaths in women and 115 deaths in men); and 7) accidents and violence (122 deaths in women and 458 deaths in men). Types of smoking-related cancer were those reported to be associated with smoking in the 2004 U.S. Surgeon General report (25). Statistical Analysis We estimated KaplanMeier survival curves from 40 to 70 years of age separately for men and women, and for the various smoking categories, using age as the time scale. We performed computations with the survfit function of S-PLUS, version 6.1 (Insightful Corp., Seattle, Washington). The limited number of individuals who were recruited before 40 years of age led to unstable survival estimates. For this reason, we calibrated all survival analyses to start at 100% at age 40 years. We estimated mortality hazard (rate) ratios comparing the persons in various smoking categories with never smokers by using the Cox proportional hazards model with age as the time scale and calculated with and without adjustment for potential confounding. The confounders were the following variables, which were all registered at the first examination: marital status (not married or married), duration of education (0 to 9 years, 10 years, 11 to 12 years, or 13 years), county of residence, and physical activity in leisure time (sedentary, moderate, intermediate, or intensive). We also used the Cox model to estimate womenmen mortality hazard ratios overall and with the different cause-of-death groups as outcomes. We performed these analyses for never smokers and for continuing smokers with additional adjustment for age when smokers began smoking and for number of cigarettes smoked per day. We performed tests for difference in regression coefficients between men and women and between smokers and nonsmokers (interaction tests), with the variance estimated as the sum of each coefficient's variance. We calculated adjusted survival first by predicting survival probabilities for each individual from the Cox model with confounders and then by averaging over all individuals in the sample (26, 27). We did this separately for women and men. We computed the average number of years of life lost between 40 and 70 years of age by summing person-time lost over the survival curves. To compute cause-specific mortality probabilities for the interval of 40 to 70 years of age, we analyzed the different cause-of-death groups in a competing risk framework. Again, we performed the computations separately for men and women and for the different smoking categories. Let Pxj be the probability of death of cause j between 40 and 70 years of age for a person in smoking group x. This probability can be estimated from the following formula: where Sx(t) is the KaplanMeier estimate of all-cause survival in smoking group x and xj (t) is the NelsonAalen estimator of cumulative cause-specific hazard, which is computed by using cause j as the outcome and treating all other causes as censoring (28). Note that the probabilities Pxj sum to Px , the all-cause probability of dying between 40 and 70 years of age for a person in smoking group x. The NelsonAalen estimator is used rather than the KaplanMeier, since cause-specific KaplanMeier estimates do not correctly account for deaths from other causes. We estimated the probability of dying between 40 and 70 years of age as a function of age when smoking began. We calculated these probabilities separately for men and women and for the 3 smoking categories (1 to 9 cigarettes per day, 10 to 19 cigarettes per day, and 20 cigarettes per day). We obtained the estimates by fitting a Cox model with a P-spline function (pspline function of S-PLUS) (29) to show the effect of age when smoking began. The P-spline allows for a flexible effect estimation, avoiding the need to assume a prespecified functional relationship between exposure and outcome, such as linear or polynomial (30). We then estimated the predicted survival from 40 to 70 years of age by using the S-PLUS survfit funct\n\n",
                "DataExportTag": "CAN1207323",
                "QuestionID": "QID308",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Smoking and Deaths between 40 and 70 Years of Age in Women and Men Context The best way to estima...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\""
                    },
                    "2": {
                        "Display": "mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "3": {
                        "Display": "\"Breast and Ovarian Cancer Screening and Hormone Therapy\""
                    },
                    "4": {
                        "Display": "breast, woman, screening, ovarian_cancer, mammography, hormone, estrogen_receptor, postmenopausal_woman, postmenopausal, hrt, estrogen, mammogram, invasive, family_history, ht"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID167",
            "SecondaryAttribute": "Solidarities and migrants' routes across Europe at large Unauthorized migration has always been a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Solidarities and migrants' routes across Europe at large Unauthorized migration has always been a contentious issue in the EU. Yet, many of the policies adopted thus far have proved ineffective in containing arrivals and blocking circulations of migrants and refugees within the EU. Furthermore, unauthorized migration has continued to challenge EU borders even during the pandemic.SOLROUTES\u00e2\u20ac\u2122 core research question is: how can the turbulence, persistence, and intensity of unauthorized movements and of the production of migrants\u00e2\u20ac\u2122 routes across \u00e2\u20ac\u0153Europe at Large\u00e2\u20ac\u009d \u00e2\u20ac\u201c originating from the externalization of EU borders to non-EU countries \u00e2\u20ac\u201c be understood?The project addresses this challenge from an innovative angle, through an ethnographic exploration of the nexus between unauthorized movements and the networks of solidarity with migrants in transit, which involve actors and practices that have been addressed just recently in migration studies. This will be achieved by exploring and observing 50 crucial nodes in migration routes within Europe, in selected countries on its fringes (Turkey, Tunisia, Morocco) and in the Outermost Regions of the EU (French Guiana, Mayotte).SOLROUTES will aim to: a) explore the functioning, articulations, and representations of local and trans-local networks which share shelter, knowledge and resources with migrants in transit; b) assess how routes are continuously reshaped by the encounter between unauthorized movements and solidarity networks; c) develop a large-scale multi-sited ethnographic approach based on live and collaborative methods and digital research; d) develop an innovative theory in migrations studies on the nexus between solidarity with migrants in transit and unauthorized movements, contributing to structure the emerging perspective of solidarities studies; e) generate Special Features \u00e2\u20ac\u201c a web series, an art exhibit, a graphic novel \u00e2\u20ac\u201c linked to the research, offering new narratives for policy makers, local communities, and other relevant audiences in the spirit of public sociology.\n\n",
                "DataExportTag": "COR28673",
                "QuestionID": "QID167",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Solidarities and migrants' routes across Europe at large Unauthorized migration has always been a...",
                "Choices": {
                    "1": {
                        "Display": "\"Research Funding and University Administration\""
                    },
                    "2": {
                        "Display": "cofund, national_contact, ncps, preparatory_phase, university, coordinator, administrative, smart_specialization, health, critical_mass, centres, website, strategic_partnership, preparation, increase_visibility"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID228",
            "SecondaryAttribute": "Solidarities and migrants' routes across Europe at large Unauthorized migration has always been a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Solidarities and migrants' routes across Europe at large Unauthorized migration has always been a contentious issue in the EU. Yet, many of the policies adopted thus far have proved ineffective in containing arrivals and blocking circulations of migrants and refugees within the EU. Furthermore, unauthorized migration has continued to challenge EU borders even during the pandemic.SOLROUTES\u00e2\u20ac\u2122 core research question is: how can the turbulence, persistence, and intensity of unauthorized movements and of the production of migrants\u00e2\u20ac\u2122 routes across \u00e2\u20ac\u0153Europe at Large\u00e2\u20ac\u009d \u00e2\u20ac\u201c originating from the externalization of EU borders to non-EU countries \u00e2\u20ac\u201c be understood?The project addresses this challenge from an innovative angle, through an ethnographic exploration of the nexus between unauthorized movements and the networks of solidarity with migrants in transit, which involve actors and practices that have been addressed just recently in migration studies. This will be achieved by exploring and observing 50 crucial nodes in migration routes within Europe, in selected countries on its fringes (Turkey, Tunisia, Morocco) and in the Outermost Regions of the EU (French Guiana, Mayotte).SOLROUTES will aim to: a) explore the functioning, articulations, and representations of local and trans-local networks which share shelter, knowledge and resources with migrants in transit; b) assess how routes are continuously reshaped by the encounter between unauthorized movements and solidarity networks; c) develop a large-scale multi-sited ethnographic approach based on live and collaborative methods and digital research; d) develop an innovative theory in migrations studies on the nexus between solidarity with migrants in transit and unauthorized movements, contributing to structure the emerging perspective of solidarities studies; e) generate Special Features \u00e2\u20ac\u201c a web series, an art exhibit, a graphic novel \u00e2\u20ac\u201c linked to the research, offering new narratives for policy makers, local communities, and other relevant audiences in the spirit of public sociology.\n\n",
                "DataExportTag": "COR28673",
                "QuestionID": "QID228",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Solidarities and migrants' routes across Europe at large Unauthorized migration has always been a...",
                "Choices": {
                    "1": {
                        "Display": "\"Research Funding and University Administration\""
                    },
                    "2": {
                        "Display": "cofund, national_contact, ncps, preparatory_phase, university, coordinator, administrative, smart_specialization, health, critical_mass, centres, website, strategic_partnership, preparation, increase_visibility"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID402",
            "SecondaryAttribute": "Solving Job-Shop Scheduling Problem Using Genetic Algorithm Approach An effective job shop schedu...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Solving Job-Shop Scheduling Problem Using Genetic Algorithm Approach An effective job shop scheduling (JSS) in the manufacturing industry is helpful to meet the production demand and reduce the production cost, and to improve the ability to compete in the ever increasing volatile market demanding multiple products. In so many combinatorial optimization problems, job shop scheduling problems have earned a reputation for being difficult to solve. Job-shop scheduling is essentially an ordering problem. A new encoding scheme for a classic job-shop scheduling problem is presented. The aim is to find an allocation for each job and to define the sequence of jobs on each machine so that the resulting schedule has a minimal completion time. Genetic algorithm that has demonstrated considerable success in providing efficient solutions to many non polynomial-hard optimization problems is used to solve job-shop scheduling problem. The schedules given by genetic algorithms are constructed using a priority rule and under several constraints. After a schedule is obtained a checking operation is applied to ensure that the solution is feasible. The approach is tested on a set of instances. The results validate the effectiveness of the algorithm. Introduction Scheduling problems are usually approached with a combination of search techniques and heuristics. Scheduling belongs to NP-complete problems. Such problems are likely to be unmanageable and cannot be solved by combinatorial search techniques. Moreover, heuristics alone cannot guarantee the best solution. They involve a competition for limited resources; as a result, they are complicated by many constraints [1]. Manufacturing environments in the real world are subject to many sources of change which are typically treated as random occurrences, such as new job releases, machine breakdowns, etc. In most manufacturing environments, there is a requirement to use the available resources as Solving Job-Shop Scheduling Problem Using Genetic Algorithm Approach.. Wathiq N. Abdullah \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0646 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0647 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0647\u062c\u0645 \u062f\u0630\u0639\u0646 \u0646\u0648\u0639\u0628\u0633\u0646 3122 242 efficiently as possible [2]. Due to their dynamic nature, scheduling problems are rather computationally complex -i.e., the time required to compute an optimal schedule increases exponentially with the size of the problem. [3] They are complex tasks that can be formulated using a constraint-based representation.[4] Reasons for scheduling complexity include [5]: (i) Scheduling is a feasibility problem. The final solution must accomplish all the problem constraints. Another objective to be satisfied is the optimization of an evaluation function, adjusting to certain criteria as cost, lateness, process time, inventory time, etc. (ii) Some scheduling problems have many constraints due to the unavailability of resources, due dates, etc. [4] Scheduling problem involves allocating limited resources to operations (activities) over time. [4] In order to complete a schedule, all the operations must be completed. [2] Job shop scheduling problem is one type of the scheduling problems.[6] ((that)) is a very important practical problem. [7] It is among the hardest combinatorial optimization problems[8]. Efficient methods of solving it can have major effects on profitability product quality. The problem is to minimize the total elapsed time between the beginning of the first operation and the completion of the last operation (the makespan). Other measures of scheduling quality exist, but shortest makespan is the simplest and most widely used criterion. In general, the difficulty of the Job-Shop Scheduling problem makes it very hard for conventional search-based methods to find near-optima in reasonable time. This has led to recent interest in using genetic algorithms to address these problems. [7] The remainder of this paper is organized as follows: the job-shop scheduling problem, the design of the system of solving job-shop scheduling problem using genetic algorithm including the representation of the chromosome, and finally, the results of the system and some concluding remarks are given. JobShop Scheduling Problem using Genetic Algorithm Since job shop scheduling is actually a constrained multi-sequencing problem, a lot of research concentrates on the problem representation by encoding operation sequences for the machines [9]. A schedule is defined by a complete and feasible ordering of operations to be processed on each machine [10]. The JSP is a scheduling problem that considers M different machines and N different jobs. Each of the jobs consists of Q operations and each of Solving Job-Shop Scheduling Problem Using Genetic Algorithm Approach.. Wathiq N. Abdullah \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0646 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0647 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0647\u062c\u0645 \u062f\u0630\u0639\u0646 \u0646\u0648\u0639\u0628\u0633\u0646 3122 243 the operations requires a different machine. All the operations of each job are processed in a fixed processing order [11] which specifies the precedence restrictions [3]. The operations of a job are totally ordered so that no operation of a job can start before the completion of its predecessor [12]. Two operations cannot be scheduled at the same time if they both require the same machine [10]. Each operation is characterized by the required machine and the processing time[11]. This time is known a job arrive at the shop all jobs are ready to start at time zero [3]. A job is processed on one machine at a time. Machines are available continuously [11] and there are no machine breakdowns [10]. Scheduling systems typically rely on priority rules, which have therefore become the subject of intense study [13]. A schedule builder produces a single schedule on the basis of a certain priority rule [14]. A priority rule is used to select the operation to be scheduled at each step of the algorithm, among the set of unscheduled operations. In some cases, the operations are selected according to a predetermined order, which is equivalent to assign to each operation a unique priority value. The operation with the smallest priority value has the highest priority. This priority value can be computed in a static way (it remains constant throughout the process) [12]. Two kinds of constraints need to be considered for the Job-Shop Problem [8] as follows: (i) Operation precedence constraint for a given job: As usual, let cjk denote the completion time of job j on machine k and let tjk denote the processing time of job j on machine k. For a job i, if the processing on machine h precedes that on machine k, we need the following constraint: cik \u2212tik >= cih ........ (1) (ii) Operation un-overlapping constraint for a given machine: For two jobs i and j, both need to be processed on machine k. If job i comes before job j, we need the following constraint [11]: cjk \u2212cik >= tjk ........ (2) The goal of the Job-Shop Problem is to choose for each operation a suitable machine and a starting time so that the maximum completion time Cmax (the makespan) is minimized. [15, 16, 17] The genetic algorithm (GA) that has received a rapidly growing interest in the combinatorial optimization community and has shown great power with very promising results from experimentation and practice of Solving Job-Shop Scheduling Problem Using Genetic Algorithm Approach.. Wathiq N. Abdullah \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0646 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629 \u0647 \u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0629\u0647\u062c\u0645 \u062f\u0630\u0639\u0646 \u0646\u0648\u0639\u0628\u0633\u0646 3122 244 many engineering areas [11] has been used with increasing frequency in the context of job shop scheduling problems [3]. Encoding a Schedule and Representation of a Chromosome As we mentioned earlier, the job-shop scheduling problem is a sequencing problem of M machines and N jobs. Each job consists of a chain of operations. All the operations of each job are processed in a fixed processing order. Suppose that the available set of total operations of N jobs be OP, the available set of machines is M, and the indices of the operation op be i and k, then we have: OP = {op ik | i = 1, 2, . . ., N and k = 1, 2, . . ., M } ........ (3) Where i is the job number, and k indicates the operation number of jobi. The ik indices of the operation op are encoded as a sequence number (index). For example, consider the problem with two jobs, and each job has three operations. For this problem, the OP= (op11, op12, op13, op21, op22, op23). Note that the op11, op12, and op13 are elements of job 1, the op21 and op22 are of job 2. Assign a number, from 1 to 6, to each operation in the order of job indices, we have, 1 = 11, 2 = 12, 3 = 13, 4 = 21, 5=22, and 6 = 31, as shown in Table 1. This sequence number is used as an index of a chromosome's gene. The chromosome's gene can be represented by two fields: the first field (Mach) consists of M-bit string (M is the number of machines), where each bit corresponds to one machine. If the operation is to be processed on a particular machine, the corresponding bit assumes value (1), otherwise it is (0). The second field of a gene is the completion time of the operation (C). A sample of a chromosome is shown in Figure 1. Generation of Initial Population Initialization is the process of generating a new sequence of chromosomes. The initialization procedure produces pop_size chromosomeswhere pop_size denotes the population sizeby setting (1) at random position in each (mach) field of a chromosome and filling the remainder bits of the field by (0's) until the whole chromosome is filled and repeat the same procedure until the initial population is filled. Evaluation of Fitness The evaluation of a chromosome's fitness involves finding the total completion time (TCT) of the operations on each machine. The total completion time on each machine can be calculated by summing the time required to process each operation on a certain machine. The total completion time having the maximum value is the\n\n",
                "DataExportTag": "AI342279",
                "QuestionID": "QID402",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Solving Job-Shop Scheduling Problem Using Genetic Algorithm Approach An effective job shop schedu...",
                "Choices": {
                    "1": {
                        "Display": "\"Optimization Techniques in Routing and Pathfinding\""
                    },
                    "2": {
                        "Display": "genetic_algorithm, heuristic, route, routing, tsp, sa, path, travel_salesman, simulated_annealing, vehicle_routing, tabu_search, ant_colony, metaheuristic, simulate_annealing, placement"
                    },
                    "3": {
                        "Display": "\"Production Scheduling and Job Shop Optimization\""
                    },
                    "4": {
                        "Display": "scheduling, job, schedule, job_shop, genetic_algorithm, production, flow_shop, sequence, heuristic, flexible_job, manufacturing, shop_scheduling, planning, assembly, constraint"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID141",
            "SecondaryAttribute": "Spectral Resolution Enhancement of Hyperspectral Images via Sparse Representations. High-spectral...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Spectral Resolution Enhancement of Hyperspectral Images via Sparse Representations. High-spectral resolution imaging provides critical insights into important computer vision tasks such as classification, tracking, and remote sensing. Modern Snapshot Spectral Imaging (SSI) systems directly acquire the entire 3D data-cube through the intelligent combination of spectral filters and detector elements. Partially because of the dramatic reduction in acquisition time, SSI systems exhibit limited spectral resolution, for example, by associating each pixel with a single spectral band in Spectrally Resolvable Detector Arrays.\n\nIn this paper, we propose a novel machine learning technique aiming to enhance the spectral resolution of imaging systems by exploiting the mathematical framework of Sparse Representations (SR). Our formal approach proposes a systematic way to estimate a high-spectral resolution pixel from a measured low-spectral resolution version by appropriately identifying a sparse representation that can directly generate the high-spectral resolution output. We enforce the sparsity constraint by learning a joint space coding dictionary from multiple low and high spectral resolution training data, and we demonstrate that one can successfully reconstruct high-spectral resolution images from limited spectral resolution measurements.",
                "DataExportTag": "AI508741",
                "QuestionID": "QID141",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Spectral Resolution Enhancement of Hyperspectral Images via Sparse Representations. High-spectral...",
                "Choices": {
                    "1": {
                        "Display": "\"Climate Monitoring and Weather Forecasting\": satellite, climate, cloud, ocean, weather, precipitation, temperature, ice, atmospheric, meteorological, snow, resolution, radar, forecast, sea_ice"
                    },
                    "2": {
                        "Display": "\"Satellite Imagery and Land Classification\": image, urban, land, map, classification, mapping, resolution, forest, satellite, imagery, vegetation, landsat, wetland, spectral, spatial"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID138",
            "SecondaryAttribute": "STATISTICAL ANALYSIS USED Chi-square test is used for evaluating the variables predicting contral...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "STATISTICAL ANALYSIS USED\nChi-square test is used for evaluating the variables predicting contralateral nodal metastasis. Finally, a multivariate analysis was performed using binomial logistic regression to identify those variables that were independently associated with the risk of contralateral nodal metastasis.\n\n",
                "DataExportTag": "66",
                "QuestionID": "QID138",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "STATISTICAL ANALYSIS USED Chi-square test is used for evaluating the variables predicting contral...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID287",
            "SecondaryAttribute": "Structure-function relationship of chimeric KdpFABC complex The structure-function relationships...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Structure-function relationship of chimeric KdpFABC complex The structure-function relationships that explain the transport mechanism of membrane proteins on a molecular level are poorly understood and define one of the most complex and interdisciplinary research fields at the interface of Biology, Chemistry and Physics. Here I propose the investigation of the KdpFABC complex for the potassium uptake into the cell. It combines features of three membrane transport systems (P-type ATPases, channels and ABC transporters), which traditionally are considered to be mechanistically and structurally distinct. I propose to decipher the mechanism of action of the KdpFABC system, which is likely to overthrow the conceptual boundaries conventionally used to describe membrane transport mechanisms. The work might lead to the design of novel antibiotics, as these systems are exclusively found in prokaryotes. The structural studies will be conducted exploiting the recent breakthroughs in high-resolution single particle cryo electron microscopy (cryo-EM) that will be established by me as experienced researcher and thereby contribute to the cutting-edge research at UG. The complementary expertise of myself in structural biology and of Prof. Slotboom, as a world-leading scientist in the functional characterization of membrane proteins, are ideally suitable for the successful completion of the research objectives. Ultimately, this project will provide me a unique opportunity to establish myself as an independent researcher and use the knowledge and skills acquired to obtain an academic position within the EU.\n\n",
                "DataExportTag": "COR29699",
                "QuestionID": "QID287",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Structure-function relationship of chimeric KdpFABC complex The structure-function relationships...",
                "Choices": {
                    "1": {
                        "Display": "\"Evolutionary Genetics and Species Adaptation\""
                    },
                    "2": {
                        "Display": "evolution, genomic, genetic, specie, trait, population, adaptation, insect, variation, fitness, phenotype, sequence, speciation, selection, lineage"
                    },
                    "3": {
                        "Display": "\"Cellular Biology and Biophysics\""
                    },
                    "4": {
                        "Display": "membrane, cellular, protein, transport, organelle, assembly, signal, actin, microscopy, cell_division, mechanical, force, channel, single_molecule, cytoskeleton"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID137",
            "SecondaryAttribute": "SUBJECTS AND METHODS Those cases in which lesions were reaching or crossing midline were included...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "SUBJECTS AND METHODS\nThose cases in which lesions were reaching or crossing midline were included in this study. All cases underwent surgery as primary modality of treatment and had bilateral neck dissection.\n\n",
                "DataExportTag": "65",
                "QuestionID": "QID137",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "SUBJECTS AND METHODS Those cases in which lesions were reaching or crossing midline were included...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID168",
            "SecondaryAttribute": "Support to policy dialogues and strengthening of cooperation\\nwith Southeast Asia The SEALING pro...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Support to policy dialogues and strengthening of cooperation\\nwith Southeast Asia The SEALING project, targeting the objective 9.1a of the FP7 ICT work programme, aims at supporting the development of Information Society policy dialogues and the strengthening of cooperation on ICT research with the ASEAN region in general (a region of strategic importance for the EU), and with Singapore in particular (a country with which a MoU on ICT has been signed).<br\/>It builds on the results achieved and the experience gained through the FP7 SEACOOP project, which has already supported, over the period 2007-2009, the development of S&T cooperation in ICT between the two regions.<br\/>On the Southeast Asian side, the SEALING partnership includes the organisation managing ICT research in each of the ten ASEAN countries and is formally supported by the ASEAN Secretariat.<br\/>On the European side, the project involves, through a Project Advisory Committee, over 20 leading companies, research institutes and academia. The project is also supported by the main European Technology Platforms and Joint Technology Initiatives in the ICT area, and by key European initiatives in Southeast Asia, namely the EU-SEA-NET INCO-Net project and the TEIN3 initiative.<br\/>The SEALING project will develop its activities over a 2-year period:\u00c2\u2022 Organization of at least 6 cooperation events in Europe and in Southeast Asia to be synchronised with policy dialogue meetings,\u00c2\u2022 Identification and analysis of ICT policy and research priorities, at bilateral and regional levels, and provision of recommendations for future cooperation initiatives,\u00c2\u2022 Development of synergies with dialogues and activities launched under EU-funded programmes (EU-SEA-NET, TEIN3, Marie Curie, etc.), at the initiative of EU Member States and Associated States, and at the ASEAN level (SCMIT),\u00c2\u2022 Development of promotion, dissemination and awareness-raising activities.\n\n",
                "DataExportTag": "COR47625",
                "QuestionID": "QID168",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Support to policy dialogues and strengthening of cooperation\\nwith Southeast Asia The SEALING pro...",
                "Choices": {
                    "1": {
                        "Display": "\"Research Funding and University Administration\""
                    },
                    "2": {
                        "Display": "cofund, national_contact, ncps, preparatory_phase, university, coordinator, administrative, smart_specialization, health, critical_mass, centres, website, strategic_partnership, preparation, increase_visibility"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID229",
            "SecondaryAttribute": "Support to policy dialogues and strengthening of cooperation\\nwith Southeast Asia The SEALING pro...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Support to policy dialogues and strengthening of cooperation\\nwith Southeast Asia The SEALING project, targeting the objective 9.1a of the FP7 ICT work programme, aims at supporting the development of Information Society policy dialogues and the strengthening of cooperation on ICT research with the ASEAN region in general (a region of strategic importance for the EU), and with Singapore in particular (a country with which a MoU on ICT has been signed).<br\/>It builds on the results achieved and the experience gained through the FP7 SEACOOP project, which has already supported, over the period 2007-2009, the development of S&T cooperation in ICT between the two regions.<br\/>On the Southeast Asian side, the SEALING partnership includes the organisation managing ICT research in each of the ten ASEAN countries and is formally supported by the ASEAN Secretariat.<br\/>On the European side, the project involves, through a Project Advisory Committee, over 20 leading companies, research institutes and academia. The project is also supported by the main European Technology Platforms and Joint Technology Initiatives in the ICT area, and by key European initiatives in Southeast Asia, namely the EU-SEA-NET INCO-Net project and the TEIN3 initiative.<br\/>The SEALING project will develop its activities over a 2-year period:\u00c2\u2022 Organization of at least 6 cooperation events in Europe and in Southeast Asia to be synchronised with policy dialogue meetings,\u00c2\u2022 Identification and analysis of ICT policy and research priorities, at bilateral and regional levels, and provision of recommendations for future cooperation initiatives,\u00c2\u2022 Development of synergies with dialogues and activities launched under EU-funded programmes (EU-SEA-NET, TEIN3, Marie Curie, etc.), at the initiative of EU Member States and Associated States, and at the ASEAN level (SCMIT),\u00c2\u2022 Development of promotion, dissemination and awareness-raising activities.\n\n",
                "DataExportTag": "COR47625",
                "QuestionID": "QID229",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Support to policy dialogues and strengthening of cooperation\\nwith Southeast Asia The SEALING pro...",
                "Choices": {
                    "1": {
                        "Display": "\"Research Funding and University Administration\""
                    },
                    "2": {
                        "Display": "cofund, national_contact, ncps, preparatory_phase, university, coordinator, administrative, smart_specialization, health, critical_mass, centres, website, strategic_partnership, preparation, increase_visibility"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID363",
            "SecondaryAttribute": "Targeting glucocorticoid resistance in T-ALL: a Systems Biology approach T-cell acute lymphoblast...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Targeting glucocorticoid resistance in T-ALL: a Systems Biology approach T-cell acute lymphoblastic leukemia (T-ALL) is an aggressive hematologic cancer that accounts for 10% to 15% of pediatric and 25% of adult ALL cases. Glucocorticoids play a central role in the treatment of T-ALL due to their capacity to induce growth arrest and apoptosis in lymphoid progenitor cells. The development of glucocortioid resistance in leukemia patients is an important clinical problem and a significant contributor to therapeutic failure. Despite numerous studies addressing the mechanisms that mediate the response of lymphoid tumor cells to glucocorticoid therapy the mechanism of glucocorticoid resistance remain incompletely understood. Activating mutations in NOTCH1 are present in over 50% of T-ALL cases and aberrant NOTCH1 signaling can transform T-cell progenitors. Importantly, proteolytic cleavage by the gamma-secretase complex is required for ligand-mediated activation of wild type NOTCH1 and for aberrant NOTCH1 signaling in T-ALLs harboring activating mutations in the NOTCH1 gene and, small molecule gamma-secretase inhibitors can effectively block the function of oncogenic NOTCH1in T-ALL. Dr. Ferrando\u00e2\u20ac\u2122s laboratory has recently shown that inhibition of NOTCH1 signaling with GSIs can reverse glucocorticoid resistance in T-ALL. Reversal of glucocorticoid resistance in this model is mediated by HES1 downregulation, increased glucocorticoid receptor autoupregualtion and improved glucocorticoid-induced upregulation of BIM, a critical proapototic factor, in glucocorticoid-induced programmed cell death. The identification of NOTCH1-HES1-glucocorticoid receptor regulatory axis in glucocorticoid resistant T-ALL serves as proof of principle for the main goal of this project, which aims to identify novel genes and pathways controlling glucocorticoid resistance in T-ALL. As second phase of this project, the previously identified pathways will be exploited to develop novel targeted therapies for the treatment of glucocorticoid resistant T-ALL.\n\n",
                "DataExportTag": "COR59555",
                "QuestionID": "QID363",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Targeting glucocorticoid resistance in T-ALL: a Systems Biology approach T-cell acute lymphoblast...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetics and Epigenetics Research\""
                    },
                    "2": {
                        "Display": "deoxyribonucleic_acid, ribonucleic_acid, chromatin, genomic, epigenetic, methylation, repair, sequence, gene_expression, chromosome, transcription, protein, genetic, mirnas, non_coding"
                    },
                    "3": {
                        "Display": "\"Cancer Research and Genetic Disorders\""
                    },
                    "4": {
                        "Display": "cancer, tumor, age, disease, mutation, patient, therapeutic, genetic, cellular, mouse, mitochondrial, ageing, aging, disorder, protein"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID108",
            "SecondaryAttribute": "Technique Our technique of patient positioning, trocar placement, and robot docking for robotic t...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Technique\nOur technique of patient positioning, trocar placement, and robot docking for robotic transabdominal kidney surgery has been described previously.2 Using robot assistance, the bowel was mobilized to expose the aorta and IVC. The renal isthmus and renal artery were dissected using robot assistance and transected using laparoscopic staplers. The renal vein and IVC were carefully dissected, and a laparoscopic ultrasound probe was used to identify the thrombus in the renal vein.\n\n",
                "DataExportTag": "36",
                "QuestionID": "QID108",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Technique Our technique of patient positioning, trocar placement, and robot docking for robotic t...",
                "Choices": {
                    "1": {
                        "Display": "An extra-large Hem-o-Lok clip was partially closed around the renal vein proximal to the tumor thrombus and was used to retract the thrombus toward the kidney (Fig. 2A). The clip was then applied, creating a space in the renal vein between the clip and the IVC, which was confirmed to be free of tumor thrombus by intraoperative ultrasonography. Additional Hem-o-Lok clips were applied on the renal vein at the junction with the IVC. The renal vein was incised circumferentially between clips, with visual confirmation of absence of thrombus at the resected margin (Fig. 2B), and the kidney was removed using a specimen retrieval bag."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID163",
            "SecondaryAttribute": "TEchnology TRAnsfer for GrOwth with twinNing The Technology Transfer Office Circle, launched by t...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "TEchnology TRAnsfer for GrOwth with twinNing The Technology Transfer Office Circle, launched by the EC in 2011, analyzed the Technology Transfer (TT) in Europe concluding that:\u201cIt is anticipated that the next decade will see profound changes in this landscape. Studies have identified the lack of scale as one of the major issues of technology transfer\u201d. So Technology Transfer, is a prevailing topic. Though it\u2019s a concept as old as ancient Greece, Archimedes could be the first Technology Broker in history, the reality is that it is still needed to overcome barriers and face common challenges to improve the Technology Transfer process. TETRAGON will use the Twinning Advanced methodology to enhance SME\u2019s innovation capacity by providing them with better innovation support in Technology Transfer. That is, provide better support to SMEs to improve TT from public research to the market.Due to the expertise of the consortium and the needs identified in their regions this project will be focused on the following models of Technology Transfer: 1) To foster an entrepreneurial environment at universities and research centers in order to increase the creation of spin-offs and to improve the exploitation of technology by existing companies. 2) To foster demand driven collaborative projects, between public researchers and private SMEs. 3) To look for innovative ways of licensing the technology, including open source, open innovation and user innovation. The project will have the support of the Netherlands Enterprise Agency (RVO) in the Twinning Advanced Methodology. TETRAGON will help to place TT in the core of innovation, to improve the TT measures and the SMEs to grow, and, consequently, foster growth of the European economy.  TETRAGON, aims at reaching almost 300 innovation agencies all over Europe, through networks as EURADA, and that 20% of this agencies adopte, at least, one of the new measures defined in the DOP, meanwhile establishing a Twinning Advanced ecosystem where agencies collaborate.\n\n",
                "DataExportTag": "COR30967",
                "QuestionID": "QID163",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "TEchnology TRAnsfer for GrOwth with twinNing The Technology Transfer Office Circle, launched by t...",
                "Choices": {
                    "1": {
                        "Display": "\"Political Science and Social Studies\""
                    },
                    "2": {
                        "Display": "labour, ethnography, peace, military, legitimacy, indigenous, election, domestic, feminist, violent, ethnic, electoral, protest, victim, populism"
                    },
                    "3": {
                        "Display": "\"Sustainable Agriculture and Eco-Tourism\""
                    },
                    "4": {
                        "Display": "farming, food, agriculture, energy, heritage, tourism, responsible, transformative, multi_actor, pathway, open_access, green_deal, consultation, resilient, toolbox"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID97",
            "SecondaryAttribute": "The antiviral activity of rodent and lagomorph SERINC3 and SERINC5 is counteracted by known viral...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The antiviral activity of rodent and lagomorph SERINC3 and SERINC5 is counteracted by known viral antagonists. A first step towards the development of a human immunodeficiency virus (HIV) animal model has been the identification and surmounting of species-specific barriers encountered by HIV along its replication cycle in cells from small animals. Serine incorporator proteins 3 (SERINC3) and 5 (SERINC5) were recently identified as restriction factors that reduce HIV-1 infectivity. Here, we compared the antiviral activity of SERINC3 and SERINC5 among mice, rats and rabbits, and their susceptibility to viral counteraction to their human counterparts. In the absence of viral antagonists, rodent and lagomorph SERINC3 and SERINC5 displayed anti-HIV activity in a similar range to human controls. Vesicular stomatitis virus G protein (VSV-G) pseudotyped virions were considerably less sensitive to restriction by all SERINC3\/5 orthologs. Interestingly, HIV-1 Nef, murine leukemia virus (MLV) GlycoGag and equine infectious anemia virus (EIAV) S2 counteracted the antiviral activity of all SERINC3\/5 orthologs with similar efficiency. Our results demonstrate that the antiviral activity of SERINC3\/5 proteins is conserved in rodents and rabbits, and can be overcome by all three previously reported viral antagonists.\n\n",
                "DataExportTag": "CAN1277767",
                "QuestionID": "QID97",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The antiviral activity of rodent and lagomorph SERINC3 and SERINC5 is counteracted by known viral...",
                "Choices": {
                    "1": {
                        "Display": "\"Gene Therapy and Cancer Treatment Research\""
                    },
                    "2": {
                        "Display": "mouse, cart_t_cell_therapy, vector, adenovirus, transfer, transduction, transgene, transduce, antitumor, animal, alzheimer_disease, chimeric_antigen, gfp, injection, oncolytic"
                    },
                    "3": {
                        "Display": "\"Virology and Gene Therapy Research\""
                    },
                    "4": {
                        "Display": "virus, hiv, vector, gene, epstein_barr_virus, strain, alzheimer_disease, kshv, transduction, immunodeficiency_virus, antiviral, adenovirus, kaposi_sarcoma, hcmv, latency"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID301",
            "SecondaryAttribute": "The antiviral activity of rodent and lagomorph SERINC3 and SERINC5 is counteracted by known viral...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The antiviral activity of rodent and lagomorph SERINC3 and SERINC5 is counteracted by known viral antagonists. A first step towards the development of a human immunodeficiency virus (HIV) animal model has been the identification and surmounting of species-specific barriers encountered by HIV along its replication cycle in cells from small animals. Serine incorporator proteins 3 (SERINC3) and 5 (SERINC5) were recently identified as restriction factors that reduce HIV-1 infectivity. Here, we compared the antiviral activity of SERINC3 and SERINC5 among mice, rats and rabbits, and their susceptibility to viral counteraction to their human counterparts. In the absence of viral antagonists, rodent and lagomorph SERINC3 and SERINC5 displayed anti-HIV activity in a similar range to human controls. Vesicular stomatitis virus G protein (VSV-G) pseudotyped virions were considerably less sensitive to restriction by all SERINC3\/5 orthologs. Interestingly, HIV-1 Nef, murine leukemia virus (MLV) GlycoGag and equine infectious anemia virus (EIAV) S2 counteracted the antiviral activity of all SERINC3\/5 orthologs with similar efficiency. Our results demonstrate that the antiviral activity of SERINC3\/5 proteins is conserved in rodents and rabbits, and can be overcome by all three previously reported viral antagonists.\n\n",
                "DataExportTag": "CAN1277767",
                "QuestionID": "QID301",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The antiviral activity of rodent and lagomorph SERINC3 and SERINC5 is counteracted by known viral...",
                "Choices": {
                    "1": {
                        "Display": "\"Gene Therapy and Cancer Treatment Research\""
                    },
                    "2": {
                        "Display": "mouse, cart_t_cell_therapy, vector, adenovirus, transfer, transduction, transgene, transduce, antitumor, animal, alzheimer_disease, chimeric_antigen, gfp, injection, oncolytic"
                    },
                    "3": {
                        "Display": "\"Virology and Gene Therapy Research\""
                    },
                    "4": {
                        "Display": "virus, hiv, vector, gene, epstein_barr_virus, strain, alzheimer_disease, kshv, transduction, immunodeficiency_virus, antiviral, adenovirus, kaposi_sarcoma, hcmv, latency"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID257",
            "SecondaryAttribute": "The Genetics of Colorectal Cancer Colon cancer is a common disease in both men and women. Because...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The Genetics of Colorectal Cancer Colon cancer is a common disease in both men and women. Because 5% of persons (1 in 20 persons) will develop colorectal cancer, this disease is an important public health issue. Colon cancer is usually observed in one of three specific patterns: sporadic, inherited, or familial. Sporadic disease, with no familial or inherited predisposition, accounts for approximately 70% of colorectal cancer in the population. Sporadic colon cancer is common in persons older than 50 years of age, probably as a result of dietary and environmental factors as well as normal aging. Fewer than 10% of patients have an inherited predisposition to colon cancer. The inherited syndromes include those in which colonic polyps are a major manifestation of disease and those in which they are not. The polyposis syndromes are subdivided into familial adenomatous polyposis and the hamartomatous polyposis syndromes. The nonpolyposis predominant syndromes include hereditary nonpolyposis colorectal cancer (HNPCC) (Lynch syndrome I) and the cancer family syndrome (Lynch syndrome II). Although uncommon, these syndromes provide insight into the biology of all types of colorectal cancer. The third and least understood pattern of colon cancer development is known as familial colon cancer. In affected families, colon cancer develops too frequently to be considered sporadic colon cancer but not in a pattern consistent with an inherited syndrome. Up to 25% of all cases of colon cancer may fall into this category. Basic Genetics The complement of DNA in the genetic code is a cell's guide to differentiation and proliferation. All cells of an organism have DNA that is virtually identical to the DNA found in the zygote. A mutation at or before this point of development is therefore termed a germline mutation. A mutation of the ova or sperm is transmitted from the parent as an inherited defect and is responsible for hereditary cancer. When a mutation occurs spontaneously in the sperm, ova, or zygote, the affected person's parents will not manifest a cancer syndrome. Successive generations, however, can inherit this de novo mutation because the abnormality can be passed on to progeny. More commonly, a spontaneous mutation occurs in a cell during the growth and development of a particular tissue or organ. This somatic mutation results in clonal proliferation of the cell containing the mutated genetic material. Sporadic colorectal cancer results from the accumulation of multiple somatic mutations in a cell (1). Genes commonly mutated in human cancer belong to one of three different classes: oncogenes, tumor suppressor genes, and mismatch repair genes (2, 3). Oncogenes are normal genes responsible for the stimulation of controlled cellular proliferation (4, 5) (Figure 1). When these genes are mutated, they result in uncontrolled proliferation and, ultimately, cancer. Tumor suppressor genes were first described in Knudson's study of the epidemiology of childhood retinoblastoma (6, 7). Knudson used the term antioncogene because the gene was thought to produce cancer in a recessive fashion at the cellular level, meaning that one normal gene was adequate to control cellular growth. Subsequently termed suppressor genes, these are normal genes whose function is lost when both copies (alleles) of the gene are inactivated (Figure 1). When a tumor suppressor gene is inherited as a germline mutation, only the mutation of the remaining normal allele is required for the gene's loss of function (Figure 2, top) (5, 6). When both copies of the gene are normal, two mutation events, or hits, are required before the gene loses function (Figure 2, bottom). This two-hit hypothesis explains why inherited disease usually manifests at an earlier age than sporadic disease, as well as the concept of suppressor genes producing cancer in a recessive fashion at the cellular level (7). Enzymes that monitor newly formed DNA and correct replication errors are called DNA mismatch repair (MMR) systems (Figure 1) (3). Defective MMR genes are associated with the so-called mutator phenotype. Cells with MMR mutations in both gene copies accumulate DNA errors throughout the genome, affecting growth regulatory genes, such as the transforming growth factor receptor (TGF-RII) gene (8). These genes are mutated in HNPCC. Figure 1. The normal function of the different classes of cancer-causing genes according to cell-cycle stage. Figure 2. Loss of suppressor-gene function. Recently, subtle genetic changes that do not affect protein structure have been recognized as a possible cause of familial colon cancer. These subtle changes, termed polymorphisms if they occur frequently in the population and do not affect protein structure, are slight variations of nucleotide base sequences in genes. They are more common in populations than the hereditary cancer gene mutations but do not usually result in clinical disease. One polymorphism, now thought to be a mutation because it ultimately results in an abnormal protein structure, is found on codon 1307 of the APC gene and is known as the I1307K APC mutation (9). This mutation is found in 6% of all Ashkenazi Jewish persons and in 28% of Ashkenazi Jewish persons with both a personal and family history of colon cancer (Figure 3) (5, 9). Figure 3. Familial colorectal cancer ( CRC ) and APC gene mutation I1307K in Ashkenazi Jewish persons. Molecular Genetics Fearon and Vogelstein (1) have described the molecular basis for sporadic colon cancer as a multistep model of carcinogenesis. This model describes an accumulation of genetic events, each conferring a selective growth advantage to an affected colon cell. These changes ultimately result in uninhibited cell growth, proliferation, and clonal tumor development. The cumulative effect of these somatic mutations is the cause of sporadic colon cancer. Four main conclusions are drawn from the proposed model of sporadic colon cancer pathogenesis: 1) Colorectal cancer results from the mutational activation of oncogenes and the inactivation of tumor suppressor genes; 2) somatic mutations in at least four or five genes of a cell are required for malignant transformation; 3) the accumulation of multiple genetic mutations rather than the sequence of mutations determines the biological behavior of the tumor, although APC mutations usually occur early in the process and mutations of the p53 suppressor gene usually occur late in the process; and 4) features of the tumorigenic process of colon cancer are applicable to other solid tumors, such as breast and pancreatic cancer (10). The most commonly inherited colon cancer syndromes are familial adenomatous polyposis and HNPCC. Each of these syndromes is the result of a specific germline mutation. In familial adenomatous polyposis, the germline mutation is always the APC gene, a tumor suppressor gene. In HNPCC, one of the MMR genes is mutated, most commonly hMLH1 or hMSH2. Several of the hamartomatous polyp syndromes have recently been associated with germline mutations. One example is the PeutzJeghers syndrome, which results from an abnormality of the STK11 tumor suppressor gene (11). Familial colon cancer in Ashkenazi Jewish persons is probably the result of an I1307K APC germline mutation, although the relative risk for tumor is much lower in a person with this mutation than in a person with one of the germline mutations noted previously (Figure 3). Unlike most germline mutations, which cause protein structure abnormalities, the I1307K APC germline mutation causes a predisposition to sporadic mutations at distant sites of the gene (which then cause protein structure abnormalities) at a later stage of development. Specific Mutations Oncogenes The oncogene ras on chromosome 12 codes for a binding protein that acts as a one-way switch for the transmission of extracellular growth signals to the nucleus and regulates cellular signal transduction. Post-translational modification of the ras protein by farnesylation is necessary for activation. Mutations of ras are detected in up to 50% of cases of sporadic colorectal cancer and in large polyps. Activation of ras leads to constitutive activity of the protein, which results in a continuous growth stimulus that can be the basis of carcinogenesis. Recognition of ras mutations may be helpful in screening and early diagnosis of colorectal cancer (12). The usefulness of a sensitive assay for the detection of ras mutations in the stool of patients with curable colorectal tumors has been studied (13). Therapeutic potential also exists because clinical trials of farnesyl transferase inhibitors, which specifically inhibit ras-mediated signal transduction, have been initiated in patients with colorectal cancer exhibiting ras mutations. The src oncogene was first identified in Rous sarcoma virus. It encodes for a transforming protein that directly modifies the cytoskeleton. Disruption of the cytoskeleton may be an early event in the process of malignant transformation and carcinogenesis (14, 15). Other oncogenes implicated in sporadic colon cancer include c-myc and c-erbB2 (Table 1) (16, 17). Table 1. Gene Mutations That Cause Colon Cancer Tumor Suppressor Genes The normal function of the APC gene is thought to be the modulation of the -catenin protein, which regulates cell signal transduction and growth (18). The APC gene inhibits -catenin, which controls cellular proliferation. Several cell-signaling pathways converge with the one for APC and may result in the same final carcinogenic event. As a result, APC mutations are important in early cell transformation, and APC is known as the gatekeeper gene (19). When APC is a germline mutation, it results in familial adenomatous polyposis; as a somatic mutation, APC is an early event in the development of sporadic colon cancer; and as an I1307K mutation, APC contributes to the development of familial colon cancer in Ashkenazi Jewish persons (Table 1). Of the known tumor suppressor genes, p53 is the most commo\n\n",
                "DataExportTag": "CAN880118",
                "QuestionID": "QID257",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The Genetics of Colorectal Cancer Colon cancer is a common disease in both men and women. Because...",
                "Choices": {
                    "1": {
                        "Display": "\"Cellular Biology and Aging Research\""
                    },
                    "2": {
                        "Display": "mitosis, stress, microtubule, homeostasis, aurora, aging, centrosome, membrane, cellular, clock, regulation, polarity, intracellular, dynamic, ion_channel"
                    },
                    "3": {
                        "Display": "\"Epigenetic Regulation and Gene Modification\""
                    },
                    "4": {
                        "Display": "pten, chromatin, histone, regulation, stress, methylation, epigenetic, autophagy, metabolic, enzyme, gene, modification, translation, acetylation, control"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID411",
            "SecondaryAttribute": "The Microbiome Composition of a Man's Penis Predicts Incident Bacterial Vaginosis in His Female S...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The Microbiome Composition of a Man's Penis Predicts Incident Bacterial Vaginosis in His Female Sex Partner With High Accuracy Background: We determined the predictive accuracy of penile bacteria for incident BV in female sex partners. In this prospective cohort, we enrolled Kenyan men aged 18\u201335 and their female sex partners aged 16 and older. We assessed BV at baseline, 1, 6, and 12 months. Incident BV was defined as a Nugent score of 7\u201310 at a follow-up visit, following a Nugent score of 0\u20136 at baseline. Amplification of the V3\u2013V4 region of the bacterial 16S rRNA gene was performed on meatal and glans\/coronal sulcus swab samples. Majority vote classifier combined the decisions of three machine learning classification algorithms (Random Forest, Support Vector Machine, K Nearest Neighbor). We report the estimate cross-validation predictive accuracy for incident BV based on baseline penile taxa. Results: The incidence of BV was 31% among 168 couples in which the woman did not have BV at baseline: 37.3% if the man was uncircumcised vs. 26.3% if the man was circumcised. Incident BV occurred at 1 month (n = 23), 6 months (n = 20), 12 months (n = 9). The predictive capacity of meatal taxa was high: sensitivity (80.7%), specificity (74.6%), accuracy (77.5%), area under the curve (88.8%). Variable importance ranking identified meatal taxa that in the vagina are associated with BV: Parvimonas, Lactobacillus iners, L. crispatus, Dialister, Sneathia sanguinegens, and Gardnerella vaginalis were among the top 10 most predictive taxa. The accuracy of glans\/coronal sulcus taxa to predict incident BV was comparable to meatal taxa accuracy, but with greater variability. Conclusions: Baseline penile microbiota accurately predicted BV incidence in women who did not have BV at baseline, with more than half of incident infections observed at 6- to 12- months after penile microbiome assessment. These results suggest interventions to manipulate the penile microbiome may reduce BV incidence in sex partners, and that potential treatment (antibiotic or live biotherapeutic) will need to be effective in reducing or altering bacteria at both the glans\/coronal sulcus and urethral sites (as represented by the meatus). The temporal association clarifies that concordance of penile microbiome with the vaginal microbiome of sex partners is not merely reflecting the vaginal microbiome, but can contribute to it.\n\n",
                "DataExportTag": "AI540566",
                "QuestionID": "QID411",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The Microbiome Composition of a Man's Penis Predicts Incident Bacterial Vaginosis in His Female S...",
                "Choices": {
                    "1": {
                        "Display": "\"Maternal Health and Pregnancy Risk Factors\""
                    },
                    "2": {
                        "Display": "age, woman, pregnancy, ci, child, risk, sex, infant, risk_factor, exposure, hiv, birth, maternal, body_mass, embryo"
                    },
                    "3": {
                        "Display": "\"Mental Health Assessment and Intervention\""
                    },
                    "4": {
                        "Display": "depression, pain, mental_health, dementia, symptom, suicide, intervention, cognitive, participant, physical, questionnaire, rehabilitation, behavior, digital, sleep"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID178",
            "SecondaryAttribute": "The Production of Sustainable Diesel-Miscible-Biofuels from the Residues and Wastes of Europe and...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The Production of Sustainable Diesel-Miscible-Biofuels from the Residues and Wastes of Europe and Latin America The increasing reliance on imported diesel fuels, in addition to annual increases in the quantities of organic wastes are threats to the EU and Latin America. This project (DIBANET) will combat these threats and help to eliminate diesel imports by developing novel technologies that will allow the sustainable production of diesel miscible biofuels from wastes. It will build on the key, complementary, strengths of researchers and industries of both regions to advance this field. This enhancement of co-operation will ensure that the whole process, from feedstock to process residues, is engineered for maximum efficiency. The links between regions will be further enhanced by the establishment of inter-regional student scholarships; 2 large brokerage events to engage all stakeholders; and a summer school for knowledge transfer. DIBANET will increase the yield from biomass, beyond the current art, of levulinic acid, a valuable platform chemical that can be combined with ethanol to make a diesel fuel. Processes will be advanced to utilise the solid residue that remains after the acid-treatment. From this residue treatment process a bio-oil and biochar will result. The bio-oil will be upgraded to produce a diesel miscible biofuel. The biochar will be examined for use as a soil amender for enhanced biomass yields. Advanced analytical techniques to benefit levulinic acid yields will be developed and employed online to allow real-time adjustment of biomass conversion conditions. All of the fuels produced will be tested to ensure compliance with current fuel requirements.\n\n",
                "DataExportTag": "COR63964",
                "QuestionID": "QID178",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The Production of Sustainable Diesel-Miscible-Biofuels from the Residues and Wastes of Europe and...",
                "Choices": {
                    "1": {
                        "Display": "\"Biofuel Production and Waste Management\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID239",
            "SecondaryAttribute": "The Production of Sustainable Diesel-Miscible-Biofuels from the Residues and Wastes of Europe and...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The Production of Sustainable Diesel-Miscible-Biofuels from the Residues and Wastes of Europe and Latin America The increasing reliance on imported diesel fuels, in addition to annual increases in the quantities of organic wastes are threats to the EU and Latin America. This project (DIBANET) will combat these threats and help to eliminate diesel imports by developing novel technologies that will allow the sustainable production of diesel miscible biofuels from wastes. It will build on the key, complementary, strengths of researchers and industries of both regions to advance this field. This enhancement of co-operation will ensure that the whole process, from feedstock to process residues, is engineered for maximum efficiency. The links between regions will be further enhanced by the establishment of inter-regional student scholarships; 2 large brokerage events to engage all stakeholders; and a summer school for knowledge transfer. DIBANET will increase the yield from biomass, beyond the current art, of levulinic acid, a valuable platform chemical that can be combined with ethanol to make a diesel fuel. Processes will be advanced to utilise the solid residue that remains after the acid-treatment. From this residue treatment process a bio-oil and biochar will result. The bio-oil will be upgraded to produce a diesel miscible biofuel. The biochar will be examined for use as a soil amender for enhanced biomass yields. Advanced analytical techniques to benefit levulinic acid yields will be developed and employed online to allow real-time adjustment of biomass conversion conditions. All of the fuels produced will be tested to ensure compliance with current fuel requirements.\n\n",
                "DataExportTag": "COR63964",
                "QuestionID": "QID239",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The Production of Sustainable Diesel-Miscible-Biofuels from the Residues and Wastes of Europe and...",
                "Choices": {
                    "1": {
                        "Display": "\"Biofuel Production and Waste Management\""
                    },
                    "2": {
                        "Display": "waste, biomass, bio_base, recycling, wood, feedstock, plant, biofuel, microalgae, bio, recovery, bioenergy, plastic, treatment, fermentation"
                    },
                    "3": {
                        "Display": "\"Material Science and Engineering\""
                    },
                    "4": {
                        "Display": "coating, composite, polymer, surface, fibre, textile, wood, plastic, mechanical_property, ceramic, glass, nanoparticle, alloy, mould, lightweight"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID288",
            "SecondaryAttribute": "The role of de novo evolution in the emergence of new genes Gene evolution has long been thought...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The role of de novo evolution in the emergence of new genes Gene evolution has long been thought to be driven primarily by duplication or transposition mechanisms, followed by divergence of the duplicated copy. However, every evolutionary lineage harbours also so-called orphan genes, which have no homologues in other evolutionary lineages i.e. which do not appear to have arisen via gene duplication mechanisms, or have diverged to a point where their origins can not be traced anymore.  Orphan genes are generally thought to be important drivers of taxon specific adaptations and interactions with the environment. New insights from comparative genomics and phylogenetic analysis suggests now that orphan genes could indeed be created through de novo evolution and it is becoming increasingly clear that this mechanism might occur at high rates, which would provide a continuous source of material for new gene functions. However, only initial evidence is available for this so far and little is known about the evolutionary dynamics and mechanisms of de novo gene emergence. The present proposal will use experimental and functional approaches to study the role and the evolutionary potential of the emergence of completely new genes from random sequences. This will open up new perspectives in understanding the evolution of genomes and the molecular mechanisms of adaptation.\n\n",
                "DataExportTag": "COR14148",
                "QuestionID": "QID288",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "The role of de novo evolution in the emergence of new genes Gene evolution has long been thought...",
                "Choices": {
                    "1": {
                        "Display": "\"Evolutionary Genetics and Species Adaptation\""
                    },
                    "2": {
                        "Display": "evolution, genomic, genetic, specie, trait, population, adaptation, insect, variation, fitness, phenotype, sequence, speciation, selection, lineage"
                    },
                    "3": {
                        "Display": "\"Cellular Biology and Biophysics\""
                    },
                    "4": {
                        "Display": "membrane, cellular, protein, transport, organelle, assembly, signal, actin, microscopy, cell_division, mechanical, force, channel, single_molecule, cytoskeleton"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID216",
            "SecondaryAttribute": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hype...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hypermedia systems and proposes adaptive characteristics for third generation adaptive hypermedia systems. First generation adaptive hypermedia systems predated the World-Wide Web (WWW) and were primarily single user adaptive hypermedia systems. Second generation systems have exploited the distributed nature and ease of authoring of the WWW to develop more robust and mature adaptive hypermedia systems. While these systems are a dramatic improvement over first generation systems, they have several limitations that limited the effectiveness of adaptation. These limitations include limited adaptation through one-dimensional, stereotypical user models, coarse granularity of adaptive support, closed adaptive hyperspaces with sharp boundaries, limited authoring support, and limited and nonconstructive communications between the user and the adaptive model. Third generation adaptive hypermedia systems must address these shortcomings to fully exploit the potential of adaptive hypermedia. Future systems must provide explicit, fine-grained adaptation support that the user can easily tailor and refine to provide highly relevant multidimensional adaptation. Future adaptive hypermedia systems must be open systems with soft boundaries that are expandable and incorporate resources from non-adaptive hypermedia with gradual degradation of support. Finally, adaptive systems must be relatively easy to build and maintain. This paper examines these characteristics of future adaptive hypermedia systems and proposes a framework for development. 1. Background 1.1 First Generation Adaptive Hypermedia Systems Numerous adaptive hypermedia systems have been implemented over the last fifteen years. These systems can be characterized as first generation or second-generation adaptive hypermedia systems based on when they were developed and what delivery mechanism was used for deployment of the systems. First generation systems span the period from 1985-1993 and are principally adaptive systems that were not distributed in nature. These systems were generally PC or Macintosh-based provided limited adaptability through stereotype-based user models and limited functionality adaptation techniques such as conditional text filters, direct guidance, stretchtext, hiding, and primitive link annotation. EPIAIM [Rosis et al. 1994], Hypadapter [Bocker et al. 1990], ITEM\/IP [Brusilovsky 1992], ISISTutor [Brusilovsky and Pesin 1994; Brusilovsky and Pesin 1994], MetaDoc [Boyle and Encarnacion 1994], and MetaDoc V [Boyle and Teh 1993] are examples of first generation adaptive hypermedia systems. ISIS-Tutor is a good example a first generation adaptive system. It is an adaptive learning environment for the information retrieval system CDS\/ISIS\/M (ISIS) [Brusilovsky and Pesin 1994; Brusilovsky and Pesin 1994]. ISISTutor provided adaptive support through frame-based presentation, link annotation, and information hiding. It presented an adaptive sequence of tasks and concepts based on the user model and maintained a four state, user knowledge model (not-ready-to-be-learned, ready-to-be-learned, in-work, and learned) on each concept within ISIS. As the user progressed through the system, ISIS-Tutor annotated each concept with color and hiding could be enabled to remove concepts that the user was not ready for. ISIS-Tutor was one of the first adaptive models to incorporate more than a bipolar knowledge model and one of the first adaptive systems to have empirical support for the effectiveness of adaptive interfaces [Brusilovsky and Pesin 1994]. The advent of the World-Wide Web, however, provided new opportunities for the development of adaptive hypermedia systems and led to new systems and more advanced adaptive techniques. 1.2 Second Generation Adaptive Hypermedia Systems Second generation systems span the period from 1993 to present and predominantly use the WWW as their delivery and presentation means. These systems have worldwide availability and are generally platform independent. Second generation systems introduced new capabilities such as adaptive multimedia presentation, map adaptation, and link sorting. They also refined existing adaptation techniques to provide greater functionality. User models became more robust and incorporated more user characteristics. AHA [Bra 1996; Bra and Calvi 1998], ARNIE-HyperMan [Mathe and Chen 1994; Rabinowitz et al. 1995; Mathe and Chen 1996], AVANTI [Fink et al. 1997; Rizzo et al. 1997; Nill 1998], Basar [Thomas 1995], CS383 [Carver et al. 1996; Carver et al. 1996], Cameleon [Laroussi and Benahmed 1999], ELM-ART [Brusilovsky et al. 1996], I-Doc [Erdem and Johnson 1998; Erdem et al. 1998], InterBook [Brusilovsky and Eklund 1998] , KN-AHS [Kobsa et al. 1994], MediaDoc [Erdem et al. 1998], RATH [Hockemeyer et al. 1998] and WebWatcher [Armstrong et al. 1995] are examples of second generation adaptive hypermedia systems. ELM-ART (The Episodic Learning Model: The Adaptive Remote Tutor) is good example of a second-generation adaptive hypermedia system. ELM-ART is a distributed intelligent tutoring system on LISP that provides course adaptation through a combination of adaptive annotation and link sorting [Brusilovsky et al. 1996]. Links are colorcoded according to user preparation for the information in the node. Red annotations indicate nodes that the user has to meet the prerequisites for, amber nodes represent information the student is ready for but not recommended, and green nodes represent nodes that the user is ready for and are recommended. ELM-ART adaptively sorts links as well so that the links that are most similar to the node that the user is currently on are presented first. ELM-ART features extensive user feedback and is highly interactive. As the user completes exercises and reads nodes, status bars change to reflect the user\u2019s progress through the course. Link annotations change color and the navigational view changes to reflect the user\u2019s newly gained knowledge. Users can directly edit the user model and override the navigational choices present to meet their educational goals. An extensive number of exercises engage the user and proved constant feedback both to the user and to the user model on the user\u2019s evaluated knowledge. While second generation adaptive hypermedia systems such as ELM-ART have dramatically improved upon the functionality of first generation systems, there are fundamental flaws associated with these systems. These limitations include limited adaptation through one-dimensional, stereotypical user models, coarse granularity of adaptive support, closed adaptive hyperspaces with sharp boundaries, limited authoring support, and limited and non-constructive communications between the user and the adaptive model. The remainder of this paper will discuss these limitations with second-generation systems and propose framework for solutions. 2. Third Generation Adaptive Hypermedia Systems 2.1 Multidimensional User Models Third generation adaptive hypermedia systems should support multidimensional user models. Current user models measure limited user characteristics to normally a single dimension such as declared or demonstrated knowledge, or hypermedia nodes visited. For example, AHA [Bra and Calvi 1998] uses the number of nodes visited while CS383 [Carver et al. 1996] consider learning styles as the basis for the user model. Actual users in a learning environment are much more complex and are both multi-dimensional and multi-faceted. Future user models must incorporate multiple dimensions of the user including expertise, user goals, interests, and preferred learning style by subject matter. These dimensions may be declared by the user, measured by the adaptive system, or combination of both approaches. Not only must the user model incorporate multiple dimensions, the importance of an individual user model dimension may vary over time. As a user progresses through hyperspace, their goals and interests may change as they learn new concepts. The user model must quickly adapt to these changes in the user model so as to present relevant information to the user. Discrepancies between declared and demonstrated user characteristics must be resolved and presentation of material adapted. The users of adaptive hypermedia systems are not one-dimensional but instead are multidimensional. Future user models in adaptive hypermedia systems should be multidimensional and adaptive as well. Providing adaptive, multidimensional user models raises a number of open research issues. Most current adaptive systems allow their users to explicitly manipulate the user model. This is most commonly done through a long list of checkboxes. Different presentation techniques will be required for users to effectively manipulate multiple user dimensions. The effective manipulation of a multidimensional user model clearly presents significant user interface issues. Additionally, it remains an open research issue as to what is the proper type and number of dimensions to measure. Adding additional dimensions will not always increase the accuracy of the user model but will always increase the complexity of the user model and the requirements to collect additional user information. There is a balance between the number of dimensions, model complexity, and the accuracy of the model. Finally, techniques for modifying the weights associated with different dimensions dynamically to better represent the user are open research issues. 2.2 Finely Grained, Multimedia Adaptation Third generation adaptive hypermedia systems should provide a fine degree of adaptation granularity and adapt more than just hypertext. All second-generation adaptive hypermedia systems provide text-based adaptation based on a user model with limited levels of user differentiation. In addition to using multidimensional user models, third generation systems must incorporate multiple levels in each user model dimension so that\n\n",
                "DataExportTag": "AI369533",
                "QuestionID": "QID216",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hype...",
                "Choices": {
                    "1": {
                        "Display": "\"Resource Allocation and Scheduling in Distributed Systems\""
                    },
                    "2": {
                        "Display": "distribute, cooperative, coordination, allocation, decentralized, resource_allocation, centralized, scheduling, sharing, heterogeneous, collaborative, decentralize, distributed, slice, slicing"
                    },
                    "3": {
                        "Display": "\"Edge Computing and Resource Allocation in UAV Networks\""
                    },
                    "4": {
                        "Display": "unmanned_aerial_vehicles, deep_learning, offload, edge_computing, vehicle, mobile, resource_allocation, deep_reinforcement_learning, mec, user, traffic, vehicular, caching, wireless, edge"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID277",
            "SecondaryAttribute": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hype...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hypermedia systems and proposes adaptive characteristics for third generation adaptive hypermedia systems. First generation adaptive hypermedia systems predated the World-Wide Web (WWW) and were primarily single user adaptive hypermedia systems. Second generation systems have exploited the distributed nature and ease of authoring of the WWW to develop more robust and mature adaptive hypermedia systems. While these systems are a dramatic improvement over first generation systems, they have several limitations that limited the effectiveness of adaptation. These limitations include limited adaptation through one-dimensional, stereotypical user models, coarse granularity of adaptive support, closed adaptive hyperspaces with sharp boundaries, limited authoring support, and limited and nonconstructive communications between the user and the adaptive model. Third generation adaptive hypermedia systems must address these shortcomings to fully exploit the potential of adaptive hypermedia. Future systems must provide explicit, fine-grained adaptation support that the user can easily tailor and refine to provide highly relevant multidimensional adaptation. Future adaptive hypermedia systems must be open systems with soft boundaries that are expandable and incorporate resources from non-adaptive hypermedia with gradual degradation of support. Finally, adaptive systems must be relatively easy to build and maintain. This paper examines these characteristics of future adaptive hypermedia systems and proposes a framework for development. 1. Background 1.1 First Generation Adaptive Hypermedia Systems Numerous adaptive hypermedia systems have been implemented over the last fifteen years. These systems can be characterized as first generation or second-generation adaptive hypermedia systems based on when they were developed and what delivery mechanism was used for deployment of the systems. First generation systems span the period from 1985-1993 and are principally adaptive systems that were not distributed in nature. These systems were generally PC or Macintosh-based provided limited adaptability through stereotype-based user models and limited functionality adaptation techniques such as conditional text filters, direct guidance, stretchtext, hiding, and primitive link annotation. EPIAIM [Rosis et al. 1994], Hypadapter [Bocker et al. 1990], ITEM\/IP [Brusilovsky 1992], ISISTutor [Brusilovsky and Pesin 1994; Brusilovsky and Pesin 1994], MetaDoc [Boyle and Encarnacion 1994], and MetaDoc V [Boyle and Teh 1993] are examples of first generation adaptive hypermedia systems. ISIS-Tutor is a good example a first generation adaptive system. It is an adaptive learning environment for the information retrieval system CDS\/ISIS\/M (ISIS) [Brusilovsky and Pesin 1994; Brusilovsky and Pesin 1994]. ISISTutor provided adaptive support through frame-based presentation, link annotation, and information hiding. It presented an adaptive sequence of tasks and concepts based on the user model and maintained a four state, user knowledge model (not-ready-to-be-learned, ready-to-be-learned, in-work, and learned) on each concept within ISIS. As the user progressed through the system, ISIS-Tutor annotated each concept with color and hiding could be enabled to remove concepts that the user was not ready for. ISIS-Tutor was one of the first adaptive models to incorporate more than a bipolar knowledge model and one of the first adaptive systems to have empirical support for the effectiveness of adaptive interfaces [Brusilovsky and Pesin 1994]. The advent of the World-Wide Web, however, provided new opportunities for the development of adaptive hypermedia systems and led to new systems and more advanced adaptive techniques. 1.2 Second Generation Adaptive Hypermedia Systems Second generation systems span the period from 1993 to present and predominantly use the WWW as their delivery and presentation means. These systems have worldwide availability and are generally platform independent. Second generation systems introduced new capabilities such as adaptive multimedia presentation, map adaptation, and link sorting. They also refined existing adaptation techniques to provide greater functionality. User models became more robust and incorporated more user characteristics. AHA [Bra 1996; Bra and Calvi 1998], ARNIE-HyperMan [Mathe and Chen 1994; Rabinowitz et al. 1995; Mathe and Chen 1996], AVANTI [Fink et al. 1997; Rizzo et al. 1997; Nill 1998], Basar [Thomas 1995], CS383 [Carver et al. 1996; Carver et al. 1996], Cameleon [Laroussi and Benahmed 1999], ELM-ART [Brusilovsky et al. 1996], I-Doc [Erdem and Johnson 1998; Erdem et al. 1998], InterBook [Brusilovsky and Eklund 1998] , KN-AHS [Kobsa et al. 1994], MediaDoc [Erdem et al. 1998], RATH [Hockemeyer et al. 1998] and WebWatcher [Armstrong et al. 1995] are examples of second generation adaptive hypermedia systems. ELM-ART (The Episodic Learning Model: The Adaptive Remote Tutor) is good example of a second-generation adaptive hypermedia system. ELM-ART is a distributed intelligent tutoring system on LISP that provides course adaptation through a combination of adaptive annotation and link sorting [Brusilovsky et al. 1996]. Links are colorcoded according to user preparation for the information in the node. Red annotations indicate nodes that the user has to meet the prerequisites for, amber nodes represent information the student is ready for but not recommended, and green nodes represent nodes that the user is ready for and are recommended. ELM-ART adaptively sorts links as well so that the links that are most similar to the node that the user is currently on are presented first. ELM-ART features extensive user feedback and is highly interactive. As the user completes exercises and reads nodes, status bars change to reflect the user\u2019s progress through the course. Link annotations change color and the navigational view changes to reflect the user\u2019s newly gained knowledge. Users can directly edit the user model and override the navigational choices present to meet their educational goals. An extensive number of exercises engage the user and proved constant feedback both to the user and to the user model on the user\u2019s evaluated knowledge. While second generation adaptive hypermedia systems such as ELM-ART have dramatically improved upon the functionality of first generation systems, there are fundamental flaws associated with these systems. These limitations include limited adaptation through one-dimensional, stereotypical user models, coarse granularity of adaptive support, closed adaptive hyperspaces with sharp boundaries, limited authoring support, and limited and non-constructive communications between the user and the adaptive model. The remainder of this paper will discuss these limitations with second-generation systems and propose framework for solutions. 2. Third Generation Adaptive Hypermedia Systems 2.1 Multidimensional User Models Third generation adaptive hypermedia systems should support multidimensional user models. Current user models measure limited user characteristics to normally a single dimension such as declared or demonstrated knowledge, or hypermedia nodes visited. For example, AHA [Bra and Calvi 1998] uses the number of nodes visited while CS383 [Carver et al. 1996] consider learning styles as the basis for the user model. Actual users in a learning environment are much more complex and are both multi-dimensional and multi-faceted. Future user models must incorporate multiple dimensions of the user including expertise, user goals, interests, and preferred learning style by subject matter. These dimensions may be declared by the user, measured by the adaptive system, or combination of both approaches. Not only must the user model incorporate multiple dimensions, the importance of an individual user model dimension may vary over time. As a user progresses through hyperspace, their goals and interests may change as they learn new concepts. The user model must quickly adapt to these changes in the user model so as to present relevant information to the user. Discrepancies between declared and demonstrated user characteristics must be resolved and presentation of material adapted. The users of adaptive hypermedia systems are not one-dimensional but instead are multidimensional. Future user models in adaptive hypermedia systems should be multidimensional and adaptive as well. Providing adaptive, multidimensional user models raises a number of open research issues. Most current adaptive systems allow their users to explicitly manipulate the user model. This is most commonly done through a long list of checkboxes. Different presentation techniques will be required for users to effectively manipulate multiple user dimensions. The effective manipulation of a multidimensional user model clearly presents significant user interface issues. Additionally, it remains an open research issue as to what is the proper type and number of dimensions to measure. Adding additional dimensions will not always increase the accuracy of the user model but will always increase the complexity of the user model and the requirements to collect additional user information. There is a balance between the number of dimensions, model complexity, and the accuracy of the model. Finally, techniques for modifying the weights associated with different dimensions dynamically to better represent the user are open research issues. 2.2 Finely Grained, Multimedia Adaptation Third generation adaptive hypermedia systems should provide a fine degree of adaptation granularity and adapt more than just hypertext. All second-generation adaptive hypermedia systems provide text-based adaptation based on a user model with limited levels of user differentiation. In addition to using multidimensional user models, third generation systems must incorporate multiple levels in each user model dimension so that\n\n",
                "DataExportTag": "AI369533",
                "QuestionID": "QID277",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hype...",
                "Choices": {
                    "1": {
                        "Display": "\"Resource Allocation and Scheduling in Distributed Systems\""
                    },
                    "2": {
                        "Display": "distribute, cooperative, coordination, allocation, decentralized, resource_allocation, centralized, scheduling, sharing, heterogeneous, collaborative, decentralize, distributed, slice, slicing"
                    },
                    "3": {
                        "Display": "\"Edge Computing and Resource Allocation in UAV Networks\""
                    },
                    "4": {
                        "Display": "unmanned_aerial_vehicles, deep_learning, offload, edge_computing, vehicle, mobile, resource_allocation, deep_reinforcement_learning, mec, user, traffic, vehicular, caching, wireless, edge"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID401",
            "SecondaryAttribute": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hype...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hypermedia systems and proposes adaptive characteristics for third generation adaptive hypermedia systems. First generation adaptive hypermedia systems predated the World-Wide Web (WWW) and were primarily single user adaptive hypermedia systems. Second generation systems have exploited the distributed nature and ease of authoring of the WWW to develop more robust and mature adaptive hypermedia systems. While these systems are a dramatic improvement over first generation systems, they have several limitations that limited the effectiveness of adaptation. These limitations include limited adaptation through one-dimensional, stereotypical user models, coarse granularity of adaptive support, closed adaptive hyperspaces with sharp boundaries, limited authoring support, and limited and nonconstructive communications between the user and the adaptive model. Third generation adaptive hypermedia systems must address these shortcomings to fully exploit the potential of adaptive hypermedia. Future systems must provide explicit, fine-grained adaptation support that the user can easily tailor and refine to provide highly relevant multidimensional adaptation. Future adaptive hypermedia systems must be open systems with soft boundaries that are expandable and incorporate resources from non-adaptive hypermedia with gradual degradation of support. Finally, adaptive systems must be relatively easy to build and maintain. This paper examines these characteristics of future adaptive hypermedia systems and proposes a framework for development. 1. Background 1.1 First Generation Adaptive Hypermedia Systems Numerous adaptive hypermedia systems have been implemented over the last fifteen years. These systems can be characterized as first generation or second-generation adaptive hypermedia systems based on when they were developed and what delivery mechanism was used for deployment of the systems. First generation systems span the period from 1985-1993 and are principally adaptive systems that were not distributed in nature. These systems were generally PC or Macintosh-based provided limited adaptability through stereotype-based user models and limited functionality adaptation techniques such as conditional text filters, direct guidance, stretchtext, hiding, and primitive link annotation. EPIAIM [Rosis et al. 1994], Hypadapter [Bocker et al. 1990], ITEM\/IP [Brusilovsky 1992], ISISTutor [Brusilovsky and Pesin 1994; Brusilovsky and Pesin 1994], MetaDoc [Boyle and Encarnacion 1994], and MetaDoc V [Boyle and Teh 1993] are examples of first generation adaptive hypermedia systems. ISIS-Tutor is a good example a first generation adaptive system. It is an adaptive learning environment for the information retrieval system CDS\/ISIS\/M (ISIS) [Brusilovsky and Pesin 1994; Brusilovsky and Pesin 1994]. ISISTutor provided adaptive support through frame-based presentation, link annotation, and information hiding. It presented an adaptive sequence of tasks and concepts based on the user model and maintained a four state, user knowledge model (not-ready-to-be-learned, ready-to-be-learned, in-work, and learned) on each concept within ISIS. As the user progressed through the system, ISIS-Tutor annotated each concept with color and hiding could be enabled to remove concepts that the user was not ready for. ISIS-Tutor was one of the first adaptive models to incorporate more than a bipolar knowledge model and one of the first adaptive systems to have empirical support for the effectiveness of adaptive interfaces [Brusilovsky and Pesin 1994]. The advent of the World-Wide Web, however, provided new opportunities for the development of adaptive hypermedia systems and led to new systems and more advanced adaptive techniques. 1.2 Second Generation Adaptive Hypermedia Systems Second generation systems span the period from 1993 to present and predominantly use the WWW as their delivery and presentation means. These systems have worldwide availability and are generally platform independent. Second generation systems introduced new capabilities such as adaptive multimedia presentation, map adaptation, and link sorting. They also refined existing adaptation techniques to provide greater functionality. User models became more robust and incorporated more user characteristics. AHA [Bra 1996; Bra and Calvi 1998], ARNIE-HyperMan [Mathe and Chen 1994; Rabinowitz et al. 1995; Mathe and Chen 1996], AVANTI [Fink et al. 1997; Rizzo et al. 1997; Nill 1998], Basar [Thomas 1995], CS383 [Carver et al. 1996; Carver et al. 1996], Cameleon [Laroussi and Benahmed 1999], ELM-ART [Brusilovsky et al. 1996], I-Doc [Erdem and Johnson 1998; Erdem et al. 1998], InterBook [Brusilovsky and Eklund 1998] , KN-AHS [Kobsa et al. 1994], MediaDoc [Erdem et al. 1998], RATH [Hockemeyer et al. 1998] and WebWatcher [Armstrong et al. 1995] are examples of second generation adaptive hypermedia systems. ELM-ART (The Episodic Learning Model: The Adaptive Remote Tutor) is good example of a second-generation adaptive hypermedia system. ELM-ART is a distributed intelligent tutoring system on LISP that provides course adaptation through a combination of adaptive annotation and link sorting [Brusilovsky et al. 1996]. Links are colorcoded according to user preparation for the information in the node. Red annotations indicate nodes that the user has to meet the prerequisites for, amber nodes represent information the student is ready for but not recommended, and green nodes represent nodes that the user is ready for and are recommended. ELM-ART adaptively sorts links as well so that the links that are most similar to the node that the user is currently on are presented first. ELM-ART features extensive user feedback and is highly interactive. As the user completes exercises and reads nodes, status bars change to reflect the user\u2019s progress through the course. Link annotations change color and the navigational view changes to reflect the user\u2019s newly gained knowledge. Users can directly edit the user model and override the navigational choices present to meet their educational goals. An extensive number of exercises engage the user and proved constant feedback both to the user and to the user model on the user\u2019s evaluated knowledge. While second generation adaptive hypermedia systems such as ELM-ART have dramatically improved upon the functionality of first generation systems, there are fundamental flaws associated with these systems. These limitations include limited adaptation through one-dimensional, stereotypical user models, coarse granularity of adaptive support, closed adaptive hyperspaces with sharp boundaries, limited authoring support, and limited and non-constructive communications between the user and the adaptive model. The remainder of this paper will discuss these limitations with second-generation systems and propose framework for solutions. 2. Third Generation Adaptive Hypermedia Systems 2.1 Multidimensional User Models Third generation adaptive hypermedia systems should support multidimensional user models. Current user models measure limited user characteristics to normally a single dimension such as declared or demonstrated knowledge, or hypermedia nodes visited. For example, AHA [Bra and Calvi 1998] uses the number of nodes visited while CS383 [Carver et al. 1996] consider learning styles as the basis for the user model. Actual users in a learning environment are much more complex and are both multi-dimensional and multi-faceted. Future user models must incorporate multiple dimensions of the user including expertise, user goals, interests, and preferred learning style by subject matter. These dimensions may be declared by the user, measured by the adaptive system, or combination of both approaches. Not only must the user model incorporate multiple dimensions, the importance of an individual user model dimension may vary over time. As a user progresses through hyperspace, their goals and interests may change as they learn new concepts. The user model must quickly adapt to these changes in the user model so as to present relevant information to the user. Discrepancies between declared and demonstrated user characteristics must be resolved and presentation of material adapted. The users of adaptive hypermedia systems are not one-dimensional but instead are multidimensional. Future user models in adaptive hypermedia systems should be multidimensional and adaptive as well. Providing adaptive, multidimensional user models raises a number of open research issues. Most current adaptive systems allow their users to explicitly manipulate the user model. This is most commonly done through a long list of checkboxes. Different presentation techniques will be required for users to effectively manipulate multiple user dimensions. The effective manipulation of a multidimensional user model clearly presents significant user interface issues. Additionally, it remains an open research issue as to what is the proper type and number of dimensions to measure. Adding additional dimensions will not always increase the accuracy of the user model but will always increase the complexity of the user model and the requirements to collect additional user information. There is a balance between the number of dimensions, model complexity, and the accuracy of the model. Finally, techniques for modifying the weights associated with different dimensions dynamically to better represent the user are open research issues. 2.2 Finely Grained, Multimedia Adaptation Third generation adaptive hypermedia systems should provide a fine degree of adaptation granularity and adapt more than just hypertext. All second-generation adaptive hypermedia systems provide text-based adaptation based on a user model with limited levels of user differentiation. In addition to using multidimensional user models, third generation systems must incorporate multiple levels in each user model dimension so that\n\n",
                "DataExportTag": "AI13165",
                "QuestionID": "QID401",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Third Generation Adaptive Hypermedia Systems This paper examines the development of adaptive hype...",
                "Choices": {
                    "1": {
                        "Display": "\"Online Community Management and Social Media Monitoring\""
                    },
                    "2": {
                        "Display": "social, social_medium, hate_speech, user, post, comment, facebook, forum, community, crowdsource, online, crowd, social_media, crowdsourcing, security_administration"
                    },
                    "3": {
                        "Display": "\"Web Search and Data Mining\""
                    },
                    "4": {
                        "Display": "web, data_mining, web_page, search_engine, text, clustering, user, query, document, blog, retrieval, association_rule, forum, internet, semantic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID393",
            "SecondaryAttribute": "This work reveals the novel finding of specificity of T3 and T4 acting on a single cell surface r...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "This work reveals the novel finding of specificity of T3 and T4 acting on a single cell surface receptor at apparently distinct sites within the molecule that regulate activation of separate downstream signaling pathways. Like the glioma cells, alveolar epithelial cells also demonstrate T3 activation of both the MAPK and PI3K pathways, but the ligand to which T3 binds in that system has not been determined yet. It will be interesting to determine whether a single integrin molecule can differentially activate nongenomic signaling pathways depending on the specific ligands. The studies of Lin et al. (13, 14) add significantly to our knowledge of the rapid nongenomic actions of thyroid hormones, demonstrating a sophisticated specificity exerted at the cell surface binding of hormone to plasma membrane integrin that affects TR activity and signaling pathway activation. Another unresolved question is whether there are intracellular interactions between the PI3K and MAPK signaling pathways after they are triggered by thyroid hormones at the plasma membrane. These findings are important biologically and also offer the opportunity to target specific pathways that may be manipulated in treating pathological states.\n\n",
                "DataExportTag": "47",
                "QuestionID": "QID393",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "This work reveals the novel finding of specificity of T3 and T4 acting on a single cell surface r...",
                "Choices": {
                    "1": {
                        "Display": "\"Gene Transcription and Regulation\""
                    },
                    "2": {
                        "Display": "promoter, transcription, bind, transcriptional, messenger_rna, gene, element, binding, splicing, enhancer, translation, deoxyribonucleic_acid, creb, region, transactivation"
                    },
                    "3": {
                        "Display": "\"Epigenetic Regulation and Gene Modification\""
                    },
                    "4": {
                        "Display": "pten, chromatin, histone, regulation, stress, methylation, epigenetic, autophagy, metabolic, enzyme, gene, modification, translation, acetylation, control"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID349",
            "SecondaryAttribute": "Timing cell cycles in multicellular development Animal development depends on precisely timed cel...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Timing cell cycles in multicellular development Animal development depends on precisely timed cell divisions, which ensure the formation of tissues and organs with specific architectures and functions. As cells differentiate and mature, they need to modify the expression of cell-cycle regulators in order to achieve tissue-specific division patterns. It is largely unknown how cells coordinate cell-cycle gene expression changes with developmental timing and how cell cycles can be altered to give rise to different tissue architectures and organ sizes.DevCycle will address these questions by studying intestinal cell cycles in nematodes. I have recently pioneered tools to measure mRNA expression changes and chromatin modifications in purified intestinal cells from C. elegans, which we will use to uncover how the expression of cell-cycle genes is changing during development and to identify novel regulators that control stage-specific gene expression. Moreover, I will implement a novel microfluidics platform for long-term imaging of larval development to identify the mechanisms that allow temporal coupling of cell cycle and development. This system will allow us to visualize and manipulate transcription factor gradients that direct intestinal cell cycles, providing unprecedented insights into how cell cycles are timed. Finally, I will analyse the mechanisms that control cell-cycle behaviour in P. redivivus, a related nematode in which intestinal cells undergo a slightly different cell-cycle pattern. Expanding our analyses to this species will empower us to investigate how cell-cycle patterns can diversify to generate phenotypic variability. Together, this research program will develop a mechanistic understanding of the temporal control of cell cycles in development, revealing new paradigms on how tissue-specific division patterns arise. This knowledge will open up future avenues to modulate cell cycles for tissue engineering purposes, as well as to control cell divisions in disease.\n\n",
                "DataExportTag": "COR54906",
                "QuestionID": "QID349",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Timing cell cycles in multicellular development Animal development depends on precisely timed cel...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetics and Epigenetics Research\""
                    },
                    "2": {
                        "Display": "deoxyribonucleic_acid, ribonucleic_acid, chromatin, genomic, epigenetic, methylation, repair, sequence, gene_expression, chromosome, transcription, protein, genetic, mirnas, non_coding"
                    },
                    "3": {
                        "Display": "\"Stem Cell Research and Tissue Regeneration\""
                    },
                    "4": {
                        "Display": "cellular, stem_cell, tissue, differentiation, single, regeneration, cell_fate, lineage, embryo, hsc, niche, mechanical, mouse, adult, organ"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID331",
            "SecondaryAttribute": "Title: Predictive coding accelerates word recognition and learning in the early stages of languag...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Title: Predictive coding accelerates word recognition and learning in the early stages of language development Running head: Predictions in children\u2019s word recognition and learning The ability to predict future events in the environment and learn from them is a fundamental component of adaptive behavior across species. Here we propose that inferring predictions facilitates speech processing and word learning in the early stages of language development. Twelveand 24-month olds\u2019 electrophysiological brain responses to heard syllables are faster and more robust when the preceding word context predicts the ending of a familiar word. For unfamiliar, novel word forms, however, word-expectancy violation generates a prediction error response, the strength of which significantly correlates with children\u2019s vocabulary scores at 12 months. These results suggest that predictive coding may accelerate word recognition and support early learning of novel words, including not only the learning of heard word forms but also their mapping to meanings. Prediction error may mediate learning via attention, since infants' attention allocation to entire learning situation in natural environments could account for the link between prediction error and the understanding of word meanings. On the whole, the present results on predictive coding support the view that principles of brain function reported across domains in humans and non-human animals apply to language and its development in the infant brain. Introduction In both humans and non-human animals, the capacity to predict future events on the basis of probabilistic inference is crucial for adaptation to the environment and, in some cases, even for survival. Predictive coding theory (Rao & Ballard, 1999; Friston, 2005, 2009, 2010) sheds light on the neural underpinnings of such predictions. According to the theory, predictive coding is an implicit process that creates an internal model about sensory input with the aim of minimizing surprise. It is based on bidirectional information flow in a hierarchical neural network. The inferred causes of the incoming sensory input are encoded by representational units located at a higher level of the network. They send predictions to prediction error units located at a lower level, where bottom-up input is compared with these top-down predictions. A match between input and prediction results in a suppressed neural response, whereas a mismatch elicits a prediction error response, which is projected back to the higher level to adjust the internal model. As a result, only input that does not match the internal model requires further processing resources. As recently reviewed by Pouget, Beck, Ma, and Latham (2013), the brain uses probabilistic inference to solve many tasks, ranging from sensory processing that is shared by species to higher cognitive functions that are specific to humans. One of the neurocognitive processes observed across species and linked with probabilistic inference is the elicitation of the mismatch negativity (MMN) component of auditory event-related potential (ERP) (N\u00e4\u00e4t\u00e4nen, Gaillard, & M\u00e4ntysalo, 1978; for reviews on MMN, see N\u00e4\u00e4t\u00e4nen, Paavilainen, Rinne, & Alho, 2007; Winkler, 2007; N\u00e4\u00e4t\u00e4nen, Astikainen, Ruusuvirta, & Huotilainen, 2010). MMN is typically elicited in an oddball paradigm, which refers to a stimulus sequence with a high probability of repetitive standard stimuli and occasional deviant stimuli. MMN reflects automatic auditory processing in the brain, since it is elicited even when participant's attention is directed away from the sounds. Winkler (2007) has proposed that the brain uses the regularities of the auditory environment to create a model that predicts the sounds that will follow. Violations of these predictions elicit the MMN, which is in line with the predictive coding theory. Furthermore, Friston (2005) has explicitly proposed that the MMN belongs to the family of prediction error responses (see also Wacongne et al., 2011). Interestingly, recent studies have suggested that the domain-general principles of predictive coding seem to also serve the processing of language. Gagnepain, Henson, and Davis (2012) have proposed that human adults use predictive coding to recognize spoken words. Specifically, their pattern of findings is compatible with the interpretation that compared with brain responses to unpredicted speech sounds, speech sounds predicted on the basis of long-term memory word representations generate reduced responses in superior temporal cortex. In contrast, speech sounds that do not match word-based predictions generate enhanced brain responses, interpreted as prediction error signals. Thus, predictive coding may, together with several other factors, contribute to speech recognition. However, the principle of predictive coding has not been associated with recognition only, but also with learning, because predictions promote reward-based associative learning in non-human animals (Schultz, Dayan, & Montague, 1997; Steinberg et al., 2013). Bridging the gap between these findings, we postulate that predictive coding could support both spoken-word recognition and word learning in infants and young children, as they show the ability of predictive coding from birth (Trainor, 2012; Haden, Nemeth, Torok, & Winkler, 2015). To test this hypothesis with recordings of auditory event-related potential (ERP), we presented 12and 24-month-old children with native language syllables in an oddball paradigm, where syllable patterns were expected to generate predictions at different levels. These predictions can be probed by violating them, which elicits the mismatch response (child equivalent of the MMN) or prediction error. The hypothesis on different levels of prediction is based on previous studies that have, on the one hand, suggested that MMN reflects hierarchical predictions derived from probabilities within sound sequences (Wacongne et al., 2011; Basirat, Dehaene, & Dehaene-Lambertz, 2014) and, on the other hand, shown that predictions and consequent prediction error responses may also be driven by word representations in the long-term memory (Gagnepain et al., 2012). Thus, when an oddball paradigm includes isolated syllables, they most likely induce predictions about the following syllables on the basis of stimulus probabilities in the stimulus sequence. Violations of these sequence-level predictions about the following items are expected to elicit mismatch responses. In contrast, when the same syllables are presented within words as second syllables, the word beginnings may induce predictions about these syllables at the level of word representations. In this case, the violations of predictions are expected to generate prediction error responses at word level. In addition to probing predictions with their violations, the oddball paradigm can be used to study the activation of long-term memory representations for words (for a review, see Pulverm\u00fcller & Shtyrov, 2006). When presented among pseudowords, words typically elicit larger MMN responses than pseudowords, because deviant syllables completing words activate the corresponding word representations resulting in enhanced responses (Pulverm\u00fcller et al. 2001; Garagnani, Wennekers, & Pulverm\u00fcller, 2008; Garagnani & Pulverm\u00fcller, 2011). According to the neurocomputational model by Garagnani and Pulverm\u00fcller (2011), local inhibition of cortical activity or the combination of inhibition and neuronal adaptation may account for MMN responses in non-speech tones, but not in 1 Since in infants and young children the brain responses elicited in an oddball paradigm and resembling MMN with respect to their function can be negative or positive in polarity, we will further use the terms mismatch response to refer to this response in children and MMN to refer to it in adults. words. The most likely explanation for their pattern of results in words is the \u201cignition\u201d and reverberation of activity in the neural network covering distant brain areas, including areas involved in speech perception and production (Garagnani & Pulverm\u00fcller, 2011, see also Pulverm\u00fcller, 1999). To reveal the effect of word-level predictions and lexical activations in a setup controlling for the effect of sequence-level predictions and neuronal adaptation in repetitive stimulation, we compared ERP responses in the two conditions that used the same syllables of interest. Firstly, in No context condition (Fig.1A), we presented the standard syllable, occurring with a high probability, in isolation. Secondly, in Context condition (Fig. 1B), we presented the same standard syllable after a familiar word beginning, enabling to interpret the syllable of interest as the word ending. Thus, the critical difference between the two conditions was the absence or presence of the context syllable. In both conditions, the standard syllable was expected to create sequence-level predictions of hearing this particular syllable (Fig. 1C and D, longer arrows). In Context condition, however, the context syllable representing a familiar word beginning was expected to create predictions at another level, namely, to create word-level predictions of hearing the ending of some familiar word (Fig. 1D, shorter arrows; for discussion on knowledge-based predictions, see Poeppel & Monahan, 2011; for phonological predictions in pseudowords, see Ylinen et al., submitted). To probe the predictions at sequence and word levels, we presented two deviant syllables (Fig. 1A and B). In No context condition, both deviants were equally unexpected with respect to sequence-level predictions and hypothesized to elicit typical mismatch responses (see Table 1). In Context condition, however, the same deviant syllables either completed a familiar word, the meaning of which the children knew, or a novel word, the meaning of which the children could not know (Fig. 1B). The involvement of words in Context condition was hypothesized to enable the interaction of sequence-level and word-level predictions and to result in different brain responses to the deviant syllables (see Table 1): The deviant syllable completing the\n\n",
                "DataExportTag": "AI429782",
                "QuestionID": "QID331",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Title: Predictive coding accelerates word recognition and learning in the early stages of languag...",
                "Choices": {
                    "1": {
                        "Display": "\"Neuroscience and Sensory Perception\""
                    },
                    "2": {
                        "Display": "spike, brain, neuronal, synaptic, animal, biological, neuroscience, sensory, cortical, stimulus, odor, synapsis, fire, hebbian, plasticity"
                    },
                    "3": {
                        "Display": "\"Neuroscience and Reinforcement Learning in Parkinson's Disease\""
                    },
                    "4": {
                        "Display": "sequence, reward, motor, reinforcement_learning, implicit, action, dopamine, trial, parkinson_disease, feedback, movement, basal_ganglia, participant, striatum, behavior"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID333",
            "SecondaryAttribute": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Towards a Generative System for Intelligent Design Support In the development of intelligent computer aided design systems, three important issues need to considered. These issues are: how to support the generation of product concepts using evolutionary computation techniques; how to use intelligent databases and constraint management systems for detailed exploration of product embodiment; and how to integrate rapid prototyping facilities for product evaluation. In this paper, we present a brief review of knowledge based design and evolutionary design and discuss ways of integrating both in the development of a generative design system. Based on this review, we present the model and its applications of a generative design system utilizing a number of AI and evolutionary computation techniques. This generative design model is intended to provide a generic computational framework for the development of intelligent design support systems. 1. Knowledge-based design Existing computer-based design systems provide little support for the evolution of design solution concepts that satisfy all physical constraints. They rely on precise geometric information to specify the model of an artifact. They cannot support early stage design tasks because the geometry of an artifact is not sufficiently definite at the early stage. The design of complex products containing many components require a number of alternative solutions to be explored, each being modified by making changes to key design parameters, until a satisfactory design solution is found. The generation and exploration of design concepts based on incomplete design requirements, and the handling of a multitude of variables and constraints embedded within 3D design objects is a challenging task for designers. Four main strategies may characterize the current developments in knowledge-based design systems: \u2022 The intelligent CAD approach; \u2022 The building block approach; \u2022 The prototype approach; and \u2022 The constraint-based approach. JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 286 The intelligent CAD approach extends the capability of a CAD system by employing heuristic knowledge on top of geometric models of the artifacts. An intelligent CAD system plays an active rather than a passive role in the design process, by incorporating a significant amount of design knowledge into the system. Functional modeling, for example, is an approach that attempts to build intelligent CAD systems through a mapping from a function space to a conceptual solution space using functional instead of geometric components and their interfaces [Chakrabarti and Tang 1996]. This approach uses abstract knowledge to represent the function and behavior of elementary design components, parts and their causal relationships. This abstract knowledge is used to generate initial conceptual solutions, which are subsequently selected, evaluated and specialized further. This approach is based on the assumption that designers use abstract and qualitative, knowledge at the early stage of the design process to generate a conceptual model of an artifact. This conceptual model can be generated without detailed geometric information such as dimensions. The building block approach formulates design process as a sequence of tasks that can be tackled individually by different classes of CAD tools and AI methods [Brown et al 1989]. Chandrasekaran classified these tasks into three categories: 1. Class 1 design tasks in which the components of the artifact being designed are unknown; 2. Class 2 design tasks in which the components of the artifact being designed are known, but the design plans, i.e. the methods of how to design, are unavailable in a compiled form; and 3. Class 3 design tasks where ways of decomposing a design problem are already known and for which compiled design plans are available for each stage of the design process In this approach, design tasks under class 3 are considered routine designs that can be conveniently supported by automated computer programs. The others are more difficult and require significant creative inputs from human designers. The prototype design approach divides design process into three different activities: prototype refinement, prototype adaptation and prototype creation based on a library of design prototypes [Gero et al 1989]. Gero classified design as: routine design where the functions, expected behaviors and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables; innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements; and creative design, in which an entirely new design problem space is to be defined [Coyne et al 1990]. Prototype refinement involves instantiating the variables in a design prototype and determining their values. Prototype adaptation becomes necessary TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT 287 when a design prototype is found to be inadequate in some way, and needs to be adapted to a new design problem, or made generally more useful for other design problem solving. Prototype creation is seen as the ultimate design endeavour in this approach. So far research based on this approach has mainly concerned the problems of prototype refinement and prototype adaptation. Little progress has been reported on the issue of prototype creation and it remains a difficult subject. The constraint-based approach models an engineering system as a constraint network or a hierarchical structure, i.e. collections of constraints that are interconnected by design variables [Smithers et al 1990]. Instantiation of this structure, or part of it, forms a basic structure of a new design problem. Computer-based constraint-based reasoning techniques are used to derive the values of unknown design variables from some initial data provided by the designers to form the solutions to the new design problem. Computational methods are used to detect constraint violation and to suggest alternative values for design variables. The application of design strategies or design methods is reflected in the way in which design variables are created, constrained, and manipulated. The original aim of AI and knowledge based design was to produce general purpose, domain independent AI tools that would automate design tasks requiring intelligence. The early promise of knowledge based design has not been fulfilled because these approaches: \u2022 did not scale within and across domains, \u2022 unable to learn and adapt to design contexts, \u2022 failed to handle complexity, consistency and unpredictability, \u2022 could not acquire knowledge satisfactorily, and \u2022 could not model and support creativity. 2. Evolutionary design Recent advancement in evolutionary computing provides new opportunities to re-examine the issue of intelligent design support. Genetic algorithms are widely accepted as powerful generative and adaptive techniques generally applicable to many design activities. Evolutionary design is an approach that utilizes different evolutionary computation techniques in different stages of the design process. Evolutionary design is capable of generating astonishing imaginative and innovative designs [Bentley 1999]. The strength of evolutionary design comes from the factor that controlled evolution can be formulated as a general purposed problem solver with ability similar to human design intelligence but with a magnitude of speed and efficiency. Traditional AI methods such as rule-based reasoning have to model design intelligence explicitly in terms of knowledge both in representation and inference. These JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 288 methods have serious drawbacks because the process of how human designers actually use this kind of knowledge is not necessarily fully understood. Bentley classified evolutionary design into four categories. These are: evolutionary design optimization that is concerned with optimizing existing designs by evolving the values of suitably constrained design parameters; creative evolutionary design that generates entirely new designs from little abstract knowledge to satisfy functional requirements; conceptual evolutionary design that deals with the production of high level conceptual frameworks of preliminary designs; and generative evolutionary design that directly produces forms of designs contributing to the emergence of implicit design concepts. These evolutionary design approaches combine several vital aspects of design intelligence in an evolutionary process, including modeling design data and information, concept formation, idea generation, optimization, learning, and evaluation. Once a design problem is properly formulated in an evolutionary process, the computer is able to generate a large number of candidate solutions before reaching at an optimum one. The candidate solutions are unpredictable but the process and the final outcome are controllable by the designers. The evolutionary design approach has an excellent potential for developing more intelligent design support tools. For example, Frazer used genetic algorithms in his evolutionary architectural design to evolve unpredicted forms of architectures and their possible interactions with the environments [Frazer 1995, 1996 and 1997]. Chakrabarti developed a functional synthesis program that generates a large number of abstract design concepts from functional requirements and abstract building blocks of engineering elements [Chakrabarti and Tang 1996]. Thornton utilised genetic algorithms as constraint management tools in the process of embodiment design [Thornton 1994]. However, the development of evolutionary design tools is still at its early stage. So far, many genetic algorithms have been used and tested only in design problems of small scale. A theoretical understanding of evolutionary design and its applications in design process is necessary. A\n\n",
                "DataExportTag": "AI80272",
                "QuestionID": "QID333",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Parallel Computing and High Performance Computing\""
                    },
                    "2": {
                        "Display": "parallel, gpu, processor, compiler, thread, cpu, distribute, speedup, parallelism, parallelization, execution, parallelize, hpc, heterogeneous, cuda"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and GPU Optimization\""
                    },
                    "4": {
                        "Display": "deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID337",
            "SecondaryAttribute": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Towards a Generative System for Intelligent Design Support In the development of intelligent computer aided design systems, three important issues need to considered. These issues are: how to support the generation of product concepts using evolutionary computation techniques; how to use intelligent databases and constraint management systems for detailed exploration of product embodiment; and how to integrate rapid prototyping facilities for product evaluation. In this paper, we present a brief review of knowledge based design and evolutionary design and discuss ways of integrating both in the development of a generative design system. Based on this review, we present the model and its applications of a generative design system utilizing a number of AI and evolutionary computation techniques. This generative design model is intended to provide a generic computational framework for the development of intelligent design support systems. 1. Knowledge-based design Existing computer-based design systems provide little support for the evolution of design solution concepts that satisfy all physical constraints. They rely on precise geometric information to specify the model of an artifact. They cannot support early stage design tasks because the geometry of an artifact is not sufficiently definite at the early stage. The design of complex products containing many components require a number of alternative solutions to be explored, each being modified by making changes to key design parameters, until a satisfactory design solution is found. The generation and exploration of design concepts based on incomplete design requirements, and the handling of a multitude of variables and constraints embedded within 3D design objects is a challenging task for designers. Four main strategies may characterize the current developments in knowledge-based design systems: \u2022 The intelligent CAD approach; \u2022 The building block approach; \u2022 The prototype approach; and \u2022 The constraint-based approach. JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 286 The intelligent CAD approach extends the capability of a CAD system by employing heuristic knowledge on top of geometric models of the artifacts. An intelligent CAD system plays an active rather than a passive role in the design process, by incorporating a significant amount of design knowledge into the system. Functional modeling, for example, is an approach that attempts to build intelligent CAD systems through a mapping from a function space to a conceptual solution space using functional instead of geometric components and their interfaces [Chakrabarti and Tang 1996]. This approach uses abstract knowledge to represent the function and behavior of elementary design components, parts and their causal relationships. This abstract knowledge is used to generate initial conceptual solutions, which are subsequently selected, evaluated and specialized further. This approach is based on the assumption that designers use abstract and qualitative, knowledge at the early stage of the design process to generate a conceptual model of an artifact. This conceptual model can be generated without detailed geometric information such as dimensions. The building block approach formulates design process as a sequence of tasks that can be tackled individually by different classes of CAD tools and AI methods [Brown et al 1989]. Chandrasekaran classified these tasks into three categories: 1. Class 1 design tasks in which the components of the artifact being designed are unknown; 2. Class 2 design tasks in which the components of the artifact being designed are known, but the design plans, i.e. the methods of how to design, are unavailable in a compiled form; and 3. Class 3 design tasks where ways of decomposing a design problem are already known and for which compiled design plans are available for each stage of the design process In this approach, design tasks under class 3 are considered routine designs that can be conveniently supported by automated computer programs. The others are more difficult and require significant creative inputs from human designers. The prototype design approach divides design process into three different activities: prototype refinement, prototype adaptation and prototype creation based on a library of design prototypes [Gero et al 1989]. Gero classified design as: routine design where the functions, expected behaviors and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables; innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements; and creative design, in which an entirely new design problem space is to be defined [Coyne et al 1990]. Prototype refinement involves instantiating the variables in a design prototype and determining their values. Prototype adaptation becomes necessary TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT 287 when a design prototype is found to be inadequate in some way, and needs to be adapted to a new design problem, or made generally more useful for other design problem solving. Prototype creation is seen as the ultimate design endeavour in this approach. So far research based on this approach has mainly concerned the problems of prototype refinement and prototype adaptation. Little progress has been reported on the issue of prototype creation and it remains a difficult subject. The constraint-based approach models an engineering system as a constraint network or a hierarchical structure, i.e. collections of constraints that are interconnected by design variables [Smithers et al 1990]. Instantiation of this structure, or part of it, forms a basic structure of a new design problem. Computer-based constraint-based reasoning techniques are used to derive the values of unknown design variables from some initial data provided by the designers to form the solutions to the new design problem. Computational methods are used to detect constraint violation and to suggest alternative values for design variables. The application of design strategies or design methods is reflected in the way in which design variables are created, constrained, and manipulated. The original aim of AI and knowledge based design was to produce general purpose, domain independent AI tools that would automate design tasks requiring intelligence. The early promise of knowledge based design has not been fulfilled because these approaches: \u2022 did not scale within and across domains, \u2022 unable to learn and adapt to design contexts, \u2022 failed to handle complexity, consistency and unpredictability, \u2022 could not acquire knowledge satisfactorily, and \u2022 could not model and support creativity. 2. Evolutionary design Recent advancement in evolutionary computing provides new opportunities to re-examine the issue of intelligent design support. Genetic algorithms are widely accepted as powerful generative and adaptive techniques generally applicable to many design activities. Evolutionary design is an approach that utilizes different evolutionary computation techniques in different stages of the design process. Evolutionary design is capable of generating astonishing imaginative and innovative designs [Bentley 1999]. The strength of evolutionary design comes from the factor that controlled evolution can be formulated as a general purposed problem solver with ability similar to human design intelligence but with a magnitude of speed and efficiency. Traditional AI methods such as rule-based reasoning have to model design intelligence explicitly in terms of knowledge both in representation and inference. These JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 288 methods have serious drawbacks because the process of how human designers actually use this kind of knowledge is not necessarily fully understood. Bentley classified evolutionary design into four categories. These are: evolutionary design optimization that is concerned with optimizing existing designs by evolving the values of suitably constrained design parameters; creative evolutionary design that generates entirely new designs from little abstract knowledge to satisfy functional requirements; conceptual evolutionary design that deals with the production of high level conceptual frameworks of preliminary designs; and generative evolutionary design that directly produces forms of designs contributing to the emergence of implicit design concepts. These evolutionary design approaches combine several vital aspects of design intelligence in an evolutionary process, including modeling design data and information, concept formation, idea generation, optimization, learning, and evaluation. Once a design problem is properly formulated in an evolutionary process, the computer is able to generate a large number of candidate solutions before reaching at an optimum one. The candidate solutions are unpredictable but the process and the final outcome are controllable by the designers. The evolutionary design approach has an excellent potential for developing more intelligent design support tools. For example, Frazer used genetic algorithms in his evolutionary architectural design to evolve unpredicted forms of architectures and their possible interactions with the environments [Frazer 1995, 1996 and 1997]. Chakrabarti developed a functional synthesis program that generates a large number of abstract design concepts from functional requirements and abstract building blocks of engineering elements [Chakrabarti and Tang 1996]. Thornton utilised genetic algorithms as constraint management tools in the process of embodiment design [Thornton 1994]. However, the development of evolutionary design tools is still at its early stage. So far, many genetic algorithms have been used and tested only in design problems of small scale. A theoretical understanding of evolutionary design and its applications in design process is necessary. A\n\n",
                "DataExportTag": "AI41974",
                "QuestionID": "QID337",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Wireless Communication and Spectrum Allocation\""
                    },
                    "2": {
                        "Display": "channel, wireless, radio, spectral, user, cognitive_radio, transmission, interference, cellular, csi, allocation, wireless_communication, deep_learning, mobile, antenna"
                    },
                    "3": {
                        "Display": "\"Edge Computing and Resource Allocation in UAV Networks\""
                    },
                    "4": {
                        "Display": "unmanned_aerial_vehicles, deep_learning, offload, edge_computing, vehicle, mobile, resource_allocation, deep_reinforcement_learning, mec, user, traffic, vehicular, caching, wireless, edge"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID340",
            "SecondaryAttribute": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Towards a Generative System for Intelligent Design Support In the development of intelligent computer aided design systems, three important issues need to considered. These issues are: how to support the generation of product concepts using evolutionary computation techniques; how to use intelligent databases and constraint management systems for detailed exploration of product embodiment; and how to integrate rapid prototyping facilities for product evaluation. In this paper, we present a brief review of knowledge based design and evolutionary design and discuss ways of integrating both in the development of a generative design system. Based on this review, we present the model and its applications of a generative design system utilizing a number of AI and evolutionary computation techniques. This generative design model is intended to provide a generic computational framework for the development of intelligent design support systems. 1. Knowledge-based design Existing computer-based design systems provide little support for the evolution of design solution concepts that satisfy all physical constraints. They rely on precise geometric information to specify the model of an artifact. They cannot support early stage design tasks because the geometry of an artifact is not sufficiently definite at the early stage. The design of complex products containing many components require a number of alternative solutions to be explored, each being modified by making changes to key design parameters, until a satisfactory design solution is found. The generation and exploration of design concepts based on incomplete design requirements, and the handling of a multitude of variables and constraints embedded within 3D design objects is a challenging task for designers. Four main strategies may characterize the current developments in knowledge-based design systems: \u2022 The intelligent CAD approach; \u2022 The building block approach; \u2022 The prototype approach; and \u2022 The constraint-based approach. JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 286 The intelligent CAD approach extends the capability of a CAD system by employing heuristic knowledge on top of geometric models of the artifacts. An intelligent CAD system plays an active rather than a passive role in the design process, by incorporating a significant amount of design knowledge into the system. Functional modeling, for example, is an approach that attempts to build intelligent CAD systems through a mapping from a function space to a conceptual solution space using functional instead of geometric components and their interfaces [Chakrabarti and Tang 1996]. This approach uses abstract knowledge to represent the function and behavior of elementary design components, parts and their causal relationships. This abstract knowledge is used to generate initial conceptual solutions, which are subsequently selected, evaluated and specialized further. This approach is based on the assumption that designers use abstract and qualitative, knowledge at the early stage of the design process to generate a conceptual model of an artifact. This conceptual model can be generated without detailed geometric information such as dimensions. The building block approach formulates design process as a sequence of tasks that can be tackled individually by different classes of CAD tools and AI methods [Brown et al 1989]. Chandrasekaran classified these tasks into three categories: 1. Class 1 design tasks in which the components of the artifact being designed are unknown; 2. Class 2 design tasks in which the components of the artifact being designed are known, but the design plans, i.e. the methods of how to design, are unavailable in a compiled form; and 3. Class 3 design tasks where ways of decomposing a design problem are already known and for which compiled design plans are available for each stage of the design process In this approach, design tasks under class 3 are considered routine designs that can be conveniently supported by automated computer programs. The others are more difficult and require significant creative inputs from human designers. The prototype design approach divides design process into three different activities: prototype refinement, prototype adaptation and prototype creation based on a library of design prototypes [Gero et al 1989]. Gero classified design as: routine design where the functions, expected behaviors and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables; innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements; and creative design, in which an entirely new design problem space is to be defined [Coyne et al 1990]. Prototype refinement involves instantiating the variables in a design prototype and determining their values. Prototype adaptation becomes necessary TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT 287 when a design prototype is found to be inadequate in some way, and needs to be adapted to a new design problem, or made generally more useful for other design problem solving. Prototype creation is seen as the ultimate design endeavour in this approach. So far research based on this approach has mainly concerned the problems of prototype refinement and prototype adaptation. Little progress has been reported on the issue of prototype creation and it remains a difficult subject. The constraint-based approach models an engineering system as a constraint network or a hierarchical structure, i.e. collections of constraints that are interconnected by design variables [Smithers et al 1990]. Instantiation of this structure, or part of it, forms a basic structure of a new design problem. Computer-based constraint-based reasoning techniques are used to derive the values of unknown design variables from some initial data provided by the designers to form the solutions to the new design problem. Computational methods are used to detect constraint violation and to suggest alternative values for design variables. The application of design strategies or design methods is reflected in the way in which design variables are created, constrained, and manipulated. The original aim of AI and knowledge based design was to produce general purpose, domain independent AI tools that would automate design tasks requiring intelligence. The early promise of knowledge based design has not been fulfilled because these approaches: \u2022 did not scale within and across domains, \u2022 unable to learn and adapt to design contexts, \u2022 failed to handle complexity, consistency and unpredictability, \u2022 could not acquire knowledge satisfactorily, and \u2022 could not model and support creativity. 2. Evolutionary design Recent advancement in evolutionary computing provides new opportunities to re-examine the issue of intelligent design support. Genetic algorithms are widely accepted as powerful generative and adaptive techniques generally applicable to many design activities. Evolutionary design is an approach that utilizes different evolutionary computation techniques in different stages of the design process. Evolutionary design is capable of generating astonishing imaginative and innovative designs [Bentley 1999]. The strength of evolutionary design comes from the factor that controlled evolution can be formulated as a general purposed problem solver with ability similar to human design intelligence but with a magnitude of speed and efficiency. Traditional AI methods such as rule-based reasoning have to model design intelligence explicitly in terms of knowledge both in representation and inference. These JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 288 methods have serious drawbacks because the process of how human designers actually use this kind of knowledge is not necessarily fully understood. Bentley classified evolutionary design into four categories. These are: evolutionary design optimization that is concerned with optimizing existing designs by evolving the values of suitably constrained design parameters; creative evolutionary design that generates entirely new designs from little abstract knowledge to satisfy functional requirements; conceptual evolutionary design that deals with the production of high level conceptual frameworks of preliminary designs; and generative evolutionary design that directly produces forms of designs contributing to the emergence of implicit design concepts. These evolutionary design approaches combine several vital aspects of design intelligence in an evolutionary process, including modeling design data and information, concept formation, idea generation, optimization, learning, and evaluation. Once a design problem is properly formulated in an evolutionary process, the computer is able to generate a large number of candidate solutions before reaching at an optimum one. The candidate solutions are unpredictable but the process and the final outcome are controllable by the designers. The evolutionary design approach has an excellent potential for developing more intelligent design support tools. For example, Frazer used genetic algorithms in his evolutionary architectural design to evolve unpredicted forms of architectures and their possible interactions with the environments [Frazer 1995, 1996 and 1997]. Chakrabarti developed a functional synthesis program that generates a large number of abstract design concepts from functional requirements and abstract building blocks of engineering elements [Chakrabarti and Tang 1996]. Thornton utilised genetic algorithms as constraint management tools in the process of embodiment design [Thornton 1994]. However, the development of evolutionary design tools is still at its early stage. So far, many genetic algorithms have been used and tested only in design problems of small scale. A theoretical understanding of evolutionary design and its applications in design process is necessary. A\n\n",
                "DataExportTag": "AI617040",
                "QuestionID": "QID340",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Self-Supervised Learning and Data Augmentation\""
                    },
                    "2": {
                        "Display": "transfer, self_supervised, augmentation, shot, annotation, deep_learning, contrastive, semi_supervised, unlabele, unlabeled, ssl, adaptation, unsupervised, pseudo_label, classification"
                    },
                    "3": {
                        "Display": "\"Deep Learning for Image and Speech Recognition\""
                    },
                    "4": {
                        "Display": "cnn, deep_learning, classification, speech_recognition, image, transfer, convolution, traffic_sign, capsule, augmentation, dcnn, networks, rnn, imagenet, extraction"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID407",
            "SecondaryAttribute": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Towards a Generative System for Intelligent Design Support In the development of intelligent computer aided design systems, three important issues need to considered. These issues are: how to support the generation of product concepts using evolutionary computation techniques; how to use intelligent databases and constraint management systems for detailed exploration of product embodiment; and how to integrate rapid prototyping facilities for product evaluation. In this paper, we present a brief review of knowledge based design and evolutionary design and discuss ways of integrating both in the development of a generative design system. Based on this review, we present the model and its applications of a generative design system utilizing a number of AI and evolutionary computation techniques. This generative design model is intended to provide a generic computational framework for the development of intelligent design support systems. 1. Knowledge-based design Existing computer-based design systems provide little support for the evolution of design solution concepts that satisfy all physical constraints. They rely on precise geometric information to specify the model of an artifact. They cannot support early stage design tasks because the geometry of an artifact is not sufficiently definite at the early stage. The design of complex products containing many components require a number of alternative solutions to be explored, each being modified by making changes to key design parameters, until a satisfactory design solution is found. The generation and exploration of design concepts based on incomplete design requirements, and the handling of a multitude of variables and constraints embedded within 3D design objects is a challenging task for designers. Four main strategies may characterize the current developments in knowledge-based design systems: \u2022 The intelligent CAD approach; \u2022 The building block approach; \u2022 The prototype approach; and \u2022 The constraint-based approach. JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 286 The intelligent CAD approach extends the capability of a CAD system by employing heuristic knowledge on top of geometric models of the artifacts. An intelligent CAD system plays an active rather than a passive role in the design process, by incorporating a significant amount of design knowledge into the system. Functional modeling, for example, is an approach that attempts to build intelligent CAD systems through a mapping from a function space to a conceptual solution space using functional instead of geometric components and their interfaces [Chakrabarti and Tang 1996]. This approach uses abstract knowledge to represent the function and behavior of elementary design components, parts and their causal relationships. This abstract knowledge is used to generate initial conceptual solutions, which are subsequently selected, evaluated and specialized further. This approach is based on the assumption that designers use abstract and qualitative, knowledge at the early stage of the design process to generate a conceptual model of an artifact. This conceptual model can be generated without detailed geometric information such as dimensions. The building block approach formulates design process as a sequence of tasks that can be tackled individually by different classes of CAD tools and AI methods [Brown et al 1989]. Chandrasekaran classified these tasks into three categories: 1. Class 1 design tasks in which the components of the artifact being designed are unknown; 2. Class 2 design tasks in which the components of the artifact being designed are known, but the design plans, i.e. the methods of how to design, are unavailable in a compiled form; and 3. Class 3 design tasks where ways of decomposing a design problem are already known and for which compiled design plans are available for each stage of the design process In this approach, design tasks under class 3 are considered routine designs that can be conveniently supported by automated computer programs. The others are more difficult and require significant creative inputs from human designers. The prototype design approach divides design process into three different activities: prototype refinement, prototype adaptation and prototype creation based on a library of design prototypes [Gero et al 1989]. Gero classified design as: routine design where the functions, expected behaviors and structural variables of the design are known, and the problem of design is one of instantiating values for structure variables; innovative design, where certain aspects of a defined design space need to be modified or extended because no existing solution within that space meets the design requirements; and creative design, in which an entirely new design problem space is to be defined [Coyne et al 1990]. Prototype refinement involves instantiating the variables in a design prototype and determining their values. Prototype adaptation becomes necessary TOWARDS A GENERATIVE SYSTEM FOR INTELLIGENT DESIGN SUPPORT 287 when a design prototype is found to be inadequate in some way, and needs to be adapted to a new design problem, or made generally more useful for other design problem solving. Prototype creation is seen as the ultimate design endeavour in this approach. So far research based on this approach has mainly concerned the problems of prototype refinement and prototype adaptation. Little progress has been reported on the issue of prototype creation and it remains a difficult subject. The constraint-based approach models an engineering system as a constraint network or a hierarchical structure, i.e. collections of constraints that are interconnected by design variables [Smithers et al 1990]. Instantiation of this structure, or part of it, forms a basic structure of a new design problem. Computer-based constraint-based reasoning techniques are used to derive the values of unknown design variables from some initial data provided by the designers to form the solutions to the new design problem. Computational methods are used to detect constraint violation and to suggest alternative values for design variables. The application of design strategies or design methods is reflected in the way in which design variables are created, constrained, and manipulated. The original aim of AI and knowledge based design was to produce general purpose, domain independent AI tools that would automate design tasks requiring intelligence. The early promise of knowledge based design has not been fulfilled because these approaches: \u2022 did not scale within and across domains, \u2022 unable to learn and adapt to design contexts, \u2022 failed to handle complexity, consistency and unpredictability, \u2022 could not acquire knowledge satisfactorily, and \u2022 could not model and support creativity. 2. Evolutionary design Recent advancement in evolutionary computing provides new opportunities to re-examine the issue of intelligent design support. Genetic algorithms are widely accepted as powerful generative and adaptive techniques generally applicable to many design activities. Evolutionary design is an approach that utilizes different evolutionary computation techniques in different stages of the design process. Evolutionary design is capable of generating astonishing imaginative and innovative designs [Bentley 1999]. The strength of evolutionary design comes from the factor that controlled evolution can be formulated as a general purposed problem solver with ability similar to human design intelligence but with a magnitude of speed and efficiency. Traditional AI methods such as rule-based reasoning have to model design intelligence explicitly in terms of knowledge both in representation and inference. These JOHN HAMILTON FRAZER, MING XI TANG, and SUN JIAN 288 methods have serious drawbacks because the process of how human designers actually use this kind of knowledge is not necessarily fully understood. Bentley classified evolutionary design into four categories. These are: evolutionary design optimization that is concerned with optimizing existing designs by evolving the values of suitably constrained design parameters; creative evolutionary design that generates entirely new designs from little abstract knowledge to satisfy functional requirements; conceptual evolutionary design that deals with the production of high level conceptual frameworks of preliminary designs; and generative evolutionary design that directly produces forms of designs contributing to the emergence of implicit design concepts. These evolutionary design approaches combine several vital aspects of design intelligence in an evolutionary process, including modeling design data and information, concept formation, idea generation, optimization, learning, and evaluation. Once a design problem is properly formulated in an evolutionary process, the computer is able to generate a large number of candidate solutions before reaching at an optimum one. The candidate solutions are unpredictable but the process and the final outcome are controllable by the designers. The evolutionary design approach has an excellent potential for developing more intelligent design support tools. For example, Frazer used genetic algorithms in his evolutionary architectural design to evolve unpredicted forms of architectures and their possible interactions with the environments [Frazer 1995, 1996 and 1997]. Chakrabarti developed a functional synthesis program that generates a large number of abstract design concepts from functional requirements and abstract building blocks of engineering elements [Chakrabarti and Tang 1996]. Thornton utilised genetic algorithms as constraint management tools in the process of embodiment design [Thornton 1994]. However, the development of evolutionary design tools is still at its early stage. So far, many genetic algorithms have been used and tested only in design problems of small scale. A theoretical understanding of evolutionary design and its applications in design process is necessary. A\n\n",
                "DataExportTag": "AI418642",
                "QuestionID": "QID407",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Towards a Generative System for Intelligent Design Support In the development of intelligent comp...",
                "Choices": {
                    "1": {
                        "Display": "\"Edge Computing and Deployment of Lightweight DNNs\""
                    },
                    "2": {
                        "Display": "dnn, deep_learning, pruning, energy, inference, gpu, dnns, deployment, mobile, automl, accelerator, deploy, latency, lightweight, edge"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and GPU Optimization\""
                    },
                    "4": {
                        "Display": "deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID213",
            "SecondaryAttribute": "Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications While Web 2.0 term...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications While Web 2.0 term is used to describe the current trend in the use of web technologies, Web 3.0 term is used to describe the next generation web, which will combine Semantic Web technologies, Web 2.0 principles and artificial intelligence. Towards this perspective, in this work we introduce a 3-tier architecture for web applications that will fit into the Web 3.0 definition. We present the fundamental features of this architecture, its components and their interaction, as well as the current technological limitations. Furthermore, some indicative application scenarios are outlined in order to illustrate the features of the proposed architecture. The aim of this architecture is to be a step towards supporting the development of intelligent semantic web applications of the near future as well as supporting the user collaboration and community-driven evolution of these applications. INTRODUCTION Current trends in Web research and development seem to revolve around two major technological pillars: Social-driven applications, a main component in the Web 2.0 domain, and the Semantic Web. It is our firm belief that web semantics and Web 2.0 are complementary visions about the near future of the Web, rather than in competition: surely they can learn from each other in order to overcome their drawbacks, in a way that enables forthcoming web applications to combine Web 2.0 principles, especially those that focus on usability, community and collaboration, with the powerful Semantic Web infrastructure, which facilitates the information sharing among applications. Recently, the term Web 3.0 is used to describe the long-term future of the web (Lassila, 2007; Hendler, 2008). Web 3.0 will surely incorporate semantic web and Web 2.0 principles, but researchers believe that it will also include some more sophisticated concepts like artificial intelligence on the web. Towards this direction, in this work we propose a 3-tier architecture for web applications that will fit into the Web 3.0, the next generation web. At the lower layer of the architecture, we introduce and describe an advanced semantic knowledge base infrastructure that can support integration of multiple disparate data sources, without requiring a concrete underlying semantic structure. In addition, the upper layers of the architecture provide greater flexibility in the user interactions with the underlying ontological data model. As a result, it supports user collaboration and community-driven evolution of the next generation web applications. This architecture gives the developers the ability to build complicated web applications which combine the philosophy of Web 2.0 applications, and the powerful technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles on the Web. Furthermore, this architecture is well suited for supporting enhanced Knowledge Systems with advanced knowledge discovery characteristics, towards the future implementation of an Internet-scale Knowledge System. For example, the proposed architecture could be used to enrich current wiki applications towards next generation semantic wiki platforms that will mash-up scattered data sources and provide intelligent search capabilities. The following text is organized in five sections. In section 2 we start by providing some broad definitions and discussing the concepts of Semantic Web and Web 2.0. Furthermore, we discuss related work and the theoretical background of the research area. In section 3, we describe in detail the proposed architecture, its components, its fundamental features and the current technological limitations. In section 4, we outline some indicative application scenarios in order to illustrate the features of the proposed architecture and prove that it can be applied today and support modern web applications. Finally, we discuss future work and summarize our conclusions. BACKGROUND As Semantic Web and Web 2.0 were firstly introduced separately by groups with completely contrary beliefs on the evolution of World Wide Web, and even targeting different audiences, there has been a common perception that both are competing approaches for organizing and emerging the Web. The Semantic Web, outlined by Berners-Lee (2001), becomes a revolutionary technological approach for organizing and exchanging information in a cross-application dimension. Strongly supported by World Wide Web Consortium and powered by heavy academic and enterprise research, Semantic Web can demonstrate standardized and well-defined approaches in language description, such as RDF (Manola, 2004), RDF(S) (Brickley, 2004) and Web Ontology Language OWL (Smith, 2004), as well as research background in ontology engineering and modeling tools, from SHOE (Heflin, 1998) to Prot\u00e9g\u00e9 (Knublauch, 2004). Semantic Web is powered by a strong AI background through its foundation on the Description Logics (DL) formalism (Baader, 2007). DL languages have become in recent years a well-studied formalism, originating from Semantic Networks and Frames and, as such, they have been extensively used in formal Semantic Web specifications and tools. These languages are of variable expressive strength which comes with the cost of increased computational complexity. Therefore, current research in this area is focused on efficient and advanced algorithms and procedures that would provide intelligent querying capabilities for the real word Web, based on DL descriptions and possibly subsets of and reductions from them that may exhibit more satisfying computational properties (Grau, 2008). One main reason for transforming the current Web to a Semantic Web is the ability to deduce new, unexpressed information that is only implied by existing descriptions. If the Web is to be considered as a huge, distributed knowledge base, then well-known AI techniques, at least for the part with sound foundations in logic, can be utilized in order to form the basis for intelligent negotiation and discovery on the Semantic Web. Such techniques may include for example deductive query answering and inference-based reasoning (Luke, 1996; Berners-Lee, 2001). On the other hand, the Web 2.0 term, introduced by Tim O\u2019Reilly (2005), represents a widely spread trend of adopting certain technologies and approaches in web development, targeting more flexible and user friendly applications, and easier distributed collaboration. The usability aspect is met by Rich Internet Applications (RIA) (Loosley, 2006) and especially Asynchronous JavaScript and XML (AJAX), which support the creation of responsive user interfaces as well as more interactive browsing experience. Collaboration conveniences come through the creation of virtual online communities of users that contribute effort and data to a common cause, achieving better results than each individual could do on his own. Finally there is a greater flexibility in data handling, enabling the development of hybrid web applications, called Mash-ups, which combine discrete data sources and services from different sites in order to provide a unified and enriched result. Therefore, the Semantic Web can provide a rich and powerful technical infrastructure for any kind of web application, while the paradigm of Web 2.0 applications can be used to provide useful guidelines, focusing on usability and collaboration. Thus, the Semantic Web and Web 2.0 principles can be combined as complementary approaches to provide more efficient web applications. Such applications could be thought to be part of next generation's web and seem to fall under the term Web 3.0 (Hendler, 2008), which lately is sort of \u201ctalk of the town\u201d (Lassila, 2007). In this context, there are several approaches; from developing AJAX tools for the Semantic Web (Oren, 2006) and studying the combination of ontologies and taxonomies (Mika, 2005), up to the proposition of sophisticated hybrid architectures, combining both of these technologies (Ankolekar, 2007). All of the above are of great use in any data-handling web application, and where there is need for a knowledge system. Especially for next generation knowledge systems that try to benefit from Web 2.0 approaches and collaborative development in order to build, or more precisely grow, Internet-scale knowledge systems (Tenenbaum, 2006). PROPOSED ARCHITECTURE In this section we propose an architecture for web applications, which provides developers the ability to structure complicated web applications, which combine the vision of Web 2.0 and the rich technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles. Such applications could be next generation semantic wikis, intelligent mash-ups, semantic portals and in general any data-handling web application that intends to provide semantic information combined with advanced intelligent querying capabilities. The information of these applications could be delivered by two main ways: i. Directly to end users through the web-based interface of a stand-alone application ii. To other programs or services, that act as intermediaries with third-party web applications, by interacting with the API of our semantic infrastructure to retrieve precisely the information they need. A conformant implementation may follow the traditional 3-tier model, which lately (Hendler, 2008) is commonly used to support web 3.0 applications, with an important variation: Where a database server would be typically used, we now use a knowledge base system, since a traditional DBMS lacks the necessary features and functions for managing and utilizing ontological knowledge. Note that each of the three layers may be physically located on different computer systems. The proposed architecture is presented in figure 1. In fact, from the designer\u2019s point of view, our architecture could be decentralized in at least two ways: i. The Semantic Web knowledge bases that data is extracted from, could be both logically and physically distribut\n\n",
                "DataExportTag": "AI836231",
                "QuestionID": "QID213",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications While Web 2.0 term...",
                "Choices": {
                    "1": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "2": {
                        "Display": "cloud, cloud_computing, server, edge, workload, job, scheduling, edge_computing, latency, distribute, deployment, vm, mobile, storage, workflow"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and GPU Optimization\""
                    },
                    "4": {
                        "Display": "deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID400",
            "SecondaryAttribute": "Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications While Web 2.0 term...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications While Web 2.0 term is used to describe the current trend in the use of web technologies, Web 3.0 term is used to describe the next generation web, which will combine Semantic Web technologies, Web 2.0 principles and artificial intelligence. Towards this perspective, in this work we introduce a 3-tier architecture for web applications that will fit into the Web 3.0 definition. We present the fundamental features of this architecture, its components and their interaction, as well as the current technological limitations. Furthermore, some indicative application scenarios are outlined in order to illustrate the features of the proposed architecture. The aim of this architecture is to be a step towards supporting the development of intelligent semantic web applications of the near future as well as supporting the user collaboration and community-driven evolution of these applications. INTRODUCTION Current trends in Web research and development seem to revolve around two major technological pillars: Social-driven applications, a main component in the Web 2.0 domain, and the Semantic Web. It is our firm belief that web semantics and Web 2.0 are complementary visions about the near future of the Web, rather than in competition: surely they can learn from each other in order to overcome their drawbacks, in a way that enables forthcoming web applications to combine Web 2.0 principles, especially those that focus on usability, community and collaboration, with the powerful Semantic Web infrastructure, which facilitates the information sharing among applications. Recently, the term Web 3.0 is used to describe the long-term future of the web (Lassila, 2007; Hendler, 2008). Web 3.0 will surely incorporate semantic web and Web 2.0 principles, but researchers believe that it will also include some more sophisticated concepts like artificial intelligence on the web. Towards this direction, in this work we propose a 3-tier architecture for web applications that will fit into the Web 3.0, the next generation web. At the lower layer of the architecture, we introduce and describe an advanced semantic knowledge base infrastructure that can support integration of multiple disparate data sources, without requiring a concrete underlying semantic structure. In addition, the upper layers of the architecture provide greater flexibility in the user interactions with the underlying ontological data model. As a result, it supports user collaboration and community-driven evolution of the next generation web applications. This architecture gives the developers the ability to build complicated web applications which combine the philosophy of Web 2.0 applications, and the powerful technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles on the Web. Furthermore, this architecture is well suited for supporting enhanced Knowledge Systems with advanced knowledge discovery characteristics, towards the future implementation of an Internet-scale Knowledge System. For example, the proposed architecture could be used to enrich current wiki applications towards next generation semantic wiki platforms that will mash-up scattered data sources and provide intelligent search capabilities. The following text is organized in five sections. In section 2 we start by providing some broad definitions and discussing the concepts of Semantic Web and Web 2.0. Furthermore, we discuss related work and the theoretical background of the research area. In section 3, we describe in detail the proposed architecture, its components, its fundamental features and the current technological limitations. In section 4, we outline some indicative application scenarios in order to illustrate the features of the proposed architecture and prove that it can be applied today and support modern web applications. Finally, we discuss future work and summarize our conclusions. BACKGROUND As Semantic Web and Web 2.0 were firstly introduced separately by groups with completely contrary beliefs on the evolution of World Wide Web, and even targeting different audiences, there has been a common perception that both are competing approaches for organizing and emerging the Web. The Semantic Web, outlined by Berners-Lee (2001), becomes a revolutionary technological approach for organizing and exchanging information in a cross-application dimension. Strongly supported by World Wide Web Consortium and powered by heavy academic and enterprise research, Semantic Web can demonstrate standardized and well-defined approaches in language description, such as RDF (Manola, 2004), RDF(S) (Brickley, 2004) and Web Ontology Language OWL (Smith, 2004), as well as research background in ontology engineering and modeling tools, from SHOE (Heflin, 1998) to Prot\u00e9g\u00e9 (Knublauch, 2004). Semantic Web is powered by a strong AI background through its foundation on the Description Logics (DL) formalism (Baader, 2007). DL languages have become in recent years a well-studied formalism, originating from Semantic Networks and Frames and, as such, they have been extensively used in formal Semantic Web specifications and tools. These languages are of variable expressive strength which comes with the cost of increased computational complexity. Therefore, current research in this area is focused on efficient and advanced algorithms and procedures that would provide intelligent querying capabilities for the real word Web, based on DL descriptions and possibly subsets of and reductions from them that may exhibit more satisfying computational properties (Grau, 2008). One main reason for transforming the current Web to a Semantic Web is the ability to deduce new, unexpressed information that is only implied by existing descriptions. If the Web is to be considered as a huge, distributed knowledge base, then well-known AI techniques, at least for the part with sound foundations in logic, can be utilized in order to form the basis for intelligent negotiation and discovery on the Semantic Web. Such techniques may include for example deductive query answering and inference-based reasoning (Luke, 1996; Berners-Lee, 2001). On the other hand, the Web 2.0 term, introduced by Tim O\u2019Reilly (2005), represents a widely spread trend of adopting certain technologies and approaches in web development, targeting more flexible and user friendly applications, and easier distributed collaboration. The usability aspect is met by Rich Internet Applications (RIA) (Loosley, 2006) and especially Asynchronous JavaScript and XML (AJAX), which support the creation of responsive user interfaces as well as more interactive browsing experience. Collaboration conveniences come through the creation of virtual online communities of users that contribute effort and data to a common cause, achieving better results than each individual could do on his own. Finally there is a greater flexibility in data handling, enabling the development of hybrid web applications, called Mash-ups, which combine discrete data sources and services from different sites in order to provide a unified and enriched result. Therefore, the Semantic Web can provide a rich and powerful technical infrastructure for any kind of web application, while the paradigm of Web 2.0 applications can be used to provide useful guidelines, focusing on usability and collaboration. Thus, the Semantic Web and Web 2.0 principles can be combined as complementary approaches to provide more efficient web applications. Such applications could be thought to be part of next generation's web and seem to fall under the term Web 3.0 (Hendler, 2008), which lately is sort of \u201ctalk of the town\u201d (Lassila, 2007). In this context, there are several approaches; from developing AJAX tools for the Semantic Web (Oren, 2006) and studying the combination of ontologies and taxonomies (Mika, 2005), up to the proposition of sophisticated hybrid architectures, combining both of these technologies (Ankolekar, 2007). All of the above are of great use in any data-handling web application, and where there is need for a knowledge system. Especially for next generation knowledge systems that try to benefit from Web 2.0 approaches and collaborative development in order to build, or more precisely grow, Internet-scale knowledge systems (Tenenbaum, 2006). PROPOSED ARCHITECTURE In this section we propose an architecture for web applications, which provides developers the ability to structure complicated web applications, which combine the vision of Web 2.0 and the rich technical infrastructure of the Semantic Web, supported by applying Artificial Intelligence principles. Such applications could be next generation semantic wikis, intelligent mash-ups, semantic portals and in general any data-handling web application that intends to provide semantic information combined with advanced intelligent querying capabilities. The information of these applications could be delivered by two main ways: i. Directly to end users through the web-based interface of a stand-alone application ii. To other programs or services, that act as intermediaries with third-party web applications, by interacting with the API of our semantic infrastructure to retrieve precisely the information they need. A conformant implementation may follow the traditional 3-tier model, which lately (Hendler, 2008) is commonly used to support web 3.0 applications, with an important variation: Where a database server would be typically used, we now use a knowledge base system, since a traditional DBMS lacks the necessary features and functions for managing and utilizing ontological knowledge. Note that each of the three layers may be physically located on different computer systems. The proposed architecture is presented in figure 1. In fact, from the designer\u2019s point of view, our architecture could be decentralized in at least two ways: i. The Semantic Web knowledge bases that data is extracted from, could be both logically and physically distribut\n\n",
                "DataExportTag": "AI830348",
                "QuestionID": "QID400",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Towards Web 3.0: A Unifying Architecture for Next Generation Web Applications While Web 2.0 term...",
                "Choices": {
                    "1": {
                        "Display": "\"Online Community Management and Social Media Monitoring\""
                    },
                    "2": {
                        "Display": "social, social_medium, hate_speech, user, post, comment, facebook, forum, community, crowdsource, online, crowd, social_media, crowdsourcing, security_administration"
                    },
                    "3": {
                        "Display": "\"Web Search and Data Mining\""
                    },
                    "4": {
                        "Display": "web, data_mining, web_page, search_engine, text, clustering, user, query, document, blog, retrieval, association_rule, forum, internet, semantic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID212",
            "SecondaryAttribute": "Track Degradation Prediction Models , Using Markov Chain , Artificial Neural and Neuro-Fuzzy Netw...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Track Degradation Prediction Models , Using Markov Chain , Artificial Neural and Neuro-Fuzzy Network Track condition is one of the most important parameters affecting the track maintenance management. In order to obtain a good track maintenance management system, it is necessary to predict track condition through the time. In this study, the track\u2019s state will be defined in terms of the Combine Track Record index (CTR) rating which can vary from 0 to 100 where 100 denotes the best possible track condition and the states are defined as five intervals of CTR, similar to those used in Iranian Railways. Four models are built and testes for the prediction of track quality, one conventional model suggested by ORE, and three new model, using markov chain, artificial neural network and neurofuzzy network. The data for our empirical application was collected from Iranian Railways network. Comparisons of the models show that all three proposed new models predict track deterioration better than the ORE model. Introduction The railway is a branch of the transportation system that is very expensive to construct but it has a long life and low operating costs. Therefore, the asset value is very high, which also leads to the possibility that maintenance might be expensive. Like other infrastructure with investment costs in the construction phase, maintenance plays a crucial role in the long-term cost effectiveness, and so maintenance management is one of the most important parts of the railway systems. Track condition is one of the most important parameters affecting the track maintenance management. In order to obtain a good track maintenance management system, it is necessary to predict track condition through the time. The cost of maintaining a track depends directly on its condition or \"state\". Maintenance and rehabilitation funds are often allocated to tracks that are in the worst state or that exhibit an accelerating rate of deterioration. Some of factors that affect the rate of deterioration of tracks include: traffic loads, weather, and construction materials. The track conditions vary considerably even when such contributing factors are similar. Therefore, it is important that a procedure that can accommodate such randomness be incorporated into a track management system. Several approaches and methods for predicting railways conditions have been proposed, and based on these, a considerable number of maintenance planning tools have been developed for railways systems in North America and Europe. A nonlinear regression model based on a product of the power functions has been proposed by the Office for Research and Experiments (ORE) of the International Union of Railways [9] to predict track deterioration. Having track degradation models, operations research techniques are commonly used to optimize track maintenance activity. Such approaches have been described by Esveld [4] and Zarembski [12]. Zhang [13] has proposed an Integrated Track Degradation Model (ITDM). ITDM simulates track degradation based on the interaction between different track components under varying traffic. It also considers several mechanistic characteristics, including train speed and axle load. Based on the ITDM, Simson et al. [11] have developed a Track Maintenance Planning Model (TMPM). It aims to deal with track maintenance planning in the medium to long term. TMPM outputs the net present value of the financial benefits of undertaking a given maintenance strategy compared with a base-case maintenance scenario. The Total Right-Of-Way Analysis and Costing System (TRACS) has been described by Martland et al. [7]. It is a system (software) developed by the Association of American Railroads (AAR) and Massachusetts Institute of Technology (MIT), in the USA. It is a computer-based tool developed to assist rail management to address change in the infrastructure. By combining engineering-based deterioration models with lifecycle costing techniques the model estimates track maintenance and renewal costs as a function of route geometry, track components, track condition, as well as traffic mix and volume. TRACS has been used by North American railroads as a tool for technology assessment costing in support of actions such as pricing, budgeting, and line consolidation. Both the ITDM and the TRACS models are based on an incremental approach where each event such as rail grinding, relining, and track renewal can be included. In recent years the application of soft computing techniques has been paid attention to predicting the future track conditions. Shafahi and Rasooli [10] have considered neuro-networks to predict future track conditions. A neuro-fuzzy decision support system for rail track maintenance planning has been described by Dell'Orco et al. [2]. During the 1990s, the International Railway Union (UIC) in conjunction with the European Rail Research Institute (ERRI) developed an expert system for track maintenance and renewal (ECOTRACK). This model builds on the fact that rules can be specified for certain maintenance activities under certain conditions. A historical database containing infrastructure information on components and current condition is also a prerequisite to use this model. ECOTRACK solves the planning problem given the rules specified and points out the activities needed at a section at certain time. Recent work on ECOTRACK at ERRI [6] has developed the model further in order to improve its functionality. To describe the condition of the track, several indexes and criteria have been defined and used in different railway systems around the world. Commonly, the track quality index (TQI) is used to define the track quality. TQI is normally determined by track geometry parameters (TGP). The term track deterioration is used to describe any changes in track geometry. Track deteriorations are classified as: unevenness, twist, alignment, and gauge. TQI is a function of these four parameters. In this study, the track\u2019s state will be defined in terms of the Combined Track Record index (CTR index) rating which can vary from 0 to 100 where 100 denotes the best possible track condition and the states are defined as five intervals of CTR index (Table 1). It is supposed that the track began its life at some time in the past in near-perfect condition. It was then a subject to a sequence of duty cycles that caused its condition to deteriorate. The duty cycle for the track in this study will be assumed to consist of one year\u2019s weather and traffic load. These discrete state and discrete time unit definition let us to express the deterioration process as a Markov chain. CTR index 0\u201350 50\u201360 60\u201370 70\u201380 80\u2013100 Track quality Failed Medium Good Very good Excellent Track state 1 2 3 4 5 Table 1: Track state classification by CTR index To have better models, in this study the tracks are categorized so that those with similar traffic loads and geographical location are collected into one class. A network of tracks based on the topography was divided into three groups of plain, hilly and mountainous areas; and based on traffic loads was divided into two groups of light and heavy traffic. We thus have six track classes (Table 2). Traffic condition Topography condition Plain areas Hilly areas Mountainous areas\n\n",
                "DataExportTag": "AI21960",
                "QuestionID": "QID212",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Track Degradation Prediction Models , Using Markov Chain , Artificial Neural and Neuro-Fuzzy Netw...",
                "Choices": {
                    "1": {
                        "Display": "\"Cloud and Edge Computing Management\""
                    },
                    "2": {
                        "Display": "cloud, cloud_computing, server, edge, workload, job, scheduling, edge_computing, latency, distribute, deployment, vm, mobile, storage, workflow"
                    },
                    "3": {
                        "Display": "\"High Performance Computing and GPU Optimization\""
                    },
                    "4": {
                        "Display": "deep_learning, accelerator, compiler, framework, gpu, workload, hpc, heterogeneous, configuration, optimization, kernel, level, tuning, abstraction, cpu"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID350",
            "SecondaryAttribute": "Translating the Global Refined Analysis of Newly transcribed RNA and Decay rates by SLAM-seq I pr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Translating the Global Refined Analysis of Newly transcribed RNA and Decay rates by SLAM-seq I propose to introduce novel analysis tools via a platform for three recently developed RNA sequencing (RNA-seq) mI propose to introduce novel analysis tools via a platform for three recently developed RNA sequencing (RNA-seq) methods (SLAM-seq, TimeLapse-seq and TUC-seq). All three methods enable the direct detection of new RNA transcribed by cells in a defined window of time within the pool of total cellular RNA. This is achieved by the introduction and subsequent detection of base-mismatches in newly transcribed RNA using RNA-seq. The ability to differentiate \u201cnew\u201d from \u201cold\u201d RNA greatly increases the temporal resolution of RNA-seq experiments. In the frame of my ERC CoG grant \u201cHERPES\u201d, we developed a computational approach (an algorithm) termed GRAND-SLAM (Globally Refined Analysis of Newly transcribed RNA and Decay rates using SLAM-seq; patent application filed) to reliably define the relative contributions of \u201cnew\u201d and \u201cold\u201d RNA. GRAND-SLAM not only directly computes the contribution of \u201cnew\u201d and \u201cold\u201d RNA but also provides credible intervals that allow to judge the precision of the obtained ratios for each gene. GRAND-SLAM thereby provides novel means to identify perturbations in RNA synthesis and decay. Furthermore, GRAND-SLAM directly reduces experiment costs by eliminating the need for control samples to determine sequencing error rates.In conclusion, SLAM-seq will become the new standard for gene expression profiling worldwide and GRAND-SLAM the computational tool to analyze the respective data. Within this PoC-project, we will present the GRAND-SLAM analysis platform and prepare its introduction on the market with prospective customers in three areas: next-generation-sequencing companies, pharmaceutical industry, and research institutes.  We will validate the analysis platform, improve its usability via direct testing with pilot customers, and develop our envisaged business strategy according to the feedback we will gain in the course of the project.\n\n",
                "DataExportTag": "COR4370",
                "QuestionID": "QID350",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Translating the Global Refined Analysis of Newly transcribed RNA and Decay rates by SLAM-seq I pr...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetics and Genomic Research\""
                    },
                    "2": {
                        "Display": "deoxyribonucleic_acid, ribonucleic_acid, chromatin, genomic, epigenetic, methylation, repair, sequence, gene_expression, chromosome, transcription, protein, genetic, mirnas, non_coding"
                    },
                    "3": {
                        "Display": "\"Cellular Biology and Biophysics\""
                    },
                    "4": {
                        "Display": "membrane, cellular, protein, transport, organelle, assembly, signal, actin, microscopy, cell_division, mechanical, force, channel, single_molecule, cytoskeleton"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID364",
            "SecondaryAttribute": "Translating the Global Refined Analysis of Newly transcribed RNA and Decay rates by SLAM-seq I pr...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Translating the Global Refined Analysis of Newly transcribed RNA and Decay rates by SLAM-seq I propose to introduce novel analysis tools via a platform for three recently developed RNA sequencing (RNA-seq) mI propose to introduce novel analysis tools via a platform for three recently developed RNA sequencing (RNA-seq) methods (SLAM-seq, TimeLapse-seq and TUC-seq). All three methods enable the direct detection of new RNA transcribed by cells in a defined window of time within the pool of total cellular RNA. This is achieved by the introduction and subsequent detection of base-mismatches in newly transcribed RNA using RNA-seq. The ability to differentiate \u201cnew\u201d from \u201cold\u201d RNA greatly increases the temporal resolution of RNA-seq experiments. In the frame of my ERC CoG grant \u201cHERPES\u201d, we developed a computational approach (an algorithm) termed GRAND-SLAM (Globally Refined Analysis of Newly transcribed RNA and Decay rates using SLAM-seq; patent application filed) to reliably define the relative contributions of \u201cnew\u201d and \u201cold\u201d RNA. GRAND-SLAM not only directly computes the contribution of \u201cnew\u201d and \u201cold\u201d RNA but also provides credible intervals that allow to judge the precision of the obtained ratios for each gene. GRAND-SLAM thereby provides novel means to identify perturbations in RNA synthesis and decay. Furthermore, GRAND-SLAM directly reduces experiment costs by eliminating the need for control samples to determine sequencing error rates.In conclusion, SLAM-seq will become the new standard for gene expression profiling worldwide and GRAND-SLAM the computational tool to analyze the respective data. Within this PoC-project, we will present the GRAND-SLAM analysis platform and prepare its introduction on the market with prospective customers in three areas: next-generation-sequencing companies, pharmaceutical industry, and research institutes.  We will validate the analysis platform, improve its usability via direct testing with pilot customers, and develop our envisaged business strategy according to the feedback we will gain in the course of the project.\n\n",
                "DataExportTag": "COR4370-1",
                "QuestionID": "QID364",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Translating the Global Refined Analysis of Newly transcribed RNA and Decay rates by SLAM-seq I pr...",
                "Choices": {
                    "1": {
                        "Display": "\"Genetics and Epigenetics Research\""
                    },
                    "2": {
                        "Display": "deoxyribonucleic_acid, ribonucleic_acid, chromatin, genomic, epigenetic, methylation, repair, sequence, gene_expression, chromosome, transcription, protein, genetic, mirnas, non_coding"
                    },
                    "3": {
                        "Display": "\"Cancer Research and Genetic Disorders\""
                    },
                    "4": {
                        "Display": "cancer, tumor, age, disease, mutation, patient, therapeutic, genetic, cellular, mouse, mitochondrial, ageing, aging, disorder, protein"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID384",
            "SecondaryAttribute": "Treatment-Free Remission After Second-Line Nilotinib Treatment in Patients With Chronic Myeloid L...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Treatment-Free Remission After Second-Line Nilotinib Treatment in Patients With Chronic Myeloid Leukemia in Chronic Phase Chronic myeloid leukemia (CML) results from DNA damage leading to a chromosomal translocation that produces the Philadelphia chromosome and resulting chimeric tyrosine kinase, BCR-ABL1 (14). The BCR-ABL1 tyrosine kinase inhibitors (TKIs) have dramatically improved outcomes for patients with CML (5). This type of leukemia progresses through 3 phases: chronic phase, accelerated phase, and blast crisis. However, with BCR-ABL1 TKI therapy, patients may remain in the chronic phase for more than a decade, with a life expectancy similar to that of the general population (57). Imatinib, nilotinib, dasatinib, and bosutinib are TKIs approved for use as frontline therapies; additional TKIs are available for later-line settings (8). During TKI treatment, patients' molecular responses (that is, BCR-ABL1 transcript levels) are monitored with real-time quantitative polymerase chain reaction (RQ-PCR) assays (8). Molecular response levels are reported relative to a standard reference representing an untreated BCR-ABL1 level (the International Scale [IS]). Established response benchmarks include major molecular response (MMR) (BCR-ABL1 IS 0.1%), MR4 (BCR-ABL1 IS 0.01%), and MR4.5 (BCR-ABL1 IS 0.0032%) (8). Treatment-free remission (TFR) after achievement of sustained deep molecular response (DMR) (for example, MR4 or MR4.5) (9, 10) represents an emerging treatment goal for patients with CML in chronic phase. Potential motivators and benefits of achieving TFR may include relief of treatment side effects, reduced risk for long-term TKI toxicity, and the ability to plan a family (1113). In the STIM1 (Stop Imatinib 1) trial, 38% of patients with sustained DMR while receiving long-term imatinib treatment had molecular recurrencefree survival at 5 years (median follow-up, 77 months) (14, 15). These results, along with findings from other TFR trials (mostly after frontline imatinib treatment) (12, 1622), confirm the feasibility of TFR after sustained DMR in patients receiving TKIs. When TFR is a treatment goal, achievement of DMR is a key prerequisite. For patients receiving frontline imatinib therapy who did not reach DMR, the ENESTcmr (Evaluating Nilotinib Efficacy and Safety in Clinical TrialsComplete Molecular Response) study demonstrated that switching to nilotinib may provide a path to achieving DMR. Among the ENESTcmr participants who did not have MR4.5 at baseline, by 2 years after enrollment, the MR4.5 rate was 43% for those randomly assigned to a nilotinib switch (n= 98) versus 21% for those assigned to continued imatinib therapy (n= 96) (23). Thus, switching to nilotinib may allow more of these patients to reach a molecular response level at which treatment discontinuation may be considered. Although studies have evaluated TFR after cessation of dasatinib or nilotinib therapy (1922), TFR in patients who achieved sustained DMR after switching from imatinib to nilotinib has not been investigated in a dedicated study. We present findings from the single-group, phase 2 ENESTop (Treatment-Free Remission After Achieving Sustained MR4.5 on Nilotinib) trial (ClinicalTrials.gov: NCT01698905). To our knowledge, ENESTop is the first large prospective study to evaluate TFR specifically in patients who achieved sustained DMR only after switching from imatinib to nilotinib. The primary objective of ENESTop was to determine the TFR rate at 48 weeks after treatment discontinuation in this patient population. Methods Patients and Study Design Eligible patients were aged 18 years or older, had Philadelphia chromosomepositive CML in chronic phase, and had an Eastern Cooperative Oncology Group performance grade of 0 to 2. Patients must have received previous TKI therapy for at least 3 years (>4 weeks with imatinib, then 2 years with nilotinib) and achieved MR4.5 while receiving nilotinib after switching from imatinib therapy. The study design is shown in Figure 1. Figure 1. ENESTop study design. Patients entered a 1-year consolidation phase and received nilotinib at the same dosage as before enrollment (300 or 400 mg twice daily or other dosage). Patients who maintained MR4.5 during the consolidation phase (that is, had no confirmed loss of MR4.5 in consecutive assessments within 4 wk) were eligible to enter the TFR phase and stop nilotinib treatment. Patients with loss of MMR or confirmed loss of MR4 (in 2 consecutive assessments within 4 wk) during the TFR phase restarted nilotinib therapy at 300 or 400 mg twice daily in the treatment reinitiation phase. BCR-ABL1 IS= BCR-ABL1 standardized to the International Scale; CML= chronic myeloid leukemia; ENESTop= Treatment-Free Remission After Achieving Sustained MR4.5 on Nilotinib; MMR= major molecular response (BCR-ABL1 IS 0.1%); MR4= BCR-ABL1 IS 0.01%; MR4.5= BCR-ABL1 IS 0.0032%; RQ-PCR= real-time quantitative polymerase chain reaction (standardized to the International Scale); TFR= treatment-free remission; TKI= tyrosine kinase inhibitor. * Confirmed loss of MR4.5 was defined as BCR-ABL1 IS >0.0032%, confirmed in a second assessment within 4 wk. Confirmed loss of MR4 was defined as BCR-ABL1 IS >0.01% in 2 consecutive assessments. Patients with confirmed loss of MR4.5 during the continuation phase continued nilotinib treatment in the prolonged continuation phase until 264 wk after the last patient entered the TFR phase or until discontinuation due to unacceptable toxicity, disease progression, investigator discretion, or withdrawal of consent. End Points and Assessments The primary efficacy end point was the proportion of patients without loss of MMR, confirmed loss of MR4, or treatment reinitiation within the first 48 weeks of the TFR phase. Mutational analyses (Sanger sequencing) were performed in patients with MMR loss or confirmed MR4 loss who had sufficient BCR-ABL1 copies. Adverse events (AEs) were assessed according to the Common Terminology Criteria for Adverse Events, version 4.03 (24). Safety was evaluated throughout the study and up to 30 days after the last dose of study drug or the last day in TFR phase. Statistical Analysis Sample-size calculation was based on the number of patients required to provide 90% power to test the null hypothesis (TFR rate 10% at 48 weeks), as detailed in the Supplement. Because of rapid recruitment, actual enrollment was higher than planned. Efficacy and safety data were reported for all patients who entered the TFR phase (TFR population) on the basis of a cutoff date of 26 November 2015, when all patients who entered the TFR phase had remained in TFR for 48 weeks or more, reinitiated treatment, or dropped out of the study; additional, longer-term analyses were based on a cutoff date of 7 November 2016. The primary end point (TFR rate at 48 weeks) was presented as a percentage with an exact 95% ClopperPearson CI. If the lower limit of the 95% CI exceeded 10%, the null hypothesis would be rejected. For the primary analysis, patients who left the study before week 48 of the TFR phase were considered nonresponders and included in the TFR population. Patients who lost MR4 for the first time (that is, not confirmed) at week 48 of the TFR phase but did not lose MMR were considered responders for the primary analysis. Supplement. Methods Treatment-free survival was defined as the time from the TFR start date to the date of the earliest occurrence of MMR loss, confirmed MR4 loss, nilotinib reinitiation, progression to accelerated phase or blast crisis, or death from any cause. Progression-free survival was defined as the time from the TFR start date to the date of the earliest occurrence of progression to accelerated phase or blast crisis or death from any cause. Overall survival was defined as the time from the TFR start date to the date of death from any cause. Treatment-free, progression-free, and overall survival were analyzed by using the KaplanMeier method. For patients who reinitiated treatment, time to recovery of a molecular response was assessed by plotting the cumulative rates of MR4 or MR4.5 regained over time. The median duration of the first musculoskeletal painrelated event, estimated by the KaplanMeier method, included data up to the 96-week cutoff date. Time to loss of MMR or confirmed loss of MR4 during the TFR phase, according to reason for switching to nilotinib, was estimated by using the KaplanMeier method on a post hoc basis. In the treatment-free survival analysis, the censoring time for patients who did not have an event on or before the cutoff date was the date of the last assessment (RQ-PCR, cytogenetic, hematologic, or extramedullary) up to the end of the TFR phase. In the analysis of time to recovery of MR4 or MR4.5 during the reinitiation phase, the censoring time for patients who did not regain the specified response was the last RQ-PCR assessment date during treatment. For patients who left the study before regaining the specified response, the censoring time was the last RQ-PCR assessment date during treatment, except for those who dropped out because of disease progression or death, in which case the censoring time was set to the longest follow-up in the treatment reinitiation phase, because the response was then impossible to reach. In the analysis of duration of the first musculoskeletal painrelated event, patients whose first event did not resolve in the TFR phase before the data cutoff date had their event duration censored at the last date they were in the TFR phase before the cutoff date. Safety results were reported for the consolidation phase and the first and second 48 weeks of the TFR phase; safety analyses were conducted to specifically assess trends in AEs over time in the overall TFR population and among the 73 patients who remained in the TFR phase for more than 48 weeks. For selected AEs of interest, analysis was conducted using groupings of terms. Ethics This study was conducted in accordance with ethical principles of the Declaration of Helsinki and local laws and regulations. All patients provided written infor\n\n",
                "DataExportTag": "CAN1258790",
                "QuestionID": "QID384",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Treatment-Free Remission After Second-Line Nilotinib Treatment in Patients With Chronic Myeloid L...",
                "Choices": {
                    "1": {
                        "Display": "\"Organ Transplantation and Graft Rejection\""
                    },
                    "2": {
                        "Display": "transplant, graft_vs_host_disease, donor, recipient, graft_versus, hsct, rejection, allogeneic_hematopoietic, allograft, engraftment, allogeneic, bmt, hct, csa, kidney"
                    },
                    "3": {
                        "Display": "\"Leukemia Research and Treatment\""
                    },
                    "4": {
                        "Display": "acute_myelogenous_leukemia, chronic_myeloid_leukemia, imatinib_treatment, mrd, cytogenetic, myeloid_leukemia, bcr_abl_protein, relapse, gene, blast, acute_lymphoblastic, minimal_residual, bcr, receptor_tyrosine_kinase, molecular"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID129",
            "SecondaryAttribute": "TRIAL REGISTRATION",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "TRIAL REGISTRATION\n",
                "DataExportTag": "57",
                "QuestionID": "QID129",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "TRIAL REGISTRATION",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID296",
            "SecondaryAttribute": "Understanding Intergenerational Transmissions: A Cross-Disciplinary Approach This project combine...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Understanding Intergenerational Transmissions: A Cross-Disciplinary Approach This project combines the best sociology and economics has to offer to establish a new understanding of intergenerational transmissions. We know that socioeconomic outcomes are correlated across generations, but we have only little understanding of the mechanisms and intergenerational transmissions which generate these correlations.In this project, we propose to combine formal models of intergenerational transmissions in economics with substantive insights from sociology to develop new and improved models of intergenerational transmissions. Furthermore, we combine longitudinal data with state-of-the-art econometric methods to analyze intergenerational transmissions of cultural endowments and educational expectations, the role of the extended family in intergenerational transmissions, and finally the utility of educational decision making.The project has the potential to significantly improve our understanding of the causes and consequences of intergenerational transmissions and, in doing so, to contribute new knowledge to inform policies to promote social mobility.\n\n",
                "DataExportTag": "COR58321",
                "QuestionID": "QID296",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Understanding Intergenerational Transmissions: A Cross-Disciplinary Approach This project combine...",
                "Choices": {
                    "1": {
                        "Display": "\"Family Behavior and Macroeconomic Impact on Child Health and Fertility\""
                    },
                    "2": {
                        "Display": "child, firm, family, behaviour, health, uncertainty, macroeconomic, shock, parent, fertility, choice, causal, longitudinal, mental_health, belief"
                    },
                    "3": {
                        "Display": "\"Global History and Cultural Studies\""
                    },
                    "4": {
                        "Display": "indigenous, colonial, elite, peace, military, ethnic, globalization, world_war, ethnography, feminist, labour, religion, domestic, diaspora, nation_state"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID286",
            "SecondaryAttribute": "Unravelling the genetic influences of reproductive behaviour and gene-environment interaction Thi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Unravelling the genetic influences of reproductive behaviour and gene-environment interaction This project will be the first to engage in a comprehensive study of the role of genes and gene-environment (GxE) interaction on reproductive behaviour. Until now, social science research has focussed on socio-environmental explanations, largely neglecting the role of genes. Due to unprecedented advances in molecular genetics over the last two decades, for the first time in history we are able to examine whether there is a genetic component to reproductive outcomes, including age at first birth, number of children and infertility. Building on my substantive empirical research, I first develop a multifactor theoretical and measurement model isolating socio-environmental and lifestyle factors. Second, I apply the most cutting-edge techniques in genetics to examine the genetic architecture of reproductive behaviour, including: the first genome-wide association study (GWAS) of reproductive choice; polygenic risk scores; and, genome-wide complex trait analysis (GCTA). Third, I focus on gene-environment interactions (GxE) to test different mechanisms of how the environment moderates genetic influences. Fourth, I propose to use genetic markers as instrumental variables (IVs) in a bi-directional Mendelian randomization (MR) analysis to determine causality and address the endogeneity of lifestyle and education in reproductive outcomes. This transdisciplinary project will produce fundamentally different results, overturn established links and deliver major breakthroughs in fertility research and beyond. This project is not only ground breaking by setting a new research agenda, but due to the inclusion of  new genetic variables and  techniques to study the causal effects of genes and their interaction with environment, will yield major innovations useful  within demography and beyond. Research builds on the proven expertise and existing collaboration with geneticists, and is carefully costed to include 2 postdocs and 2 PhDs.\n\n",
                "DataExportTag": "COR9930",
                "QuestionID": "QID286",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Unravelling the genetic influences of reproductive behaviour and gene-environment interaction Thi...",
                "Choices": {
                    "1": {
                        "Display": "\"Evolutionary Genetics and Species Adaptation\""
                    },
                    "2": {
                        "Display": "evolution, genomic, genetic, specie, trait, population, adaptation, insect, variation, fitness, phenotype, sequence, speciation, selection, lineage"
                    },
                    "3": {
                        "Display": "\"Cancer Research and Aging Studies\""
                    },
                    "4": {
                        "Display": "cancer, tumor, age, disease, mutation, patient, therapeutic, genetic, cellular, mouse, mitochondrial, ageing, aging, disorder, protein"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID345",
            "SecondaryAttribute": "Urgent health challenges for the next decade 2030: World Health Organization As a new year and a...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Urgent health challenges for the next decade 2030: World Health Organization As a new year and a new decade kickoff, WHO has released a list of urgent, global health challenges in January 2020. This list, developed with input from WHO experts around the world, reflects a deep concern that leaders are failing to invest enough resources in core health priorities and systems. This puts lives, livelihoods, and economies in jeopardy. None of these issues are simple to address, but they are within reach. Public health is ultimately a political choice. All the challenges in this list demand a response from more than just the health sector. With the deadline for the 2030 Sustainable Development Goals quickly approaching, the United Nations General Assembly has underscored that the next 10 years must be the \"decade of action\". -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------According to WHO Director-General, we need to realize that health is an investment in the future. Countries invest heavily in protecting their people from terrorist attacks, but not against the attack of a virus, which could be far more deadly, and far more damaging economically and socially. A pandemic could bring economies and nations to their knees. Which is why health security cannot be a matter for ministries of health alone. Coronavirus disease 2019 (COVID-19) is a respiratory tract infection caused by a newly emergent coronavirus, SARS-CoV-2, that was first recognized in Wuhan, China, in December 2019. Genetic sequencing of the virus suggests that SARS-CoV-2 is a betacoronavirus closely linked to the SARS virus. The World Health Organization (WHO) has declared the COVID-19 outbreak a pandemic on 11 March 2020 and till the 19th April 2020 WHO reports the 2,241,359 confirmed cases, 152,551 deaths and more than 168 countries, areas or territories with cases worldwide affected.1,2 Stopping infectious diseases What\u2019s the challenge? Infectious diseases like HIV, tuberculosis, viral hepatitis, malaria, neglected tropical diseases and sexually-transmitted infections will kill an estimated 4 million people in 2020, most of them 114 NEUROPHARMAC JOURNAL 5 (2020) 113-117 poor. Meanwhile, vaccine-preventable diseases continue to kill, such as measles, which took 140,000 lives in 2019, many of them children. Although polio has been driven to the brink of eradication, there were 156 cases of wild poliovirus last year, the most since 2014. The root causes are insufficient levels of financing and the weakness of health systems in endemic countries, coupled with a lack of commitment from wealthy countries. What\u2019s WHO doing? There\u2019s an urgent need for greater political will and increased funding for essential health services; strengthening routine immunization; improving the quality and availability of data to inform planning, and more efforts to mitigate the effects of drug resistance. There\u2019s also a need to invest in research and development of new diagnostics, medicines and vaccines. Together with partners, WHO is working to end polio as soon as possible. Preparing for epidemics What\u2019s the challenge? Every year, the world spends far more responding to disease outbreaks, natural disasters and other health emergencies than it does preparing for and preventing them. A pandemic of a new, highly infectious, airborne virus most likely a strain of influenza to which most people lack immunity is inevitable. It is not a matter of if another pandemic will strike, but when, and when it strikes it will spread fast, potentially threatening millions of lives. Meanwhile, vector-borne diseases like dengue, malaria, Zika, chikungunya, and yellow fever are spreading as mosquito populations move into new areas, fanned by climate change. What is WHO doing? WHO is advising countries on evidence-based investments to strengthen health systems and infrastructure to keep populations safe when health emergencies strike. The 2019 Global Preparedness Monitoring Board report identified seven concrete steps that countries and multilateral institutions should adopt, including more international cooperation, greater domestic focus on preparedness and increased funding. Protecting the medicines that protect us What\u2019s the challenge? Anti-microbial resistance (AMR) threatens to send modern medicine back decades to the preantibiotic era, when even routine surgeries were hazardous. The rise of AMR stems from myriad factors that have come together to create a terrifying brew, including unregulated prescription and use of antibiotics, lack of access to quality and affordable medicines, and lack of clean water, sanitation, hygiene and infection prevention and control. What is WHO doing? WHO is working with national and international authorities in the environment, agriculture and animal sectors to reduce the threat of AMR by addressing its root causes, while advocating for research and development into new antibiotics. Protecting people from dangerous products What\u2019s the challenge? Lack of food, unsafe food and unhealthy diets are responsible for almost one-third of today\u2019s global disease burden. Hunger and food insecurity continue to plague millions, with food shortages being perniciously exploited as weapons of war. At the same time, as people consume foods and drinks high in sugar, saturated fat, trans fat and salt, overweight, obesity and diet-related diseases are on the rise globally. Meanwhile, tobacco use is declining in a few but rising in most countries, and evidence is building about the health risks of ecigarettes. What is WHO doing? WHO is working with countries to develop evidence-based public policies, investments and private sector reforms to reshape food systems, and provide healthy and sustainable diets. In 2019 the food industry committed to eliminating trans fat by 2023, but more is needed. WHO is working with countries to build political commitment and capacity to strengthen the implementation of evidence-based tobacco control policies. Investing in the people who defend our health What\u2019s the challenge? 115 NEUROPHARMAC JOURNAL 5 (2020) 113-117 Chronic under-investment in the education and employment of health workers, coupled with a failure to ensure decent pay, has led to health worker shortages all over the world. This jeopardizes the health and social care services and sustainable health systems. The world will need 18 million additional health workers by 2030, primarily in lowand middle-income countries, including 9 million nurses and midwives. What is WHO doing? To trigger action and encourage investment in education, skills and jobs, the World Health Assembly has designated 2020 the Year of the Nurse and the Midwife. WHO, with partners, will issue a comprehensive State of the World\u2019s Nursing report on World Health Day in April. We\u2019re working with countries to stimulate new investment to train health workers and pay them decent salaries. Keeping adolescents safe What\u2019s the challenge? More than 1 million adolescents aged 10-19 years die every year. The leading causes of death in this age group are road injury, HIV, suicide, lower respiratory infections, and interpersonal violence. Harmful use of alcohol, tobacco and drug use, lack of physical activity, unprotected sex, and previous exposure to child maltreatment all increase the risks for these causes of death. What is WHO doing? In 2020, WHO will issue new guidance for policymakers, health practitioners and educators, called Helping Adolescents Thrive. The aim is to promote adolescents\u2019 mental health and prevent the use of drugs, alcohol, self-harm and interpersonal violence, as well as provide young people with information on preventing HIV and other sexually transmitted infections, contraception, and care during pregnancy and childbirth. WHO will continue to work with governments on improving emergency trauma care following severe injuries (e.g. due to gunshots and road traffic crashes). Earning public trust What\u2019s the challenge? Trust helps to shape whether patients are likely to rely on health services and follow a health worker\u2019s advice around vaccinations, taking medicines, or using condoms. Public health is compromised by the uncontrolled dissemination of misinformation in social media, as well as through the erosion of trust in public institutions. The anti-vaccination movement has been a significant factor in the rise of deaths in preventable diseases. What is WHO doing? WHO is working with countries to strengthen primary health care, so people can access effective and affordable services easily, from people they know and trust, in their communities. We're working with Facebook, Pinterest and other social media platforms to ensure their users receive reliable information about vaccines and other health issues. Building scientific literacy and health education is vital. There\u2019s a need, too, for self-reflection: scientists and the public health community need to do a better job of listening to the communities they serve. Finally, we must invest in better public health data information systems. Harnessing new technologies What\u2019s the challenge? New technologies are revolutionizing our ability to prevent, diagnose and treat many diseases. Genome editing, synthetic biology, and digital health technologies such as artificial intelligence can solve many problems, but also raise new questions and challenges for monitoring and regulation. Without a deeper understanding of their ethical and social implications, these new technologies, which include the capacity to create new organisms, could harm the people they are intended to help. What is WHO doing? In 2019 WHO set up new advisory committees for human genome editing and digital health, bringing together the world\u2019s leading experts to review the evidence and provide guidance. WHO is also working with countries to enable them to plan, adapt, and benefit from new tools that provide clinical and public health solu\n\n",
                "DataExportTag": "AI874788",
                "QuestionID": "QID345",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Urgent health challenges for the next decade 2030: World Health Organization As a new year and a...",
                "Choices": {
                    "1": {
                        "Display": "\"Healthcare Cost-Effectiveness and Drug Therapy Management\""
                    },
                    "2": {
                        "Display": "treatment, patient, cost, therapy, dose, effectiveness, trial, drug, clinical_trial, antibiotic, chemotherapy, treat, intervention, incremental, decision_tree"
                    },
                    "3": {
                        "Display": "\"Healthcare Diagnosis and Disease Prediction\""
                    },
                    "4": {
                        "Display": "diagnosis, disease, prediction, expert, heart_disease, symptom, patient, healthcare, doctor, pandemic, parkinson_disease, classification, data_mining, fuzzy_logic, world"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID375",
            "SecondaryAttribute": "Urinary Biomarkers for Diagnosis of Bladder Cancer Bladder cancer is the fourth most commonly dia...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Urinary Biomarkers for Diagnosis of Bladder Cancer Bladder cancer is the fourth most commonly diagnosed cancer in U.S. men and the 10th most commonly diagnosed cancer in U.S. women (1). Standard methods for diagnosis of bladder cancer involve cytologic evaluation of urine, imaging tests, and cystoscopy (2, 3). Because cystoscopy is uncomfortable and costly, alternative diagnostic methods have been sought. Urine-based biomarkers have been developed as potential alternatives or adjuncts to standard tests for the initial diagnosis of bladder cancer or identification of recurrent disease (4). Six urinary biomarkers have been approved by the U.S. Food and Drug Administration (FDA) for diagnosis or surveillance of bladder cancer: quantitative nuclear matrix protein 22 (NMP22) (Alere NMP22 [Alere]), qualitative NMP22 (BladderChek [Alere]), qualitative bladder tumor antigen (BTA) (BTA stat [Polymedco]), quantitative BTA (BTA TRAK [Polymedco]), fluorescence in situ hybridization (FISH) (UroVysion [Abbott Molecular]), and fluorescent immunohistochemistry (ImmunoCyt [Scimedx]). The qualitative NMP22 and BTA tests can be performed as point-of-care tests, and the others are performed in a laboratory. One additional test, Cxbladder (Pacific Edge Diagnostics USA), is a laboratory-developed test that does not require FDA approval. Other biomarkers have been developed but are not FDA-approved. The purpose of this study was to systematically review the evidence on the comparative accuracy of urinary biomarkers for diagnosis of bladder cancer. It was done as part of a larger review (5) on the evaluation and treatment of nonmuscle-invasive bladder cancer that was nominated to the Agency for Healthcare Research and Quality (AHRQ) by the American Urological Association for use in updating its guidelines. Methods Detailed methods and data for this review, including the analytic framework, key questions, search strategies, inclusion criteria, study data extraction, and quality ratings, are available in the full report (5). The protocol was developed using a standardized process (6) with input from experts and the public and is registered in the PROSPERO database (7). This article focuses on the accuracy of urinary biomarkers for initial diagnosis of bladder cancer or for diagnosis of recurrent disease, including any variance in diagnostic accuracy based on tumor characteristics, patient characteristics, or the nature of presenting signs or symptoms. Data Sources and Searches A research librarian searched multiple electronic databases, including Ovid MEDLINE (January 1990 through June 2015), the Cochrane Central Register of Controlled Trials, and the Cochrane Database of Systematic Reviews (through June 2015). We also reviewed reference lists and searched ClinicalTrials.gov. Study Selection Two investigators independently reviewed abstracts and full-text articles against prespecified eligibility criteria. We included cross-sectional and cohort studies on the diagnostic accuracy of urinary biomarkers in adults who had signs or symptoms of bladder cancer or were undergoing surveillance for recurrent disease after treatment. We focused on urinary biomarkers approved by the FDA for the diagnosis of bladder cancer (quantitative or qualitative NMP22, qualitative or quantitative BTA, FISH, and ImmunoCyt) or classified by the FDA as a laboratory-developed test (Cxbladder). We excluded studies that used a casecontrol design; studies that did not evaluate the diagnostic accuracy of biomarkers against standard diagnostic methods (cystoscopy and histopathology); and studies on the accuracy of biomarkers for screening in assessing prognosis, guiding therapy, or monitoring response to treatment. Data Extraction and Quality Assessment One investigator extracted details about the setting, tests evaluated, definition of a positive test result, study design, reference standard, inclusion criteria, population characteristics, proportion found to have bladder cancer, bladder cancer stage and grade, results, and funding sources. We constructed 22 tables with the number of true-positive, false-positive, true-negative, and false-negative results from published sample sizes, prevalence, sensitivity, and specificity. A second investigator verified extractions for accuracy. Two investigators independently assessed the risk of bias for each study as low, moderate, or high using criteria adapted from QUADAS-2 (Quality Assessment of Diagnostic Accuracy Studies 2) (8). Discrepancies were resolved through discussion and consensus. Data Synthesis and Analysis We performed meta-analyses for sensitivity and specificity using a bivariate logistic mixed-effects model (9) with SAS, version 10.0 (SAS Institute) (10). We assumed random effects with a bivariate normal distribution and measured statistical heterogeneity with the random-effects variance (2). When few studies were available for an analysis, we used the moment estimates of correlation between sensitivity and specificity in the bivariate model. We calculated positive and negative likelihood ratios (LRs) using the summarized sensitivity and specificity (11, 12). Because studies of a particular biomarker generally used the same definition for a positive test result, we did not plot summary receiver-operating characteristic curves (13). For head-to-head comparisons, we used the same bivariate logistic mixed-effects model, with an added indicator variable for the tests. We conducted analyses for each biomarker by using data from all patients and data stratified according to whether testing was performed for initial diagnosis (evaluation of symptoms) or diagnosis of recurrence (surveillance). We also performed analyses stratified by study design features (such as retrospective or prospective or use of a prespecified threshold to define a positive test result), risk of bias (overall and whether the study performed blinding to the results of the index test), the country in which the study was conducted, and tumor grade and stage (14). We assessed the strength of evidence (SOE) for each body of evidence as high, moderate, low, or insufficient based on aggregate study quality, precision, consistency, and directness. Role of the Funding Source This project was funded under contract HHSA290201200014I from the AHRQ, U.S. Department of Health and Human Services. AHRQ staff assisted in developing the scope and key questions. The AHRQ had no role in study selection, quality assessment, or synthesis. Results The literature flow diagram (Figure 1) summarizes the search and selection of articles. Database searches resulted in 4358 potentially relevant articles. After dual review of abstracts and titles, we selected 262 articles for full-text dual review and determined that 57 studies (in 60 publications) met our inclusion criteria (Appendix Table 1) (15-74). Nineteen studies evaluated quantitative NMP22, 4 evaluated qualitative NMP22, 23 evaluated qualitative BTA, 4 evaluated quantitative BTA, 10 evaluated FISH, 13 evaluated ImmunoCyt, and 1 evaluated Cxbladder. Sample sizes ranged from 26 to 3916, mean age ranged from 54 to 77 years, the proportion of male patients ranged from 57% to 88%, and the proportion diagnosed with bladder cancer ranged from 3% to 81%. Eight studies focused on diagnostic testing for signs and symptoms suggestive of bladder cancer, 16 focused on surveillance of previously treated bladder cancer, and 19 evaluated mixed populations. Forty-three studies were conducted in the United States or Europe. We rated 2 studies as having low risk of bias (20, 21), 3 as having high risk of bias (25, 62, 68), and the remainder as having medium risk of bias. Frequent methodological shortcomings were failure to report blinded interpretation of the reference standard, failure to report enrollment of a random or consecutive sample of patients, or failure to report predefined criteria for a positive test result. Figure 1. Summary of evidence search and selection. * Cochrane Central Register of Controlled Trials and Cochrane Database of Systematic Reviews. Includes prior reports, reference lists of relevant articles, and systematic reviews. Appendix Table 1. Biomarker Study Characteristics Appendix Table 1 Continued Appendix Table 1 Continued Appendix Table 1 Continued Quantitative NMP22 Sensitivity of quantitative NMP22 was 0.69 (95% CI, 0.62 to 0.75), and specificity was 0.77 (CI, 0.70 to 0.83) (19 studies), for a positive LR of 3.05 (CI, 2.28 to 4.10) and a negative LR of 0.40 (CI, 0.32 to 0.50) (Appendix Figure 1). Exclusion of 2 studies that used a cutoff other than >10 U\/mL for a positive test result (18, 37) resulted in similar sensitivity and specificity. Diagnostic accuracy was similar for evaluation of symptoms and for surveillance. Excluding 1 study with high risk of bias (68) and restricting the analysis to prospective studies, those conducted in the United States or Europe, or those that used a prespecified threshold for a positive test result had little effect on pooled estimates. Restricting the analysis to 3 studies with blinded reference standard interpretation resulted in higher specificity (0.89 [CI, 0.78 to 0.95]) (15, 42, 58). Appendix Figure 1. Sensitivity and specificity of quantitative NMP22. NMP22 = nuclear matrix protein 22; TN = true-negative; TP = true-positive. Qualitative NMP22 Sensitivity of qualitative NMP22 was 0.58 (CI, 0.39 to 0.75), and specificity was 0.88 (CI, 0.78 to 0.94) (4 studies), for a positive LR of 4.89 (CI, 3.23 to 7.40) and a negative LR of 0.48 (CI, 0.33 to 0.71) (Appendix Figure 2) (20, 21, 23, 37). Restricting the analysis to 2 studies with low risk of bias resulted in similar estimates (sensitivity, 0.53 [CI, 0.29 to 0.75]; specificity, 0.87 [CI, 0.74 to 0.94]) (20, 21). Subgroup and sensitivity analyses were limited by small numbers of studies. Appendix Figure 2. Sensitivity and specificity of qualitative NMP22. NMP22 = nuclear matrix protein 22; TN = true-negative; TP = true-positive. Qualitative BTA Sensitivity of qualit\n\n",
                "DataExportTag": "CAN271147",
                "QuestionID": "QID375",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Urinary Biomarkers for Diagnosis of Bladder Cancer Bladder cancer is the fourth most commonly dia...",
                "Choices": {
                    "1": {
                        "Display": "\"Exercise and Stress Response Monitoring\""
                    },
                    "2": {
                        "Display": "crp_levels, exercise, serum, plasma, concentration, blood, reactive, hs_crp, urine, cpb, lactate, hscrp, cortisol, exercise_training, training"
                    },
                    "3": {
                        "Display": "\"Cardiovascular Diseases and Heart Failure Treatment\""
                    },
                    "4": {
                        "Display": "heart_failure, left_ventricular, atrial_fibrillation, left_ventricle, cardiovascular, myocardial, hf, chf, leave_ventricular, ejection_fraction, heart, anthracycline, arrhythmia, leave_ventricle, doxorubicin"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID259",
            "SecondaryAttribute": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developmen...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Development and Validation of a New Predictive Model Context Existing breast cancer prediction tools do not account for breast density, a strong risk factor for breast cancer and have been studied in white women only. Contribution The authors developed a breast cancer risk prediction model that incorporates a measure of breast density routinely reported with mammography. Its predictions were accurate, but it had only modest ability to distinguish women who did not develop cancer from those who did, and it misclassified risk in some subgroups. Implication The model requires validation in additional populations. A breast cancer prediction model that incorporates breast density does well in some but not all domains of predicting risk. Its accuracy should be better characterized before it is used clinically. The Editors In 2007, breast cancer will have been diagnosed in more than 178000 women in the United States, and more than 40000 women will have died of breast cancer (1). Most of these women never had their risk for breast cancer assessed, and even fewer considered chemoprevention (25). Providing women with an estimate of their risk for breast cancer would provide an opportunity for them to consider options to decrease their risk. Women at low short-term risk for breast cancer may experience less anxiety about their health and would be less likely to benefit from prevention efforts. Women at very high risk may warrant additional screening tests, such as breast magnetic resonance imaging (6), and might benefit from chemoprevention of breast cancer with tamoxifen or raloxifene. The standard risk assessment model available to practitioners (the Gail model) (7) identifies only a minority of women who eventually develop breast cancer being at high risk (8). Better breast cancer risk prediction tools are needed (9). The radiographic appearance of the breast has been consistently shown to be a major risk factor for breast cancer, whether it is defined by a qualitative assessment of the parenchymal pattern or a quantitative measure of percentage of density (1012). Women in whom more than 50% of total breast area is mammographically dense have high breast density and are at 3- to 5-fold greater risk for breast cancer than women in whom breast density is less than 25% (10, 1316). The increased risk for breast cancer associated with breast density is due in part to the lower sensitivity of mammography in dense breasts (1719), but the association remains strong after accounting for masking (20, 21). Mammographically dense breast tissue is rich in epithelium and stroma (10), and the association could represent activation of epithelial cells or fibroblasts (2225). Recently, several models have been published that incorporate breast density: One uses a continuous measure of breast density that is not available to clinicians and has not been validated (26), and the other predicts 1-year risk for breast cancer (27). We previously demonstrated that a simple model based on age, ethnicity, and a categorical measure of breast density had predictive accuracy similar to that of the Gail model in a multiethnic cohort of women receiving screening mammograms in northern California (28). We expand on that work by using data from more than 1 million ethnically diverse women throughout the United States to develop and validate a risk assessment tool that incorporates breast density and therefore might improve breast cancer screening and prevention efforts. Methods Study Population We included 1095484 women age 35 years or older who had had at least 1 mammogram with breast density measured by using the Breast Imaging Reporting and Data System (BI-RADS) classification system in any of the 7 mammography registries participating in the National Cancer Institutefunded Breast Cancer Surveillance Consortium (BCSC) (available at breastscreening.cancer.gov) (29). The BCSC is a community-based, ethnically and geographically diverse sample that broadly represents the United States (30). We excluded women who had a diagnosis of breast cancer before their first eligible mammography examination. Because our goal was to develop a model of long-term risk for invasive breast cancer, we excluded women with cancer diagnosed in the first 6 months of follow-up to minimize the number of cases of cancer included in the model that were diagnosed on the basis of the mammogram used for risk assessment. Women were also excluded if they had breast implants. Women in whom ductal carcinoma in situ was diagnosed were censored at the time of diagnosis in the primary analysis. When women had several mammograms, we based our analysis on findings from the first mammogram. Each registry obtains annual approval from its institutional review board for consenting processes or a waiver of consent, enrollment of participants, and ongoing data linkage for research purposes. All registries have received a Certificate of Confidentiality from the federal government that protects the identities of research participants. Measurement of Risk Factors Patient information was obtained primarily from self-report at the time of mammography. We selected 2 risk factors in addition to breast density for inclusion in the model on the basis of simplicity (yes or no) and a high attributable risk: history of breast cancer in a first-degree relative and history of a breast biopsy. Body mass index was later considered for addition to the model, but it was excluded to maintain parsimony and because it had minimal effect on model discrimination (the increase in the concordance statistic [c-statistic] was only 0.003). For modeling and validation, missing data for relatives with breast cancer and number of breast biopsies were set to 0. The 5-year Gail risk was computed for each woman by using the algorithms provided by the National Cancer Institute to calculate the Gail model risk for individual women (31). For Gail model calculations, missing data were coded as specified by that model (age at menarche as14 years, age at first live birth as<20 years, number of breast biopsies as 0, and number of first-degree relatives as 0). Ethnicity was coded by using the expanded race and ethnicity definition currently used in the Surveillance, Epidemiology, and End Results (SEER) database and U.S. Vital Statistics (non-Hispanic White, non-Hispanic Black, Asian or Pacific Islander, Native American\/Alaskan Native, Hispanic, or other). We classified women who self-identified as mixed or other race with participants who did not report race and ethnicity. Breast Density Community radiologists at each site classified breast density on screening mammograms as part of routine clinical practice by using the American College of Radiology BI-RADS density categories (32): almost entirely fat (category 1), scattered fibroglandular densities (category 2), heterogeneously dense (category 3), and extremely dense (category 4). The BI-RADS category 2 was used as the reference group for breast density because it formed the largest group. Ascertainment of Breast Cancer Cases Breast cancer outcomes (invasive cancer and ductal carcinoma in situ) were obtained at each site through linkage with the regional population-based SEER program, state tumor registries, and pathology databases. Vital Status Vital status was obtained through linkage to SEER registries, state tumor registries, and the individual state vital statistics or the National Death Index. Model Development We used a proportional hazards model of invasive breast cancer to estimate the hazard ratios for each BI-RADS breast density category. Women entered the model 6 months after the index mammogram and were censored at the time of death, diagnosis of ductal carcinoma in situ, or the end of follow-up. All models were adjusted for age (in 5-year intervals) and race and ethnicity. The strength of the breast density association with breast cancer was greater for women younger than age 65 years (P for interaction< 0.001). Thus, separate models were fitted for women younger than age 65 years and for women age 65 years or older. No other interaction terms were included in the final model. We calculated similar estimates for first-degree relatives with breast cancer (yes or no) and a personal history of breast biopsy (yes or no) from the BCSC. All predictors met the proportional hazards assumption that was assessed by loglog plots and by including interaction terms with time for each predictor variable. We then developed an absolute risk model by using methods described in the Appendix Figure. The model primarily estimates predicted incidence of invasive breast cancer by using age, race or ethnicity, and breast density. These estimates are then adjusted for family history and biopsy history if available. We based our estimates of breast cancer incidence on the SEER age- and ethnicity-specific risk for invasive breast cancer (1992 to 2002) (33). Age-specific incidence for each ethnic group was estimated by fitting a third-order polynomial model to the SEER data. Age-specific incidence rates for the Native American and Alaskan Native group were inconsistent in SEER, so we excluded this group from further analyses. We calculated the baseline risk for the model by adjusting SEER incidence for the population's attributable risk for each breast density subgroup. We estimated the age- and ethnicity-specific distribution of mammographic breast density needed for these calculations by using data from a larger set of 3343047 mammograms from the BCSC. The distribution of breast density varied statistically significantly by age and by race or ethnicity (P< 0.001 for each comparison). The model used these variations by age and race to distribute the 5-year risk for invasive breast cancer across the 4 breast density subgroups. We used the methods described by Gail and colleagues (7) to translate the hazard ratios and risk factor distributions into absolute risks. The age-, sex-, and ethnicity-specific competing risks for death for women were calcula\n\n",
                "DataExportTag": "CAN677383",
                "QuestionID": "QID259",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developmen...",
                "Choices": {
                    "1": {
                        "Display": "\"Scientific Drug Discovery and Prevention Technologies\""
                    },
                    "2": {
                        "Display": "challenge, decade, overview, scientific, drug, concept, decision, update, biological, evolve, publish, prevention, benefit, technology, question"
                    },
                    "3": {
                        "Display": "\"Childhood Illness and Family Psychosocial Experience\""
                    },
                    "4": {
                        "Display": "child, parent, survivor, family, pediatric, woman, experience, adolescent, qualitative, life, illness, childhood, interview, psychosocial, mother"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID307",
            "SecondaryAttribute": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developmen...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Development and Validation of a New Predictive Model Context Existing breast cancer prediction tools do not account for breast density, a strong risk factor for breast cancer and have been studied in white women only. Contribution The authors developed a breast cancer risk prediction model that incorporates a measure of breast density routinely reported with mammography. Its predictions were accurate, but it had only modest ability to distinguish women who did not develop cancer from those who did, and it misclassified risk in some subgroups. Implication The model requires validation in additional populations. A breast cancer prediction model that incorporates breast density does well in some but not all domains of predicting risk. Its accuracy should be better characterized before it is used clinically. The Editors In 2007, breast cancer will have been diagnosed in more than 178000 women in the United States, and more than 40000 women will have died of breast cancer (1). Most of these women never had their risk for breast cancer assessed, and even fewer considered chemoprevention (25). Providing women with an estimate of their risk for breast cancer would provide an opportunity for them to consider options to decrease their risk. Women at low short-term risk for breast cancer may experience less anxiety about their health and would be less likely to benefit from prevention efforts. Women at very high risk may warrant additional screening tests, such as breast magnetic resonance imaging (6), and might benefit from chemoprevention of breast cancer with tamoxifen or raloxifene. The standard risk assessment model available to practitioners (the Gail model) (7) identifies only a minority of women who eventually develop breast cancer being at high risk (8). Better breast cancer risk prediction tools are needed (9). The radiographic appearance of the breast has been consistently shown to be a major risk factor for breast cancer, whether it is defined by a qualitative assessment of the parenchymal pattern or a quantitative measure of percentage of density (1012). Women in whom more than 50% of total breast area is mammographically dense have high breast density and are at 3- to 5-fold greater risk for breast cancer than women in whom breast density is less than 25% (10, 1316). The increased risk for breast cancer associated with breast density is due in part to the lower sensitivity of mammography in dense breasts (1719), but the association remains strong after accounting for masking (20, 21). Mammographically dense breast tissue is rich in epithelium and stroma (10), and the association could represent activation of epithelial cells or fibroblasts (2225). Recently, several models have been published that incorporate breast density: One uses a continuous measure of breast density that is not available to clinicians and has not been validated (26), and the other predicts 1-year risk for breast cancer (27). We previously demonstrated that a simple model based on age, ethnicity, and a categorical measure of breast density had predictive accuracy similar to that of the Gail model in a multiethnic cohort of women receiving screening mammograms in northern California (28). We expand on that work by using data from more than 1 million ethnically diverse women throughout the United States to develop and validate a risk assessment tool that incorporates breast density and therefore might improve breast cancer screening and prevention efforts. Methods Study Population We included 1095484 women age 35 years or older who had had at least 1 mammogram with breast density measured by using the Breast Imaging Reporting and Data System (BI-RADS) classification system in any of the 7 mammography registries participating in the National Cancer Institutefunded Breast Cancer Surveillance Consortium (BCSC) (available at breastscreening.cancer.gov) (29). The BCSC is a community-based, ethnically and geographically diverse sample that broadly represents the United States (30). We excluded women who had a diagnosis of breast cancer before their first eligible mammography examination. Because our goal was to develop a model of long-term risk for invasive breast cancer, we excluded women with cancer diagnosed in the first 6 months of follow-up to minimize the number of cases of cancer included in the model that were diagnosed on the basis of the mammogram used for risk assessment. Women were also excluded if they had breast implants. Women in whom ductal carcinoma in situ was diagnosed were censored at the time of diagnosis in the primary analysis. When women had several mammograms, we based our analysis on findings from the first mammogram. Each registry obtains annual approval from its institutional review board for consenting processes or a waiver of consent, enrollment of participants, and ongoing data linkage for research purposes. All registries have received a Certificate of Confidentiality from the federal government that protects the identities of research participants. Measurement of Risk Factors Patient information was obtained primarily from self-report at the time of mammography. We selected 2 risk factors in addition to breast density for inclusion in the model on the basis of simplicity (yes or no) and a high attributable risk: history of breast cancer in a first-degree relative and history of a breast biopsy. Body mass index was later considered for addition to the model, but it was excluded to maintain parsimony and because it had minimal effect on model discrimination (the increase in the concordance statistic [c-statistic] was only 0.003). For modeling and validation, missing data for relatives with breast cancer and number of breast biopsies were set to 0. The 5-year Gail risk was computed for each woman by using the algorithms provided by the National Cancer Institute to calculate the Gail model risk for individual women (31). For Gail model calculations, missing data were coded as specified by that model (age at menarche as14 years, age at first live birth as<20 years, number of breast biopsies as 0, and number of first-degree relatives as 0). Ethnicity was coded by using the expanded race and ethnicity definition currently used in the Surveillance, Epidemiology, and End Results (SEER) database and U.S. Vital Statistics (non-Hispanic White, non-Hispanic Black, Asian or Pacific Islander, Native American\/Alaskan Native, Hispanic, or other). We classified women who self-identified as mixed or other race with participants who did not report race and ethnicity. Breast Density Community radiologists at each site classified breast density on screening mammograms as part of routine clinical practice by using the American College of Radiology BI-RADS density categories (32): almost entirely fat (category 1), scattered fibroglandular densities (category 2), heterogeneously dense (category 3), and extremely dense (category 4). The BI-RADS category 2 was used as the reference group for breast density because it formed the largest group. Ascertainment of Breast Cancer Cases Breast cancer outcomes (invasive cancer and ductal carcinoma in situ) were obtained at each site through linkage with the regional population-based SEER program, state tumor registries, and pathology databases. Vital Status Vital status was obtained through linkage to SEER registries, state tumor registries, and the individual state vital statistics or the National Death Index. Model Development We used a proportional hazards model of invasive breast cancer to estimate the hazard ratios for each BI-RADS breast density category. Women entered the model 6 months after the index mammogram and were censored at the time of death, diagnosis of ductal carcinoma in situ, or the end of follow-up. All models were adjusted for age (in 5-year intervals) and race and ethnicity. The strength of the breast density association with breast cancer was greater for women younger than age 65 years (P for interaction< 0.001). Thus, separate models were fitted for women younger than age 65 years and for women age 65 years or older. No other interaction terms were included in the final model. We calculated similar estimates for first-degree relatives with breast cancer (yes or no) and a personal history of breast biopsy (yes or no) from the BCSC. All predictors met the proportional hazards assumption that was assessed by loglog plots and by including interaction terms with time for each predictor variable. We then developed an absolute risk model by using methods described in the Appendix Figure. The model primarily estimates predicted incidence of invasive breast cancer by using age, race or ethnicity, and breast density. These estimates are then adjusted for family history and biopsy history if available. We based our estimates of breast cancer incidence on the SEER age- and ethnicity-specific risk for invasive breast cancer (1992 to 2002) (33). Age-specific incidence for each ethnic group was estimated by fitting a third-order polynomial model to the SEER data. Age-specific incidence rates for the Native American and Alaskan Native group were inconsistent in SEER, so we excluded this group from further analyses. We calculated the baseline risk for the model by adjusting SEER incidence for the population's attributable risk for each breast density subgroup. We estimated the age- and ethnicity-specific distribution of mammographic breast density needed for these calculations by using data from a larger set of 3343047 mammograms from the BCSC. The distribution of breast density varied statistically significantly by age and by race or ethnicity (P< 0.001 for each comparison). The model used these variations by age and race to distribute the 5-year risk for invasive breast cancer across the 4 breast density subgroups. We used the methods described by Gail and colleagues (7) to translate the hazard ratios and risk factor distributions into absolute risks. The age-, sex-, and ethnicity-specific competing risks for death for women were calcula\n\n",
                "DataExportTag": "CAN442469",
                "QuestionID": "QID307",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developmen...",
                "Choices": {
                    "1": {
                        "Display": "\"Hospitalization and Mortality Risk Analysis\""
                    },
                    "2": {
                        "Display": "mortality, heart_rate, death, hazard_ratio, intensive_care, hospitalization, admission, comorbiditie, comorbidity, hazard, die, statin, cox_proportional, hospital, cvd"
                    },
                    "3": {
                        "Display": "\"Breast and Ovarian Cancer Screening and Hormone Therapy\""
                    },
                    "4": {
                        "Display": "breast, woman, screening, ovarian_cancer, mammography, hormone, estrogen_receptor, postmenopausal_woman, postmenopausal, hrt, estrogen, mammogram, invasive, family_history, ht"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID382",
            "SecondaryAttribute": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developmen...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Development and Validation of a New Predictive Model Context Existing breast cancer prediction tools do not account for breast density, a strong risk factor for breast cancer and have been studied in white women only. Contribution The authors developed a breast cancer risk prediction model that incorporates a measure of breast density routinely reported with mammography. Its predictions were accurate, but it had only modest ability to distinguish women who did not develop cancer from those who did, and it misclassified risk in some subgroups. Implication The model requires validation in additional populations. A breast cancer prediction model that incorporates breast density does well in some but not all domains of predicting risk. Its accuracy should be better characterized before it is used clinically. The Editors In 2007, breast cancer will have been diagnosed in more than 178000 women in the United States, and more than 40000 women will have died of breast cancer (1). Most of these women never had their risk for breast cancer assessed, and even fewer considered chemoprevention (25). Providing women with an estimate of their risk for breast cancer would provide an opportunity for them to consider options to decrease their risk. Women at low short-term risk for breast cancer may experience less anxiety about their health and would be less likely to benefit from prevention efforts. Women at very high risk may warrant additional screening tests, such as breast magnetic resonance imaging (6), and might benefit from chemoprevention of breast cancer with tamoxifen or raloxifene. The standard risk assessment model available to practitioners (the Gail model) (7) identifies only a minority of women who eventually develop breast cancer being at high risk (8). Better breast cancer risk prediction tools are needed (9). The radiographic appearance of the breast has been consistently shown to be a major risk factor for breast cancer, whether it is defined by a qualitative assessment of the parenchymal pattern or a quantitative measure of percentage of density (1012). Women in whom more than 50% of total breast area is mammographically dense have high breast density and are at 3- to 5-fold greater risk for breast cancer than women in whom breast density is less than 25% (10, 1316). The increased risk for breast cancer associated with breast density is due in part to the lower sensitivity of mammography in dense breasts (1719), but the association remains strong after accounting for masking (20, 21). Mammographically dense breast tissue is rich in epithelium and stroma (10), and the association could represent activation of epithelial cells or fibroblasts (2225). Recently, several models have been published that incorporate breast density: One uses a continuous measure of breast density that is not available to clinicians and has not been validated (26), and the other predicts 1-year risk for breast cancer (27). We previously demonstrated that a simple model based on age, ethnicity, and a categorical measure of breast density had predictive accuracy similar to that of the Gail model in a multiethnic cohort of women receiving screening mammograms in northern California (28). We expand on that work by using data from more than 1 million ethnically diverse women throughout the United States to develop and validate a risk assessment tool that incorporates breast density and therefore might improve breast cancer screening and prevention efforts. Methods Study Population We included 1095484 women age 35 years or older who had had at least 1 mammogram with breast density measured by using the Breast Imaging Reporting and Data System (BI-RADS) classification system in any of the 7 mammography registries participating in the National Cancer Institutefunded Breast Cancer Surveillance Consortium (BCSC) (available at breastscreening.cancer.gov) (29). The BCSC is a community-based, ethnically and geographically diverse sample that broadly represents the United States (30). We excluded women who had a diagnosis of breast cancer before their first eligible mammography examination. Because our goal was to develop a model of long-term risk for invasive breast cancer, we excluded women with cancer diagnosed in the first 6 months of follow-up to minimize the number of cases of cancer included in the model that were diagnosed on the basis of the mammogram used for risk assessment. Women were also excluded if they had breast implants. Women in whom ductal carcinoma in situ was diagnosed were censored at the time of diagnosis in the primary analysis. When women had several mammograms, we based our analysis on findings from the first mammogram. Each registry obtains annual approval from its institutional review board for consenting processes or a waiver of consent, enrollment of participants, and ongoing data linkage for research purposes. All registries have received a Certificate of Confidentiality from the federal government that protects the identities of research participants. Measurement of Risk Factors Patient information was obtained primarily from self-report at the time of mammography. We selected 2 risk factors in addition to breast density for inclusion in the model on the basis of simplicity (yes or no) and a high attributable risk: history of breast cancer in a first-degree relative and history of a breast biopsy. Body mass index was later considered for addition to the model, but it was excluded to maintain parsimony and because it had minimal effect on model discrimination (the increase in the concordance statistic [c-statistic] was only 0.003). For modeling and validation, missing data for relatives with breast cancer and number of breast biopsies were set to 0. The 5-year Gail risk was computed for each woman by using the algorithms provided by the National Cancer Institute to calculate the Gail model risk for individual women (31). For Gail model calculations, missing data were coded as specified by that model (age at menarche as14 years, age at first live birth as<20 years, number of breast biopsies as 0, and number of first-degree relatives as 0). Ethnicity was coded by using the expanded race and ethnicity definition currently used in the Surveillance, Epidemiology, and End Results (SEER) database and U.S. Vital Statistics (non-Hispanic White, non-Hispanic Black, Asian or Pacific Islander, Native American\/Alaskan Native, Hispanic, or other). We classified women who self-identified as mixed or other race with participants who did not report race and ethnicity. Breast Density Community radiologists at each site classified breast density on screening mammograms as part of routine clinical practice by using the American College of Radiology BI-RADS density categories (32): almost entirely fat (category 1), scattered fibroglandular densities (category 2), heterogeneously dense (category 3), and extremely dense (category 4). The BI-RADS category 2 was used as the reference group for breast density because it formed the largest group. Ascertainment of Breast Cancer Cases Breast cancer outcomes (invasive cancer and ductal carcinoma in situ) were obtained at each site through linkage with the regional population-based SEER program, state tumor registries, and pathology databases. Vital Status Vital status was obtained through linkage to SEER registries, state tumor registries, and the individual state vital statistics or the National Death Index. Model Development We used a proportional hazards model of invasive breast cancer to estimate the hazard ratios for each BI-RADS breast density category. Women entered the model 6 months after the index mammogram and were censored at the time of death, diagnosis of ductal carcinoma in situ, or the end of follow-up. All models were adjusted for age (in 5-year intervals) and race and ethnicity. The strength of the breast density association with breast cancer was greater for women younger than age 65 years (P for interaction< 0.001). Thus, separate models were fitted for women younger than age 65 years and for women age 65 years or older. No other interaction terms were included in the final model. We calculated similar estimates for first-degree relatives with breast cancer (yes or no) and a personal history of breast biopsy (yes or no) from the BCSC. All predictors met the proportional hazards assumption that was assessed by loglog plots and by including interaction terms with time for each predictor variable. We then developed an absolute risk model by using methods described in the Appendix Figure. The model primarily estimates predicted incidence of invasive breast cancer by using age, race or ethnicity, and breast density. These estimates are then adjusted for family history and biopsy history if available. We based our estimates of breast cancer incidence on the SEER age- and ethnicity-specific risk for invasive breast cancer (1992 to 2002) (33). Age-specific incidence for each ethnic group was estimated by fitting a third-order polynomial model to the SEER data. Age-specific incidence rates for the Native American and Alaskan Native group were inconsistent in SEER, so we excluded this group from further analyses. We calculated the baseline risk for the model by adjusting SEER incidence for the population's attributable risk for each breast density subgroup. We estimated the age- and ethnicity-specific distribution of mammographic breast density needed for these calculations by using data from a larger set of 3343047 mammograms from the BCSC. The distribution of breast density varied statistically significantly by age and by race or ethnicity (P< 0.001 for each comparison). The model used these variations by age and race to distribute the 5-year risk for invasive breast cancer across the 4 breast density subgroups. We used the methods described by Gail and colleagues (7) to translate the hazard ratios and risk factor distributions into absolute risks. The age-, sex-, and ethnicity-specific competing risks for death for women were calcula\n\n",
                "DataExportTag": "CAN1195891",
                "QuestionID": "QID382",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Using Clinical Factors and Mammographic Breast Density to Estimate Breast Cancer Risk: Developmen...",
                "Choices": {
                    "1": {
                        "Display": "\"Cancer Metastasis and Cell Migration\""
                    },
                    "2": {
                        "Display": "integrin, metastasis, matrix_metalloproteinase_inhibitors, adhesion, invasion, migration, extracellular_matrix, fak, focal_adhesion, motility, upa_activator, lung, matrix, upar, fibronectin"
                    },
                    "3": {
                        "Display": "\"Breast Cancer and Brain Tumor Research\""
                    },
                    "4": {
                        "Display": "breast, glioma, glioblastoma, triple_negative_breast_cancer, estrogen_receptor, cerebral, malondialdehyde, estrogen, estrogen_receptor_alpha, mammary, tamoxifen, gsc, resistance, astrocytoma, epidermal_growth_factor"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID150",
            "SecondaryAttribute": "Using events for the scalable federation of heterogeneous components The thesis of this paper is...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Using events for the scalable federation of heterogeneous components The thesis of this paper is that, using our eventbased development principles, components that were not designed to interoperate, can be made to work together quickly and easily. The only requirement is that each component must be made event-based by adding an interface for registering interest in events and an interface for injecting actions. A component noti es an event to a distributed client if the parameters of an event, internal to the component, match the parameters of a particular registration. Heterogeneous components can be federated using event-based rules; rules can respond to events from any component by injecting actions into any other component. We show that the event paradigm is scalable by illustrating how event-based components can be located worldwide, using a federation of event brokers. Additionally, we illustrate with 3 event-based systems we have developed: a component-based multimedia system, a multi-user virtual worlds system and an augmented reality system for mobile users. Finally, we show how the event paradigm is also scalable enough to allow event federation of entire systems, not just single components. We illustrate by showing how we have federated the operation of the 3 featured eventbased systems. This enables, for example, real-world mobile users to appear as avatars in the appropriate locations in the VR world, and for these avatars to move in response to actual user movements. 1 Principles of Event-based Systems This section introduces distributed systems development using events. The framework presented here uses events to \\glue\" together components. By using events as a uniform way of informing other components of activities that have occurred, systems\/applications can be built around a generic eventresponse paradigm. Using this approach, the entire Internet can become a plug-and-play domain. An event is an asynchronous message containing parametrised details of an activity that has occurred within a component, or has been detected by the component. Services publish details of classes of event they actively monitor for, clients register interest with services, and services notify clients, as appropriate. Single events, or events as part of an ordered combination of events, noti ed to clients, can be used to trigger further actions within the system\/application. Using events as the uniform interchange of activity between the possibly distributed components, simplies the construction of a complex system\/application. Our recent work is making fundamental contributions to the di\u00c6cult problem of how to compose or federate existing systems to achieve interoperability. A related problem that is relevant within a single domain is how to add new object types (in our case, event classes) to a dynamically running system. In this paper, we use the term active system to describe a large system or application composed of components interoperating via event federation. 1.1 Components and Glue Event-based programming relies on two main concepts: Existing event-based components are used as the building blocks of applications. These may be distributed around the local network or Internet. They can inform interested users of pertinent events, such as a change in the location of a mobile user. The power of event-based system building is that no component is designed to work speci cally with any other component. To compose many event-based components into an active system, a federator component must be written. This is the bespoke part of the system, composed of event-based rules that describe actions to take in response to the receipt of certain events from the components within the system. Rules act as a distributed systems integration glue, using any class of event from any components to drive any other components by injecting any actions. Examples are moving a user interface to the application owner's new location in response to an event that the user has moved, or recon guring a multimedia application in response to an event detailing a change in bandwidth availability. In this paper we specify event-based rules in the following way: rule <name> = <event expression>; {<list of invocations>} In a multi-user application, for example a multimedia conference, each user owns a set of components, but the overall system is composed of the interacting set of all users' components. In such a situation, it is likely that each user will have a separate federator component to take care of his\/her own components. In the remainder of this section, we illustrate eventbased programming concepts using two case studies. The rst shows how a stand-alone drawing board can be made event-based and then used as a component in a CSCW application. The second shows how event services allow clients to tailor their range of interest. For this we use the example of a location service, which is part of our augmented environments framework. The location service is responsible for keeping clients location-aware. 1.2 Event Taxonomy and Brokerage One event source can generate events of one or more event classes. An event class is analogous to an object class. It has typed attributes, instances of which uniquely identify a captured activity. For example, an event source that provides information about the locations of users can o er monitoring facilities for the following class of event: LocationEvent(Domain,Name,Type,Location). Class LocationEvent identi es that an entity has changed location in a speci c domain. The type of the entity can be, for example, a person or a piece of equipment. To actually detect movement of entities, we used active badges [7], a form of electronic tag. Distributed event-based objects advertise the event classes they can monitor for by exporting them to an event broker. Clients can query the event broker, to search for appropriate event sources. An event taxonomy describes the set and organisation of event classes in a particular domain. We are investigating the organisation of event classes into class hierarchies of events, to support automatic propagation of interest in sub-classes. For example, consider the class LocationEvent. If we want to track the whereabouts of user Mark.Spiteri, we register interest with a local event broker. We intend that that should be all that is required to track him wherever he goes in our domain (and even world-wide { see below). He may be tracked by many di erent means, such as active badge or login detection within a building, outside by GPS [1]; in either, by security camera image recognition etc. Each class of specialist tracking can be encoded as a sub-class of LocationEvent. The event broker will automatically register our interest with all known subclasses. Event sub-classing is also useful in allowing specialisation of event classes, whilst supporting compatibility with super-classes. For example, a new event class FineGrainedLocation can provide a 3D x on a user, rather than just a named location. This can be built on top of a GPS system or a ne-grained o\u00c6ce-based system [15], thus providing latitude, longitude and altitude information. In our design, the event broker is the custodian of the class hierarchy for its local event domain. Querying the event broker for a particular class of event returns all event sources that can monitor for the class and all its sub-classes. It is also possible to query a sub-class directly, events of which provide the specialised information not available in the super-class. By using a class hierarchy, new subclasses may be added dynamically and registration of interest in a class may be extended to a new subclass. We are investigating how to use this mechanism to add new services that notify new event classes without recompiling clients. Because event interfaces do not always describe the purpose of the event class, we are extending event brokers to support the association of free text with event classes. The broker will support searches, similar to Web search engines, returning event classes that match text queries. We are employing a locally developed search engine in our implementation [12]. 1.3 Registration for Filtering Some event sources generate thousands of events a second. Transmission of all of these is network intensive and requires a client of the event source to perform ltering to decide whether a particular event is of interest. Our event system allows interested parties to register interest in events with certain characteristics. Only events that match the registered patterns are dispatched to the interested parties, thus reducing network tra\u00c6c and ltering requirements. To register interest with an event source, clients provide an event template. This is an event instance, with elds for exact match lled in and those for wildcard match expressed as variables. Examples for class LocationEvent are as follows: LocationEvent(\"cl.cam.ac.uk\", \"John.Bates\", \"Person\", L) { Report wherever John is seen in the Computer Lab. LocationEvent(\"cl.cam.ac.uk\", P, \"Person\", \"Meeting Room\") { Report when anyone is seen in the Computer Lab meeting room. LocationEvent(D, P, \"Person\", L) { Report when anyone is seen in any of the places we receive events from. When event instances are noti ed to clients, values for all class attributes are given, e.g. the instance LocationEvent(\"cl.cam.ac.uk\", \"John.Bates\", \"Person\", \"Meeting Room\") means John has been seen in the meeting room. As a comparison to another event-based component, the drawing board advertises event class LineDrawn(user,x1,y1,x2,y2). This allows us to nd out about lines that are drawn on the board. We will show how this is useful shortly. Our policy is to apply access control on event registration. We use the locally developed OASIS system [8]. We are currently investigating to what extent authorisation should extend automatically to new subclasses. For example, it might be that a given client is allowed to register with an active badge-based location service but i\n\n",
                "DataExportTag": "AI422569",
                "QuestionID": "QID150",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Using events for the scalable federation of heterogeneous components The thesis of this paper is...",
                "Choices": {
                    "1": {
                        "Display": "\"Academic Research and Bibliometric Analysis\""
                    },
                    "2": {
                        "Display": "community, discipline, big_data, academic, bibliometric, citation, article, progress, trend, industry, mathematics, cover, tutorial, advance, area"
                    },
                    "3": {
                        "Display": "\"Cognitive Psychology and Philosophy of Mind\""
                    },
                    "4": {
                        "Display": "cognitive, human, theory, mind, consciousness, cognition, brain, agent, psychology, philosophy, social, mathematics, philosophical, concept, turing"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID395",
            "SecondaryAttribute": "Using events for the scalable federation of heterogeneous components The thesis of this paper is...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Using events for the scalable federation of heterogeneous components The thesis of this paper is that, using our eventbased development principles, components that were not designed to interoperate, can be made to work together quickly and easily. The only requirement is that each component must be made event-based by adding an interface for registering interest in events and an interface for injecting actions. A component noti es an event to a distributed client if the parameters of an event, internal to the component, match the parameters of a particular registration. Heterogeneous components can be federated using event-based rules; rules can respond to events from any component by injecting actions into any other component. We show that the event paradigm is scalable by illustrating how event-based components can be located worldwide, using a federation of event brokers. Additionally, we illustrate with 3 event-based systems we have developed: a component-based multimedia system, a multi-user virtual worlds system and an augmented reality system for mobile users. Finally, we show how the event paradigm is also scalable enough to allow event federation of entire systems, not just single components. We illustrate by showing how we have federated the operation of the 3 featured eventbased systems. This enables, for example, real-world mobile users to appear as avatars in the appropriate locations in the VR world, and for these avatars to move in response to actual user movements. 1 Principles of Event-based Systems This section introduces distributed systems development using events. The framework presented here uses events to \\glue\" together components. By using events as a uniform way of informing other components of activities that have occurred, systems\/applications can be built around a generic eventresponse paradigm. Using this approach, the entire Internet can become a plug-and-play domain. An event is an asynchronous message containing parametrised details of an activity that has occurred within a component, or has been detected by the component. Services publish details of classes of event they actively monitor for, clients register interest with services, and services notify clients, as appropriate. Single events, or events as part of an ordered combination of events, noti ed to clients, can be used to trigger further actions within the system\/application. Using events as the uniform interchange of activity between the possibly distributed components, simplies the construction of a complex system\/application. Our recent work is making fundamental contributions to the di\u00c6cult problem of how to compose or federate existing systems to achieve interoperability. A related problem that is relevant within a single domain is how to add new object types (in our case, event classes) to a dynamically running system. In this paper, we use the term active system to describe a large system or application composed of components interoperating via event federation. 1.1 Components and Glue Event-based programming relies on two main concepts: Existing event-based components are used as the building blocks of applications. These may be distributed around the local network or Internet. They can inform interested users of pertinent events, such as a change in the location of a mobile user. The power of event-based system building is that no component is designed to work speci cally with any other component. To compose many event-based components into an active system, a federator component must be written. This is the bespoke part of the system, composed of event-based rules that describe actions to take in response to the receipt of certain events from the components within the system. Rules act as a distributed systems integration glue, using any class of event from any components to drive any other components by injecting any actions. Examples are moving a user interface to the application owner's new location in response to an event that the user has moved, or recon guring a multimedia application in response to an event detailing a change in bandwidth availability. In this paper we specify event-based rules in the following way: rule <name> = <event expression>; {<list of invocations>} In a multi-user application, for example a multimedia conference, each user owns a set of components, but the overall system is composed of the interacting set of all users' components. In such a situation, it is likely that each user will have a separate federator component to take care of his\/her own components. In the remainder of this section, we illustrate eventbased programming concepts using two case studies. The rst shows how a stand-alone drawing board can be made event-based and then used as a component in a CSCW application. The second shows how event services allow clients to tailor their range of interest. For this we use the example of a location service, which is part of our augmented environments framework. The location service is responsible for keeping clients location-aware. 1.2 Event Taxonomy and Brokerage One event source can generate events of one or more event classes. An event class is analogous to an object class. It has typed attributes, instances of which uniquely identify a captured activity. For example, an event source that provides information about the locations of users can o er monitoring facilities for the following class of event: LocationEvent(Domain,Name,Type,Location). Class LocationEvent identi es that an entity has changed location in a speci c domain. The type of the entity can be, for example, a person or a piece of equipment. To actually detect movement of entities, we used active badges [7], a form of electronic tag. Distributed event-based objects advertise the event classes they can monitor for by exporting them to an event broker. Clients can query the event broker, to search for appropriate event sources. An event taxonomy describes the set and organisation of event classes in a particular domain. We are investigating the organisation of event classes into class hierarchies of events, to support automatic propagation of interest in sub-classes. For example, consider the class LocationEvent. If we want to track the whereabouts of user Mark.Spiteri, we register interest with a local event broker. We intend that that should be all that is required to track him wherever he goes in our domain (and even world-wide { see below). He may be tracked by many di erent means, such as active badge or login detection within a building, outside by GPS [1]; in either, by security camera image recognition etc. Each class of specialist tracking can be encoded as a sub-class of LocationEvent. The event broker will automatically register our interest with all known subclasses. Event sub-classing is also useful in allowing specialisation of event classes, whilst supporting compatibility with super-classes. For example, a new event class FineGrainedLocation can provide a 3D x on a user, rather than just a named location. This can be built on top of a GPS system or a ne-grained o\u00c6ce-based system [15], thus providing latitude, longitude and altitude information. In our design, the event broker is the custodian of the class hierarchy for its local event domain. Querying the event broker for a particular class of event returns all event sources that can monitor for the class and all its sub-classes. It is also possible to query a sub-class directly, events of which provide the specialised information not available in the super-class. By using a class hierarchy, new subclasses may be added dynamically and registration of interest in a class may be extended to a new subclass. We are investigating how to use this mechanism to add new services that notify new event classes without recompiling clients. Because event interfaces do not always describe the purpose of the event class, we are extending event brokers to support the association of free text with event classes. The broker will support searches, similar to Web search engines, returning event classes that match text queries. We are employing a locally developed search engine in our implementation [12]. 1.3 Registration for Filtering Some event sources generate thousands of events a second. Transmission of all of these is network intensive and requires a client of the event source to perform ltering to decide whether a particular event is of interest. Our event system allows interested parties to register interest in events with certain characteristics. Only events that match the registered patterns are dispatched to the interested parties, thus reducing network tra\u00c6c and ltering requirements. To register interest with an event source, clients provide an event template. This is an event instance, with elds for exact match lled in and those for wildcard match expressed as variables. Examples for class LocationEvent are as follows: LocationEvent(\"cl.cam.ac.uk\", \"John.Bates\", \"Person\", L) { Report wherever John is seen in the Computer Lab. LocationEvent(\"cl.cam.ac.uk\", P, \"Person\", \"Meeting Room\") { Report when anyone is seen in the Computer Lab meeting room. LocationEvent(D, P, \"Person\", L) { Report when anyone is seen in any of the places we receive events from. When event instances are noti ed to clients, values for all class attributes are given, e.g. the instance LocationEvent(\"cl.cam.ac.uk\", \"John.Bates\", \"Person\", \"Meeting Room\") means John has been seen in the meeting room. As a comparison to another event-based component, the drawing board advertises event class LineDrawn(user,x1,y1,x2,y2). This allows us to nd out about lines that are drawn on the board. We will show how this is useful shortly. Our policy is to apply access control on event registration. We use the locally developed OASIS system [8]. We are currently investigating to what extent authorisation should extend automatically to new subclasses. For example, it might be that a given client is allowed to register with an active badge-based location service but i\n\n",
                "DataExportTag": "AI1094659",
                "QuestionID": "QID395",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Using events for the scalable federation of heterogeneous components The thesis of this paper is...",
                "Choices": {
                    "1": {
                        "Display": "\"Smart Home Monitoring and Activity Recognition\""
                    },
                    "2": {
                        "Display": "activity, sensor, smartphone, monitoring, wearable, smart, human, har, smart_home, accelerometer, home, fall, remote, monitor, wheelchair"
                    },
                    "3": {
                        "Display": "\"Human Activity Recognition and Posture Classification\""
                    },
                    "4": {
                        "Display": "activity, human, speech_recognition, fall, classification, gait, body, posture, sensor, action, walk, motion, svm, wearable, sport"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID189",
            "SecondaryAttribute": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospital data. BACKGROUND Improving quality and effectiveness of healthcare is one of the priorities of health policies. Hospital or physician volume represents a measurable variable with an impact on effectiveness of healthcare. An Italian law calls for the definition of \u00abqualitative, structural, technological, and quantitative standards of hospital care\u00bb. There is a need for an evaluation of the available scientific evidence in order to identify qualitative, structural, technological, and quantitative standards of hospital care, including the volume of care above or below which the public and private hospitals may be accredited (or not) to provide specific healthcare interventions. OBJECTIVES To identify conditions\/interventions for which an association between volume and outcome has been investigated. To identify conditions\/interventions for which an association between volume and outcome has been proved. To analyze the distribution of Italian health providers by volume of activity. To measure the association between volume of care and outcomes of the health providers of the Italian National Health Service (NHS). METHODS Systematic review An overview of systematic reviews was performed searching PubMed, EMBASE, and The Cochrane Library up to November 2016. Studies were evaluated by 2 researchers independently; quality assessment was performed using the AMSTAR checklist. For each health condition and outcome, if available, total number of studies, participants, high volume cut-off values, and metanalysis have been reported. According to the considered outcomes, health topics were classified into 3 groups: positive association: a positive association was demonstrated in the majority of studies\/participants and\/or a pooled measure (metanalysis) with positive results was reported; lack of association: both studies and\/or metanalysis showed no association; no sufficient evidence of association: both results of single studies and metanalysis do not allow to draw firm conclusions on the association between volume and outcome. Analysis of the distribution of Italian hospitals by volume of activity and the association between volume of activity and outcomes: the Italian National Outcome evaluation Programme 2016 The analyses were performed using the Hospital Information System and the National Tax Register (year 2015). For each condition, the number of hospitals by volume of activity was calculated. Hospitals with a volume lower than 3-5 cases\/year were excluded. For conditions with more than 1,500 cases\/year and frequency of outcome \u22651%, the association between volume of care and outcome was analyzed estimating risk-adjusted outcomes. RESULTS Bibliographic searches identified 80 reviews, evaluating 48 different clinical areas. The main outcome considered was intrahospital\/30-day mortality. The other outcomes vary depending on the type of condition or intervention in study. The relationship between hospital volume and outcomes was considered in 47 out of 48 conditions: 34 conditions showed evidence of a positive association; \u2022 14 conditions consider cancer surgery for bladder, breast, colon, rectum, colon rectum, oesophagus, kidney, liver, lung, ovaries, pancreas, prostate, stomach, head and neck; \u2022 11 conditions consider cardiocerebrovascular area: nonruptured and ruptured abdominal aortic aneurysm, acute myocardial infarction, brain aneurysm, carotid endarterectomy, coronary angioplasty, coronary artery bypass, paediatric heart surgery, revascularization of lower limbs, stroke, subarachnoid haemorrhage; \u2022 2 conditions consider orthopaedic area: knee arthroplasty, hip fracture; \u2022 7 conditions consider other areas: AIDS, bariatric surgery, cholecystectomy, intensive care unit, neonatal intensive care unit, sepsis, and traumas; for 3 conditions, no association was demonstrated: hip arthroplasty, dialysis, and thyroidectomy. for the remaining 10 conditions, the available evidence does not allow to draw firm conclusions about the association between hospital volume and considered outcomes: surgery for testicular cancer and intracranial tumours, paediatric oncology, aortofemoral bypass, cardiac catheterization, appendectomy, colectomy, inguinal hernia, respiratory failure, and hysterectomy. The relationship between volume of clinician\/surgeon and outcomes was assessed only through the literature re view; to date, it is not possible to analyze this association for Italian health provider hospitals, since information on the clinician\/surgeon on the hospital discharge chart is missing. The literature found a positive association for 21 conditions: 9 consider surgery for cancer: bladder, breast, colon, colon rectum, pancreas, prostate, rectum, stomach, and head and neck; 5 consider the cardiocerebrovascular area: ruptured and nonruptured abdominal aortic aneurysm, carotid endarterectomy, paediatric heart surgery, and revascularization of the lower limbs; 2 consider the orthopaedic area: knee and hip arthroplasty; 5 consider other areas: AIDS, bariatric surgery, hysterectomy, intensive care unit, and thyroidectomy. The analysis of the distribution of Italian hospitals concerned the 34 conditions for which the systematic review has shown a positive volume-outcome association. For the following, it was possible to conduct the analysis of the association using national data: unruptured abdominal aortic aneurysm, coronary angioplasty, hip arthroplasty, knee arthroplasty, coronary artery bypass, cancer surgery (colon, liver, breast, pancreas, lung, prostate, kidney, and stomach), laparoscopic cholecystectomy, hip fracture, stroke, acute myocardial infarction. For these conditions, the association between volume and outcome of care was observed. For laparoscopic cholecystectomy and surgery of the breast and stomach cancer, the association between the volume of the discharge (o dismissal) operating unit and the outcome was analyzed. The outcomes differ depending on the condition studied. The shape of the relationship is variable among different conditions, with heterogeneous slope of the curves. DISCUSSION For many conditions, the overview of systematic reviews has shown a strong evidence of association between higher volumes and better outcomes. The quality of the available reviews can be considered good for the consistency of the results between the studies and for the strength of the association; however, this does not mean that the included studies are of good quality. Analyzing national data, potential confounders, including age and comorbidities, have been considered. The systematic review of the literature does not permit to identify predefined volume thresholds. The analysis of national data shows a strong improvement in outcomes in the first part of the curve (from very low to higher volumes) for most conditions. In some cases, the improvement in outcomes remains gradual or constant with the increasing volume of care; in other, the analysis could allow the identification of threshold values beyond which the outcome does not further improve. However, a good knowledge of the relationship between effectiveness of treatments and costs, the geographical distribution and the accessibility to healthcare services are necessary to choose the minimum volumes of care, under which specific health procedures could not been provided in the NHS. Some potential biases due to the use of information systems data should also be considered. The different way of coding among hospitals could lead to a different selection of cases for some conditions. Regarding the definition of the exposure (volume of care), a possible bias could result from misclassification of health providers with high volume of activity. Performing the intervention in different departments\/ units of the same hospital would result in an overestimation of the volume of care measured for hospital rather than for department\/unit. For the conditions with a further fragmentation within the same structure, the association between volumes of discharge department and outcomes has also been evaluated. In this case, the two curves were different. The limit is to attribute the outcome to the discharge unit, which in case of surgery may not be the intervention unit. A similar bias could occur if the main determinant of the outcome of treatment was the caseload of each surgeon. The results of the analysis may be biased when different operators in the same hospital\/unit carried out the same procedure. In any case, the observed association between volumes and outcome is very strong, and it is unlikely to be attributable to biases of the study design. Another aspect on which there is still little evidence is the interaction between volume of the hospital and of the surgeon. A MEDICARE study suggests that in some conditions, especially for specialized surgery, the effect of the surgeon's volume of activity is different depending on the structure volume, whereas it would not differ for some less specialized surgery conditions. The data here presented still show extremely fragmented volumes of both clinical and surgical areas, with a predominance of very low volume structures. Health systems operate, by definition, in a context of limited resources, especially when the amount of resources to allocate to the health system is reduced. In such conditions, the rationalization of the organization of health services based on the volume of care may make resources available to improve the effectiveness of interventions. The identification and certification of services and providers with high volume of activity can help to reduce differences in the access to non-effective procedures. To produce additional evidence to guide the reorganization of the national healthcare system, it will be necessary to design further primary studies to evaluate the effectiveness and safety of policies aimed at concentrating interventions in structures with high volumes of activity.\n\n",
                "DataExportTag": "CAN1449576",
                "QuestionID": "QID189",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospi...",
                "Choices": {
                    "1": {
                        "Display": "\"Endoscopic Procedures and Esophageal Disorders\""
                    },
                    "2": {
                        "Display": "endoscopic, esophageal, esd, endoscopy, endoscopic_submucosal, perforation, dysphagia, emr, tube, airway, resection, bleeding, anastomosis, bleed, stricture"
                    },
                    "3": {
                        "Display": "\"Surgical Procedures and Complications in Thoracic and Gastrointestinal Medicine\""
                    },
                    "4": {
                        "Display": "rectal, resection, pancreatic, lung, esophagectomy, esophageal, vat, anastomosis, video_assist, lobectomy, anastomotic_leakage, total_mesorectal, mortality, gastric, thoracic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID250",
            "SecondaryAttribute": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospital data. BACKGROUND Improving quality and effectiveness of healthcare is one of the priorities of health policies. Hospital or physician volume represents a measurable variable with an impact on effectiveness of healthcare. An Italian law calls for the definition of \u00abqualitative, structural, technological, and quantitative standards of hospital care\u00bb. There is a need for an evaluation of the available scientific evidence in order to identify qualitative, structural, technological, and quantitative standards of hospital care, including the volume of care above or below which the public and private hospitals may be accredited (or not) to provide specific healthcare interventions. OBJECTIVES To identify conditions\/interventions for which an association between volume and outcome has been investigated. To identify conditions\/interventions for which an association between volume and outcome has been proved. To analyze the distribution of Italian health providers by volume of activity. To measure the association between volume of care and outcomes of the health providers of the Italian National Health Service (NHS). METHODS Systematic review An overview of systematic reviews was performed searching PubMed, EMBASE, and The Cochrane Library up to November 2016. Studies were evaluated by 2 researchers independently; quality assessment was performed using the AMSTAR checklist. For each health condition and outcome, if available, total number of studies, participants, high volume cut-off values, and metanalysis have been reported. According to the considered outcomes, health topics were classified into 3 groups: positive association: a positive association was demonstrated in the majority of studies\/participants and\/or a pooled measure (metanalysis) with positive results was reported; lack of association: both studies and\/or metanalysis showed no association; no sufficient evidence of association: both results of single studies and metanalysis do not allow to draw firm conclusions on the association between volume and outcome. Analysis of the distribution of Italian hospitals by volume of activity and the association between volume of activity and outcomes: the Italian National Outcome evaluation Programme 2016 The analyses were performed using the Hospital Information System and the National Tax Register (year 2015). For each condition, the number of hospitals by volume of activity was calculated. Hospitals with a volume lower than 3-5 cases\/year were excluded. For conditions with more than 1,500 cases\/year and frequency of outcome \u22651%, the association between volume of care and outcome was analyzed estimating risk-adjusted outcomes. RESULTS Bibliographic searches identified 80 reviews, evaluating 48 different clinical areas. The main outcome considered was intrahospital\/30-day mortality. The other outcomes vary depending on the type of condition or intervention in study. The relationship between hospital volume and outcomes was considered in 47 out of 48 conditions: 34 conditions showed evidence of a positive association; \u2022 14 conditions consider cancer surgery for bladder, breast, colon, rectum, colon rectum, oesophagus, kidney, liver, lung, ovaries, pancreas, prostate, stomach, head and neck; \u2022 11 conditions consider cardiocerebrovascular area: nonruptured and ruptured abdominal aortic aneurysm, acute myocardial infarction, brain aneurysm, carotid endarterectomy, coronary angioplasty, coronary artery bypass, paediatric heart surgery, revascularization of lower limbs, stroke, subarachnoid haemorrhage; \u2022 2 conditions consider orthopaedic area: knee arthroplasty, hip fracture; \u2022 7 conditions consider other areas: AIDS, bariatric surgery, cholecystectomy, intensive care unit, neonatal intensive care unit, sepsis, and traumas; for 3 conditions, no association was demonstrated: hip arthroplasty, dialysis, and thyroidectomy. for the remaining 10 conditions, the available evidence does not allow to draw firm conclusions about the association between hospital volume and considered outcomes: surgery for testicular cancer and intracranial tumours, paediatric oncology, aortofemoral bypass, cardiac catheterization, appendectomy, colectomy, inguinal hernia, respiratory failure, and hysterectomy. The relationship between volume of clinician\/surgeon and outcomes was assessed only through the literature re view; to date, it is not possible to analyze this association for Italian health provider hospitals, since information on the clinician\/surgeon on the hospital discharge chart is missing. The literature found a positive association for 21 conditions: 9 consider surgery for cancer: bladder, breast, colon, colon rectum, pancreas, prostate, rectum, stomach, and head and neck; 5 consider the cardiocerebrovascular area: ruptured and nonruptured abdominal aortic aneurysm, carotid endarterectomy, paediatric heart surgery, and revascularization of the lower limbs; 2 consider the orthopaedic area: knee and hip arthroplasty; 5 consider other areas: AIDS, bariatric surgery, hysterectomy, intensive care unit, and thyroidectomy. The analysis of the distribution of Italian hospitals concerned the 34 conditions for which the systematic review has shown a positive volume-outcome association. For the following, it was possible to conduct the analysis of the association using national data: unruptured abdominal aortic aneurysm, coronary angioplasty, hip arthroplasty, knee arthroplasty, coronary artery bypass, cancer surgery (colon, liver, breast, pancreas, lung, prostate, kidney, and stomach), laparoscopic cholecystectomy, hip fracture, stroke, acute myocardial infarction. For these conditions, the association between volume and outcome of care was observed. For laparoscopic cholecystectomy and surgery of the breast and stomach cancer, the association between the volume of the discharge (o dismissal) operating unit and the outcome was analyzed. The outcomes differ depending on the condition studied. The shape of the relationship is variable among different conditions, with heterogeneous slope of the curves. DISCUSSION For many conditions, the overview of systematic reviews has shown a strong evidence of association between higher volumes and better outcomes. The quality of the available reviews can be considered good for the consistency of the results between the studies and for the strength of the association; however, this does not mean that the included studies are of good quality. Analyzing national data, potential confounders, including age and comorbidities, have been considered. The systematic review of the literature does not permit to identify predefined volume thresholds. The analysis of national data shows a strong improvement in outcomes in the first part of the curve (from very low to higher volumes) for most conditions. In some cases, the improvement in outcomes remains gradual or constant with the increasing volume of care; in other, the analysis could allow the identification of threshold values beyond which the outcome does not further improve. However, a good knowledge of the relationship between effectiveness of treatments and costs, the geographical distribution and the accessibility to healthcare services are necessary to choose the minimum volumes of care, under which specific health procedures could not been provided in the NHS. Some potential biases due to the use of information systems data should also be considered. The different way of coding among hospitals could lead to a different selection of cases for some conditions. Regarding the definition of the exposure (volume of care), a possible bias could result from misclassification of health providers with high volume of activity. Performing the intervention in different departments\/ units of the same hospital would result in an overestimation of the volume of care measured for hospital rather than for department\/unit. For the conditions with a further fragmentation within the same structure, the association between volumes of discharge department and outcomes has also been evaluated. In this case, the two curves were different. The limit is to attribute the outcome to the discharge unit, which in case of surgery may not be the intervention unit. A similar bias could occur if the main determinant of the outcome of treatment was the caseload of each surgeon. The results of the analysis may be biased when different operators in the same hospital\/unit carried out the same procedure. In any case, the observed association between volumes and outcome is very strong, and it is unlikely to be attributable to biases of the study design. Another aspect on which there is still little evidence is the interaction between volume of the hospital and of the surgeon. A MEDICARE study suggests that in some conditions, especially for specialized surgery, the effect of the surgeon's volume of activity is different depending on the structure volume, whereas it would not differ for some less specialized surgery conditions. The data here presented still show extremely fragmented volumes of both clinical and surgical areas, with a predominance of very low volume structures. Health systems operate, by definition, in a context of limited resources, especially when the amount of resources to allocate to the health system is reduced. In such conditions, the rationalization of the organization of health services based on the volume of care may make resources available to improve the effectiveness of interventions. The identification and certification of services and providers with high volume of activity can help to reduce differences in the access to non-effective procedures. To produce additional evidence to guide the reorganization of the national healthcare system, it will be necessary to design further primary studies to evaluate the effectiveness and safety of policies aimed at concentrating interventions in structures with high volumes of activity.\n\n",
                "DataExportTag": "CAN1449576",
                "QuestionID": "QID250",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospi...",
                "Choices": {
                    "1": {
                        "Display": "\"Endoscopic Procedures and Esophageal Disorders\""
                    },
                    "2": {
                        "Display": "endoscopic, esophageal, esd, endoscopy, endoscopic_submucosal, perforation, dysphagia, emr, tube, airway, resection, bleeding, anastomosis, bleed, stricture"
                    },
                    "3": {
                        "Display": "\"Surgical Procedures and Complications in Thoracic and Gastrointestinal Medicine\""
                    },
                    "4": {
                        "Display": "rectal, resection, pancreatic, lung, esophagectomy, esophageal, vat, anastomosis, video_assist, lobectomy, anastomotic_leakage, total_mesorectal, mortality, gastric, thoracic"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID369",
            "SecondaryAttribute": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospi...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospital data. BACKGROUND Improving quality and effectiveness of healthcare is one of the priorities of health policies. Hospital or physician volume represents a measurable variable with an impact on effectiveness of healthcare. An Italian law calls for the definition of \u00abqualitative, structural, technological, and quantitative standards of hospital care\u00bb. There is a need for an evaluation of the available scientific evidence in order to identify qualitative, structural, technological, and quantitative standards of hospital care, including the volume of care above or below which the public and private hospitals may be accredited (or not) to provide specific healthcare interventions. OBJECTIVES To identify conditions\/interventions for which an association between volume and outcome has been investigated. To identify conditions\/interventions for which an association between volume and outcome has been proved. To analyze the distribution of Italian health providers by volume of activity. To measure the association between volume of care and outcomes of the health providers of the Italian National Health Service (NHS). METHODS Systematic review An overview of systematic reviews was performed searching PubMed, EMBASE, and The Cochrane Library up to November 2016. Studies were evaluated by 2 researchers independently; quality assessment was performed using the AMSTAR checklist. For each health condition and outcome, if available, total number of studies, participants, high volume cut-off values, and metanalysis have been reported. According to the considered outcomes, health topics were classified into 3 groups: positive association: a positive association was demonstrated in the majority of studies\/participants and\/or a pooled measure (metanalysis) with positive results was reported; lack of association: both studies and\/or metanalysis showed no association; no sufficient evidence of association: both results of single studies and metanalysis do not allow to draw firm conclusions on the association between volume and outcome. Analysis of the distribution of Italian hospitals by volume of activity and the association between volume of activity and outcomes: the Italian National Outcome evaluation Programme 2016 The analyses were performed using the Hospital Information System and the National Tax Register (year 2015). For each condition, the number of hospitals by volume of activity was calculated. Hospitals with a volume lower than 3-5 cases\/year were excluded. For conditions with more than 1,500 cases\/year and frequency of outcome \u22651%, the association between volume of care and outcome was analyzed estimating risk-adjusted outcomes. RESULTS Bibliographic searches identified 80 reviews, evaluating 48 different clinical areas. The main outcome considered was intrahospital\/30-day mortality. The other outcomes vary depending on the type of condition or intervention in study. The relationship between hospital volume and outcomes was considered in 47 out of 48 conditions: 34 conditions showed evidence of a positive association; \u2022 14 conditions consider cancer surgery for bladder, breast, colon, rectum, colon rectum, oesophagus, kidney, liver, lung, ovaries, pancreas, prostate, stomach, head and neck; \u2022 11 conditions consider cardiocerebrovascular area: nonruptured and ruptured abdominal aortic aneurysm, acute myocardial infarction, brain aneurysm, carotid endarterectomy, coronary angioplasty, coronary artery bypass, paediatric heart surgery, revascularization of lower limbs, stroke, subarachnoid haemorrhage; \u2022 2 conditions consider orthopaedic area: knee arthroplasty, hip fracture; \u2022 7 conditions consider other areas: AIDS, bariatric surgery, cholecystectomy, intensive care unit, neonatal intensive care unit, sepsis, and traumas; for 3 conditions, no association was demonstrated: hip arthroplasty, dialysis, and thyroidectomy. for the remaining 10 conditions, the available evidence does not allow to draw firm conclusions about the association between hospital volume and considered outcomes: surgery for testicular cancer and intracranial tumours, paediatric oncology, aortofemoral bypass, cardiac catheterization, appendectomy, colectomy, inguinal hernia, respiratory failure, and hysterectomy. The relationship between volume of clinician\/surgeon and outcomes was assessed only through the literature re view; to date, it is not possible to analyze this association for Italian health provider hospitals, since information on the clinician\/surgeon on the hospital discharge chart is missing. The literature found a positive association for 21 conditions: 9 consider surgery for cancer: bladder, breast, colon, colon rectum, pancreas, prostate, rectum, stomach, and head and neck; 5 consider the cardiocerebrovascular area: ruptured and nonruptured abdominal aortic aneurysm, carotid endarterectomy, paediatric heart surgery, and revascularization of the lower limbs; 2 consider the orthopaedic area: knee and hip arthroplasty; 5 consider other areas: AIDS, bariatric surgery, hysterectomy, intensive care unit, and thyroidectomy. The analysis of the distribution of Italian hospitals concerned the 34 conditions for which the systematic review has shown a positive volume-outcome association. For the following, it was possible to conduct the analysis of the association using national data: unruptured abdominal aortic aneurysm, coronary angioplasty, hip arthroplasty, knee arthroplasty, coronary artery bypass, cancer surgery (colon, liver, breast, pancreas, lung, prostate, kidney, and stomach), laparoscopic cholecystectomy, hip fracture, stroke, acute myocardial infarction. For these conditions, the association between volume and outcome of care was observed. For laparoscopic cholecystectomy and surgery of the breast and stomach cancer, the association between the volume of the discharge (o dismissal) operating unit and the outcome was analyzed. The outcomes differ depending on the condition studied. The shape of the relationship is variable among different conditions, with heterogeneous slope of the curves. DISCUSSION For many conditions, the overview of systematic reviews has shown a strong evidence of association between higher volumes and better outcomes. The quality of the available reviews can be considered good for the consistency of the results between the studies and for the strength of the association; however, this does not mean that the included studies are of good quality. Analyzing national data, potential confounders, including age and comorbidities, have been considered. The systematic review of the literature does not permit to identify predefined volume thresholds. The analysis of national data shows a strong improvement in outcomes in the first part of the curve (from very low to higher volumes) for most conditions. In some cases, the improvement in outcomes remains gradual or constant with the increasing volume of care; in other, the analysis could allow the identification of threshold values beyond which the outcome does not further improve. However, a good knowledge of the relationship between effectiveness of treatments and costs, the geographical distribution and the accessibility to healthcare services are necessary to choose the minimum volumes of care, under which specific health procedures could not been provided in the NHS. Some potential biases due to the use of information systems data should also be considered. The different way of coding among hospitals could lead to a different selection of cases for some conditions. Regarding the definition of the exposure (volume of care), a possible bias could result from misclassification of health providers with high volume of activity. Performing the intervention in different departments\/ units of the same hospital would result in an overestimation of the volume of care measured for hospital rather than for department\/unit. For the conditions with a further fragmentation within the same structure, the association between volumes of discharge department and outcomes has also been evaluated. In this case, the two curves were different. The limit is to attribute the outcome to the discharge unit, which in case of surgery may not be the intervention unit. A similar bias could occur if the main determinant of the outcome of treatment was the caseload of each surgeon. The results of the analysis may be biased when different operators in the same hospital\/unit carried out the same procedure. In any case, the observed association between volumes and outcome is very strong, and it is unlikely to be attributable to biases of the study design. Another aspect on which there is still little evidence is the interaction between volume of the hospital and of the surgeon. A MEDICARE study suggests that in some conditions, especially for specialized surgery, the effect of the surgeon's volume of activity is different depending on the structure volume, whereas it would not differ for some less specialized surgery conditions. The data here presented still show extremely fragmented volumes of both clinical and surgical areas, with a predominance of very low volume structures. Health systems operate, by definition, in a context of limited resources, especially when the amount of resources to allocate to the health system is reduced. In such conditions, the rationalization of the organization of health services based on the volume of care may make resources available to improve the effectiveness of interventions. The identification and certification of services and providers with high volume of activity can help to reduce differences in the access to non-effective procedures. To produce additional evidence to guide the reorganization of the national healthcare system, it will be necessary to design further primary studies to evaluate the effectiveness and safety of policies aimed at concentrating interventions in structures with high volumes of activity.\n\n",
                "DataExportTag": "CAN894538",
                "QuestionID": "QID369",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Volume and health outcomes: evidence from systematic reviews and from evaluation of Italian hospi...",
                "Choices": {
                    "1": {
                        "Display": "\"Endocrine Disorders and Hormone Diseases\""
                    },
                    "2": {
                        "Display": "pituitary, adrenal, growth_hormone, cortisol, acth, acromegaly, adenoma, adrenocortical, prl, hormone, prolactin, pheochromocytoma, cushing_syndrome, endocrine, cushing"
                    },
                    "3": {
                        "Display": "\"Kidney Disease and Dialysis Management\""
                    },
                    "4": {
                        "Display": "renal, vitamin_d, kidney, ckd, pth, calcium, thyroid, urinary, hemodialysis, hypercalcemia, urine, dialysis, creatinine, phosphate, parathyroid_hormone"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID162",
            "SecondaryAttribute": "Web Graph: Learning Models for Prediction and Evolution Monitoring Data intensive systems flouris...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Web Graph: Learning Models for Prediction and Evolution Monitoring Data intensive systems flourish in the last decades with an ever increasing rate of data production. A characteristic such case is the Web graph. Given the dynamism of the Web we aim to study the Web graph in terms of learning models and monitoring its evolution. The main problems we study are: \u00e2\u20ac\u00a2 identification of trends and patterns in the web graph, using the spectral properties of the evolving web adjacency matrix. \u00e2\u20ac\u00a2 monitoring of web pages\u00e2\u20ac\u2122 ranking over time, and prediction of pages web ranking. \u00e2\u20ac\u00a2 learn models for the evolving web graph with statistical learning techniques. The results of the proposed research will be a framework of approaches and algorithms that will enable effective and efficient: - Query based top-k list predictions (future and historical ones) - Prediction based crawling: based on our ranking predictive modeling, crawling resources can be optimized maintaining at the same time a satisfactory top-k quality All the above are profoundly beneficial for resource management in the context of large scale Web search, and the added value of the above will be the potential use of these techniques by the Web search industry.\n\n",
                "DataExportTag": "COR13744",
                "QuestionID": "QID162",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Web Graph: Learning Models for Prediction and Evolution Monitoring Data intensive systems flouris...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\""
                    },
                    "2": {
                        "Display": "computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "3": {
                        "Display": "\"Nuclear Transport and Digital Twin Experimentation\""
                    },
                    "4": {
                        "Display": "nuclear, transport, interoperable, enabler, experimentation, agile, certification, toolbox, factory, testbed, bim, broad, federation, vertical, digital_twin"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID223",
            "SecondaryAttribute": "Web Graph: Learning Models for Prediction and Evolution Monitoring Data intensive systems flouris...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Web Graph: Learning Models for Prediction and Evolution Monitoring Data intensive systems flourish in the last decades with an ever increasing rate of data production. A characteristic such case is the Web graph. Given the dynamism of the Web we aim to study the Web graph in terms of learning models and monitoring its evolution. The main problems we study are: \u00e2\u20ac\u00a2 identification of trends and patterns in the web graph, using the spectral properties of the evolving web adjacency matrix. \u00e2\u20ac\u00a2 monitoring of web pages\u00e2\u20ac\u2122 ranking over time, and prediction of pages web ranking. \u00e2\u20ac\u00a2 learn models for the evolving web graph with statistical learning techniques. The results of the proposed research will be a framework of approaches and algorithms that will enable effective and efficient: - Query based top-k list predictions (future and historical ones) - Prediction based crawling: based on our ranking predictive modeling, crawling resources can be optimized maintaining at the same time a satisfactory top-k quality All the above are profoundly beneficial for resource management in the context of large scale Web search, and the added value of the above will be the potential use of these techniques by the Web search industry.\n\n",
                "DataExportTag": "COR13744",
                "QuestionID": "QID223",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "Web Graph: Learning Models for Prediction and Evolution Monitoring Data intensive systems flouris...",
                "Choices": {
                    "1": {
                        "Display": "\"High Performance Computing and Cryptography\": computation, hpc, machine_learning, heterogeneous, code, cryptography, hardware, programming, exascale, cps, storage, embed, milliliter, verification, processor"
                    },
                    "2": {
                        "Display": "\"Nuclear Transport and Digital Twin Experimentation\": nuclear, transport, interoperable, enabler, experimentation, agile, certification, toolbox, factory, testbed, bim, broad, federation, vertical, digital_twin"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "NextChoiceId": 3,
                "NextAnswerId": 1
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID185",
            "SecondaryAttribute": "With regard to ctl evasion, tumour losses of mhci have been thoroughly studied (our group has mor...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "With regard to ctl evasion, tumour losses of mhci have been thoroughly studied (our group has more than 200 papers on file) and have, in most instances (although not invariably), been associated with poor outcome (reviewed in Garrido et al. 2). Interestingly, the principle of mhc-i loss also applies to the members of the so-called antigen-processing machinery, such as the transporter associated with antigen processing (tap), the endoplasmic reticulum aminopeptidase associated with antigen processing (eraap in mice and erap1 and erap2 in humans), and tapasin. These are in charge of, respectively, translocation (into the endoplasmic reticulum), final trimming, and editing of peptide antigens (Figure 1) before loading onto mhc-i. After our initial observation of linked expression patterns between mhc-i and members of the antigen-processing machinery 3, coordinated downregulation of some of these molecules was shown to correlate with poor prognosis 4.\n\n",
                "DataExportTag": "25",
                "QuestionID": "QID185",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "With regard to ctl evasion, tumour losses of mhci have been thoroughly studied (our group has mor...",
                "Choices": {
                    "1": {
                        "Display": "Immunotherapeutic approaches, including the massive administration of dominant tumour antigens in peptide-based T-cell therapy (mostly pursued in melanoma and incorrectly called \u201cvaccination\u201d), impose an even greater selective pressure, possibly leading to an increased advantage for tumour cell variants lacking the antigen-presenting mhc-i molecule or the protein antigen that contains the immunogenic peptide epitope (or both) 2,5. Particularly when irreversible, mhc-i loss in cancer patients has been claimed to negatively affect prognosis 2."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "Assuming that spontaneous and immunotherapy-induced mhc-i losses are drivers and not passengers of tumour progression, it remains to be explained why they do not incite recognition and tumour lysis by nk cells (Figure 1). Porgador et al. 6 described a very high prevalence (5 in 13 cases) of irreversible complete mhc-i losses in patients treated with various immunotherapeutic regimens. Despite the cells being very sensitive targets of autologous nk cells in vitro, clinical outcome was reported to be poor. Likewise, Pende et al. 7 observed that long-term tumour cell lines, even when established from patients not undergoing immunotherapy, do not express enough mhc-i to protect themselves from nk recognition. Why, then, can these tumours evade in the face of a brisk in vitro nk response?"
                    },
                    "2": {
                        "Display": "A possible interpretation is that simple cytotoxicity readouts do not reflect the lytic behaviour of immune effectors in vivo. After all, if antitumour T-cell counts and activity in vitro are not entirely predictive of clinical responsiveness to vaccination 8, why should nk cell responses in vivo be faithfully recapitulated in an in vitro assay? Alternatively, it might be hypothesized that nk cells have nothing to do with tumour immune surveillance, at least in humans. Indeed, lymphoid cell infiltrates contain many more T cells than nk cells, and only T cells are positively associated with a favorable outcome 9. Whatever the interpretation, a drastic objection is that certain subsets of nk cells may be important at early stages, but may be long gone by the time the tumour becomes clinically evident and hits the pathology slide."
                    },
                    "3": {
                        "Display": "If nk cells are indeed important, tumours low in mhc-i may elude them either by exploiting certain \u201cgaps\u201d in the inhibitory nk receptor repertoire 10 or, analogous with viral immuno-evasion strategies 11, by \u201creplacing\u201d mhc-i self-inhibitory signals with other inhibitory ligands such as the non-classical mhc-i human leukocyte antigens G (hla-g) and E (hla-e) 12\u201314. However, at least hla-e behaves not only as an inhibitory, but also as a triggering ligand 15. In addition, hla-e expression may not be restricted to tumours with mhc-i loss as required by the \u201creplacement\u201d model 16,17. Finally, and quite surprisingly, hla-e is associated with a good prognosis, at least in certain tumour histotypes 18\u201320. It will be of considerable interest to find out if and how tumours use nk-decoy tactics."
                    },
                    "4": {
                        "Display": "Although there are simpler ways to explain mhc-i\u2013driven tumour evasion from both ctl and nk cells, those explanations have received considerably less attention than the foregoing mechanisms. A straightforward assumption is that, besides mhc-i losses adopted by ctl-sensitive tumours, there are mechanisms of mhc-i gains, and those mechanisms are preferred by another set of tumours that are particularly sensitive to nk lysis. It might be envisaged that the opposing influences of ctl and nk cells prevent any major change in mhc-i expression, making less-aggressive tumours resemble their normal counterparts. By contrast, aggressive tumours may escape by adopting whichever immuno-evasion strategy is the most advantageous in the context of the immune response mounted by an individual host. Indeed, a Gaussian distribution of mhc-i expression around \u201cnormal\u201d values was observed in vitro and in vivo in a variety of solid tumours 3,21, mhc-i losses and mhc-i gains both being associated with poor prognosis in colorectal carcinoma 22."
                    },
                    "5": {
                        "Display": "Given the opposing effects of mhc-i molecules on ctl and nk cells (Figure 1), an mhc-i phenotype efficiently triggering both effectors is a contradiction in terms. For instance, in the classical paper that pioneered the \u201cmissing self\u201d hypothesis, a tap-defective mutant of the murine lymphoma RMA, called RMAS, was shown to be rejected essentially by nk cells 1."
                    },
                    "6": {
                        "Display": "Recently, rna interference of the same RMA cells for eraap (just downstream of tap in the antigen-processing machinery pathway 23) similarly resulted in tumour rejection 24, but in addition to nk cells, T cells (CD4 and CD8 alike) were also involved. It appears that poorly folded mhc-i molecules synthesized in the absence of eraap can be \u201cseen\u201d as abnormal by several immune effectors. Quite interestingly, only a few human tumours express low erap1 and erap2 levels 25,26, suggesting that the spontaneous occurrence of this altered, two-edge phenotype is counterselected in vivo."
                    },
                    "7": {
                        "Display": "In conclusion, it is fairly clear what tumours look like when they are \u201cout of the hands\u201d of the immune system, but we know much less of \u201creal\u201d tumours under immunologic scrutiny and during immunoediting in vivo. If ctl and nk cells must both be \u201ctuned in\u201d to reject tumours, many more immunoevasive mhc-i (and non-mhc-i 27) phenotypes remain to be discovered."
                    },
                    "8": {
                        "Display": "\"Medical Imaging and Therapeutic Techniques\""
                    },
                    "9": {
                        "Display": "imaging, temperature, ultrasound, optical, oct, tissue, hifu, vivo, heating, optical_coherence, probe, hyperthermia, phantom, real_time, laser"
                    },
                    "10": {
                        "Display": "\"Medical Imaging and Cancer Treatment\""
                    },
                    "11": {
                        "Display": "imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6",
                    "7",
                    "8",
                    "9",
                    "10",
                    "11"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID246",
            "SecondaryAttribute": "With regard to ctl evasion, tumour losses of mhci have been thoroughly studied (our group has mor...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "With regard to ctl evasion, tumour losses of mhci have been thoroughly studied (our group has more than 200 papers on file) and have, in most instances (although not invariably), been associated with poor outcome (reviewed in Garrido et al. 2). Interestingly, the principle of mhc-i loss also applies to the members of the so-called antigen-processing machinery, such as the transporter associated with antigen processing (tap), the endoplasmic reticulum aminopeptidase associated with antigen processing (eraap in mice and erap1 and erap2 in humans), and tapasin. These are in charge of, respectively, translocation (into the endoplasmic reticulum), final trimming, and editing of peptide antigens (Figure 1) before loading onto mhc-i. After our initial observation of linked expression patterns between mhc-i and members of the antigen-processing machinery 3, coordinated downregulation of some of these molecules was shown to correlate with poor prognosis 4.\n\n",
                "DataExportTag": "25",
                "QuestionID": "QID246",
                "QuestionType": "Matrix",
                "Selector": "Likert",
                "SubSelector": "SingleAnswer",
                "QuestionDescription": "With regard to ctl evasion, tumour losses of mhci have been thoroughly studied (our group has mor...",
                "Choices": {
                    "1": {
                        "Display": "Immunotherapeutic approaches, including the massive administration of dominant tumour antigens in peptide-based T-cell therapy (mostly pursued in melanoma and incorrectly called \u201cvaccination\u201d), impose an even greater selective pressure, possibly leading to an increased advantage for tumour cell variants lacking the antigen-presenting mhc-i molecule or the protein antigen that contains the immunogenic peptide epitope (or both) 2,5. Particularly when irreversible, mhc-i loss in cancer patients has been claimed to negatively affect prognosis 2."
                    }
                },
                "ChoiceOrder": [
                    "1"
                ],
                "Randomization": {
                    "Advanced": null,
                    "TotalRandSubset": "",
                    "Type": "All"
                },
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": [],
                "Answers": {
                    "1": {
                        "Display": "Assuming that spontaneous and immunotherapy-induced mhc-i losses are drivers and not passengers of tumour progression, it remains to be explained why they do not incite recognition and tumour lysis by nk cells (Figure 1). Porgador et al. 6 described a very high prevalence (5 in 13 cases) of irreversible complete mhc-i losses in patients treated with various immunotherapeutic regimens. Despite the cells being very sensitive targets of autologous nk cells in vitro, clinical outcome was reported to be poor. Likewise, Pende et al. 7 observed that long-term tumour cell lines, even when established from patients not undergoing immunotherapy, do not express enough mhc-i to protect themselves from nk recognition. Why, then, can these tumours evade in the face of a brisk in vitro nk response?"
                    },
                    "2": {
                        "Display": "A possible interpretation is that simple cytotoxicity readouts do not reflect the lytic behaviour of immune effectors in vivo. After all, if antitumour T-cell counts and activity in vitro are not entirely predictive of clinical responsiveness to vaccination 8, why should nk cell responses in vivo be faithfully recapitulated in an in vitro assay? Alternatively, it might be hypothesized that nk cells have nothing to do with tumour immune surveillance, at least in humans. Indeed, lymphoid cell infiltrates contain many more T cells than nk cells, and only T cells are positively associated with a favorable outcome 9. Whatever the interpretation, a drastic objection is that certain subsets of nk cells may be important at early stages, but may be long gone by the time the tumour becomes clinically evident and hits the pathology slide."
                    },
                    "3": {
                        "Display": "If nk cells are indeed important, tumours low in mhc-i may elude them either by exploiting certain \u201cgaps\u201d in the inhibitory nk receptor repertoire 10 or, analogous with viral immuno-evasion strategies 11, by \u201creplacing\u201d mhc-i self-inhibitory signals with other inhibitory ligands such as the non-classical mhc-i human leukocyte antigens G (hla-g) and E (hla-e) 12\u201314. However, at least hla-e behaves not only as an inhibitory, but also as a triggering ligand 15. In addition, hla-e expression may not be restricted to tumours with mhc-i loss as required by the \u201creplacement\u201d model 16,17. Finally, and quite surprisingly, hla-e is associated with a good prognosis, at least in certain tumour histotypes 18\u201320. It will be of considerable interest to find out if and how tumours use nk-decoy tactics."
                    },
                    "4": {
                        "Display": "Although there are simpler ways to explain mhc-i\u2013driven tumour evasion from both ctl and nk cells, those explanations have received considerably less attention than the foregoing mechanisms. A straightforward assumption is that, besides mhc-i losses adopted by ctl-sensitive tumours, there are mechanisms of mhc-i gains, and those mechanisms are preferred by another set of tumours that are particularly sensitive to nk lysis. It might be envisaged that the opposing influences of ctl and nk cells prevent any major change in mhc-i expression, making less-aggressive tumours resemble their normal counterparts. By contrast, aggressive tumours may escape by adopting whichever immuno-evasion strategy is the most advantageous in the context of the immune response mounted by an individual host. Indeed, a Gaussian distribution of mhc-i expression around \u201cnormal\u201d values was observed in vitro and in vivo in a variety of solid tumours 3,21, mhc-i losses and mhc-i gains both being associated with poor prognosis in colorectal carcinoma 22."
                    },
                    "5": {
                        "Display": "Given the opposing effects of mhc-i molecules on ctl and nk cells (Figure 1), an mhc-i phenotype efficiently triggering both effectors is a contradiction in terms. For instance, in the classical paper that pioneered the \u201cmissing self\u201d hypothesis, a tap-defective mutant of the murine lymphoma RMA, called RMAS, was shown to be rejected essentially by nk cells 1."
                    },
                    "6": {
                        "Display": "Recently, rna interference of the same RMA cells for eraap (just downstream of tap in the antigen-processing machinery pathway 23) similarly resulted in tumour rejection 24, but in addition to nk cells, T cells (CD4 and CD8 alike) were also involved. It appears that poorly folded mhc-i molecules synthesized in the absence of eraap can be \u201cseen\u201d as abnormal by several immune effectors. Quite interestingly, only a few human tumours express low erap1 and erap2 levels 25,26, suggesting that the spontaneous occurrence of this altered, two-edge phenotype is counterselected in vivo."
                    },
                    "7": {
                        "Display": "In conclusion, it is fairly clear what tumours look like when they are \u201cout of the hands\u201d of the immune system, but we know much less of \u201creal\u201d tumours under immunologic scrutiny and during immunoediting in vivo. If ctl and nk cells must both be \u201ctuned in\u201d to reject tumours, many more immunoevasive mhc-i (and non-mhc-i 27) phenotypes remain to be discovered."
                    },
                    "8": {
                        "Display": "\"Medical Imaging and Therapeutic Techniques\""
                    },
                    "9": {
                        "Display": "imaging, temperature, ultrasound, optical, oct, tissue, hifu, vivo, heating, optical_coherence, probe, hyperthermia, phantom, real_time, laser"
                    },
                    "10": {
                        "Display": "\"Medical Imaging and Cancer Treatment\""
                    },
                    "11": {
                        "Display": "imaging, temperature, optical, breast, vivo, tissue, ultrasound, probe, fluorescence, phantom, oct, heating, hyperthermia, mouse, ablation"
                    }
                },
                "AnswerOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6",
                    "7",
                    "8",
                    "9",
                    "10",
                    "11"
                ]
            }
        },
        {
            "SurveyID": "SV_1BSk0CWt552laHI",
            "Element": "SQ",
            "PrimaryAttribute": "QID385",
            "SecondaryAttribute": "[Renal graft outcome in patients with associated liver transplant]. INTRODUCTION Nearly 50% of li...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "[Renal graft outcome in patients with associated liver transplant]. INTRODUCTION\nNearly 50% of liver transplant recipients have some degree of renal failure; patients in haemodialysis treatment have a higher risk of suffering hepatic diseases related to viral infections or concomitant pathologies. Improvement in surgical and organ preservation techniques and immunosuppressive therapy has permitted multiorganic transplants in patients needing both liver and kidney organs.\n\n",
                "DataExportTag": "CAN1254539",
                "QuestionID": "QID385",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "QuestionDescription": "[Renal graft outcome in patients with associated liver transplant]. INTRODUCTION Nearly 50% of li...",
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "Language": []
            }
        }
    ]
}