{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2948ba6b-068d-40be-9a81-a11c51568a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "import pathlib\n",
    "import pickle\n",
    "import glob\n",
    "import tomotopy as tp\n",
    "from scipy.sparse import issparse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e044121-6e1a-487c-bfe1-5781d5a414b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = \"/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/data/models_v3\"\n",
    "datasets = [\"cordis\", \"cancer\", \"ai\"]\n",
    "n_words = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a9a90-7d14-415c-9b80-c07c90bc25e4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3d4a66e-0d14-4883-92ee-7a9e0e296947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def compute_CLNPMI(parent_diff_words, child_diff_words, all_bow, vocab):\n",
    "    npmi_list = list()\n",
    "\n",
    "    for p_w in parent_diff_words:\n",
    "        flag_n = all_bow[:, vocab[p_w]] > 0\n",
    "        p_n = np.sum(flag_n) / len(all_bow)\n",
    "\n",
    "        for c_w in child_diff_words:\n",
    "            flag_l = all_bow[:, vocab[c_w]] > 0\n",
    "            p_l = np.sum(flag_l)\n",
    "            p_nl = np.sum(flag_n * flag_l)\n",
    "\n",
    "            if p_nl == len(all_bow):\n",
    "                npmi_score = 1\n",
    "            else:\n",
    "                p_l = p_l / len(all_bow)\n",
    "                p_nl = p_nl / len(all_bow)\n",
    "                p_nl += 1e-10\n",
    "                npmi_score = np.log(p_nl / (p_l * p_n)) / -np.log(p_nl)\n",
    "\n",
    "            npmi_list.append(npmi_score)\n",
    "\n",
    "    return npmi_list\n",
    "\"\"\"\n",
    "def compute_CLNPMI(parent_diff_words, child_diff_words, all_bow, vocab):\n",
    "    npmi_list = list()\n",
    "\n",
    "    # Longitud de la matriz\n",
    "    total_docs = all_bow.shape[0]\n",
    "\n",
    "    for p_w in parent_diff_words:\n",
    "        # Índice de la palabra \"parent\" en el vocabulario\n",
    "        p_idx = vocab[p_w]\n",
    "\n",
    "        # Filtramos documentos donde aparece la palabra p_w\n",
    "        if issparse(all_bow):\n",
    "            flag_n = all_bow[:, p_idx].toarray().flatten() > 0\n",
    "        else:\n",
    "            flag_n = all_bow[:, p_idx] > 0\n",
    "        p_n = np.sum(flag_n) / total_docs\n",
    "\n",
    "        for c_w in child_diff_words:\n",
    "            # Índice de la palabra \"child\" en el vocabulario\n",
    "            try:\n",
    "                c_idx = vocab[c_w]\n",
    "                # Filtramos documentos donde aparece la palabra c_w\n",
    "                if issparse(all_bow):\n",
    "                    flag_l = all_bow[:, c_idx].toarray().flatten() > 0\n",
    "                else:\n",
    "                    flag_l = all_bow[:, c_idx] > 0\n",
    "                p_l = np.sum(flag_l)\n",
    "    \n",
    "                # Coincidencia entre documentos que contienen p_w y c_w\n",
    "                p_nl = np.sum(flag_n & flag_l)\n",
    "                if p_l == 0:\n",
    "                    import pdb; pdb.set_trace()\n",
    "    \n",
    "                if p_nl == total_docs:\n",
    "                    npmi_score = 1\n",
    "                else:\n",
    "                    p_l = p_l / total_docs\n",
    "                    p_nl = p_nl / total_docs\n",
    "                    p_nl += 1e-10\n",
    "                    npmi_score = np.log(p_nl / (p_l * p_n)) / -np.log(p_nl)\n",
    "\n",
    "                # if p_l = 0 implica que la palabra no aparece en ningún documento\n",
    "                \n",
    "                npmi_list.append(npmi_score)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"word {c_w} not in vocab\")\n",
    "\n",
    "    return npmi_list\n",
    "\n",
    "\n",
    "def compute_TD(texts):\n",
    "    K = len(texts)\n",
    "    T = len(texts[0].split())\n",
    "    vectorizer = CountVectorizer()\n",
    "    counter = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "    TF = counter.sum(axis=0)\n",
    "    TD = (TF == 1).sum() / (K * T)\n",
    "\n",
    "    return TD\n",
    "\n",
    "def get_CLNPMI(PC_pair_groups, all_bow, vocab):\n",
    "    CNPMI_list = list()\n",
    "    for group in tqdm(PC_pair_groups):\n",
    "        layer_CNPMI = []\n",
    "    \n",
    "       \n",
    "        parent_words = set(group[0])\n",
    "        child_words = set(group[1])\n",
    "\n",
    "        inter = parent_words.intersection(child_words)\n",
    "        parent_diff_words = list(parent_words.difference(inter))\n",
    "        child_diff_words = list(child_words.difference(inter))\n",
    "        \n",
    "        npmi_list = compute_CLNPMI(parent_diff_words, child_diff_words, all_bow, vocab)\n",
    "\n",
    "        # Handle repetitive word pair NPMI assignments if necessary\n",
    "        num_repetition = (\n",
    "            len(parent_words) - len(parent_diff_words)\n",
    "        ) * (len(child_words) - len(child_diff_words))\n",
    "        npmi_list.extend([-1] * num_repetition)\n",
    "\n",
    "        layer_CNPMI.extend(npmi_list)\n",
    "    \n",
    "        CNPMI_list.append(np.mean(layer_CNPMI))  # Append the group's result\n",
    "\n",
    "\n",
    "    return CNPMI_list\n",
    "\n",
    "def compute_diff_topic_pair(topic_words_a, topic_words_b):\n",
    "    word_counter = Counter()\n",
    "    word_counter.update(topic_words_a)\n",
    "    word_counter.update(topic_words_b)\n",
    "    diff = (np.asarray(list(word_counter.values())) == 1).sum() / (len(topic_words_a) + len(topic_words_b))\n",
    "    return diff\n",
    "\n",
    "\n",
    "def get_topics_difference(topic_pair_groups):\n",
    "    diff_list = list()\n",
    "    for group in tqdm(topic_pair_groups):\n",
    "        layer_diff = list()\n",
    "        diff = compute_diff_topic_pair(group[0], group[1])\n",
    "        layer_diff.append(diff)\n",
    "        diff_list.append(np.mean(layer_diff))\n",
    "\n",
    "    return diff_list\n",
    "\n",
    "def get_Sibling_TD(sibling_groups):\n",
    "    sibling_TD = list()\n",
    "    for group in sibling_groups:\n",
    "        layer_sibling_TD = list()\n",
    "        for sibling_topics in group:\n",
    "            TD = compute_TD(sibling_topics)\n",
    "            layer_sibling_TD.append(TD)\n",
    "        sibling_TD.append(np.mean(layer_sibling_TD))\n",
    "    return sibling_TD\n",
    "\n",
    "#### HPAM\n",
    "def assign_subtopics_to_supertopics(model):\n",
    "    \"\"\"\n",
    "    Asigna cada subtópico a un supertópico basado en la máxima probabilidad condicional.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo HPAM entrenado.\n",
    "\n",
    "    Returns:\n",
    "        hierarchy: Diccionario donde las claves son supertópicos (nivel 1) y los valores son listas\n",
    "                   de subtópicos (nivel 2) que cuelgan de ellos.\n",
    "    \"\"\"\n",
    "    hierarchy = {k1: [] for k1 in range(model.k1)}  # Inicializa el diccionario para supertópicos\n",
    "\n",
    "    for k2 in range(model.k2):  # Iterar sobre los subtópicos\n",
    "        max_prob = 0\n",
    "        assigned_super_topic = None\n",
    "        \n",
    "        for k1 in range(model.k1):  # Iterar sobre los supertópicos\n",
    "            # Obtener la probabilidad del subtópico dado el supertópico\n",
    "            prob = model.get_sub_topic_dist(k1)[k2]\n",
    "            if prob > max_prob:  # Asignar al supertópico con mayor probabilidad\n",
    "                max_prob = prob\n",
    "                assigned_super_topic = k1\n",
    "        \n",
    "        if assigned_super_topic is not None:\n",
    "            hierarchy[assigned_super_topic].append(k2)\n",
    "    \n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6352303-bf3d-44b1-866e-238f649f610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mallet_corpus_to_df(corpusFile: pathlib.Path):\n",
    "    \"\"\"Converts a Mallet corpus file (i.e., file required for the Mallet import command) to a pandas DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpusFile: pathlib.Path\n",
    "        Path to the Mallet corpus file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :   pandas.DataFrame\n",
    "        DataFrame with the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "        corpusFile, encoding=\"utf-8\").readlines()]\n",
    "    indexes = [line.rsplit(' 0 ')[0].strip() for line in open(\n",
    "        corpusFile, encoding=\"utf-8\").readlines()]\n",
    "    corpus_dict = {\n",
    "        'id': indexes,\n",
    "        'text': corpus\n",
    "    }\n",
    "    return pd.DataFrame(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd0e2f5-a33d-4058-93f0-4404b76fdc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_models(path_models:pathlib.Path):\n",
    "    dfs = []\n",
    "    for entry in path_models.iterdir():\n",
    "        # check if it is a root model\n",
    "        if entry.name.startswith(\"htm\"):#root\n",
    "            \n",
    "            # Path to the root model\n",
    "            path = entry\n",
    "\n",
    "            # Thr and exp_tpc do not apply for the root model\n",
    "            thr = -1\n",
    "            exp_tpc = -1\n",
    "\n",
    "            # Experiment iteration\n",
    "            iter_ = -1\n",
    "            \n",
    "            # tr_topics\n",
    "            tr_topics = int(path.name.split(\"_\")[1])\n",
    "\n",
    "            # Size of the topics\n",
    "            alphas = np.load(path.joinpath('TMmodel/alphas.npy')).tolist()\n",
    "            alphas = list(map(lambda x: x * 100, alphas))\n",
    "\n",
    "            # Coherences (CV and NPMI)\n",
    "            cohrs_cv = np.load(path.joinpath('TMmodel/c_v_ref_coherence.npy')).tolist()\n",
    "            cohrs_npmi = np.load(path.joinpath('TMmodel/c_npmi_ref_coherence.npy')).tolist()\n",
    "\n",
    "            # Topics' entropies\n",
    "            entropies = np.load(path.joinpath('TMmodel/topic_entropy.npy')).tolist()\n",
    "\n",
    "            # TD\n",
    "            td = np.load(path.joinpath('TMmodel/td.npy'))\n",
    "\n",
    "            # IRBO \n",
    "            rbo = np.load(path.joinpath('TMmodel/rbo.npy'))\n",
    "            \n",
    "            # tpc_Desc\n",
    "            with path.joinpath('TMmodel/tpc_descriptions.txt').open('r', encoding='utf8') as fin:\n",
    "                tpc_descriptions = [el.strip() for el in fin.readlines()]\n",
    "\n",
    "            # Ids of the topics\n",
    "            tpc_ids = np.arange(0,len(alphas),1)\n",
    "\n",
    "            # Corpus size\n",
    "            if path.joinpath('corpus.txt').is_file():\n",
    "                corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                    path.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                size = len(corpus)\n",
    "            elif path.joinpath('corpus.parquet').is_dir():\n",
    "                dfc = pd.read_parquet(path.joinpath('corpus.parquet'))\n",
    "                size = len(dfc)\n",
    "\n",
    "            # Create dataframe for the root model\n",
    "            root_tpc_df = pd.DataFrame(\n",
    "                {'iter': [iter_] * len(alphas),\n",
    "                 'path': [path] * len(alphas),\n",
    "                 'cohrs_cv': cohrs_cv,\n",
    "                 'cohrs_npmi': cohrs_npmi,\n",
    "                 'entropies': entropies,\n",
    "                 'td': [td] * len(alphas),\n",
    "                 'rbo': [rbo] * len(alphas),\n",
    "                 'alphas': alphas,\n",
    "                 'tpc_ids': tpc_ids,\n",
    "                 'thr': [thr] * len(alphas),\n",
    "                 'exp_tpc': [exp_tpc] * len(alphas),\n",
    "                 'tr_tpcs': [tr_topics] * len(alphas),\n",
    "                 'tpc_descriptions': tpc_descriptions,\n",
    "                 'father': [iter_] * len(alphas),\n",
    "                'father_tpc_descriptions':[iter_] * len(alphas),\n",
    "                })\n",
    "\n",
    "            # Get root size\n",
    "            if root_tpc_df.iloc[0].path.joinpath('corpus.txt').is_file():\n",
    "                corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                            root_tpc_df.iloc[0].path.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                root_size = len(corpus)\n",
    "            elif root_tpc_df.iloc[0].path.joinpath('corpus.parquet').is_dir() or root_tpc_df.iloc[0].path.joinpath('corpus.parquet').is_file():\n",
    "                dfc = pd.read_parquet(root_tpc_df.iloc[0].path.joinpath('corpus.parquet'))\n",
    "                root_size = len(dfc) \n",
    "\n",
    "            root_tpc_df[\"root_size\"] = [root_size] * len(alphas)\n",
    "\n",
    "            # Append to the list of dataframes to concatenate them\n",
    "            dfs.append(root_tpc_df)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.sort_values(by=['iter'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_submodules(df:pd.DataFrame):\n",
    "    # Iter over each root model (according to its corresponding iteration, iter)\n",
    "    concats = [df]\n",
    "    not_finished = []\n",
    "    paths_root = df.path.unique()\n",
    "    print(paths_root)\n",
    "    for path_root in paths_root:\n",
    "        print(path_root)\n",
    "        root_size = df[df.path == path_root].iloc[0].root_size\n",
    "        \n",
    "        for entry in path_root.iterdir():\n",
    "            if entry.joinpath('TMmodel/topic_coherence.npy').is_file() and not entry.as_posix().endswith(\"old\"):\n",
    "                try:\n",
    "                    if \"submodel_htm-ws\" in entry.as_posix():\n",
    "                        thr_ = 0\n",
    "                        size = 0\n",
    "                    else:\n",
    "                        thr_ = float(entry.as_posix().split(\"thr_\")[1].split(\"_\")[0])\n",
    "                        \n",
    "                        if entry.joinpath('corpus.txt').is_file():\n",
    "                            corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                                        entry.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                            size = len(corpus)\n",
    "                        elif entry.joinpath('corpus.parquet').is_dir():\n",
    "                            dfc = pd.read_parquet(entry.joinpath('corpus.parquet'))\n",
    "                            size = len(dfc)\n",
    "                        size = size * 100 / root_size\n",
    "                        \n",
    "                    # get iter\n",
    "                    iter_ = int(entry.name.split(\"_iter_\")[1].split(\"_\")[0])\n",
    "                    \n",
    "                    # get topic from which the submodel is generated\n",
    "                    exp_tpc = int(entry.as_posix().split(\"from_topic_\")[1].split(\"_\")[0])   \n",
    "                    aux = df[(df.path == path_root)]\n",
    "                    father_tpc_desc = aux[aux.tpc_ids == exp_tpc].tpc_descriptions.values.tolist()[0]\n",
    "    \n",
    "                    # Size of the topics\n",
    "                    alphas = np.load(entry.joinpath('TMmodel/alphas.npy')).tolist()\n",
    "                    alphas = list(map(lambda x: x * 100, alphas))\n",
    "    \n",
    "                    # Alphas submodel is the mean of the cohr of its topics\n",
    "                    alpha = np.mean(alphas)\n",
    "    \n",
    "                    # Coheerences (CV and NPMI)\n",
    "                    cohrs_cv = np.load(entry.joinpath('TMmodel/c_v_ref_coherence.npy')).tolist()\n",
    "                    cohrs_npmi = np.load(entry.joinpath('TMmodel/c_npmi_ref_coherence.npy'), allow_pickle=True).tolist()\n",
    "                    if cohrs_npmi is None:\n",
    "                        cohrs_npmi = [0]*len(cohrs_cv)\n",
    "    \n",
    "                    # cohr submodel is the mean of the cohr of its topics\n",
    "                    cohr_cv = np.mean(cohrs_cv)\n",
    "                    cohr_npmi = np.mean(cohrs_npmi)\n",
    "    \n",
    "                    # TD\n",
    "                    td = np.load(entry.joinpath('TMmodel/td.npy'))\n",
    "    \n",
    "                    # IRBO \n",
    "                    rbo = np.load(entry.joinpath('TMmodel/rbo.npy'))\n",
    "    \n",
    "                    # Topics' entropies\n",
    "                    entropy = np.mean(np.load(entry.joinpath('TMmodel/topic_entropy.npy')).tolist())\n",
    "    \n",
    "                    tr_tpcs = int(entry.as_posix().split(\"train_with_\")[1].split(\"_\")[0])\n",
    "    \n",
    "                    # tpc_Desc\n",
    "                    with entry.joinpath('TMmodel/tpc_descriptions.txt').open('r', encoding='utf8') as fin:\n",
    "                        tpc_descriptions = [el.strip() for el in fin.readlines()]\n",
    "                    \n",
    "                    root_tpc_df = pd.DataFrame(\n",
    "                    {'iter': [iter_],\n",
    "                     'path': [entry],\n",
    "                     'cohrs_cv': [cohr_cv],\n",
    "                     'cohrs_npmi': [cohr_npmi],\n",
    "                     'entropies': [entropy],\n",
    "                     'alphas': [alpha],\n",
    "                     'td': [td],\n",
    "                     'rbo': [rbo],\n",
    "                     'tpc_ids': [exp_tpc],\n",
    "                     'thr': [thr_],\n",
    "                     'exp_tpc': [exp_tpc],\n",
    "                     'size': [size],\n",
    "                     'tr_tpcs': [tr_tpcs],\n",
    "                     'tpc_descriptions': [tpc_descriptions],\n",
    "                     'father': [path_root.as_posix()],\n",
    "                     'father_tpc_descriptions':[father_tpc_desc] \n",
    "                    })\n",
    "                                    \n",
    "                    concats.append(root_tpc_df)\n",
    "                except Exception as e:\n",
    "                    not_finished.append(entry)\n",
    "    \n",
    "            else:\n",
    "                not_finished.append(entry)\n",
    "    df = pd.concat(concats)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_df_all_models(path_models:pathlib.Path):\n",
    "    df_root = get_root_models(path_models)\n",
    "    df_all = get_submodules(df_root)\n",
    "    return df_root, df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c67e8e9-acb9-49c2-bbd6-889c7fa66f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = mallet_corpus_to_df(\"/export/usuarios_ml4ds/lbartolome/Datasets/Cancer/htm_variability_models/htm_20_tpcs_20230927/submodel_htm-ws_from_topic_2_train_with_10_iter_0_20230927/corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd1fad3f-a324-401f-b9f6-c6b1c2887078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>symptom symptom rapid obstructive symptom persist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>nuclear_magnetic_resonance cerebral nuclear_ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>upper_extremity swelling adolescent emergency ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>anterior undertake asian origin region male ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>choroidal_melanoma finding collaborative_ocula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290153</th>\n",
       "      <td>1505161</td>\n",
       "      <td>necrosis cardiovascular destruction diagnosis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290154</th>\n",
       "      <td>1505162</td>\n",
       "      <td>amyloidosis face cutaneous localize amyloidosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290155</th>\n",
       "      <td>1505174</td>\n",
       "      <td>frequency pediatric affect child supratentoria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290156</th>\n",
       "      <td>1505176</td>\n",
       "      <td>biopsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290157</th>\n",
       "      <td>1505177</td>\n",
       "      <td>progressive pyoderma_gangrenosum leg unrespons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290158 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0             1  symptom symptom rapid obstructive symptom persist\n",
       "1             6  nuclear_magnetic_resonance cerebral nuclear_ma...\n",
       "2            13  upper_extremity swelling adolescent emergency ...\n",
       "3            17  anterior undertake asian origin region male ca...\n",
       "4            19  choroidal_melanoma finding collaborative_ocula...\n",
       "...         ...                                                ...\n",
       "290153  1505161  necrosis cardiovascular destruction diagnosis ...\n",
       "290154  1505162  amyloidosis face cutaneous localize amyloidosi...\n",
       "290155  1505174  frequency pediatric affect child supratentoria...\n",
       "290156  1505176                                             biopsy\n",
       "290157  1505177  progressive pyoderma_gangrenosum leg unrespons...\n",
       "\n",
       "[290158 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69ef8a0c-288d-42e1-95c2-806a3d6a61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = mallet_corpus_to_df(\"/export/usuarios_ml4ds/lbartolome/Datasets/Cancer/htm_variability_models/htm_20_tpcs_20230927/corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9423853d-7841-494f-805e-eb9950e601b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28995998</td>\n",
       "      <td>trend size diagnose choroidal_melanoma finding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>1425898</td>\n",
       "      <td>association dual modality positron_emission_to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>3333418</td>\n",
       "      <td>upregulation müller_glial proliferative_vitreo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>205664745</td>\n",
       "      <td>necrotic choroidal_melanoma effusion choroidal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9703</th>\n",
       "      <td>1877393</td>\n",
       "      <td>metastasis ocular melanoma left_ventricle sync...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498909</th>\n",
       "      <td>210867516</td>\n",
       "      <td>choroidal_melanoma modulate long_noncode rnas_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500723</th>\n",
       "      <td>26462877</td>\n",
       "      <td>choroidal_melanoma child aware author choroida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501410</th>\n",
       "      <td>38416345</td>\n",
       "      <td>diagnosis choroidal_melanoma abstract utility ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502379</th>\n",
       "      <td>12796779</td>\n",
       "      <td>new_zealand experience brachytherapy choroidal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505121</th>\n",
       "      <td>31661411</td>\n",
       "      <td>experience stereotactic_radiosurgery linear_ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>539 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text\n",
       "19        28995998  trend size diagnose choroidal_melanoma finding...\n",
       "3470       1425898  association dual modality positron_emission_to...\n",
       "3518       3333418  upregulation müller_glial proliferative_vitreo...\n",
       "7994     205664745  necrotic choroidal_melanoma effusion choroidal...\n",
       "9703       1877393  metastasis ocular melanoma left_ventricle sync...\n",
       "...            ...                                                ...\n",
       "1498909  210867516  choroidal_melanoma modulate long_noncode rnas_...\n",
       "1500723   26462877  choroidal_melanoma child aware author choroida...\n",
       "1501410   38416345  diagnosis choroidal_melanoma abstract utility ...\n",
       "1502379   12796779  new_zealand experience brachytherapy choroidal...\n",
       "1505121   31661411  experience stereotactic_radiosurgery linear_ac...\n",
       "\n",
       "[539 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root[root.text.str.contains(\"choroidal_melanoma\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9d57c-5677-4dbe-a139-71f7f346d9ed",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc804d65-d1b4-4c06-a25c-1f472d9478e3",
   "metadata": {},
   "source": [
    "### Hyperminer and Traco baselines (directly load from TopMost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fe843d8-aacf-4e01-b75b-60e6f9ec270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating topmost.... \n"
     ]
    }
   ],
   "source": [
    "path_models = \"/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/data/models_v3\"\n",
    "datasets = [\"cordis\", \"cancer\", \"ai\"]\n",
    "n_words = 15\n",
    "\n",
    "print(\"--- Calculating topmost.... \")\n",
    "algorithms = [\"hyperminer\", \"traco\"]\n",
    "\n",
    "results_df = []\n",
    "for algo in algorithms:\n",
    "    for dtset in datasets:\n",
    "        search_key = f\"{path_models}/{algo}/{dtset}/{dtset}_trained_model_iter_*.pkl\"\n",
    "        results_files = glob.glob(search_key)\n",
    "        for i, file in enumerate(results_files):\n",
    "            #if i <= 0: #TODO: REMOVE\n",
    "            with open(file, \"rb\") as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "                results = loaded_data[\"hierarchy_quality_results\"]\n",
    "                results[\"dataset\"] = dtset\n",
    "                results[\"algorithm\"] = algo\n",
    "                results[\"iter\"] = i\n",
    "                if isinstance(results, dict):\n",
    "                    results = [results] \n",
    "                results_df.append(pd.DataFrame(results))\n",
    "results_df_topmost_new = pd.concat(results_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5f082d6-bb8f-4348-b619-8e95cdd59a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         PCC       PCD  Sibling_TD      PnCD dataset   algorithm  iter\n",
      "0  -0.021947  0.831111    0.697778  0.978444  cordis  hyperminer     0\n",
      "1  -0.069262  0.781111    0.673889  0.981111  cordis  hyperminer     1\n",
      "2  -0.120262  0.742222    0.700556  0.979333  cordis  hyperminer     2\n",
      "3  -0.079284  0.768333    0.666833  0.993632  cancer  hyperminer     0\n",
      "4  -0.034296  0.819667    0.707500  0.992281  cancer  hyperminer     1\n",
      "5  -0.056633  0.797667    0.730500  0.996018  cancer  hyperminer     2\n",
      "6  -0.231284  0.677333    0.624667  0.986193      ai  hyperminer     0\n",
      "7  -0.231291  0.670667    0.633500  0.984000      ai  hyperminer     1\n",
      "8  -0.175837  0.707000    0.613500  0.988930      ai  hyperminer     2\n",
      "9  -0.316054  0.992222    0.994444  0.999778  cordis       traco     0\n",
      "10 -0.302709  0.994444    0.998889  1.000000  cordis       traco     1\n",
      "11 -0.305776  0.995556    0.996667  1.000000  cordis       traco     2\n",
      "12 -0.267206  0.957667    0.972667  0.999860  cancer       traco     0\n",
      "13 -0.256034  0.990333    0.996667  0.999895  cancer       traco     1\n",
      "14 -0.260944  0.988667    0.990667  0.999965  cancer       traco     2\n",
      "15 -0.226881  0.995333    0.990333  0.999930      ai       traco     0\n",
      "16 -0.256937  0.954333    0.973167  0.999930      ai       traco     1\n",
      "17 -0.257608  0.949000    0.970667  0.999895      ai       traco     2\n"
     ]
    }
   ],
   "source": [
    "print(results_df_topmost_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5e63fc-c3f3-4ddd-9673-b94135c60a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv(\"traco_hyper.csv\")\n",
    "#results_traco_hyper = pd.read_csv(\"traco_hyper.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a97dd-4de8-4cf1-a0e9-a0097bde2be8",
   "metadata": {},
   "source": [
    "### hLDA and HDP baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea77147-fb28-44eb-8009-dd4af7121bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating tomo.... \n",
      "Extracting for cordis...\n",
      "CORPUS: ['methods', 'processes', 'embedded_systems', 'embed', 'critical', 'variety', 'couple', 'constraint', 'enabler', 'interoperable', 'absence', 'recognize', 'limit', 'factor', 'term', 'com', 'enabler', 'significant', 'conclusive', 'particular', 'formalization', 'viewpoint', 'multi_criterion', 'component', 'space_exploration', 'comprise', 'multi_criterion', 'addition', 'intend', 'deploy', 'customizable', 'possible', 'available', 'significant', 'step', 'term', 'help', 'standardization', 'rely', 'point_view', 'rely', 'maturity', 'input', 'trl', 'maturity', 'trl', 'aspect', 'aspect']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 65889.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for cancer...\n",
      "CORPUS: ['ribonucleic_acid', 'meet', 'train', 'jena', 'participant', 'join', 'young', 'scientist', 'meeting', 'german_society', 'biological', 'entitle', 'ribonucleic_acid', 'excellent', 'speaker', 'world', 'graduate_student', 'young', 'leader', 'enjoy', 'meeting', 'familiar', 'atmosphere', 'exchange', 'inspire', 'new', 'vibrant', 'scientific', 'discussion', 'fascinating', 'exciting', 'non_coding', 'ribonucleic_acid', 'microrna', 'pirna', 'long', 'non_coding', 'ribonucleic_acid', 'diabetes', 'neurodegenerative']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3800/3800 [00:00<00:00, 53309.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for ai...\n",
      "CORPUS: ['remark', 'speech_recognition', 'tea', 'sources', 'sensor', 'signal', 'study', 'capacity', 'deep_learning', 'distinguish', 'tea', 'source', 'aroma', 'aroma', 'tea', 'source', 'contain', 'sensor', 'response', 'measure', 'gas', 'sense', 'mass', 'sensitive', 'chemical', 'sensor', 'evaluate', 'speech_recognition', 'deep_learning', 'aroma', 'speech_recognition', 'experiment', 'frequency', 'analysis', 'continuous', 'wavelet_transform', 'morlet', 'mother_wavelet', 'extraction', 'feature', 'sensor', 'signal', 'deep_learning', 'achieve', 'speech_recognition', 'tea', 'source', 'gas', 'indoor', 'air', 'speech_recognition', 'deep_learning', 'obtain', 'pattern', 'speech_recognition', 'naive_bayes', 'random_forest', 'experimental', 'demonstrate', 'effectiveness', 'deep_learning']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3800/3800 [00:00<00:00, 67266.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for cordis...\n",
      "CORPUS: ['methods', 'processes', 'embedded_systems', 'embed', 'critical', 'variety', 'couple', 'constraint', 'enabler', 'interoperable', 'absence', 'recognize', 'limit', 'factor', 'term', 'com', 'enabler', 'significant', 'conclusive', 'particular', 'formalization', 'viewpoint', 'multi_criterion', 'component', 'space_exploration', 'comprise', 'multi_criterion', 'addition', 'intend', 'deploy', 'customizable', 'possible', 'available', 'significant', 'step', 'term', 'help', 'standardization', 'rely', 'point_view', 'rely', 'maturity', 'input', 'trl', 'maturity', 'trl', 'aspect', 'aspect']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 58434.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for cancer...\n",
      "CORPUS: ['ribonucleic_acid', 'meet', 'train', 'jena', 'participant', 'join', 'young', 'scientist', 'meeting', 'german_society', 'biological', 'entitle', 'ribonucleic_acid', 'excellent', 'speaker', 'world', 'graduate_student', 'young', 'leader', 'enjoy', 'meeting', 'familiar', 'atmosphere', 'exchange', 'inspire', 'new', 'vibrant', 'scientific', 'discussion', 'fascinating', 'exciting', 'non_coding', 'ribonucleic_acid', 'microrna', 'pirna', 'long', 'non_coding', 'ribonucleic_acid', 'diabetes', 'neurodegenerative']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 55234.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for ai...\n",
      "CORPUS: ['remark', 'speech_recognition', 'tea', 'sources', 'sensor', 'signal', 'study', 'capacity', 'deep_learning', 'distinguish', 'tea', 'source', 'aroma', 'aroma', 'tea', 'source', 'contain', 'sensor', 'response', 'measure', 'gas', 'sense', 'mass', 'sensitive', 'chemical', 'sensor', 'evaluate', 'speech_recognition', 'deep_learning', 'aroma', 'speech_recognition', 'experiment', 'frequency', 'analysis', 'continuous', 'wavelet_transform', 'morlet', 'mother_wavelet', 'extraction', 'feature', 'sensor', 'signal', 'deep_learning', 'achieve', 'speech_recognition', 'tea', 'source', 'gas', 'indoor', 'air', 'speech_recognition', 'deep_learning', 'obtain', 'pattern', 'speech_recognition', 'naive_bayes', 'random_forest', 'experimental', 'demonstrate', 'effectiveness', 'deep_learning']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:00<00:00, 28428.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
      "0   cordis      hpam     0 -0.092341  0.964444    0.955414  0.965778\n",
      "1   cordis      hpam     1 -0.051832  0.941111    0.926507  0.964889\n",
      "2   cordis      hpam     2 -0.087348  0.963333    0.920876  0.962667\n",
      "3   cancer      hpam     0 -0.246210  0.995667    0.977453  0.994772\n",
      "4   cancer      hpam     1 -0.273212  0.998000    0.980479  0.996632\n",
      "5   cancer      hpam     2 -0.271764  0.996667    0.981429  0.996614\n",
      "6       ai      hpam     0 -0.160386  0.991000    0.968773  0.991263\n",
      "7       ai      hpam     1 -0.164539  0.989333    0.957675  0.991263\n",
      "8       ai      hpam     2 -0.181021  0.992667    0.982173  0.992877\n",
      "9   cordis      hlda     0 -0.072132  0.946667    0.982173  0.000000\n",
      "10  cordis      hlda     1 -0.061312  0.946667    0.982173  0.000000\n",
      "11  cordis      hlda     2 -0.066540  0.958730    0.982173  0.000000\n",
      "12  cancer      hlda     0 -0.206344  0.957447    0.982173  0.000000\n",
      "13  cancer      hlda     1 -0.183586  0.965812    0.982173  0.000000\n",
      "14  cancer      hlda     2 -0.243461  0.941844    0.982173  0.000000\n",
      "15      ai      hlda     0 -0.208731  0.957447    0.982173  0.000000\n",
      "16      ai      hlda     1 -0.168554  0.981560    0.982173  0.000000\n",
      "17      ai      hlda     2 -0.171353  0.945299    0.982173  0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Calculating tomo.... \")\n",
    "\n",
    "results_df_tomo = []\n",
    "for algo in [\"hpam\", \"hlda\"]: #[\"hdp\", \"hpam\"]\n",
    "    for dtset in datasets: \n",
    "        print(f\"Extracting for {dtset}...\")\n",
    "\n",
    "        ### Load corpus information\n",
    "        corpus_path_ = \"/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/data/tomo_corpus_objects/XXXX.pkl\"\n",
    "        # Cancer, CORDIS, S2CS-AI\n",
    "\n",
    "        if dtset == \"cordis\":\n",
    "            corpus_path = corpus_path_.replace(\"XXXX\", \"CORDIS\")\n",
    "        elif dtset == \"cancer\":\n",
    "            corpus_path = corpus_path_.replace(\"XXXX\", \"Cancer\")\n",
    "        else:\n",
    "            corpus_path = corpus_path_.replace(\"XXXX\", \"S2CS-AI\") \n",
    "\n",
    "        with open(corpus_path, 'rb') as f:\n",
    "            corpus = pickle.load(f)\n",
    "\n",
    "        if dtset == \"cordis\":\n",
    "            # reduce by a factor of 10\n",
    "            corpus = corpus[:len(corpus)//10]\n",
    "        else:\n",
    "            # reduce by a factor of 100\n",
    "            corpus = corpus[:len(corpus)//100]\n",
    "\n",
    "        print(f\"CORPUS: {corpus[0]}\")\n",
    "        \n",
    "        corpus_join = [\" \".join(doc) for doc in corpus]\n",
    "        vectorizer = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "        bow = vectorizer.fit_transform(corpus_join).toarray()\n",
    "        vocab = vectorizer.vocabulary_\n",
    "\n",
    "        print(f\"Vocabulary and BoW calculated\")\n",
    "\n",
    "        ### Load model information\n",
    "        search_key = f\"{path_models}/{algo}/{dtset}/run*.bin\"\n",
    "        results_files = glob.glob(search_key)\n",
    "        for iter_, file in enumerate(results_files):\n",
    "            if algo == \"hlda\":\n",
    "                mdl = tp.HLDAModel.load(file)\n",
    "    \n",
    "                # get topic words\n",
    "                betas = np.array([mdl.get_topic_word_dist(el) for el in range(mdl.k)])\n",
    "                tpc_descs = []\n",
    "                for i in range(mdl.k):\n",
    "                    words = [mdl.vocabs[idx2] for idx2 in np.argsort(betas[i])[::-1][0:n_words]]\n",
    "                    tpc_descs.append((i, words))\n",
    "    \n",
    "                # get groups\n",
    "                root_desc = tpc_descs[0][1]\n",
    "                tpc_descs = [el[1] for el in tpc_descs if el[0] !=0]\n",
    "    \n",
    "                PC_pair_groups = [[root_desc, el] for el in tpc_descs]\n",
    "    \n",
    "                # sibling_groups: length == num_layers\n",
    "                # each element in the list is a group of sibling topics at a layer.\n",
    "                sibling_groups = [# conjunto de all sibling topics (one per layer)\n",
    "                    [\n",
    "                        [\" \".join(el) for el in tpc_descs]\n",
    "                    ]\n",
    "                ]\n",
    "            else:\n",
    "\n",
    "                mdl = tp.HPAModel.load(file)\n",
    "                \n",
    "                supertopics_lst = []  # Lista para los tópicos de nivel 1\n",
    "                subtopics_lst = []    # Lista para los tópicos de nivel 2\n",
    "                \n",
    "                # Extraer super tópicos (nivel 1)\n",
    "                for k1 in range(mdl.k1):  # Iterar sobre los tópicos de nivel 1\n",
    "                    words = [word for word, _ in mdl.get_topic_words(k1, top_n=n_words)]\n",
    "                    supertopics_lst.append(words)\n",
    "                \n",
    "                # Extraer sub tópicos (nivel 2)\n",
    "                for k2 in range(mdl.k1, mdl.k1 + mdl.k2):  # Iterar sobre los tópicos de nivel 2\n",
    "                    words = [word for word, _ in mdl.get_topic_words(k2, top_n=n_words)]\n",
    "                    subtopics_lst.append(words)\n",
    "                    \n",
    "                # Asignar subtópicos a supertópicos\n",
    "                hierarchy = assign_subtopics_to_supertopics(mdl)\n",
    "                \n",
    "                # Imprimir la jerarquía\n",
    "                #print(\"Jerarquía de supertópicos y subtópicos:\")\n",
    "                #for super_topic, sub_topics in hierarchy.items():\n",
    "                #    print(f\"Super tópico {super_topic}: Sub tópicos {sub_topics}\")\n",
    "                \n",
    "                # PC_pair_groups: pares de supertópicos y sus subtópicos\n",
    "                PC_pair_groups = []\n",
    "                for super_topic, sub_topic_ids in hierarchy.items():\n",
    "                    for sub_topic in sub_topic_ids:\n",
    "                        PC_pair_groups.append([supertopics_lst[super_topic], subtopics_lst[sub_topic]])\n",
    "                \n",
    "                # sibling_topics: combinaciones de palabras de supertópicos y subtópicos\n",
    "                sibling_topics = [\n",
    "                    [[\" \".join(supertopics_lst[a]), \" \".join(supertopics_lst[b])] for a, b in combinations(range(len(supertopics_lst)), 2)],  # Nivel 1\n",
    "                    [[\" \".join(subtopics_lst[a]), \" \".join(subtopics_lst[b])] for a, b in combinations(range(len(subtopics_lst)), 2)]        # Nivel 2\n",
    "                ]\n",
    "                \n",
    "                # P_noC_pair_groups: supertópicos con subtópicos que no son directamente suyos\n",
    "                P_noC_pair_groups = []\n",
    "                for super_topic, sub_topic_ids in hierarchy.items():\n",
    "                    non_child_subtopics = [\n",
    "                        sub_topic for sub_topic in range(len(subtopics_lst))\n",
    "                        if sub_topic not in sub_topic_ids\n",
    "                    ]\n",
    "                    for sub_topic in non_child_subtopics:\n",
    "                        P_noC_pair_groups.append([supertopics_lst[super_topic], subtopics_lst[sub_topic]])\n",
    "                                \n",
    "\n",
    "            results = defaultdict()\n",
    "            results[\"dataset\"] = dtset\n",
    "            results[\"algorithm\"] = algo\n",
    "            results[\"iter\"] = iter_\n",
    "            results[\"PCC\"] = np.mean(get_CLNPMI(PC_pair_groups, bow, vocab)) # Parent and Child topic Coherence (PCC)\n",
    "            #print(f\"PCC : {results['PCC']}\")\n",
    "            results[\"PCD\"] = np.mean(get_topics_difference(PC_pair_groups)) # Parent and Child topic Diversity (PCD)\n",
    "            #print(f\"PCD : {results['PCD']}\")\n",
    "            results[\"Sibling_TD\"] = np.mean(get_Sibling_TD(sibling_topics)) # Sibling Topic Diversity (SD)\n",
    "            #print(f\"Sibling_TD : {results['Sibling_TD']}\")\n",
    "            results[\"PnCD\"] = np.mean(get_topics_difference(P_noC_pair_groups)) if algo == \"hpam\" else 0\n",
    "            #print(f\"PnCD : {results['PnCD']}\")\n",
    "\n",
    "            results_df_tomo.append(pd.DataFrame([results]))\n",
    "            \n",
    "results_df_tomo = pd.concat(results_df_tomo, ignore_index=True)\n",
    "print(results_df_tomo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30220e2a-72eb-40f1-93a9-72ab3c9340ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_tomo  = pd.read_csv(\"tomo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebd19f-20ef-4f31-a3e6-564c67e7bdf9",
   "metadata": {},
   "source": [
    "### HTM-WS / HTM-DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81b079-6753-42a8-a650-26a1bbd9154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Calculating ours.... \")\n",
    "algorithms = [\"ws\", \"ds\"]\n",
    "PATH_MODELS = \"/export/usuarios_ml4ds/lbartolome/Datasets/XXXX/htm_variability_models\"\n",
    "PATH_MODELS_PREPROC = \"/export/usuarios_ml4ds/lbartolome/Datasets/XXXX/models_preproc/iter_0\"\n",
    "PATH_CORPUS = \"/export/usuarios_ml4ds/lbartolome/Datasets/XXXX/models_preproc/iter_0/corpus.txt\"\n",
    "\n",
    "results_df_ours = []\n",
    "\n",
    "for dtset in datasets:\n",
    "\n",
    "    print(f\"Executing for dtset: {dtset}\")\n",
    "\n",
    "    ##### GET CORPUS, VOCAB AND BOW #####\n",
    "    if dtset == \"cordis\":\n",
    "        path_corpus = PATH_CORPUS.replace(\"XXXX\", \"CORDIS\")\n",
    "        path_models = pathlib.Path(PATH_MODELS.replace(\"XXXX\", \"CORDIS\"))\n",
    "        model_path_preproc = PATH_MODELS_PREPROC.replace(\"XXXX\", \"CORDIS\")\n",
    "        nr_topics = \"6\"\n",
    "    elif dtset == \"cancer\":\n",
    "        path_corpus = PATH_CORPUS.replace(\"XXXX\", \"Cancer\")\n",
    "        path_models = pathlib.Path(PATH_MODELS.replace(\"XXXX\", \"Cancer\"))\n",
    "        model_path_preproc = PATH_MODELS_PREPROC.replace(\"XXXX\", \"Cancer\")\n",
    "        nr_topics = \"20\"\n",
    "    else:\n",
    "        path_corpus = PATH_CORPUS.replace(\"XXXX\", \"S2CS-AI\") \n",
    "        path_models = pathlib.Path(PATH_MODELS.replace(\"XXXX\", \"S2CS-AI\"))\n",
    "        model_path_preproc = PATH_MODELS_PREPROC.replace(\"XXXX\", \"S2CS-AI\")\n",
    "        nr_topics = \"20\"\n",
    "        \n",
    "    df = mallet_corpus_to_df(path_corpus)\n",
    "    \n",
    "    vocab_w2id = {}\n",
    "    with (pathlib.Path(model_path_preproc)/'vocabulary.txt').open('r', encoding='utf8') as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            wd = line.strip()\n",
    "            vocab_w2id[wd] = i\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary=vocab_w2id.keys(), tokenizer=lambda x: x.split())\n",
    "    bow = vectorizer.fit_transform(df.text.values.tolist())\n",
    "    bow = bow#.toarray()\n",
    "\n",
    "    ##### LOAD MODELS #####\n",
    "    df_root, df_all = get_df_all_models(path_models)\n",
    "    father = [el.as_posix() for el in list(df_root.path.unique()) if nr_topics in el.as_posix()][0]\n",
    "    print(f\"This is the father: {father}\")\n",
    "\n",
    "    for algo in algorithms:\n",
    "        for iter_ in [0,1,2]:\n",
    "            PC_pair_groups = []\n",
    "            P_noC_pair_groups = []\n",
    "            sibling_groups = []\n",
    "            \n",
    "            filtered_df = df_all[\n",
    "                (df_all.father == father) &\n",
    "                (df_all.tr_tpcs == 10) &\n",
    "                (df_all.iter == iter_) & # Cambia iter por iter_ si este es el nombre correcto\n",
    "                (df_all.thr.isin([0, 0.6]))  # Descomentar si se requiere\n",
    "            ]\n",
    "    \n",
    "            # append the descriptions of topics at the first level\n",
    "            sibling_groups.append([\" \".join(el.split(\", \")) for el in filtered_df.iloc[0].tpc_descriptions])\n",
    "        \n",
    "            second_level_siblings = []\n",
    "            for _, submodel in filtered_df.iterrows():  # Iterate over filtered submodels\n",
    "                if algo in submodel.path.as_posix():\n",
    "                    for submodel_topic in submodel.tpc_descriptions:\n",
    "                        # Add Parent-Child (PC) pair\n",
    "                        pair_PC = [submodel.father_tpc_descriptions.split(\", \"), submodel_topic.split(\", \")]\n",
    "                        PC_pair_groups.append(pair_PC)\n",
    "                    \n",
    "                    second_level_siblings += [\" \".join(el.split(\", \")) for el in submodel.tpc_descriptions]\n",
    "                    # Add Parent non-Child (P_noC) pair\n",
    "                    for _, submodel_j in filtered_df.iterrows():\n",
    "                        if submodel.exp_tpc != submodel_j.exp_tpc:  # Ensure submodel_j is different from submodel\n",
    "                            if algo in submodel_j.path.as_posix():\n",
    "                                for submodel_topic in submodel.tpc_descriptions:\n",
    "                                    pair_P_noC = [submodel_j.father_tpc_descriptions.split(\", \"), submodel_topic.split(\", \")]\n",
    "                                    P_noC_pair_groups.append(pair_P_noC)\n",
    "                                    \n",
    "            sibling_groups.append(second_level_siblings)\n",
    "            sibling_groups = [sibling_groups]\n",
    "            \n",
    "            results = defaultdict()\n",
    "            results[\"dataset\"] = dtset\n",
    "            results[\"algorithm\"] = algo\n",
    "            results[\"iter\"] = iter_\n",
    "            results[\"PCC\"] = np.mean(get_CLNPMI(PC_pair_groups, bow, vocab_w2id))\n",
    "            results[\"PCD\"] = np.mean(get_topics_difference(PC_pair_groups))\n",
    "            results[\"Sibling_TD\"] = np.mean(get_Sibling_TD(sibling_groups))\n",
    "            results[\"PnCD\"] =np.mean(get_topics_difference(P_noC_pair_groups))\n",
    "    \n",
    "            results_df_ours.append(pd.DataFrame([results]))\n",
    "            \n",
    "results_df_ours = pd.concat(results_df_ours, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9db273a-feda-48d5-b8f3-794d97caf22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[  dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cordis        ws     0 -0.022061  0.853333    0.696667  0.987333,\n",
       "   dataset algorithm  iter      PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cordis        ws     1 -0.02204  0.851111    0.744444  0.987778,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD   PnCD\n",
       " 0  cordis        ws     2 -0.039863  0.841111    0.722778  0.988,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cordis        ds     0 -0.025439  0.849444    0.455833  0.982889,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cordis        ds     1 -0.025545  0.854444    0.501667  0.982333,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cordis        ds     2 -0.041924  0.838889      0.4925  0.982333,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cancer        ws     0  0.011878  0.844333    0.722333  0.990386,\n",
       "   dataset algorithm  iter      PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cancer        ws     1  0.00665  0.837333    0.678333  0.990754,\n",
       "   dataset algorithm  iter       PCC    PCD  Sibling_TD      PnCD\n",
       " 0  cancer        ws     2  0.010274  0.843    0.670833  0.991105,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cancer        ds     0  0.002499  0.834667    0.507333  0.984526,\n",
       "   dataset algorithm  iter       PCC     PCD  Sibling_TD      PnCD\n",
       " 0  cancer        ds     1  0.000488  0.8325     0.46675  0.984588,\n",
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       " 0  cancer        ds     2  0.006755  0.839167      0.4565  0.984904]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b559fb-84e5-4536-a266-f2df4e2d1796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>iter</th>\n",
       "      <th>PCC</th>\n",
       "      <th>PCD</th>\n",
       "      <th>Sibling_TD</th>\n",
       "      <th>PnCD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>ws</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.987333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>ws</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.022040</td>\n",
       "      <td>0.851111</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.987778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>ws</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>0.841111</td>\n",
       "      <td>0.722778</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>ds</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.025439</td>\n",
       "      <td>0.849444</td>\n",
       "      <td>0.455833</td>\n",
       "      <td>0.982889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>ds</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.025545</td>\n",
       "      <td>0.854444</td>\n",
       "      <td>0.501667</td>\n",
       "      <td>0.982333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>ds</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.041924</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.982333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>ws</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.844333</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.990386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>ws</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.837333</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>0.990754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>ws</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.991105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>ds</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.834667</td>\n",
       "      <td>0.507333</td>\n",
       "      <td>0.984526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>ds</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.466750</td>\n",
       "      <td>0.984588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>ds</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.839167</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.984904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       "0  cordis        ws     0 -0.022061  0.853333    0.696667  0.987333\n",
       "0  cordis        ws     1 -0.022040  0.851111    0.744444  0.987778\n",
       "0  cordis        ws     2 -0.039863  0.841111    0.722778  0.988000\n",
       "0  cordis        ds     0 -0.025439  0.849444    0.455833  0.982889\n",
       "0  cordis        ds     1 -0.025545  0.854444    0.501667  0.982333\n",
       "0  cordis        ds     2 -0.041924  0.838889    0.492500  0.982333\n",
       "0  cancer        ws     0  0.011878  0.844333    0.722333  0.990386\n",
       "0  cancer        ws     1  0.006650  0.837333    0.678333  0.990754\n",
       "0  cancer        ws     2  0.010274  0.843000    0.670833  0.991105\n",
       "0  cancer        ds     0  0.002499  0.834667    0.507333  0.984526\n",
       "0  cancer        ds     1  0.000488  0.832500    0.466750  0.984588\n",
       "0  cancer        ds     2  0.006755  0.839167    0.456500  0.984904"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = pd.concat(results_df_ours)\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64abc9fc-f6b7-4099-b770-7366db7cf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aux.to_csv(\"ours_cordis_cancer.csv\")\n",
    "aux=pd.read_csv(\"ours_cordis_cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679dd88b-15f8-43df-9dbf-0db9264b61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_results = pd.concat([results_traco_hyper, results_df_tomo, aux])\n",
    "#all_results.to_csv(\"all_results_20dec.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c993a86-42ec-4e88-a85c-1ff3adb7c41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCC</th>\n",
       "      <th>PCD</th>\n",
       "      <th>Sibling_TD</th>\n",
       "      <th>PnCD</th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>iter</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.021947</td>\n",
       "      <td>0.831111</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.978444</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.069262</td>\n",
       "      <td>0.781111</td>\n",
       "      <td>0.673889</td>\n",
       "      <td>0.981111</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.120262</td>\n",
       "      <td>0.742222</td>\n",
       "      <td>0.700556</td>\n",
       "      <td>0.979333</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.079284</td>\n",
       "      <td>0.768333</td>\n",
       "      <td>0.666833</td>\n",
       "      <td>0.993632</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.034296</td>\n",
       "      <td>0.819667</td>\n",
       "      <td>0.707500</td>\n",
       "      <td>0.992281</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.056633</td>\n",
       "      <td>0.797667</td>\n",
       "      <td>0.730500</td>\n",
       "      <td>0.996018</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.231284</td>\n",
       "      <td>0.677333</td>\n",
       "      <td>0.624667</td>\n",
       "      <td>0.986193</td>\n",
       "      <td>ai</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.231291</td>\n",
       "      <td>0.670667</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>ai</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.175837</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.613500</td>\n",
       "      <td>0.988930</td>\n",
       "      <td>ai</td>\n",
       "      <td>hyperminer</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.316054</td>\n",
       "      <td>0.992222</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>cordis</td>\n",
       "      <td>traco</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.302709</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.998889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>cordis</td>\n",
       "      <td>traco</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.305776</td>\n",
       "      <td>0.995556</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>cordis</td>\n",
       "      <td>traco</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.267206</td>\n",
       "      <td>0.957667</td>\n",
       "      <td>0.972667</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>cancer</td>\n",
       "      <td>traco</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.256034</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.999895</td>\n",
       "      <td>cancer</td>\n",
       "      <td>traco</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.260944</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>0.990667</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>cancer</td>\n",
       "      <td>traco</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.226881</td>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>ai</td>\n",
       "      <td>traco</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.256937</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.973167</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>ai</td>\n",
       "      <td>traco</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.257608</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>0.970667</td>\n",
       "      <td>0.999895</td>\n",
       "      <td>ai</td>\n",
       "      <td>traco</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.092341</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.955414</td>\n",
       "      <td>0.965778</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hpam</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.051832</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>0.926507</td>\n",
       "      <td>0.964889</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hpam</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.087348</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.920876</td>\n",
       "      <td>0.962667</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hpam</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.246210</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.977453</td>\n",
       "      <td>0.994772</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hpam</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.273212</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.980479</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hpam</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.271764</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>0.996614</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hpam</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.160386</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.968773</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>ai</td>\n",
       "      <td>hpam</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.164539</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.957675</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>ai</td>\n",
       "      <td>hpam</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.181021</td>\n",
       "      <td>0.992667</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.992877</td>\n",
       "      <td>ai</td>\n",
       "      <td>hpam</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.072132</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hlda</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.061312</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hlda</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.066540</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cordis</td>\n",
       "      <td>hlda</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.206344</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hlda</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.183586</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hlda</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.243461</td>\n",
       "      <td>0.941844</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cancer</td>\n",
       "      <td>hlda</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.208731</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ai</td>\n",
       "      <td>hlda</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.168554</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ai</td>\n",
       "      <td>hlda</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.171353</td>\n",
       "      <td>0.945299</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ai</td>\n",
       "      <td>hlda</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>cordis</td>\n",
       "      <td>ws</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022040</td>\n",
       "      <td>0.851111</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.987778</td>\n",
       "      <td>cordis</td>\n",
       "      <td>ws</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039863</td>\n",
       "      <td>0.841111</td>\n",
       "      <td>0.722778</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>cordis</td>\n",
       "      <td>ws</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025439</td>\n",
       "      <td>0.849444</td>\n",
       "      <td>0.455833</td>\n",
       "      <td>0.982889</td>\n",
       "      <td>cordis</td>\n",
       "      <td>ds</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025545</td>\n",
       "      <td>0.854444</td>\n",
       "      <td>0.501667</td>\n",
       "      <td>0.982333</td>\n",
       "      <td>cordis</td>\n",
       "      <td>ds</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.041924</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.982333</td>\n",
       "      <td>cordis</td>\n",
       "      <td>ds</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.844333</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.990386</td>\n",
       "      <td>cancer</td>\n",
       "      <td>ws</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.837333</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>0.990754</td>\n",
       "      <td>cancer</td>\n",
       "      <td>ws</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.991105</td>\n",
       "      <td>cancer</td>\n",
       "      <td>ws</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.834667</td>\n",
       "      <td>0.507333</td>\n",
       "      <td>0.984526</td>\n",
       "      <td>cancer</td>\n",
       "      <td>ds</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.466750</td>\n",
       "      <td>0.984588</td>\n",
       "      <td>cancer</td>\n",
       "      <td>ds</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.839167</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.984904</td>\n",
       "      <td>cancer</td>\n",
       "      <td>ds</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.805667</td>\n",
       "      <td>0.670333</td>\n",
       "      <td>0.989702</td>\n",
       "      <td>ai</td>\n",
       "      <td>ws</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014571</td>\n",
       "      <td>0.811333</td>\n",
       "      <td>0.614833</td>\n",
       "      <td>0.990053</td>\n",
       "      <td>ai</td>\n",
       "      <td>ws</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013211</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.989544</td>\n",
       "      <td>ai</td>\n",
       "      <td>ws</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.806498</td>\n",
       "      <td>0.438608</td>\n",
       "      <td>0.980660</td>\n",
       "      <td>ai</td>\n",
       "      <td>ds</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.424603</td>\n",
       "      <td>0.982660</td>\n",
       "      <td>ai</td>\n",
       "      <td>ds</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.491917</td>\n",
       "      <td>0.983570</td>\n",
       "      <td>ai</td>\n",
       "      <td>ds</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCC       PCD  Sibling_TD      PnCD dataset   algorithm  iter  \\\n",
       "0  -0.021947  0.831111    0.697778  0.978444  cordis  hyperminer     0   \n",
       "1  -0.069262  0.781111    0.673889  0.981111  cordis  hyperminer     1   \n",
       "2  -0.120262  0.742222    0.700556  0.979333  cordis  hyperminer     2   \n",
       "3  -0.079284  0.768333    0.666833  0.993632  cancer  hyperminer     0   \n",
       "4  -0.034296  0.819667    0.707500  0.992281  cancer  hyperminer     1   \n",
       "5  -0.056633  0.797667    0.730500  0.996018  cancer  hyperminer     2   \n",
       "6  -0.231284  0.677333    0.624667  0.986193      ai  hyperminer     0   \n",
       "7  -0.231291  0.670667    0.633500  0.984000      ai  hyperminer     1   \n",
       "8  -0.175837  0.707000    0.613500  0.988930      ai  hyperminer     2   \n",
       "9  -0.316054  0.992222    0.994444  0.999778  cordis       traco     0   \n",
       "10 -0.302709  0.994444    0.998889  1.000000  cordis       traco     1   \n",
       "11 -0.305776  0.995556    0.996667  1.000000  cordis       traco     2   \n",
       "12 -0.267206  0.957667    0.972667  0.999860  cancer       traco     0   \n",
       "13 -0.256034  0.990333    0.996667  0.999895  cancer       traco     1   \n",
       "14 -0.260944  0.988667    0.990667  0.999965  cancer       traco     2   \n",
       "15 -0.226881  0.995333    0.990333  0.999930      ai       traco     0   \n",
       "16 -0.256937  0.954333    0.973167  0.999930      ai       traco     1   \n",
       "17 -0.257608  0.949000    0.970667  0.999895      ai       traco     2   \n",
       "0  -0.092341  0.964444    0.955414  0.965778  cordis        hpam     0   \n",
       "1  -0.051832  0.941111    0.926507  0.964889  cordis        hpam     1   \n",
       "2  -0.087348  0.963333    0.920876  0.962667  cordis        hpam     2   \n",
       "3  -0.246210  0.995667    0.977453  0.994772  cancer        hpam     0   \n",
       "4  -0.273212  0.998000    0.980479  0.996632  cancer        hpam     1   \n",
       "5  -0.271764  0.996667    0.981429  0.996614  cancer        hpam     2   \n",
       "6  -0.160386  0.991000    0.968773  0.991263      ai        hpam     0   \n",
       "7  -0.164539  0.989333    0.957675  0.991263      ai        hpam     1   \n",
       "8  -0.181021  0.992667    0.982173  0.992877      ai        hpam     2   \n",
       "9  -0.072132  0.946667    0.982173  0.000000  cordis        hlda     0   \n",
       "10 -0.061312  0.946667    0.982173  0.000000  cordis        hlda     1   \n",
       "11 -0.066540  0.958730    0.982173  0.000000  cordis        hlda     2   \n",
       "12 -0.206344  0.957447    0.982173  0.000000  cancer        hlda     0   \n",
       "13 -0.183586  0.965812    0.982173  0.000000  cancer        hlda     1   \n",
       "14 -0.243461  0.941844    0.982173  0.000000  cancer        hlda     2   \n",
       "15 -0.208731  0.957447    0.982173  0.000000      ai        hlda     0   \n",
       "16 -0.168554  0.981560    0.982173  0.000000      ai        hlda     1   \n",
       "17 -0.171353  0.945299    0.982173  0.000000      ai        hlda     2   \n",
       "0  -0.022061  0.853333    0.696667  0.987333  cordis          ws     0   \n",
       "1  -0.022040  0.851111    0.744444  0.987778  cordis          ws     1   \n",
       "2  -0.039863  0.841111    0.722778  0.988000  cordis          ws     2   \n",
       "3  -0.025439  0.849444    0.455833  0.982889  cordis          ds     0   \n",
       "4  -0.025545  0.854444    0.501667  0.982333  cordis          ds     1   \n",
       "5  -0.041924  0.838889    0.492500  0.982333  cordis          ds     2   \n",
       "6   0.011878  0.844333    0.722333  0.990386  cancer          ws     0   \n",
       "7   0.006650  0.837333    0.678333  0.990754  cancer          ws     1   \n",
       "8   0.010274  0.843000    0.670833  0.991105  cancer          ws     2   \n",
       "9   0.002499  0.834667    0.507333  0.984526  cancer          ds     0   \n",
       "10  0.000488  0.832500    0.466750  0.984588  cancer          ds     1   \n",
       "11  0.006755  0.839167    0.456500  0.984904  cancer          ds     2   \n",
       "0   0.004027  0.805667    0.670333  0.989702      ai          ws     0   \n",
       "1   0.014571  0.811333    0.614833  0.990053      ai          ws     1   \n",
       "2   0.013211  0.810667    0.676333  0.989544      ai          ws     2   \n",
       "3   0.000232  0.806498    0.438608  0.980660      ai          ds     0   \n",
       "4   0.003045  0.807619    0.424603  0.982660      ai          ds     1   \n",
       "5   0.006190  0.810000    0.491917  0.983570      ai          ds     2   \n",
       "\n",
       "    Unnamed: 0  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         NaN  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "0          0.0  \n",
       "1          1.0  \n",
       "2          2.0  \n",
       "3          3.0  \n",
       "4          4.0  \n",
       "5          5.0  \n",
       "6          6.0  \n",
       "7          7.0  \n",
       "8          8.0  \n",
       "9          9.0  \n",
       "10        10.0  \n",
       "11        11.0  \n",
       "12        12.0  \n",
       "13        13.0  \n",
       "14        14.0  \n",
       "15        15.0  \n",
       "16        16.0  \n",
       "17        17.0  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "5          0.0  \n",
       "6          0.0  \n",
       "7          0.0  \n",
       "8          0.0  \n",
       "9          0.0  \n",
       "10         0.0  \n",
       "11         0.0  \n",
       "0          0.0  \n",
       "1          1.0  \n",
       "2          2.0  \n",
       "3          3.0  \n",
       "4          4.0  \n",
       "5          5.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ai = pd.read_csv(\"ours_ai_from_copy.csv\")\n",
    "all_results_with_ai = pd.concat([results_df_topmost_new, results_df_tomo, aux, df_ai])\n",
    "#all_results_with_ai.to_csv(\"all_results_20dec_with_ai.csv\")\n",
    "all_results_with_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d655940d-6ea5-4b44-a183-ade9a0d532b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCC</th>\n",
       "      <th>PCD</th>\n",
       "      <th>Sibling_TD</th>\n",
       "      <th>PnCD</th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>iter</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.280082</td>\n",
       "      <td>0.978667</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>cordis</td>\n",
       "      <td>traco</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.267206</td>\n",
       "      <td>0.957667</td>\n",
       "      <td>0.972667</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>cancer</td>\n",
       "      <td>traco</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.226881</td>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>ai</td>\n",
       "      <td>traco</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PCC       PCD  Sibling_TD      PnCD dataset algorithm  iter  \\\n",
       "3 -0.280082  0.978667    0.998667  0.999556  cordis     traco     0   \n",
       "4 -0.267206  0.957667    0.972667  0.999860  cancer     traco     0   \n",
       "5 -0.226881  0.995333    0.990333  0.999930      ai     traco     0   \n",
       "\n",
       "   Unnamed: 0  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "5         NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_with_ai[all_results_with_ai.algorithm==\"traco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36192bde-a8fe-4084-8488-c046466b4144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PCC</th>\n",
       "      <th>PCD</th>\n",
       "      <th>Sibling_TD</th>\n",
       "      <th>PnCD</th>\n",
       "      <th>iter</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">traco</th>\n",
       "      <th>ai</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordis</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   PCC  PCD  Sibling_TD  PnCD  iter  Unnamed: 0\n",
       "algorithm dataset                                              \n",
       "traco     ai       NaN  NaN         NaN   NaN   NaN         NaN\n",
       "          cancer   NaN  NaN         NaN   NaN   NaN         NaN\n",
       "          cordis   NaN  NaN         NaN   NaN   NaN         NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_with_ai[all_results_with_ai.algorithm==\"traco\"].groupby([\"algorithm\", \"dataset\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba449246-a58f-43ad-918b-3d7b62a2c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[ht]\n",
      "\\centering\n",
      "\\caption{Resultados de los modelos por dataset y métrica.}\n",
      "\\resizebox{\\textwidth}{!}{%\n",
      "\\begin{tabular}{llcccc}\n",
      "\\toprule\n",
      "Model & Dataset & PCC & PCD & DC & PnDC \\\\\n",
      "\\midrule\n",
      "hLDA & CORDIS & -0.067 $\\pm$ 0.005 & 0.951 $\\pm$ 0.007 & 0.982 $\\pm$ 0.000 & 0.000 $\\pm$ 0.000 \\\\\n",
      "  & S2-Cancer & -0.211 $\\pm$ 0.030 & 0.955 $\\pm$ 0.012 & 0.982 $\\pm$ 0.000 & 0.000 $\\pm$ 0.000 \\\\\n",
      "  & S2-AI & -0.183 $\\pm$ 0.022 & 0.961 $\\pm$ 0.018 & 0.982 $\\pm$ 0.000 & 0.000 $\\pm$ 0.000 \\\\\n",
      "HPAM & CORDIS & -0.077 $\\pm$ 0.022 & 0.956 $\\pm$ 0.013 & 0.934 $\\pm$ 0.019 & 0.964 $\\pm$ 0.002 \\\\\n",
      "  & S2-Cancer & -0.264 $\\pm$ 0.015 & 0.997 $\\pm$ 0.001 & 0.980 $\\pm$ 0.002 & 0.996 $\\pm$ 0.001 \\\\\n",
      "  & S2-AI & -0.169 $\\pm$ 0.011 & 0.991 $\\pm$ 0.002 & 0.970 $\\pm$ 0.012 & 0.992 $\\pm$ 0.001 \\\\\n",
      "HyperMiner & CORDIS & -0.070 $\\pm$ 0.049 & 0.785 $\\pm$ 0.045 & 0.691 $\\pm$ 0.015 & 0.980 $\\pm$ 0.001 \\\\\n",
      "  & S2-Cancer & -0.057 $\\pm$ 0.022 & 0.795 $\\pm$ 0.026 & 0.702 $\\pm$ 0.032 & 0.994 $\\pm$ 0.002 \\\\\n",
      "  & S2-AI & -0.213 $\\pm$ 0.032 & 0.685 $\\pm$ 0.019 & 0.624 $\\pm$ 0.010 & 0.986 $\\pm$ 0.002 \\\\\n",
      "TraCo & CORDIS & -0.308 $\\pm$ 0.007 & 0.994 $\\pm$ 0.002 & 0.997 $\\pm$ 0.002 & 1.000 $\\pm$ 0.000 \\\\\n",
      "  & S2-Cancer & -0.261 $\\pm$ 0.006 & 0.979 $\\pm$ 0.018 & 0.987 $\\pm$ 0.012 & 1.000 $\\pm$ 0.000 \\\\\n",
      "  & S2-AI & -0.247 $\\pm$ 0.018 & 0.966 $\\pm$ 0.025 & 0.978 $\\pm$ 0.011 & 1.000 $\\pm$ 0.000 \\\\\n",
      "\\midrule\n",
      "\\ws{} & CORDIS & -0.028 $\\pm$ 0.010 & 0.849 $\\pm$ 0.007 & 0.721 $\\pm$ 0.024 & 0.988 $\\pm$ 0.000 \\\\\n",
      "  & S2-Cancer & 0.010 $\\pm$ 0.003 & 0.842 $\\pm$ 0.004 & 0.691 $\\pm$ 0.028 & 0.991 $\\pm$ 0.000 \\\\\n",
      "  & S2-AI & 0.011 $\\pm$ 0.006 & 0.809 $\\pm$ 0.003 & 0.654 $\\pm$ 0.034 & 0.990 $\\pm$ 0.000 \\\\\n",
      "\\ds{} & CORDIS & -0.031 $\\pm$ 0.009 & 0.848 $\\pm$ 0.008 & 0.483 $\\pm$ 0.024 & 0.983 $\\pm$ 0.000 \\\\\n",
      "  & S2-Cancer & 0.003 $\\pm$ 0.003 & 0.835 $\\pm$ 0.003 & 0.477 $\\pm$ 0.027 & 0.985 $\\pm$ 0.000 \\\\\n",
      "  & S2-AI & 0.003 $\\pm$ 0.003 & 0.808 $\\pm$ 0.002 & 0.452 $\\pm$ 0.036 & 0.982 $\\pm$ 0.001 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def format_value(mean, std):\n",
    "    return f\"{mean:.3f} $\\\\pm$ {std:.3f}\"\n",
    "\n",
    "# Diccionario para formatear los nombres de algoritmos y datasets\n",
    "algorithm_format = {\n",
    "    \"hlda\": \"hLDA\",\n",
    "    \"hpam\": \"HPAM\",\n",
    "    \"hyperminer\": \"HyperMiner\",\n",
    "    \"traco\": \"TraCo\",\n",
    "    \"ws\": \"\\\\ws{}\",\n",
    "    \"ds\": \"\\\\ds{}\",\n",
    "}\n",
    "dataset_format = {\n",
    "    \"cordis\": \"CORDIS\",\n",
    "    \"cancer\": \"S2-Cancer\",\n",
    "    \"ai\": \"S2-AI\",\n",
    "}\n",
    "\n",
    "# Orden de los algoritmos y datasets\n",
    "algorithm_order = [\"hlda\", \"hpam\", \"hyperminer\", \"traco\", \"ws\", \"ds\"]\n",
    "dataset_order = [\"cordis\", \"cancer\", \"ai\"]\n",
    "\n",
    "# Agrupar por 'algorithm' y 'dataset' para calcular media y desviación estándar\n",
    "mean_df = all_results_with_ai.groupby([\"algorithm\", \"dataset\"]).mean()\n",
    "std_df = all_results_with_ai.groupby([\"algorithm\", \"dataset\"]).std()\n",
    "\n",
    "# Crear la estructura de la tabla en LaTeX\n",
    "latex_table = \"\\\\begin{table}[ht]\\n\\\\centering\\n\\\\caption{Resultados de los modelos por dataset y métrica.}\\n\"\n",
    "latex_table += \"\\\\resizebox{\\\\textwidth}{!}{%\\n\"\n",
    "latex_table += \"\\\\begin{tabular}{llcccc}\\n\\\\toprule\\n\"\n",
    "latex_table += \"Model & Dataset & PCC & PCD & DC & PnDC \\\\\\\\\\n\\\\midrule\\n\"\n",
    "\n",
    "# Iterar sobre los algoritmos en el orden especificado\n",
    "for algorithm in algorithm_order:\n",
    "    # Formatear el nombre del algoritmo\n",
    "    formatted_algorithm = algorithm_format.get(algorithm, algorithm)\n",
    "    \n",
    "    rows = []\n",
    "    for dataset in dataset_order:\n",
    "        # Verificar si los datos existen para este algoritmo y dataset\n",
    "        if (algorithm, dataset) in mean_df.index:\n",
    "            mean_row = mean_df.loc[(algorithm, dataset)]\n",
    "            std_row = std_df.loc[(algorithm, dataset)]\n",
    "            row = [\n",
    "                format_value(mean_row[\"PCC\"], std_row[\"PCC\"]),\n",
    "                format_value(mean_row[\"PCD\"], std_row[\"PCD\"]),\n",
    "                format_value(mean_row[\"Sibling_TD\"], std_row[\"Sibling_TD\"]),\n",
    "                format_value(mean_row[\"PnCD\"], std_row[\"PnCD\"]),\n",
    "            ]\n",
    "            # Formatear el nombre del dataset\n",
    "            formatted_dataset = dataset_format.get(dataset, dataset)\n",
    "            rows.append(f\" & {formatted_dataset} & \" + \" & \".join(row) + \" \\\\\\\\\\n\")\n",
    "    \n",
    "    # Añadir el algoritmo y las filas asociadas\n",
    "    if rows:\n",
    "        latex_table += f\"{formatted_algorithm}{rows[0]}\"\n",
    "        for r in rows[1:]:\n",
    "            latex_table += f\" {r}\"\n",
    "\n",
    "    # Añadir línea divisoria después de `traco`\n",
    "    if algorithm == \"traco\":\n",
    "        latex_table += \"\\\\midrule\\n\"\n",
    "\n",
    "latex_table += \"\\\\bottomrule\\n\\\\end{tabular}%\\n\"\n",
    "latex_table += \"}\\n\\\\end{table}\"\n",
    "\n",
    "# Mostrar o guardar la tabla\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
