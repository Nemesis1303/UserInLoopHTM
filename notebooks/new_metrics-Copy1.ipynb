{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2948ba6b-068d-40be-9a81-a11c51568a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "import pathlib\n",
    "import pickle\n",
    "import glob\n",
    "import tomotopy as tp\n",
    "from scipy.sparse import issparse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e044121-6e1a-487c-bfe1-5781d5a414b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = \"/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/data/models_v3\"\n",
    "datasets = [\"cordis\", \"cancer\", \"ai\"]\n",
    "n_words = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a9a90-7d14-415c-9b80-c07c90bc25e4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d4a66e-0d14-4883-92ee-7a9e0e296947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def compute_CLNPMI(parent_diff_words, child_diff_words, all_bow, vocab):\n",
    "    npmi_list = list()\n",
    "\n",
    "    for p_w in parent_diff_words:\n",
    "        flag_n = all_bow[:, vocab[p_w]] > 0\n",
    "        p_n = np.sum(flag_n) / len(all_bow)\n",
    "\n",
    "        for c_w in child_diff_words:\n",
    "            flag_l = all_bow[:, vocab[c_w]] > 0\n",
    "            p_l = np.sum(flag_l)\n",
    "            p_nl = np.sum(flag_n * flag_l)\n",
    "\n",
    "            if p_nl == len(all_bow):\n",
    "                npmi_score = 1\n",
    "            else:\n",
    "                p_l = p_l / len(all_bow)\n",
    "                p_nl = p_nl / len(all_bow)\n",
    "                p_nl += 1e-10\n",
    "                npmi_score = np.log(p_nl / (p_l * p_n)) / -np.log(p_nl)\n",
    "\n",
    "            npmi_list.append(npmi_score)\n",
    "\n",
    "    return npmi_list\n",
    "\"\"\"\n",
    "def compute_CLNPMI(parent_diff_words, child_diff_words, all_bow, vocab):\n",
    "    npmi_list = list()\n",
    "\n",
    "    # Longitud de la matriz\n",
    "    total_docs = all_bow.shape[0]\n",
    "\n",
    "    for p_w in parent_diff_words:\n",
    "        # Índice de la palabra \"parent\" en el vocabulario\n",
    "        p_idx = vocab[p_w]\n",
    "\n",
    "        # Filtramos documentos donde aparece la palabra p_w\n",
    "        if issparse(all_bow):\n",
    "            flag_n = all_bow[:, p_idx].toarray().flatten() > 0\n",
    "        else:\n",
    "            flag_n = all_bow[:, p_idx] > 0\n",
    "        p_n = np.sum(flag_n) / total_docs\n",
    "\n",
    "        for c_w in child_diff_words:\n",
    "            # Índice de la palabra \"child\" en el vocabulario\n",
    "            try:\n",
    "                c_idx = vocab[c_w]\n",
    "                # Filtramos documentos donde aparece la palabra c_w\n",
    "                if issparse(all_bow):\n",
    "                    flag_l = all_bow[:, c_idx].toarray().flatten() > 0\n",
    "                else:\n",
    "                    flag_l = all_bow[:, c_idx] > 0\n",
    "                p_l = np.sum(flag_l)\n",
    "    \n",
    "                # Coincidencia entre documentos que contienen p_w y c_w\n",
    "                p_nl = np.sum(flag_n & flag_l)\n",
    "                if p_l == 0:\n",
    "                    import pdb; pdb.set_trace()\n",
    "    \n",
    "                if p_nl == total_docs:\n",
    "                    npmi_score = 1\n",
    "                else:\n",
    "                    p_l = p_l / total_docs\n",
    "                    p_nl = p_nl / total_docs\n",
    "                    p_nl += 1e-10\n",
    "                    npmi_score = np.log(p_nl / (p_l * p_n)) / -np.log(p_nl)\n",
    "\n",
    "                # if p_l = 0 implica que la palabra no aparece en ningún documento\n",
    "                \n",
    "                npmi_list.append(npmi_score)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"word {c_w} not in vocab\")\n",
    "\n",
    "    return npmi_list\n",
    "\n",
    "\n",
    "def compute_TD(texts):\n",
    "    K = len(texts)\n",
    "    T = len(texts[0].split())\n",
    "    vectorizer = CountVectorizer()\n",
    "    counter = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "    TF = counter.sum(axis=0)\n",
    "    TD = (TF == 1).sum() / (K * T)\n",
    "\n",
    "    return TD\n",
    "\n",
    "def get_CLNPMI(PC_pair_groups, all_bow, vocab):\n",
    "    CNPMI_list = list()\n",
    "    for group in tqdm(PC_pair_groups):\n",
    "        layer_CNPMI = []\n",
    "    \n",
    "       \n",
    "        parent_words = set(group[0])\n",
    "        child_words = set(group[1])\n",
    "\n",
    "        inter = parent_words.intersection(child_words)\n",
    "        parent_diff_words = list(parent_words.difference(inter))\n",
    "        child_diff_words = list(child_words.difference(inter))\n",
    "        \n",
    "        npmi_list = compute_CLNPMI(parent_diff_words, child_diff_words, all_bow, vocab)\n",
    "\n",
    "        # Handle repetitive word pair NPMI assignments if necessary\n",
    "        num_repetition = (\n",
    "            len(parent_words) - len(parent_diff_words)\n",
    "        ) * (len(child_words) - len(child_diff_words))\n",
    "        npmi_list.extend([-1] * num_repetition)\n",
    "\n",
    "        layer_CNPMI.extend(npmi_list)\n",
    "    \n",
    "        CNPMI_list.append(np.mean(layer_CNPMI))  # Append the group's result\n",
    "\n",
    "\n",
    "    return CNPMI_list\n",
    "\n",
    "def compute_diff_topic_pair(topic_words_a, topic_words_b):\n",
    "    word_counter = Counter()\n",
    "    word_counter.update(topic_words_a)\n",
    "    word_counter.update(topic_words_b)\n",
    "    diff = (np.asarray(list(word_counter.values())) == 1).sum() / (len(topic_words_a) + len(topic_words_b))\n",
    "    return diff\n",
    "\n",
    "\n",
    "def get_topics_difference(topic_pair_groups):\n",
    "    diff_list = list()\n",
    "    for group in tqdm(topic_pair_groups):\n",
    "        layer_diff = list()\n",
    "        diff = compute_diff_topic_pair(group[0], group[1])\n",
    "        layer_diff.append(diff)\n",
    "        diff_list.append(np.mean(layer_diff))\n",
    "\n",
    "    return diff_list\n",
    "\n",
    "def get_Sibling_TD(sibling_groups):\n",
    "    sibling_TD = list()\n",
    "    for group in sibling_groups:\n",
    "        layer_sibling_TD = list()\n",
    "        for sibling_topics in group:\n",
    "            TD = compute_TD(sibling_topics)\n",
    "            layer_sibling_TD.append(TD)\n",
    "        sibling_TD.append(np.mean(layer_sibling_TD))\n",
    "    return sibling_TD\n",
    "\n",
    "#### HPAM\n",
    "def assign_subtopics_to_supertopics(model):\n",
    "    \"\"\"\n",
    "    Asigna cada subtópico a un supertópico basado en la máxima probabilidad condicional.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo HPAM entrenado.\n",
    "\n",
    "    Returns:\n",
    "        hierarchy: Diccionario donde las claves son supertópicos (nivel 1) y los valores son listas\n",
    "                   de subtópicos (nivel 2) que cuelgan de ellos.\n",
    "    \"\"\"\n",
    "    hierarchy = {k1: [] for k1 in range(model.k1)}  # Inicializa el diccionario para supertópicos\n",
    "\n",
    "    for k2 in range(model.k2):  # Iterar sobre los subtópicos\n",
    "        max_prob = 0\n",
    "        assigned_super_topic = None\n",
    "        \n",
    "        for k1 in range(model.k1):  # Iterar sobre los supertópicos\n",
    "            # Obtener la probabilidad del subtópico dado el supertópico\n",
    "            prob = model.get_sub_topic_dist(k1)[k2]\n",
    "            if prob > max_prob:  # Asignar al supertópico con mayor probabilidad\n",
    "                max_prob = prob\n",
    "                assigned_super_topic = k1\n",
    "        \n",
    "        if assigned_super_topic is not None:\n",
    "            hierarchy[assigned_super_topic].append(k2)\n",
    "    \n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6352303-bf3d-44b1-866e-238f649f610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mallet_corpus_to_df(corpusFile: pathlib.Path):\n",
    "    \"\"\"Converts a Mallet corpus file (i.e., file required for the Mallet import command) to a pandas DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpusFile: pathlib.Path\n",
    "        Path to the Mallet corpus file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :   pandas.DataFrame\n",
    "        DataFrame with the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "        corpusFile, encoding=\"utf-8\").readlines()]\n",
    "    indexes = [line.rsplit(' 0 ')[0].strip() for line in open(\n",
    "        corpusFile, encoding=\"utf-8\").readlines()]\n",
    "    corpus_dict = {\n",
    "        'id': indexes,\n",
    "        'text': corpus\n",
    "    }\n",
    "    return pd.DataFrame(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd0e2f5-a33d-4058-93f0-4404b76fdc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_models(path_models:pathlib.Path):\n",
    "    dfs = []\n",
    "    for entry in path_models.iterdir():\n",
    "        # check if it is a root model\n",
    "        if entry.name.startswith(\"htm\"):#root\n",
    "            \n",
    "            # Path to the root model\n",
    "            path = entry\n",
    "\n",
    "            # Thr and exp_tpc do not apply for the root model\n",
    "            thr = -1\n",
    "            exp_tpc = -1\n",
    "\n",
    "            # Experiment iteration\n",
    "            iter_ = -1\n",
    "            \n",
    "            # tr_topics\n",
    "            tr_topics = int(path.name.split(\"_\")[1])\n",
    "\n",
    "            # Size of the topics\n",
    "            alphas = np.load(path.joinpath('TMmodel/alphas.npy')).tolist()\n",
    "            alphas = list(map(lambda x: x * 100, alphas))\n",
    "\n",
    "            # Coherences (CV and NPMI)\n",
    "            cohrs_cv = np.load(path.joinpath('TMmodel/c_v_ref_coherence.npy')).tolist()\n",
    "            cohrs_npmi = np.load(path.joinpath('TMmodel/c_npmi_ref_coherence.npy')).tolist()\n",
    "\n",
    "            # Topics' entropies\n",
    "            entropies = np.load(path.joinpath('TMmodel/topic_entropy.npy')).tolist()\n",
    "\n",
    "            # TD\n",
    "            td = np.load(path.joinpath('TMmodel/td.npy'))\n",
    "\n",
    "            # IRBO \n",
    "            rbo = np.load(path.joinpath('TMmodel/rbo.npy'))\n",
    "            \n",
    "            # tpc_Desc\n",
    "            with path.joinpath('TMmodel/tpc_descriptions.txt').open('r', encoding='utf8') as fin:\n",
    "                tpc_descriptions = [el.strip() for el in fin.readlines()]\n",
    "\n",
    "            # Ids of the topics\n",
    "            tpc_ids = np.arange(0,len(alphas),1)\n",
    "\n",
    "            # Corpus size\n",
    "            if path.joinpath('corpus.txt').is_file():\n",
    "                corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                    path.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                size = len(corpus)\n",
    "            elif path.joinpath('corpus.parquet').is_dir():\n",
    "                dfc = pd.read_parquet(path.joinpath('corpus.parquet'))\n",
    "                size = len(dfc)\n",
    "\n",
    "            # Create dataframe for the root model\n",
    "            root_tpc_df = pd.DataFrame(\n",
    "                {'iter': [iter_] * len(alphas),\n",
    "                 'path': [path] * len(alphas),\n",
    "                 'cohrs_cv': cohrs_cv,\n",
    "                 'cohrs_npmi': cohrs_npmi,\n",
    "                 'entropies': entropies,\n",
    "                 'td': [td] * len(alphas),\n",
    "                 'rbo': [rbo] * len(alphas),\n",
    "                 'alphas': alphas,\n",
    "                 'tpc_ids': tpc_ids,\n",
    "                 'thr': [thr] * len(alphas),\n",
    "                 'exp_tpc': [exp_tpc] * len(alphas),\n",
    "                 'tr_tpcs': [tr_topics] * len(alphas),\n",
    "                 'tpc_descriptions': tpc_descriptions,\n",
    "                 'father': [iter_] * len(alphas),\n",
    "                'father_tpc_descriptions':[iter_] * len(alphas),\n",
    "                })\n",
    "\n",
    "            # Get root size\n",
    "            if root_tpc_df.iloc[0].path.joinpath('corpus.txt').is_file():\n",
    "                corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                            root_tpc_df.iloc[0].path.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                root_size = len(corpus)\n",
    "            elif root_tpc_df.iloc[0].path.joinpath('corpus.parquet').is_dir() or root_tpc_df.iloc[0].path.joinpath('corpus.parquet').is_file():\n",
    "                dfc = pd.read_parquet(root_tpc_df.iloc[0].path.joinpath('corpus.parquet'))\n",
    "                root_size = len(dfc) \n",
    "\n",
    "            root_tpc_df[\"root_size\"] = [root_size] * len(alphas)\n",
    "\n",
    "            # Append to the list of dataframes to concatenate them\n",
    "            dfs.append(root_tpc_df)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.sort_values(by=['iter'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_submodules(df:pd.DataFrame):\n",
    "    # Iter over each root model (according to its corresponding iteration, iter)\n",
    "    concats = [df]\n",
    "    not_finished = []\n",
    "    paths_root = df.path.unique()\n",
    "    print(paths_root)\n",
    "    for path_root in paths_root:\n",
    "        print(path_root)\n",
    "        root_size = df[df.path == path_root].iloc[0].root_size\n",
    "        \n",
    "        for entry in path_root.iterdir():\n",
    "            if entry.joinpath('TMmodel/topic_coherence.npy').is_file() and not entry.as_posix().endswith(\"old\"):\n",
    "                try:\n",
    "                    if \"submodel_htm-ws\" in entry.as_posix():\n",
    "                        thr_ = 0\n",
    "                        size = 0\n",
    "                    else:\n",
    "                        thr_ = float(entry.as_posix().split(\"thr_\")[1].split(\"_\")[0])\n",
    "                        \n",
    "                        if entry.joinpath('corpus.txt').is_file():\n",
    "                            corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                                        entry.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                            size = len(corpus)\n",
    "                        elif entry.joinpath('corpus.parquet').is_dir():\n",
    "                            dfc = pd.read_parquet(entry.joinpath('corpus.parquet'))\n",
    "                            size = len(dfc)\n",
    "                        size = size * 100 / root_size\n",
    "                        \n",
    "                    # get iter\n",
    "                    iter_ = int(entry.name.split(\"_iter_\")[1].split(\"_\")[0])\n",
    "                    \n",
    "                    # get topic from which the submodel is generated\n",
    "                    exp_tpc = int(entry.as_posix().split(\"from_topic_\")[1].split(\"_\")[0])   \n",
    "                    aux = df[(df.path == path_root)]\n",
    "                    father_tpc_desc = aux[aux.tpc_ids == exp_tpc].tpc_descriptions.values.tolist()[0]\n",
    "    \n",
    "                    # Size of the topics\n",
    "                    alphas = np.load(entry.joinpath('TMmodel/alphas.npy')).tolist()\n",
    "                    alphas = list(map(lambda x: x * 100, alphas))\n",
    "    \n",
    "                    # Alphas submodel is the mean of the cohr of its topics\n",
    "                    alpha = np.mean(alphas)\n",
    "    \n",
    "                    # Coheerences (CV and NPMI)\n",
    "                    cohrs_cv = np.load(entry.joinpath('TMmodel/c_v_ref_coherence.npy')).tolist()\n",
    "                    cohrs_npmi = np.load(entry.joinpath('TMmodel/c_npmi_ref_coherence.npy'), allow_pickle=True).tolist()\n",
    "                    if cohrs_npmi is None:\n",
    "                        cohrs_npmi = [0]*len(cohrs_cv)\n",
    "    \n",
    "                    # cohr submodel is the mean of the cohr of its topics\n",
    "                    cohr_cv = np.mean(cohrs_cv)\n",
    "                    cohr_npmi = np.mean(cohrs_npmi)\n",
    "    \n",
    "                    # TD\n",
    "                    td = np.load(entry.joinpath('TMmodel/td.npy'))\n",
    "    \n",
    "                    # IRBO \n",
    "                    rbo = np.load(entry.joinpath('TMmodel/rbo.npy'))\n",
    "    \n",
    "                    # Topics' entropies\n",
    "                    entropy = np.mean(np.load(entry.joinpath('TMmodel/topic_entropy.npy')).tolist())\n",
    "    \n",
    "                    tr_tpcs = int(entry.as_posix().split(\"train_with_\")[1].split(\"_\")[0])\n",
    "    \n",
    "                    # tpc_Desc\n",
    "                    with entry.joinpath('TMmodel/tpc_descriptions.txt').open('r', encoding='utf8') as fin:\n",
    "                        tpc_descriptions = [el.strip() for el in fin.readlines()]\n",
    "                    \n",
    "                    root_tpc_df = pd.DataFrame(\n",
    "                    {'iter': [iter_],\n",
    "                     'path': [entry],\n",
    "                     'cohrs_cv': [cohr_cv],\n",
    "                     'cohrs_npmi': [cohr_npmi],\n",
    "                     'entropies': [entropy],\n",
    "                     'alphas': [alpha],\n",
    "                     'td': [td],\n",
    "                     'rbo': [rbo],\n",
    "                     'tpc_ids': [exp_tpc],\n",
    "                     'thr': [thr_],\n",
    "                     'exp_tpc': [exp_tpc],\n",
    "                     'size': [size],\n",
    "                     'tr_tpcs': [tr_tpcs],\n",
    "                     'tpc_descriptions': [tpc_descriptions],\n",
    "                     'father': [path_root.as_posix()],\n",
    "                     'father_tpc_descriptions':[father_tpc_desc] \n",
    "                    })\n",
    "                                    \n",
    "                    concats.append(root_tpc_df)\n",
    "                except Exception as e:\n",
    "                    not_finished.append(entry)\n",
    "    \n",
    "            else:\n",
    "                not_finished.append(entry)\n",
    "    df = pd.concat(concats)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_df_all_models(path_models:pathlib.Path):\n",
    "    df_root = get_root_models(path_models)\n",
    "    df_all = get_submodules(df_root)\n",
    "    return df_root, df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9d57c-5677-4dbe-a139-71f7f346d9ed",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc804d65-d1b4-4c06-a25c-1f472d9478e3",
   "metadata": {},
   "source": [
    "### Hyperminer and Traco baselines (directly load from TopMost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe843d8-aacf-4e01-b75b-60e6f9ec270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating topmost.... \n"
     ]
    }
   ],
   "source": [
    "print(\"--- Calculating topmost.... \")\n",
    "algorithms = [\"hyperminer\", \"traco\"]\n",
    "\n",
    "results_df = []\n",
    "for algo in algorithms:\n",
    "    for dtset in datasets:\n",
    "        search_key = f\"{path_models}/{algo}/{dtset}/{dtset}_trained_model_iter_*.pkl\"\n",
    "        results_files = glob.glob(search_key)\n",
    "        for i, file in enumerate(results_files):\n",
    "            with open(file, \"rb\") as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "                results = loaded_data[\"hierarchy_quality_results\"]\n",
    "                results[\"dataset\"] = dtset\n",
    "                results[\"algorithm\"] = algo\n",
    "                results[\"iter\"] = i\n",
    "                if isinstance(results, dict):\n",
    "                    results = [results] \n",
    "                results_df.append(pd.DataFrame(results))\n",
    "results_df = pd.concat(results_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5f082d6-bb8f-4348-b619-8e95cdd59a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         PCC       PCD  Sibling_TD      PnCD dataset   algorithm  iter\n",
      "0  -0.219832  0.676000    0.730667  0.981481  cordis  hyperminer     0\n",
      "1  -0.190614  0.690667    0.742667  0.985481  cordis  hyperminer     1\n",
      "2  -0.163313  0.694667    0.712000  0.979852  cordis  hyperminer     2\n",
      "3  -0.376846  0.560000    0.935000  0.994236  cancer  hyperminer     0\n",
      "4  -0.421895  0.531667    0.930000  0.995069  cancer  hyperminer     1\n",
      "5  -0.395936  0.548333    0.896667  0.993264  cancer  hyperminer     2\n",
      "6  -0.450961  0.510000    0.890000  0.991875      ai  hyperminer     0\n",
      "7  -0.416273  0.535000    0.903333  0.993333      ai  hyperminer     1\n",
      "8  -0.541641  0.451667    0.861667  0.991389      ai  hyperminer     2\n",
      "9  -0.280082  0.978667    0.998667  0.999556  cordis       traco     0\n",
      "10 -0.274823  0.978667    0.998667  0.999852  cordis       traco     1\n",
      "11 -0.289854  0.980000    0.998667  0.999556  cordis       traco     2\n",
      "12 -0.256897  0.913333    0.998333  0.999861  cancer       traco     0\n",
      "13 -0.299988  0.863333    0.968333  0.999653  cancer       traco     1\n",
      "14 -0.259012  0.916667    1.000000  0.999722  cancer       traco     2\n",
      "15 -0.230079  0.935000    1.000000  0.999792      ai       traco     0\n",
      "16 -0.231914  0.931667    0.996667  0.999722      ai       traco     1\n",
      "17 -0.231038  0.928333    1.000000  0.999861      ai       traco     2\n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5e63fc-c3f3-4ddd-9673-b94135c60a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"traco_hyper.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a97dd-4de8-4cf1-a0e9-a0097bde2be8",
   "metadata": {},
   "source": [
    "### hLDA and HDP baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea77147-fb28-44eb-8009-dd4af7121bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating tomo.... \n",
      "Extracting for cordis...\n",
      "CORPUS: ['methods', 'processes', 'embedded_systems', 'embed', 'critical', 'variety', 'couple', 'constraint', 'enabler', 'interoperable', 'absence', 'recognize', 'limit', 'factor', 'term', 'com', 'enabler', 'significant', 'conclusive', 'particular', 'formalization', 'viewpoint', 'multi_criterion', 'component', 'space_exploration', 'comprise', 'multi_criterion', 'addition', 'intend', 'deploy', 'customizable', 'possible', 'available', 'significant', 'step', 'term', 'help', 'standardization', 'rely', 'point_view', 'rely', 'maturity', 'input', 'trl', 'maturity', 'trl', 'aspect', 'aspect']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 65889.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for cancer...\n",
      "CORPUS: ['ribonucleic_acid', 'meet', 'train', 'jena', 'participant', 'join', 'young', 'scientist', 'meeting', 'german_society', 'biological', 'entitle', 'ribonucleic_acid', 'excellent', 'speaker', 'world', 'graduate_student', 'young', 'leader', 'enjoy', 'meeting', 'familiar', 'atmosphere', 'exchange', 'inspire', 'new', 'vibrant', 'scientific', 'discussion', 'fascinating', 'exciting', 'non_coding', 'ribonucleic_acid', 'microrna', 'pirna', 'long', 'non_coding', 'ribonucleic_acid', 'diabetes', 'neurodegenerative']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3800/3800 [00:00<00:00, 53309.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for ai...\n",
      "CORPUS: ['remark', 'speech_recognition', 'tea', 'sources', 'sensor', 'signal', 'study', 'capacity', 'deep_learning', 'distinguish', 'tea', 'source', 'aroma', 'aroma', 'tea', 'source', 'contain', 'sensor', 'response', 'measure', 'gas', 'sense', 'mass', 'sensitive', 'chemical', 'sensor', 'evaluate', 'speech_recognition', 'deep_learning', 'aroma', 'speech_recognition', 'experiment', 'frequency', 'analysis', 'continuous', 'wavelet_transform', 'morlet', 'mother_wavelet', 'extraction', 'feature', 'sensor', 'signal', 'deep_learning', 'achieve', 'speech_recognition', 'tea', 'source', 'gas', 'indoor', 'air', 'speech_recognition', 'deep_learning', 'obtain', 'pattern', 'speech_recognition', 'naive_bayes', 'random_forest', 'experimental', 'demonstrate', 'effectiveness', 'deep_learning']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3800/3800 [00:00<00:00, 67266.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for cordis...\n",
      "CORPUS: ['methods', 'processes', 'embedded_systems', 'embed', 'critical', 'variety', 'couple', 'constraint', 'enabler', 'interoperable', 'absence', 'recognize', 'limit', 'factor', 'term', 'com', 'enabler', 'significant', 'conclusive', 'particular', 'formalization', 'viewpoint', 'multi_criterion', 'component', 'space_exploration', 'comprise', 'multi_criterion', 'addition', 'intend', 'deploy', 'customizable', 'possible', 'available', 'significant', 'step', 'term', 'help', 'standardization', 'rely', 'point_view', 'rely', 'maturity', 'input', 'trl', 'maturity', 'trl', 'aspect', 'aspect']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 58434.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for cancer...\n",
      "CORPUS: ['ribonucleic_acid', 'meet', 'train', 'jena', 'participant', 'join', 'young', 'scientist', 'meeting', 'german_society', 'biological', 'entitle', 'ribonucleic_acid', 'excellent', 'speaker', 'world', 'graduate_student', 'young', 'leader', 'enjoy', 'meeting', 'familiar', 'atmosphere', 'exchange', 'inspire', 'new', 'vibrant', 'scientific', 'discussion', 'fascinating', 'exciting', 'non_coding', 'ribonucleic_acid', 'microrna', 'pirna', 'long', 'non_coding', 'ribonucleic_acid', 'diabetes', 'neurodegenerative']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 55234.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for ai...\n",
      "CORPUS: ['remark', 'speech_recognition', 'tea', 'sources', 'sensor', 'signal', 'study', 'capacity', 'deep_learning', 'distinguish', 'tea', 'source', 'aroma', 'aroma', 'tea', 'source', 'contain', 'sensor', 'response', 'measure', 'gas', 'sense', 'mass', 'sensitive', 'chemical', 'sensor', 'evaluate', 'speech_recognition', 'deep_learning', 'aroma', 'speech_recognition', 'experiment', 'frequency', 'analysis', 'continuous', 'wavelet_transform', 'morlet', 'mother_wavelet', 'extraction', 'feature', 'sensor', 'signal', 'deep_learning', 'achieve', 'speech_recognition', 'tea', 'source', 'gas', 'indoor', 'air', 'speech_recognition', 'deep_learning', 'obtain', 'pattern', 'speech_recognition', 'naive_bayes', 'random_forest', 'experimental', 'demonstrate', 'effectiveness', 'deep_learning']\n",
      "Vocabulary and BoW calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:00<00:00, 28428.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
      "0   cordis      hpam     0 -0.092341  0.964444    0.955414  0.965778\n",
      "1   cordis      hpam     1 -0.051832  0.941111    0.926507  0.964889\n",
      "2   cordis      hpam     2 -0.087348  0.963333    0.920876  0.962667\n",
      "3   cancer      hpam     0 -0.246210  0.995667    0.977453  0.994772\n",
      "4   cancer      hpam     1 -0.273212  0.998000    0.980479  0.996632\n",
      "5   cancer      hpam     2 -0.271764  0.996667    0.981429  0.996614\n",
      "6       ai      hpam     0 -0.160386  0.991000    0.968773  0.991263\n",
      "7       ai      hpam     1 -0.164539  0.989333    0.957675  0.991263\n",
      "8       ai      hpam     2 -0.181021  0.992667    0.982173  0.992877\n",
      "9   cordis      hlda     0 -0.072132  0.946667    0.982173  0.000000\n",
      "10  cordis      hlda     1 -0.061312  0.946667    0.982173  0.000000\n",
      "11  cordis      hlda     2 -0.066540  0.958730    0.982173  0.000000\n",
      "12  cancer      hlda     0 -0.206344  0.957447    0.982173  0.000000\n",
      "13  cancer      hlda     1 -0.183586  0.965812    0.982173  0.000000\n",
      "14  cancer      hlda     2 -0.243461  0.941844    0.982173  0.000000\n",
      "15      ai      hlda     0 -0.208731  0.957447    0.982173  0.000000\n",
      "16      ai      hlda     1 -0.168554  0.981560    0.982173  0.000000\n",
      "17      ai      hlda     2 -0.171353  0.945299    0.982173  0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Calculating tomo.... \")\n",
    "\n",
    "results_df_tomo = []\n",
    "for algo in [\"hpam\", \"hlda\"]: #[\"hdp\", \"hpam\"]\n",
    "    for dtset in datasets: \n",
    "        print(f\"Extracting for {dtset}...\")\n",
    "\n",
    "        ### Load corpus information\n",
    "        corpus_path_ = \"/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/data/tomo_corpus_objects/XXXX.pkl\"\n",
    "        # Cancer, CORDIS, S2CS-AI\n",
    "\n",
    "        if dtset == \"cordis\":\n",
    "            corpus_path = corpus_path_.replace(\"XXXX\", \"CORDIS\")\n",
    "        elif dtset == \"cancer\":\n",
    "            corpus_path = corpus_path_.replace(\"XXXX\", \"Cancer\")\n",
    "        else:\n",
    "            corpus_path = corpus_path_.replace(\"XXXX\", \"S2CS-AI\") \n",
    "\n",
    "        with open(corpus_path, 'rb') as f:\n",
    "            corpus = pickle.load(f)\n",
    "\n",
    "        if dtset == \"cordis\":\n",
    "            # reduce by a factor of 10\n",
    "            corpus = corpus[:len(corpus)//10]\n",
    "        else:\n",
    "            # reduce by a factor of 100\n",
    "            corpus = corpus[:len(corpus)//100]\n",
    "\n",
    "        print(f\"CORPUS: {corpus[0]}\")\n",
    "        \n",
    "        corpus_join = [\" \".join(doc) for doc in corpus]\n",
    "        vectorizer = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "        bow = vectorizer.fit_transform(corpus_join).toarray()\n",
    "        vocab = vectorizer.vocabulary_\n",
    "\n",
    "        print(f\"Vocabulary and BoW calculated\")\n",
    "\n",
    "        ### Load model information\n",
    "        search_key = f\"{path_models}/{algo}/{dtset}/run*.bin\"\n",
    "        results_files = glob.glob(search_key)\n",
    "        for iter_, file in enumerate(results_files):\n",
    "            if algo == \"hlda\":\n",
    "                mdl = tp.HLDAModel.load(file)\n",
    "    \n",
    "                # get topic words\n",
    "                betas = np.array([mdl.get_topic_word_dist(el) for el in range(mdl.k)])\n",
    "                tpc_descs = []\n",
    "                for i in range(mdl.k):\n",
    "                    words = [mdl.vocabs[idx2] for idx2 in np.argsort(betas[i])[::-1][0:n_words]]\n",
    "                    tpc_descs.append((i, words))\n",
    "    \n",
    "                # get groups\n",
    "                root_desc = tpc_descs[0][1]\n",
    "                tpc_descs = [el[1] for el in tpc_descs if el[0] !=0]\n",
    "    \n",
    "                PC_pair_groups = [[root_desc, el] for el in tpc_descs]\n",
    "    \n",
    "                # sibling_groups: length == num_layers\n",
    "                # each element in the list is a group of sibling topics at a layer.\n",
    "                sibling_groups = [# conjunto de all sibling topics (one per layer)\n",
    "                    [\n",
    "                        [\" \".join(el) for el in tpc_descs]\n",
    "                    ]\n",
    "                ]\n",
    "            else:\n",
    "\n",
    "                mdl = tp.HPAModel.load(file)\n",
    "                \n",
    "                supertopics_lst = []  # Lista para los tópicos de nivel 1\n",
    "                subtopics_lst = []    # Lista para los tópicos de nivel 2\n",
    "                \n",
    "                # Extraer super tópicos (nivel 1)\n",
    "                for k1 in range(mdl.k1):  # Iterar sobre los tópicos de nivel 1\n",
    "                    words = [word for word, _ in mdl.get_topic_words(k1, top_n=n_words)]\n",
    "                    supertopics_lst.append(words)\n",
    "                \n",
    "                # Extraer sub tópicos (nivel 2)\n",
    "                for k2 in range(mdl.k1, mdl.k1 + mdl.k2):  # Iterar sobre los tópicos de nivel 2\n",
    "                    words = [word for word, _ in mdl.get_topic_words(k2, top_n=n_words)]\n",
    "                    subtopics_lst.append(words)\n",
    "                    \n",
    "                # Asignar subtópicos a supertópicos\n",
    "                hierarchy = assign_subtopics_to_supertopics(mdl)\n",
    "                \n",
    "                # Imprimir la jerarquía\n",
    "                #print(\"Jerarquía de supertópicos y subtópicos:\")\n",
    "                #for super_topic, sub_topics in hierarchy.items():\n",
    "                #    print(f\"Super tópico {super_topic}: Sub tópicos {sub_topics}\")\n",
    "                \n",
    "                # PC_pair_groups: pares de supertópicos y sus subtópicos\n",
    "                PC_pair_groups = []\n",
    "                for super_topic, sub_topic_ids in hierarchy.items():\n",
    "                    for sub_topic in sub_topic_ids:\n",
    "                        PC_pair_groups.append([supertopics_lst[super_topic], subtopics_lst[sub_topic]])\n",
    "                \n",
    "                # sibling_topics: combinaciones de palabras de supertópicos y subtópicos\n",
    "                sibling_topics = [\n",
    "                    [[\" \".join(supertopics_lst[a]), \" \".join(supertopics_lst[b])] for a, b in combinations(range(len(supertopics_lst)), 2)],  # Nivel 1\n",
    "                    [[\" \".join(subtopics_lst[a]), \" \".join(subtopics_lst[b])] for a, b in combinations(range(len(subtopics_lst)), 2)]        # Nivel 2\n",
    "                ]\n",
    "                \n",
    "                # P_noC_pair_groups: supertópicos con subtópicos que no son directamente suyos\n",
    "                P_noC_pair_groups = []\n",
    "                for super_topic, sub_topic_ids in hierarchy.items():\n",
    "                    non_child_subtopics = [\n",
    "                        sub_topic for sub_topic in range(len(subtopics_lst))\n",
    "                        if sub_topic not in sub_topic_ids\n",
    "                    ]\n",
    "                    for sub_topic in non_child_subtopics:\n",
    "                        P_noC_pair_groups.append([supertopics_lst[super_topic], subtopics_lst[sub_topic]])\n",
    "                                \n",
    "\n",
    "            results = defaultdict()\n",
    "            results[\"dataset\"] = dtset\n",
    "            results[\"algorithm\"] = algo\n",
    "            results[\"iter\"] = iter_\n",
    "            results[\"PCC\"] = np.mean(get_CLNPMI(PC_pair_groups, bow, vocab)) # Parent and Child topic Coherence (PCC)\n",
    "            #print(f\"PCC : {results['PCC']}\")\n",
    "            results[\"PCD\"] = np.mean(get_topics_difference(PC_pair_groups)) # Parent and Child topic Diversity (PCD)\n",
    "            #print(f\"PCD : {results['PCD']}\")\n",
    "            results[\"Sibling_TD\"] = np.mean(get_Sibling_TD(sibling_topics)) # Sibling Topic Diversity (SD)\n",
    "            #print(f\"Sibling_TD : {results['Sibling_TD']}\")\n",
    "            results[\"PnCD\"] = np.mean(get_topics_difference(P_noC_pair_groups)) if algo == \"hpam\" else 0\n",
    "            #print(f\"PnCD : {results['PnCD']}\")\n",
    "\n",
    "            results_df_tomo.append(pd.DataFrame([results]))\n",
    "            \n",
    "results_df_tomo = pd.concat(results_df_tomo, ignore_index=True)\n",
    "print(results_df_tomo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30220e2a-72eb-40f1-93a9-72ab3c9340ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>iter</th>\n",
       "      <th>PCC</th>\n",
       "      <th>PCD</th>\n",
       "      <th>Sibling_TD</th>\n",
       "      <th>PnCD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cordis</td>\n",
       "      <td>hpam</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.092341</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.955414</td>\n",
       "      <td>0.965778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cordis</td>\n",
       "      <td>hpam</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.051832</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>0.926507</td>\n",
       "      <td>0.964889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cordis</td>\n",
       "      <td>hpam</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.920876</td>\n",
       "      <td>0.962667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancer</td>\n",
       "      <td>hpam</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.246210</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.977453</td>\n",
       "      <td>0.994772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cancer</td>\n",
       "      <td>hpam</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.273212</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.980479</td>\n",
       "      <td>0.996632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cancer</td>\n",
       "      <td>hpam</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.271764</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>0.996614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ai</td>\n",
       "      <td>hpam</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160386</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.968773</td>\n",
       "      <td>0.991263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ai</td>\n",
       "      <td>hpam</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.164539</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.957675</td>\n",
       "      <td>0.991263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ai</td>\n",
       "      <td>hpam</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.181021</td>\n",
       "      <td>0.992667</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.992877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cordis</td>\n",
       "      <td>hlda</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.072132</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cordis</td>\n",
       "      <td>hlda</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.061312</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cordis</td>\n",
       "      <td>hlda</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.066540</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cancer</td>\n",
       "      <td>hlda</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.206344</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cancer</td>\n",
       "      <td>hlda</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.183586</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cancer</td>\n",
       "      <td>hlda</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.243461</td>\n",
       "      <td>0.941844</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ai</td>\n",
       "      <td>hlda</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.208731</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ai</td>\n",
       "      <td>hlda</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.168554</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ai</td>\n",
       "      <td>hlda</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.171353</td>\n",
       "      <td>0.945299</td>\n",
       "      <td>0.982173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       "0   cordis      hpam     0 -0.092341  0.964444    0.955414  0.965778\n",
       "1   cordis      hpam     1 -0.051832  0.941111    0.926507  0.964889\n",
       "2   cordis      hpam     2 -0.087348  0.963333    0.920876  0.962667\n",
       "3   cancer      hpam     0 -0.246210  0.995667    0.977453  0.994772\n",
       "4   cancer      hpam     1 -0.273212  0.998000    0.980479  0.996632\n",
       "5   cancer      hpam     2 -0.271764  0.996667    0.981429  0.996614\n",
       "6       ai      hpam     0 -0.160386  0.991000    0.968773  0.991263\n",
       "7       ai      hpam     1 -0.164539  0.989333    0.957675  0.991263\n",
       "8       ai      hpam     2 -0.181021  0.992667    0.982173  0.992877\n",
       "9   cordis      hlda     0 -0.072132  0.946667    0.982173  0.000000\n",
       "10  cordis      hlda     1 -0.061312  0.946667    0.982173  0.000000\n",
       "11  cordis      hlda     2 -0.066540  0.958730    0.982173  0.000000\n",
       "12  cancer      hlda     0 -0.206344  0.957447    0.982173  0.000000\n",
       "13  cancer      hlda     1 -0.183586  0.965812    0.982173  0.000000\n",
       "14  cancer      hlda     2 -0.243461  0.941844    0.982173  0.000000\n",
       "15      ai      hlda     0 -0.208731  0.957447    0.982173  0.000000\n",
       "16      ai      hlda     1 -0.168554  0.981560    0.982173  0.000000\n",
       "17      ai      hlda     2 -0.171353  0.945299    0.982173  0.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_tomo #.to_csv(\"tomo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebd19f-20ef-4f31-a3e6-564c67e7bdf9",
   "metadata": {},
   "source": [
    "### HTM-WS / HTM-DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a81b079-6753-42a8-a650-26a1bbd9154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating ours.... \n",
      "Executing for dtset: ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/gpu_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/export/usuarios_ml4ds/lbartolome/Repos/my_repos/UserInLoopHTM/gpu_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/export/usuarios_ml4ds/lbartolome/Datasets/S2CS-AI/htm_variability_models/htm_20_tpcs_20230929')]\n",
      "/export/usuarios_ml4ds/lbartolome/Datasets/S2CS-AI/htm_variability_models/htm_20_tpcs_20230929\n",
      "This is the father: /export/usuarios_ml4ds/lbartolome/Datasets/S2CS-AI/htm_variability_models/htm_20_tpcs_20230929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15200/15200 [00:00<00:00, 63919.93it/s]"
     ]
    }
   ],
   "source": [
    "print(\"--- Calculating ours.... \")\n",
    "algorithms = [\"ws\", \"ds\"]\n",
    "PATH_MODELS = \"/export/usuarios_ml4ds/lbartolome/Datasets/XXXX/htm_variability_models\"\n",
    "PATH_MODELS_PREPROC = \"/export/usuarios_ml4ds/lbartolome/Datasets/XXXX/models_preproc/iter_0\"\n",
    "PATH_CORPUS = \"/export/usuarios_ml4ds/lbartolome/Datasets/XXXX/models_preproc/iter_0/corpus.txt\"\n",
    "\n",
    "results_df_ours = []\n",
    "\n",
    "for dtset in [\"ai\"]:\n",
    "\n",
    "    print(f\"Executing for dtset: {dtset}\")\n",
    "\n",
    "    ##### GET CORPUS, VOCAB AND BOW #####\n",
    "    if dtset == \"cordis\":\n",
    "        path_corpus = PATH_CORPUS.replace(\"XXXX\", \"CORDIS\")\n",
    "        path_models = pathlib.Path(PATH_MODELS.replace(\"XXXX\", \"CORDIS\"))\n",
    "        model_path_preproc = PATH_MODELS_PREPROC.replace(\"XXXX\", \"CORDIS\")\n",
    "        nr_topics = \"6\"\n",
    "    elif dtset == \"cancer\":\n",
    "        path_corpus = PATH_CORPUS.replace(\"XXXX\", \"Cancer\")\n",
    "        path_models = pathlib.Path(PATH_MODELS.replace(\"XXXX\", \"Cancer\"))\n",
    "        model_path_preproc = PATH_MODELS_PREPROC.replace(\"XXXX\", \"Cancer\")\n",
    "        nr_topics = \"20\"\n",
    "    else:\n",
    "        path_corpus = PATH_CORPUS.replace(\"XXXX\", \"S2CS-AI\") \n",
    "        path_models = pathlib.Path(PATH_MODELS.replace(\"XXXX\", \"S2CS-AI\"))\n",
    "        model_path_preproc = PATH_MODELS_PREPROC.replace(\"XXXX\", \"S2CS-AI\")\n",
    "        nr_topics = \"20\"\n",
    "        \n",
    "    df = mallet_corpus_to_df(path_corpus)\n",
    "    \n",
    "    vocab_w2id = {}\n",
    "    with (pathlib.Path(model_path_preproc)/'vocabulary.txt').open('r', encoding='utf8') as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            wd = line.strip()\n",
    "            vocab_w2id[wd] = i\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary=vocab_w2id.keys(), tokenizer=lambda x: x.split())\n",
    "    bow = vectorizer.fit_transform(df.text.values.tolist())\n",
    "    bow = bow#.toarray()\n",
    "\n",
    "    ##### LOAD MODELS #####\n",
    "    df_root, df_all = get_df_all_models(path_models)\n",
    "    father = [el.as_posix() for el in list(df_root.path.unique()) if nr_topics in el.as_posix()][0]\n",
    "    print(f\"This is the father: {father}\")\n",
    "\n",
    "    for algo in algorithms:\n",
    "        for iter_ in [0,1,2]:\n",
    "            PC_pair_groups = []\n",
    "            P_noC_pair_groups = []\n",
    "            sibling_groups = []\n",
    "            \n",
    "            filtered_df = df_all[\n",
    "                (df_all.father == father) &\n",
    "                (df_all.tr_tpcs == 10) &\n",
    "                (df_all.iter == iter_) & # Cambia iter por iter_ si este es el nombre correcto\n",
    "                (df_all.thr.isin([0, 0.6]))  # Descomentar si se requiere\n",
    "            ]\n",
    "    \n",
    "            # append the descriptions of topics at the first level\n",
    "            sibling_groups.append([\" \".join(el.split(\", \")) for el in filtered_df.iloc[0].tpc_descriptions])\n",
    "        \n",
    "            second_level_siblings = []\n",
    "            for _, submodel in filtered_df.iterrows():  # Iterate over filtered submodels\n",
    "                if algo in submodel.path.as_posix():\n",
    "                    for submodel_topic in submodel.tpc_descriptions:\n",
    "                        # Add Parent-Child (PC) pair\n",
    "                        pair_PC = [submodel.father_tpc_descriptions.split(\", \"), submodel_topic.split(\", \")]\n",
    "                        PC_pair_groups.append(pair_PC)\n",
    "                    \n",
    "                    second_level_siblings += [\" \".join(el.split(\", \")) for el in submodel.tpc_descriptions]\n",
    "                    # Add Parent non-Child (P_noC) pair\n",
    "                    for _, submodel_j in filtered_df.iterrows():\n",
    "                        if submodel.exp_tpc != submodel_j.exp_tpc:  # Ensure submodel_j is different from submodel\n",
    "                            if algo in submodel_j.path.as_posix():\n",
    "                                for submodel_topic in submodel.tpc_descriptions:\n",
    "                                    pair_P_noC = [submodel_j.father_tpc_descriptions.split(\", \"), submodel_topic.split(\", \")]\n",
    "                                    P_noC_pair_groups.append(pair_P_noC)\n",
    "                                    \n",
    "            sibling_groups.append(second_level_siblings)\n",
    "            sibling_groups = [sibling_groups]\n",
    "            \n",
    "            results = defaultdict()\n",
    "            results[\"dataset\"] = dtset\n",
    "            results[\"algorithm\"] = algo\n",
    "            results[\"iter\"] = iter_\n",
    "            results[\"PCC\"] = np.mean(get_CLNPMI(PC_pair_groups, bow, vocab_w2id))\n",
    "            results[\"PCD\"] = np.mean(get_topics_difference(PC_pair_groups))\n",
    "            results[\"Sibling_TD\"] = np.mean(get_Sibling_TD(sibling_groups))\n",
    "            results[\"PnCD\"] =np.mean(get_topics_difference(P_noC_pair_groups))\n",
    "    \n",
    "            results_df_ours.append(pd.DataFrame([results]))\n",
    "            \n",
    "results_df_ours = pd.concat(results_df_ours, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64abc9fc-f6b7-4099-b770-7366db7cf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_ours.to_csv(\"ours_ai_from_copy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676d7851-6b60-415f-854b-32860e308544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>iter</th>\n",
       "      <th>PCC</th>\n",
       "      <th>PCD</th>\n",
       "      <th>Sibling_TD</th>\n",
       "      <th>PnCD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai</td>\n",
       "      <td>ws</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.805667</td>\n",
       "      <td>0.670333</td>\n",
       "      <td>0.989702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>ws</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014571</td>\n",
       "      <td>0.811333</td>\n",
       "      <td>0.614833</td>\n",
       "      <td>0.990053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ai</td>\n",
       "      <td>ws</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013211</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.989544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai</td>\n",
       "      <td>ds</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.806498</td>\n",
       "      <td>0.438608</td>\n",
       "      <td>0.980660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai</td>\n",
       "      <td>ds</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.424603</td>\n",
       "      <td>0.982660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ai</td>\n",
       "      <td>ds</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.491917</td>\n",
       "      <td>0.983570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset algorithm  iter       PCC       PCD  Sibling_TD      PnCD\n",
       "0      ai        ws     0  0.004027  0.805667    0.670333  0.989702\n",
       "1      ai        ws     1  0.014571  0.811333    0.614833  0.990053\n",
       "2      ai        ws     2  0.013211  0.810667    0.676333  0.989544\n",
       "3      ai        ds     0  0.000232  0.806498    0.438608  0.980660\n",
       "4      ai        ds     1  0.003045  0.807619    0.424603  0.982660\n",
       "5      ai        ds     2  0.006190  0.810000    0.491917  0.983570"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_ours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
